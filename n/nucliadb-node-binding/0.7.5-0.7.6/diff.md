# Comparing `tmp/nucliadb_node_binding-0.7.5.tar.gz` & `tmp/nucliadb_node_binding-0.7.6.tar.gz`

## Comparing `nucliadb_node_binding-0.7.5.tar` & `nucliadb_node_binding-0.7.6.tar`

### file list

```diff
@@ -1,198 +1,212 @@
--rw-r--r--   0        0        0      552 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/Cargo.toml
--rw-r--r--   0      501       20     2217 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/build.rs
--rw-r--r--   0      501       20    26090 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/fdbwriter.rs
--rw-r--r--   0      501       20        0 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/google.protobuf.rs
--rw-r--r--   0      501       20     8504 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/knowledgebox.rs
--rw-r--r--   0      501       20     1281 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/lib.rs
--rw-r--r--   0      501       20    74127 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/nodereader.rs
--rw-r--r--   0      501       20     9709 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/noderesources.rs
--rw-r--r--   0      501       20    58264 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/nodewriter.rs
--rw-r--r--   0      501       20    26661 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/resources.rs
--rw-r--r--   0      501       20     5879 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/utils.rs
--rw-r--r--   0        0        0      653 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/Cargo.toml
--rw-r--r--   0      501       20     1502 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/README.md
--rw-r--r--   0      501       20     1140 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/error.rs
--rw-r--r--   0      501       20     6863 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/lib.rs
--rw-r--r--   0      501       20     3968 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/listener.rs
--rw-r--r--   0      501       20     2677 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/main.rs
--rw-r--r--   0      501       20     7112 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/publisher.rs
--rw-r--r--   0        0        0      491 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/Cargo.toml
--rw-r--r--   0      501       20       18 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/.gitignore
--rw-r--r--   0      501       20     1873 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/build.rs
--rw-r--r--   0      501       20     8117 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs
--rw-r--r--   0      501       20      998 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/lib.rs
--rw-r--r--   0      501       20    37178 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/reader.rs
--rw-r--r--   0      501       20     4339 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/schema.rs
--rw-r--r--   0      501       20    17866 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/search_query.rs
--rw-r--r--   0      501       20    11743 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/search_response.rs
--rw-r--r--   0      501       20    17048 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/writer.rs
--rw-r--r--   0      501       20     1495 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/ca.json
--rw-r--r--   0      501       20     4724 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/en.json
--rw-r--r--   0      501       20     4516 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/es.json
--rw-r--r--   0      501       20     5233 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/fr.json
--rw-r--r--   0        0        0      284 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/Cargo.toml
--rw-r--r--   0      501       20      917 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/lib.rs
--rw-r--r--   0      501       20    29168 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/reader.rs
--rw-r--r--   0      501       20     2878 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/schema.rs
--rw-r--r--   0      501       20     3688 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/search_query.rs
--rw-r--r--   0      501       20    11567 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/writer.rs
--rw-r--r--   0        0        0     3041 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/Cargo.toml
--rw-r--r--   0      501       20     1045 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/.rustc_info.json
--rw-r--r--   0      501       20     1503 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/README.md
--rwxr-xr-x   0      501       20       66 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/entrypoint.sh
--rw-r--r--   0      501       20     1757 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/__init__.py
--rw-r--r--   0      501       20     5345 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/app.py
--rw-r--r--   0      501       20    11122 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/pull.py
--rw-r--r--   0      501       20     2719 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/reader.py
--rw-r--r--   0      501       20     2255 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/service.py
--rw-r--r--   0      501       20     4031 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/servicer.py
--rw-r--r--   0      501       20     1216 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/settings.py
--rw-r--r--   0      501       20     8230 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/shadow_shards.py
--rw-r--r--   0      501       20      835 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py
--rw-r--r--   0      501       20     1055 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py
--rw-r--r--   0      501       20     7610 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/fixtures.py
--rw-r--r--   0      501       20     6077 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/test_indexing.py
--rw-r--r--   0      501       20     4961 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/test_shadow_shards.py
--rw-r--r--   0      501       20     1552 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/test_sidecar_servicer.py
--rw-r--r--   0      501       20     2449 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/writer.py
--rw-r--r--   0      501       20       88 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/requirements-sources.txt
--rw-r--r--   0      501       20      242 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/requirements.txt
--rw-r--r--   0      501       20      122 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/setup.cfg
--rw-r--r--   0      501       20     1349 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/setup.py
--rw-r--r--   0      501       20     2202 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/bin/payload_test.rs
--rw-r--r--   0      501       20     3088 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/bin/reader.rs
--rw-r--r--   0      501       20     9657 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/bin/writer.rs
--rw-r--r--   0      501       20     8336 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/env.rs
--rw-r--r--   0      501       20     1278 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/lib.rs
--rw-r--r--   0      501       20     6220 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/node_metadata.rs
--rw-r--r--   0      501       20    18124 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/reader/grpc_driver.rs
--rw-r--r--   0      501       20    10199 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/reader/mod.rs
--rw-r--r--   0      501       20     1286 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/mod.rs
--rw-r--r--   0      501       20    22559 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/reader.rs
--rw-r--r--   0      501       20     8831 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/versions.rs
--rw-r--r--   0      501       20    18752 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/writer.rs
--rw-r--r--   0      501       20     3987 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/shard_metadata.rs
--rw-r--r--   0      501       20     3897 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/telemetry.rs
--rw-r--r--   0      501       20     2875 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/utils.rs
--rw-r--r--   0      501       20    25015 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/writer/grpc_driver.rs
--rw-r--r--   0      501       20     9286 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/writer/mod.rs
--rw-r--r--   0      501       20       42 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/test.sh
--rw-r--r--   0      501       20     1420 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/common/constants.rs
--rw-r--r--   0      501       20     1133 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/common/mod.rs
--rw-r--r--   0      501       20     5325 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/common/node_services.rs
--rw-r--r--   0      501       20    22116 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/test_search_relations.rs
--rw-r--r--   0      501       20     6704 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/test_search_sorting.rs
--rw-r--r--   0      501       20     3422 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/test_shards.rs
--rw-r--r--   0        0        0      687 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/Cargo.toml
--rw-r--r--   0      501       20     9337 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/bfs_engine.rs
--rw-r--r--   0      501       20     1761 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/errors.rs
--rw-r--r--   0      501       20    21723 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/graph_db.rs
--rw-r--r--   0      501       20     1831 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/graph_test_utils.rs
--rw-r--r--   0      501       20    10747 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/index.rs
--rw-r--r--   0      501       20     1003 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/lib.rs
--rw-r--r--   0      501       20     5752 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/node_dictionary.rs
--rw-r--r--   0      501       20     5251 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/relations_io.rs
--rw-r--r--   0      501       20     2692 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/bfs.rs
--rw-r--r--   0      501       20      970 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/mod.rs
--rw-r--r--   0      501       20    13511 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/reader.rs
--rw-r--r--   0      501       20    14392 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/tests.rs
--rw-r--r--   0      501       20     3071 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/utils.rs
--rw-r--r--   0      501       20     9918 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/writer.rs
--rw-r--r--   0        0        0      696 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/Cargo.toml
--rw-r--r--   0      501       20       80 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/MANIFEST.in
--rw-r--r--   0      501       20      335 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/Makefile
--rw-r--r--   0      501       20     5604 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/README.md
--rw-r--r--   0      501       20        6 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/VERSION
--rw-r--r--   0      501       20      899 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py
--rw-r--r--   0      501       20    12027 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py
--rw-r--r--   0      501       20     1254 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py
--rw-r--r--   0      501       20     2925 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py
--rw-r--r--   0      501       20     2615 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi.py
--rw-r--r--   0      501       20    14670 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py
--rw-r--r--   0      501       20     7231 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py
--rw-r--r--   0      501       20     8226 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py
--rw-r--r--   0      501       20     5178 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/metrics.py
--rw-r--r--   0      501       20        0 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/py.typed
--rw-r--r--   0      501       20     1165 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py
--rw-r--r--   0      501       20      833 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py
--rw-r--r--   0      501       20      960 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py
--rw-r--r--   0      501       20      292 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/Makefile
--rw-r--r--   0      501       20      833 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py
--rw-r--r--   0      501       20     1111 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
--rw-r--r--   0      501       20     2340 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py
--rw-r--r--   0      501       20     1411 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.pyi
--rw-r--r--   0      501       20     2822 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py
--rw-r--r--   0      501       20     1084 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi
--rw-r--r--   0      501       20     1126 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto
--rw-r--r--   0      501       20     2290 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py
--rw-r--r--   0      501       20     1190 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.pyi
--rw-r--r--   0      501       20     2677 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py
--rw-r--r--   0      501       20      957 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi
--rw-r--r--   0      501       20     3050 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/test_telemetry.py
--rw-r--r--   0      501       20    12609 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py
--rw-r--r--   0      501       20     3724 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py
--rw-r--r--   0      501       20     3967 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_metrics.py
--rw-r--r--   0      501       20     3883 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py
--rw-r--r--   0      501       20     4419 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/utils.py
--rw-r--r--   0      501       20      492 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/requirements.txt
--rw-r--r--   0      501       20      434 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/setup.cfg
--rw-r--r--   0      501       20     1620 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/setup.py
--rw-r--r--   0      501       20     2497 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/blocking.rs
--rw-r--r--   0      501       20     1218 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/lib.rs
--rw-r--r--   0      501       20     4934 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/payload.rs
--rw-r--r--   0      501       20    12829 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/sender.rs
--rw-r--r--   0      501       20     3166 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/sink.rs
--rw-r--r--   0      501       20     1867 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/sync.rs
--rw-r--r--   0        0        0      435 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/Cargo.toml
--rw-r--r--   0      501       20     5774 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/fs_state.rs
--rw-r--r--   0      501       20     4038 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/lib.rs
--rw-r--r--   0      501       20     1920 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/paragraphs.rs
--rw-r--r--   0      501       20     1601 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/relations.rs
--rw-r--r--   0      501       20     1798 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/texts.rs
--rw-r--r--   0      501       20     1729 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/vectors.rs
--rw-r--r--   0        0        0     1217 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/Cargo.toml
--rw-r--r--   0      501       20      869 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/LICENSE-AGPL
--rw-r--r--   0      501       20      194 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/README.md
--rwxr-xr-x   0      501       20       75 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/entrypoint.sh
--rw-r--r--   0      501       20     6889 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/bin/manager.rs
--rw-r--r--   0      501       20      267 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/error.rs
--rw-r--r--   0      501       20      908 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/key.rs
--rw-r--r--   0      501       20      166 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/lib.rs
--rw-r--r--   0      501       20    15097 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/node.rs
--rw-r--r--   0      501       20     1186 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/register.rs
--rwxr-xr-x   0      501       20     1333 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/tests/cluster_reader.py
--rw-r--r--   0      501       20     7072 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/tests/integration.rs
--rw-r--r--   0      501       20     3185 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/tests/test2.py
--rw-r--r--   0        0        0      604 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/Cargo.toml
--rw-r--r--   0      501       20       69 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/README.md
--rw-r--r--   0      501       20     9788 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs
--rw-r--r--   0      501       20    14921 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/mod.rs
--rw-r--r--   0      501       20    11727 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/node.rs
--rw-r--r--   0      501       20    11617 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/ops_hnsw.rs
--rw-r--r--   0      501       20     4063 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs
--rw-r--r--   0      501       20     7213 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/tests.rs
--rw-r--r--   0      501       20     4295 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs
--rw-r--r--   0      501       20     2911 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs
--rw-r--r--   0      501       20     8801 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs
--rw-r--r--   0      501       20    11759 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/state.rs
--rw-r--r--   0      501       20     1439 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs
--rw-r--r--   0      501       20     6957 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs
--rw-r--r--   0      501       20    13487 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs
--rw-r--r--   0      501       20     2049 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/mod.rs
--rw-r--r--   0      501       20     6202 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/trie.rs
--rw-r--r--   0      501       20     2986 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/trie_ram.rs
--rw-r--r--   0      501       20     4492 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/vector.rs
--rw-r--r--   0      501       20     5338 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/formula/mod.rs
--rw-r--r--   0      501       20     3886 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/indexset/mod.rs
--rw-r--r--   0      501       20     4483 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/indexset/state.rs
--rw-r--r--   0      501       20     1585 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/lib.rs
--rw-r--r--   0      501       20     1264 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/service/mod.rs
--rw-r--r--   0      501       20    12597 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/service/reader.rs
--rw-r--r--   0      501       20    20136 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/service/writer.rs
--rw-r--r--   0        0        0      934 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/Cargo.toml
--rw-r--r--   0      501       20     1281 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/CHANGELOG.md
--rw-r--r--   0      501       20       52 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/README.md
--rw-r--r--   0      501       20     1162 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/pyproject.toml
--rw-r--r--   0      501       20    23139 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/src/lib.rs
--rw-r--r--   0      501       20     2041 2023-03-24 13:21:29.000000 nucliadb_node_binding-0.7.5/test.py
--rw-r--r--   0        0        0      392 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.5/PKG-INFO
+-rw-r--r--   0        0        0      491 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/Cargo.toml
+-rw-r--r--   0        0        0       19 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/.gitignore
+-rw-r--r--   0        0        0     2160 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/build.rs
+-rw-r--r--   0        0        0     8365 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs
+-rw-r--r--   0        0        0     1025 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/lib.rs
+-rw-r--r--   0        0        0    40864 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/reader.rs
+-rw-r--r--   0        0        0     4464 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/schema.rs
+-rw-r--r--   0        0        0    18383 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/search_query.rs
+-rw-r--r--   0        0        0    12074 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/search_response.rs
+-rw-r--r--   0        0        0    18426 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/writer.rs
+-rw-r--r--   0        0        0     1496 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/ca.json
+-rw-r--r--   0        0        0     4724 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/en.json
+-rw-r--r--   0        0        0     4517 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/es.json
+-rw-r--r--   0        0        0     5233 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/fr.json
+-rw-r--r--   0        0        0     3123 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/Cargo.toml
+-rw-r--r--   0        0        0     1045 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/.rustc_info.json
+-rw-r--r--   0        0        0      927 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/Makefile
+-rw-r--r--   0        0        0     1534 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/README.md
+-rw-r--r--   0        0        0       72 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/entrypoint.sh
+-rw-r--r--   0        0        0     1800 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/__init__.py
+-rw-r--r--   0        0        0     3386 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/app.py
+-rw-r--r--   0        0        0    15567 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/pull.py
+-rw-r--r--   0        0        0     2445 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/reader.py
+-rw-r--r--   0        0        0     1961 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/service.py
+-rw-r--r--   0        0        0     4132 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/servicer.py
+-rw-r--r--   0        0        0     1657 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/settings.py
+-rw-r--r--   0        0        0     8485 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/shadow_shards.py
+-rw-r--r--   0        0        0      854 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py
+-rw-r--r--   0        0        0     1082 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py
+-rw-r--r--   0        0        0     8625 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/fixtures.py
+-rw-r--r--   0        0        0     7770 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_indexing.py
+-rw-r--r--   0        0        0     5131 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_shadow_shards.py
+-rw-r--r--   0        0        0     1592 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_sidecar_servicer.py
+-rw-r--r--   0        0        0     1866 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_app.py
+-rw-r--r--   0        0        0     6015 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/unit/test_pull.py
+-rw-r--r--   0        0        0     2181 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/writer.py
+-rw-r--r--   0        0        0       99 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/requirements-sources.txt
+-rw-r--r--   0        0        0      214 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/requirements.txt
+-rw-r--r--   0        0        0      137 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/setup.cfg
+-rw-r--r--   0        0        0     1399 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/setup.py
+-rw-r--r--   0        0        0     2209 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/bin/payload_test.rs
+-rw-r--r--   0        0        0     3460 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/bin/reader.rs
+-rw-r--r--   0        0        0    10193 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/bin/writer.rs
+-rw-r--r--   0        0        0     9014 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/env.rs
+-rw-r--r--   0        0        0     1206 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/http_server/metrics_service.rs
+-rw-r--r--   0        0        0     1559 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/http_server/mod.rs
+-rw-r--r--   0        0        0     1329 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/lib.rs
+-rw-r--r--   0        0        0     5481 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/node_metadata.rs
+-rw-r--r--   0        0        0    18823 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/reader/grpc_driver.rs
+-rw-r--r--   0        0        0    10688 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/reader/mod.rs
+-rw-r--r--   0        0        0     1320 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/mod.rs
+-rw-r--r--   0        0        0    25266 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/reader.rs
+-rw-r--r--   0        0        0     9051 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/versions.rs
+-rw-r--r--   0        0        0    21256 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/writer.rs
+-rw-r--r--   0        0        0     4114 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/shard_metadata.rs
+-rw-r--r--   0        0        0     4004 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/telemetry.rs
+-rw-r--r--   0        0        0     2960 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/utils.rs
+-rw-r--r--   0        0        0    25718 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/writer/grpc_driver.rs
+-rw-r--r--   0        0        0     9476 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/writer/mod.rs
+-rw-r--r--   0        0        0       43 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/test.sh
+-rw-r--r--   0        0        0     1454 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/common/constants.rs
+-rw-r--r--   0        0        0     1162 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/common/mod.rs
+-rw-r--r--   0        0        0     5487 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/common/node_services.rs
+-rw-r--r--   0        0        0    22733 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/test_search_relations.rs
+-rw-r--r--   0        0        0     6891 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/test_search_sorting.rs
+-rw-r--r--   0        0        0     3451 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/test_shards.rs
+-rw-r--r--   0        0        0      719 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/Cargo.toml
+-rw-r--r--   0        0        0       84 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/MANIFEST.in
+-rw-r--r--   0        0        0     1302 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/Makefile
+-rw-r--r--   0        0        0     5804 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/README.md
+-rw-r--r--   0        0        0        7 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/VERSION
+-rw-r--r--   0        0        0      921 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py
+-rw-r--r--   0        0        0    12339 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py
+-rw-r--r--   0        0        0     1289 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py
+-rw-r--r--   0        0        0     3024 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py
+-rw-r--r--   0        0        0     2929 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/__init__.py
+-rw-r--r--   0        0        0     5603 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/metrics.py
+-rw-r--r--   0        0        0    13481 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/tracing.py
+-rw-r--r--   0        0        0    15218 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py
+-rw-r--r--   0        0        0     5384 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc_metrics.py
+-rw-r--r--   0        0        0     7420 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py
+-rw-r--r--   0        0        0     8457 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py
+-rw-r--r--   0        0        0     6457 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/logs.py
+-rw-r--r--   0        0        0     7333 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/metrics.py
+-rw-r--r--   0        0        0        0 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/py.typed
+-rw-r--r--   0        0        0     1570 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py
+-rw-r--r--   0        0        0      851 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py
+-rw-r--r--   0        0        0      984 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py
+-rw-r--r--   0        0        0      294 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/Makefile
+-rw-r--r--   0        0        0      851 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py
+-rw-r--r--   0        0        0     1149 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
+-rw-r--r--   0        0        0     2395 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py
+-rw-r--r--   0        0        0     1464 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.pyi
+-rw-r--r--   0        0        0     2903 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py
+-rw-r--r--   0        0        0     1120 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi
+-rw-r--r--   0        0        0     1163 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto
+-rw-r--r--   0        0        0     2347 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py
+-rw-r--r--   0        0        0     1235 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.pyi
+-rw-r--r--   0        0        0     2758 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py
+-rw-r--r--   0        0        0      990 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi
+-rw-r--r--   0        0        0     7421 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_fastapi.py
+-rw-r--r--   0        0        0     3376 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_telemetry.py
+-rw-r--r--   0        0        0    12955 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py
+-rw-r--r--   0        0        0     3823 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py
+-rw-r--r--   0        0        0     4646 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_logs.py
+-rw-r--r--   0        0        0     6199 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_metrics.py
+-rw-r--r--   0        0        0     3984 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py
+-rw-r--r--   0        0        0     5335 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/utils.py
+-rw-r--r--   0        0        0      625 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/requirements.txt
+-rw-r--r--   0        0        0      583 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/setup.cfg
+-rw-r--r--   0        0        0     1670 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/setup.py
+-rw-r--r--   0        0        0     2567 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/blocking.rs
+-rw-r--r--   0        0        0     1247 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/lib.rs
+-rw-r--r--   0        0        0     5080 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/payload.rs
+-rw-r--r--   0        0        0    13230 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/sender.rs
+-rw-r--r--   0        0        0     3272 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/sink.rs
+-rw-r--r--   0        0        0     1915 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/sync.rs
+-rw-r--r--   0        0        0      604 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/Cargo.toml
+-rw-r--r--   0        0        0       74 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/README.md
+-rw-r--r--   0        0        0    10081 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs
+-rw-r--r--   0        0        0    15448 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/mod.rs
+-rw-r--r--   0        0        0    12011 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/node.rs
+-rw-r--r--   0        0        0    11943 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/ops_hnsw.rs
+-rw-r--r--   0        0        0     4199 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs
+-rw-r--r--   0        0        0     7454 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/tests.rs
+-rw-r--r--   0        0        0     4423 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs
+-rw-r--r--   0        0        0     2995 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs
+-rw-r--r--   0        0        0     9056 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs
+-rw-r--r--   0        0        0    12089 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/state.rs
+-rw-r--r--   0        0        0     1477 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs
+-rw-r--r--   0        0        0     7142 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs
+-rw-r--r--   0        0        0    13871 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs
+-rw-r--r--   0        0        0     2115 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/mod.rs
+-rw-r--r--   0        0        0     6382 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/trie.rs
+-rw-r--r--   0        0        0     3083 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/trie_ram.rs
+-rw-r--r--   0        0        0     4632 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/vector.rs
+-rw-r--r--   0        0        0     5500 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/formula/mod.rs
+-rw-r--r--   0        0        0     4000 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/indexset/mod.rs
+-rw-r--r--   0        0        0     4602 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/indexset/state.rs
+-rw-r--r--   0        0        0     1632 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/lib.rs
+-rw-r--r--   0        0        0     1301 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/service/mod.rs
+-rw-r--r--   0        0        0    13579 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/service/reader.rs
+-rw-r--r--   0        0        0    22224 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/service/writer.rs
+-rw-r--r--   0        0        0      526 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/Cargo.toml
+-rw-r--r--   0        0        0     1488 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/context.rs
+-rw-r--r--   0        0        0     5986 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/fs_state.rs
+-rw-r--r--   0        0        0     4204 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/lib.rs
+-rw-r--r--   0        0        0     3766 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/metrics/mod.rs
+-rw-r--r--   0        0        0     2718 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/metrics/request_time.rs
+-rw-r--r--   0        0        0     1976 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/paragraphs.rs
+-rw-r--r--   0        0        0     1647 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/relations.rs
+-rw-r--r--   0        0        0     1852 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/texts.rs
+-rw-r--r--   0        0        0     1781 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/vectors.rs
+-rw-r--r--   0        0        0      687 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/Cargo.toml
+-rw-r--r--   0        0        0     9600 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/bfs_engine.rs
+-rw-r--r--   0        0        0     1812 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/errors.rs
+-rw-r--r--   0        0        0    22269 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/graph_db.rs
+-rw-r--r--   0        0        0     1895 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/graph_test_utils.rs
+-rw-r--r--   0        0        0    11054 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/index.rs
+-rw-r--r--   0        0        0     1032 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/lib.rs
+-rw-r--r--   0        0        0     5908 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/node_dictionary.rs
+-rw-r--r--   0        0        0     5413 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/relations_io.rs
+-rw-r--r--   0        0        0     2770 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/bfs.rs
+-rw-r--r--   0        0        0      999 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/mod.rs
+-rw-r--r--   0        0        0    15505 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/reader.rs
+-rw-r--r--   0        0        0    14827 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/tests.rs
+-rw-r--r--   0        0        0     3158 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/utils.rs
+-rw-r--r--   0        0        0    11621 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/writer.rs
+-rw-r--r--   0        0        0      574 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/Cargo.toml
+-rw-r--r--   0        0        0     2215 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/build.rs
+-rw-r--r--   0        0        0    26906 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/fdbwriter.rs
+-rw-r--r--   0        0        0        0 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/google.protobuf.rs
+-rw-r--r--   0        0        0     7067 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/knowledgebox.rs
+-rw-r--r--   0        0        0     1321 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/lib.rs
+-rw-r--r--   0        0        0    76517 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/nodereader.rs
+-rw-r--r--   0        0        0     9975 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/noderesources.rs
+-rw-r--r--   0        0        0    60235 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/nodewriter.rs
+-rw-r--r--   0        0        0    27367 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/resources.rs
+-rw-r--r--   0        0        0     6040 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/utils.rs
+-rw-r--r--   0        0        0      284 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/Cargo.toml
+-rw-r--r--   0        0        0      941 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/lib.rs
+-rw-r--r--   0        0        0    30065 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/reader.rs
+-rw-r--r--   0        0        0     2970 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/schema.rs
+-rw-r--r--   0        0        0     7166 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/search_query.rs
+-rw-r--r--   0        0        0    12671 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/writer.rs
+-rw-r--r--   0        0        0     1263 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/Cargo.toml
+-rw-r--r--   0        0        0      888 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/LICENSE-AGPL
+-rw-r--r--   0        0        0      199 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/README.md
+-rw-r--r--   0        0        0       81 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/entrypoint.sh
+-rw-r--r--   0        0        0     7109 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/bin/manager.rs
+-rw-r--r--   0        0        0      277 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/error.rs
+-rw-r--r--   0        0        0      951 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/key.rs
+-rw-r--r--   0        0        0      174 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/lib.rs
+-rw-r--r--   0        0        0    15579 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/node.rs
+-rw-r--r--   0        0        0     1227 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/register.rs
+-rw-r--r--   0        0        0     1368 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/tests/cluster_reader.py
+-rw-r--r--   0        0        0     7291 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/tests/integration.rs
+-rw-r--r--   0        0        0     3277 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/tests/test2.py
+-rw-r--r--   0        0        0      675 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/Cargo.toml
+-rw-r--r--   0        0        0     1549 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/README.md
+-rw-r--r--   0        0        0     1172 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/error.rs
+-rw-r--r--   0        0        0     7047 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/lib.rs
+-rw-r--r--   0        0        0     4095 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/listener.rs
+-rw-r--r--   0        0        0     2769 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/main.rs
+-rw-r--r--   0        0        0     7315 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/publisher.rs
+-rw-r--r--   0        0        0      938 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/Cargo.toml
+-rw-r--r--   0        0        0     1470 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/CHANGELOG.md
+-rw-r--r--   0        0        0       53 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/README.md
+-rw-r--r--   0        0        0     1192 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/pyproject.toml
+-rw-r--r--   0        0        0    23646 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/src/lib.rs
+-rw-r--r--   0        0        0     2097 2023-04-26 13:06:27.000000 nucliadb_node_binding-0.7.6/test.py
+-rw-r--r--   0        0        0      393 1970-01-01 00:00:00.000000 nucliadb_node_binding-0.7.6/PKG-INFO
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/build.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/build.rs`

 * *Files 19% similar despite different names*

```diff
@@ -1,62 +1,60 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io::Result;
-
-fn main() -> Result<()> {
-    println!("cargo:rerun-if-changed=../knowledgebox.proto");
-    println!("cargo:rerun-if-changed=../resources.proto");
-    println!("cargo:rerun-if-changed=../noderesources.proto");
-    println!("cargo:rerun-if-changed=../utils.proto");
-    println!("cargo:rerun-if-changed=../writer.proto");
-    println!("cargo:rerun-if-changed=../nodesidecar.proto");
-    println!("cargo:rerun-if-changed=../nodewriter.proto");
-    println!("cargo:rerun-if-changed=../nodereader.proto");
-
-    let mut prost_config = prost_build::Config::default();
-
-    prost_config
-        .out_dir("src")
-        .compile_protos(
-            &[
-                "nucliadb_protos/utils.proto",
-                "nucliadb_protos/knowledgebox.proto",
-                "nucliadb_protos/resources.proto",
-                "nucliadb_protos/noderesources.proto",
-                "nucliadb_protos/writer.proto",
-                "nucliadb_protos/nodewriter.proto",
-                "nucliadb_protos/nodereader.proto",
-            ],
-            &["../../"],
-        )?;
-
-    tonic_build::configure()
-        .build_server(true)
-        .out_dir("src")
-        .compile(
-            &[
-                "nucliadb_protos/nodewriter.proto",
-                "nucliadb_protos/nodereader.proto",
-            ],
-            &["../../"],
-        )?;
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io::Result;
+
+fn main() -> Result<()> {
+    println!("cargo:rerun-if-changed=../knowledgebox.proto");
+    println!("cargo:rerun-if-changed=../resources.proto");
+    println!("cargo:rerun-if-changed=../noderesources.proto");
+    println!("cargo:rerun-if-changed=../utils.proto");
+    println!("cargo:rerun-if-changed=../writer.proto");
+    println!("cargo:rerun-if-changed=../nodesidecar.proto");
+    println!("cargo:rerun-if-changed=../nodewriter.proto");
+    println!("cargo:rerun-if-changed=../nodereader.proto");
+
+    let mut prost_config = prost_build::Config::default();
+
+    prost_config.out_dir("src").compile_protos(
+        &[
+            "nucliadb_protos/utils.proto",
+            "nucliadb_protos/knowledgebox.proto",
+            "nucliadb_protos/resources.proto",
+            "nucliadb_protos/noderesources.proto",
+            "nucliadb_protos/writer.proto",
+            "nucliadb_protos/nodewriter.proto",
+            "nucliadb_protos/nodereader.proto",
+        ],
+        &["../../"],
+    )?;
+
+    tonic_build::configure()
+        .build_server(true)
+        .out_dir("src")
+        .compile(
+            &[
+                "nucliadb_protos/nodewriter.proto",
+                "nucliadb_protos/nodereader.proto",
+            ],
+            &["../../"],
+        )?;
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/fdbwriter.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/fdbwriter.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,699 +1,702 @@
-// We receive this information throw an stream system
-
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Audit {
-    #[prost(string, tag="1")]
-    pub user: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="2")]
-    pub when: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(string, tag="3")]
-    pub origin: ::prost::alloc::string::String,
-    #[prost(enumeration="audit::Source", tag="4")]
-    pub source: i32,
-}
-/// Nested message and enum types in `Audit`.
-pub mod audit {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Source {
-        Http = 0,
-        Dashboard = 1,
-        Desktop = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Error {
-    #[prost(string, tag="1")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(enumeration="super::resources::FieldType", tag="2")]
-    pub field_type: i32,
-    #[prost(string, tag="3")]
-    pub error: ::prost::alloc::string::String,
-    #[prost(enumeration="error::ErrorCode", tag="4")]
-    pub code: i32,
-}
-/// Nested message and enum types in `Error`.
-pub mod error {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum ErrorCode {
-        Generic = 0,
-        Extract = 1,
-        Process = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct BrokerMessage {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="4")]
-    pub slug: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="5")]
-    pub audit: ::core::option::Option<Audit>,
-    #[prost(enumeration="broker_message::MessageType", tag="6")]
-    pub r#type: i32,
-    #[prost(string, tag="7")]
-    pub multiid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="8")]
-    pub basic: ::core::option::Option<super::resources::Basic>,
-    #[prost(message, optional, tag="9")]
-    pub origin: ::core::option::Option<super::resources::Origin>,
-    #[prost(message, repeated, tag="10")]
-    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
-    /// Field Conversations
-    #[prost(map="string, message", tag="11")]
-    pub conversations: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::Conversation>,
-    /// Field Layout
-    #[prost(map="string, message", tag="12")]
-    pub layouts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLayout>,
-    /// Field Text
-    #[prost(map="string, message", tag="13")]
-    pub texts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldText>,
-    /// Field keyword
-    #[prost(map="string, message", tag="14")]
-    pub keywordsets: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldKeywordset>,
-    /// Field Datetime
-    #[prost(map="string, message", tag="15")]
-    pub datetimes: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldDatetime>,
-    /// Field Links
-    #[prost(map="string, message", tag="16")]
-    pub links: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLink>,
-    /// Field File
-    #[prost(map="string, message", tag="17")]
-    pub files: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldFile>,
-    /// Link extracted extra info
-    #[prost(message, repeated, tag="18")]
-    pub link_extracted_data: ::prost::alloc::vec::Vec<super::resources::LinkExtractedData>,
-    /// File extracted extra info
-    #[prost(message, repeated, tag="19")]
-    pub file_extracted_data: ::prost::alloc::vec::Vec<super::resources::FileExtractedData>,
-    /// Field Extracted/Computed information
-    #[prost(message, repeated, tag="20")]
-    pub extracted_text: ::prost::alloc::vec::Vec<super::resources::ExtractedTextWrapper>,
-    #[prost(message, repeated, tag="21")]
-    pub field_metadata: ::prost::alloc::vec::Vec<super::resources::FieldComputedMetadataWrapper>,
-    #[prost(message, repeated, tag="22")]
-    pub field_vectors: ::prost::alloc::vec::Vec<super::resources::ExtractedVectorsWrapper>,
-    /// Resource Large Computed Metadata
-    #[prost(message, repeated, tag="23")]
-    pub field_large_metadata: ::prost::alloc::vec::Vec<super::resources::LargeComputedMetadataWrapper>,
-    #[prost(message, repeated, tag="24")]
-    pub delete_fields: ::prost::alloc::vec::Vec<super::resources::FieldId>,
-    #[prost(int32, tag="25")]
-    pub origin_seq: i32,
-    #[prost(float, tag="26")]
-    pub slow_processing_time: f32,
-    #[prost(float, tag="28")]
-    pub pre_processing_time: f32,
-    #[prost(message, optional, tag="29")]
-    pub done_time: ::core::option::Option<::prost_types::Timestamp>,
-    /// Not needed anymore
-    #[deprecated]
-    #[prost(int64, tag="30")]
-    pub txseqid: i64,
-    #[prost(message, repeated, tag="31")]
-    pub errors: ::prost::alloc::vec::Vec<Error>,
-    #[prost(string, tag="32")]
-    pub processing_id: ::prost::alloc::string::String,
-    #[prost(enumeration="broker_message::MessageSource", tag="33")]
-    pub source: i32,
-    #[prost(int64, tag="34")]
-    pub account_seq: i64,
-    #[prost(message, repeated, tag="35")]
-    pub user_vectors: ::prost::alloc::vec::Vec<super::resources::UserVectorsWrapper>,
-}
-/// Nested message and enum types in `BrokerMessage`.
-pub mod broker_message {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum MessageType {
-        Autocommit = 0,
-        Multi = 1,
-        Commit = 2,
-        Rollback = 3,
-        Delete = 4,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum MessageSource {
-        Writer = 0,
-        Processor = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct WriterStatusResponse {
-    #[prost(string, repeated, tag="1")]
-    pub knowledgeboxes: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// map of last message processed
-    #[prost(map="string, int64", tag="2")]
-    pub msgid: ::std::collections::HashMap<::prost::alloc::string::String, i64>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct WriterStatusRequest {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetLabelsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DelLabelsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub id: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelsResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub labels: ::core::option::Option<super::knowledgebox::Labels>,
-    #[prost(enumeration="get_labels_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetLabelsResponse`.
-pub mod get_labels_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListEntitiesGroupsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListEntitiesGroupsResponse {
-    #[prost(map="string, message", tag="1")]
-    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroupSummary>,
-    #[prost(enumeration="list_entities_groups_response::Status", tag="2")]
-    pub status: i32,
-}
-/// Nested message and enum types in `ListEntitiesGroupsResponse`.
-pub mod list_entities_groups_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(map="string, message", tag="2")]
-    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroup>,
-    #[prost(enumeration="get_entities_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetEntitiesResponse`.
-pub mod get_entities_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DelEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct MergeEntitiesRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub from: ::core::option::Option<merge_entities_request::EntityId>,
-    #[prost(message, optional, tag="3")]
-    pub to: ::core::option::Option<merge_entities_request::EntityId>,
-}
-/// Nested message and enum types in `MergeEntitiesRequest`.
-pub mod merge_entities_request {
-    #[derive(Clone, PartialEq, ::prost::Message)]
-    pub struct EntityId {
-        #[prost(string, tag="1")]
-        pub group: ::prost::alloc::string::String,
-        #[prost(string, tag="2")]
-        pub entity: ::prost::alloc::string::String,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub labelset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetLabelSetResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
-    #[prost(enumeration="get_label_set_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetLabelSetResponse`.
-pub mod get_label_set_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesGroupRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub group: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetEntitiesGroupResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub group: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
-    #[prost(enumeration="get_entities_group_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetEntitiesGroupResponse`.
-pub mod get_entities_group_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        KbNotFound = 1,
-        EntitiesGroupNotFound = 2,
-        Error = 3,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetWidgetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub widget: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetWidgetResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub widget: ::core::option::Option<super::knowledgebox::Widget>,
-    #[prost(enumeration="get_widget_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetWidgetResponse`.
-pub mod get_widget_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetVectorSetsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetVectorSetsResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub vectorsets: ::core::option::Option<super::knowledgebox::VectorSets>,
-    #[prost(enumeration="get_vector_sets_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetVectorSetsResponse`.
-pub mod get_vector_sets_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-        Error = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DelVectorSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub vectorset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetVectorSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub vectorset: ::core::option::Option<super::knowledgebox::VectorSet>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetWidgetsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetWidgetsResponse {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(map="string, message", tag="2")]
-    pub widgets: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Widget>,
-    #[prost(enumeration="get_widgets_response::Status", tag="3")]
-    pub status: i32,
-}
-/// Nested message and enum types in `GetWidgetsResponse`.
-pub mod get_widgets_response {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Notfound = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetWidgetsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub widget: ::core::option::Option<super::knowledgebox::Widget>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DetWidgetsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(string, tag="2")]
-    pub widget: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct OpStatusWriter {
-    #[prost(enumeration="op_status_writer::Status", tag="1")]
-    pub status: i32,
-}
-/// Nested message and enum types in `OpStatusWriter`.
-pub mod op_status_writer {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Error = 1,
-        Notfound = 2,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Notification {
-    #[prost(int32, tag="1")]
-    pub partition: i32,
-    #[prost(string, tag="2")]
-    pub multi: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="4")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(int64, tag="5")]
-    pub seqid: i64,
-    #[prost(enumeration="notification::Action", tag="6")]
-    pub action: i32,
-}
-/// Nested message and enum types in `Notification`.
-pub mod notification {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Action {
-        Commit = 0,
-        Abort = 1,
-    }
-}
-//// The member information.
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Member {
-    //// Member ID.　A string of the UUID.
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    //// Cluster listen address. string of IP and port number.
-    //// E.g. 127.0.0.1:5000
-    #[prost(string, tag="2")]
-    pub listen_address: ::prost::alloc::string::String,
-    //// If true, it means self.
-    #[prost(bool, tag="3")]
-    pub is_self: bool,
-    //// Io, Ingest, Search, Train.
-    #[prost(enumeration="member::Type", tag="4")]
-    pub r#type: i32,
-    //// Dummy Member
-    #[prost(bool, tag="5")]
-    pub dummy: bool,
-    //// The load score of the member.
-    #[prost(float, tag="6")]
-    pub load_score: f32,
-    //// The number of shards in the node.
-    #[prost(uint32, tag="7")]
-    pub shard_count: u32,
-}
-/// Nested message and enum types in `Member`.
-pub mod member {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Type {
-        Io = 0,
-        Search = 1,
-        Ingest = 2,
-        Train = 3,
-        Unknown = 4,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListMembersRequest {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ListMembersResponse {
-    #[prost(message, repeated, tag="1")]
-    pub members: ::prost::alloc::vec::Vec<Member>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShadowShard {
-    #[prost(message, optional, tag="1")]
-    pub shard: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(string, tag="2")]
-    pub node: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardReplica {
-    #[prost(message, optional, tag="1")]
-    pub shard: ::core::option::Option<super::noderesources::ShardCreated>,
-    #[prost(string, tag="2")]
-    pub node: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub shadow_replica: ::core::option::Option<ShadowShard>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShardObject {
-    #[prost(string, tag="1")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(message, repeated, tag="3")]
-    pub replicas: ::prost::alloc::vec::Vec<ShardReplica>,
-    #[prost(message, optional, tag="4")]
-    pub timestamp: ::core::option::Option<::prost_types::Timestamp>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Shards {
-    #[prost(message, repeated, tag="1")]
-    pub shards: ::prost::alloc::vec::Vec<ShardObject>,
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(int32, tag="3")]
-    pub actual: i32,
-    #[prost(enumeration="super::utils::VectorSimilarity", tag="4")]
-    pub similarity: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceFieldId {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub rid: ::prost::alloc::string::String,
-    #[prost(enumeration="super::resources::FieldType", tag="3")]
-    pub field_type: i32,
-    #[prost(string, tag="4")]
-    pub field: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexResource {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub rid: ::prost::alloc::string::String,
-    #[prost(bool, tag="3")]
-    pub reindex_vectors: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexStatus {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceFieldExistsResponse {
-    #[prost(bool, tag="1")]
-    pub found: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceIdRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub slug: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResourceIdResponse {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ExportRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetVectorsRequest {
-    #[prost(message, optional, tag="1")]
-    pub vectors: ::core::option::Option<super::utils::VectorObject>,
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub rid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="4")]
-    pub field: ::core::option::Option<super::resources::FieldId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetVectorsResponse {
-    #[prost(bool, tag="1")]
-    pub found: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FileRequest {
-    #[prost(string, tag="1")]
-    pub bucket: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub key: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct BinaryData {
-    #[prost(bytes="vec", tag="1")]
-    pub data: ::prost::alloc::vec::Vec<u8>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct BinaryMetadata {
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub key: ::prost::alloc::string::String,
-    #[prost(int32, tag="4")]
-    pub size: i32,
-    #[prost(string, tag="5")]
-    pub filename: ::prost::alloc::string::String,
-    #[prost(string, tag="6")]
-    pub content_type: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UploadBinaryData {
-    #[prost(int32, tag="1")]
-    pub count: i32,
-    #[prost(oneof="upload_binary_data::Data", tags="2, 3")]
-    pub data: ::core::option::Option<upload_binary_data::Data>,
-}
-/// Nested message and enum types in `UploadBinaryData`.
-pub mod upload_binary_data {
-    #[derive(Clone, PartialEq, ::prost::Oneof)]
-    pub enum Data {
-        #[prost(message, tag="2")]
-        Metadata(super::BinaryMetadata),
-        #[prost(bytes, tag="3")]
-        Payload(::prost::alloc::vec::Vec<u8>),
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FileUploaded {
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct CreateShadowShardRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="2")]
-    pub replica: ::core::option::Option<super::noderesources::ShardId>,
-    /// node where the shadow shard is created
-    #[prost(string, tag="3")]
-    pub node: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DeleteShadowShardRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="2")]
-    pub replica: ::core::option::Option<super::noderesources::ShardId>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ShadowShardResponse {
-    #[prost(message, optional, tag="1")]
-    pub shadow_shard: ::core::option::Option<ShadowShard>,
-    #[prost(bool, tag="2")]
-    pub success: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SynonymsRequest {
-    #[prost(string, tag="1")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetSynonymsRequest {
-    #[prost(message, optional, tag="1")]
-    pub kbid: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
-    #[prost(message, optional, tag="2")]
-    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetSynonymsResponse {
-    #[prost(message, optional, tag="1")]
-    pub status: ::core::option::Option<OpStatusWriter>,
-    #[prost(message, optional, tag="2")]
-    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
-}
+// We receive this information throw an stream system
+
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Audit {
+    #[prost(string, tag="1")]
+    pub user: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="2")]
+    pub when: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(string, tag="3")]
+    pub origin: ::prost::alloc::string::String,
+    #[prost(enumeration="audit::Source", tag="4")]
+    pub source: i32,
+}
+/// Nested message and enum types in `Audit`.
+pub mod audit {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Source {
+        Http = 0,
+        Dashboard = 1,
+        Desktop = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Error {
+    #[prost(string, tag="1")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(enumeration="super::resources::FieldType", tag="2")]
+    pub field_type: i32,
+    #[prost(string, tag="3")]
+    pub error: ::prost::alloc::string::String,
+    #[prost(enumeration="error::ErrorCode", tag="4")]
+    pub code: i32,
+}
+/// Nested message and enum types in `Error`.
+pub mod error {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum ErrorCode {
+        Generic = 0,
+        Extract = 1,
+        Process = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct BrokerMessage {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="4")]
+    pub slug: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="5")]
+    pub audit: ::core::option::Option<Audit>,
+    #[prost(enumeration="broker_message::MessageType", tag="6")]
+    pub r#type: i32,
+    #[prost(string, tag="7")]
+    pub multiid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="8")]
+    pub basic: ::core::option::Option<super::resources::Basic>,
+    #[prost(message, optional, tag="9")]
+    pub origin: ::core::option::Option<super::resources::Origin>,
+    #[prost(message, repeated, tag="10")]
+    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
+    /// Field Conversations
+    #[prost(map="string, message", tag="11")]
+    pub conversations: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::Conversation>,
+    /// Field Layout
+    #[prost(map="string, message", tag="12")]
+    pub layouts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLayout>,
+    /// Field Text
+    #[prost(map="string, message", tag="13")]
+    pub texts: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldText>,
+    /// Field keyword
+    #[prost(map="string, message", tag="14")]
+    pub keywordsets: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldKeywordset>,
+    /// Field Datetime
+    #[prost(map="string, message", tag="15")]
+    pub datetimes: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldDatetime>,
+    /// Field Links
+    #[prost(map="string, message", tag="16")]
+    pub links: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldLink>,
+    /// Field File
+    #[prost(map="string, message", tag="17")]
+    pub files: ::std::collections::HashMap<::prost::alloc::string::String, super::resources::FieldFile>,
+    /// Link extracted extra info
+    #[prost(message, repeated, tag="18")]
+    pub link_extracted_data: ::prost::alloc::vec::Vec<super::resources::LinkExtractedData>,
+    /// File extracted extra info
+    #[prost(message, repeated, tag="19")]
+    pub file_extracted_data: ::prost::alloc::vec::Vec<super::resources::FileExtractedData>,
+    /// Field Extracted/Computed information
+    #[prost(message, repeated, tag="20")]
+    pub extracted_text: ::prost::alloc::vec::Vec<super::resources::ExtractedTextWrapper>,
+    #[prost(message, repeated, tag="21")]
+    pub field_metadata: ::prost::alloc::vec::Vec<super::resources::FieldComputedMetadataWrapper>,
+    #[prost(message, repeated, tag="22")]
+    pub field_vectors: ::prost::alloc::vec::Vec<super::resources::ExtractedVectorsWrapper>,
+    /// Resource Large Computed Metadata
+    #[prost(message, repeated, tag="23")]
+    pub field_large_metadata: ::prost::alloc::vec::Vec<super::resources::LargeComputedMetadataWrapper>,
+    #[prost(message, repeated, tag="24")]
+    pub delete_fields: ::prost::alloc::vec::Vec<super::resources::FieldId>,
+    #[prost(int32, tag="25")]
+    pub origin_seq: i32,
+    #[prost(float, tag="26")]
+    pub slow_processing_time: f32,
+    #[prost(float, tag="28")]
+    pub pre_processing_time: f32,
+    #[prost(message, optional, tag="29")]
+    pub done_time: ::core::option::Option<::prost_types::Timestamp>,
+    /// Not needed anymore
+    #[deprecated]
+    #[prost(int64, tag="30")]
+    pub txseqid: i64,
+    #[prost(message, repeated, tag="31")]
+    pub errors: ::prost::alloc::vec::Vec<Error>,
+    #[prost(string, tag="32")]
+    pub processing_id: ::prost::alloc::string::String,
+    #[prost(enumeration="broker_message::MessageSource", tag="33")]
+    pub source: i32,
+    #[prost(int64, tag="34")]
+    pub account_seq: i64,
+    #[prost(message, repeated, tag="35")]
+    pub user_vectors: ::prost::alloc::vec::Vec<super::resources::UserVectorsWrapper>,
+    /// If true, force reindex all paragraphs in a resource
+    #[prost(bool, tag="36")]
+    pub reindex: bool,
+}
+/// Nested message and enum types in `BrokerMessage`.
+pub mod broker_message {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum MessageType {
+        Autocommit = 0,
+        Multi = 1,
+        Commit = 2,
+        Rollback = 3,
+        Delete = 4,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum MessageSource {
+        Writer = 0,
+        Processor = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct WriterStatusResponse {
+    #[prost(string, repeated, tag="1")]
+    pub knowledgeboxes: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// map of last message processed
+    #[prost(map="string, int64", tag="2")]
+    pub msgid: ::std::collections::HashMap<::prost::alloc::string::String, i64>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct WriterStatusRequest {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetLabelsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DelLabelsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub id: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelsResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub labels: ::core::option::Option<super::knowledgebox::Labels>,
+    #[prost(enumeration="get_labels_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetLabelsResponse`.
+pub mod get_labels_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewEntitiesGroupRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewEntitiesGroupResponse {
+    #[prost(enumeration="new_entities_group_response::Status", tag="1")]
+    pub status: i32,
+}
+/// Nested message and enum types in `NewEntitiesGroupResponse`.
+pub mod new_entities_group_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Error = 1,
+        KbNotFound = 2,
+        AlreadyExists = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub entities: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UpdateEntitiesGroupRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+    /// entity_id: Entity
+    #[prost(map="string, message", tag="3")]
+    pub add: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Entity>,
+    /// entity_id: Entity
+    #[prost(map="string, message", tag="4")]
+    pub update: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::Entity>,
+    /// entity_id
+    #[prost(string, repeated, tag="5")]
+    pub delete: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(string, tag="6")]
+    pub title: ::prost::alloc::string::String,
+    #[prost(string, tag="7")]
+    pub color: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UpdateEntitiesGroupResponse {
+    #[prost(enumeration="update_entities_group_response::Status", tag="1")]
+    pub status: i32,
+}
+/// Nested message and enum types in `UpdateEntitiesGroupResponse`.
+pub mod update_entities_group_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Error = 1,
+        KbNotFound = 2,
+        EntitiesGroupNotFound = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListEntitiesGroupsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListEntitiesGroupsResponse {
+    #[prost(map="string, message", tag="1")]
+    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroupSummary>,
+    #[prost(enumeration="list_entities_groups_response::Status", tag="2")]
+    pub status: i32,
+}
+/// Nested message and enum types in `ListEntitiesGroupsResponse`.
+pub mod list_entities_groups_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(map="string, message", tag="2")]
+    pub groups: ::std::collections::HashMap<::prost::alloc::string::String, super::knowledgebox::EntitiesGroup>,
+    #[prost(enumeration="get_entities_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetEntitiesResponse`.
+pub mod get_entities_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DelEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct MergeEntitiesRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub from: ::core::option::Option<merge_entities_request::EntityId>,
+    #[prost(message, optional, tag="3")]
+    pub to: ::core::option::Option<merge_entities_request::EntityId>,
+}
+/// Nested message and enum types in `MergeEntitiesRequest`.
+pub mod merge_entities_request {
+    #[derive(Clone, PartialEq, ::prost::Message)]
+    pub struct EntityId {
+        #[prost(string, tag="1")]
+        pub group: ::prost::alloc::string::String,
+        #[prost(string, tag="2")]
+        pub entity: ::prost::alloc::string::String,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub labelset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetLabelSetResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub labelset: ::core::option::Option<super::knowledgebox::LabelSet>,
+    #[prost(enumeration="get_label_set_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetLabelSetResponse`.
+pub mod get_label_set_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesGroupRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub group: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetEntitiesGroupResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub group: ::core::option::Option<super::knowledgebox::EntitiesGroup>,
+    #[prost(enumeration="get_entities_group_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetEntitiesGroupResponse`.
+pub mod get_entities_group_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        KbNotFound = 1,
+        EntitiesGroupNotFound = 2,
+        Error = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetVectorSetsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetVectorSetsResponse {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub vectorsets: ::core::option::Option<super::knowledgebox::VectorSets>,
+    #[prost(enumeration="get_vector_sets_response::Status", tag="3")]
+    pub status: i32,
+}
+/// Nested message and enum types in `GetVectorSetsResponse`.
+pub mod get_vector_sets_response {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Notfound = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DelVectorSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub vectorset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetVectorSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub kb: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(string, tag="2")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub vectorset: ::core::option::Option<super::knowledgebox::VectorSet>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct OpStatusWriter {
+    #[prost(enumeration="op_status_writer::Status", tag="1")]
+    pub status: i32,
+}
+/// Nested message and enum types in `OpStatusWriter`.
+pub mod op_status_writer {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Error = 1,
+        Notfound = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Notification {
+    #[prost(int32, tag="1")]
+    pub partition: i32,
+    #[prost(string, tag="2")]
+    pub multi: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="4")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(int64, tag="5")]
+    pub seqid: i64,
+    #[prost(enumeration="notification::Action", tag="6")]
+    pub action: i32,
+}
+/// Nested message and enum types in `Notification`.
+pub mod notification {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Action {
+        Commit = 0,
+        Abort = 1,
+    }
+}
+//// The member information.
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Member {
+    //// Member ID.　A string of the UUID.
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    //// Cluster listen address. string of IP and port number.
+    //// E.g. 127.0.0.1:5000
+    #[prost(string, tag="2")]
+    pub listen_address: ::prost::alloc::string::String,
+    //// If true, it means self.
+    #[prost(bool, tag="3")]
+    pub is_self: bool,
+    //// Io, Ingest, Search, Train.
+    #[prost(enumeration="member::Type", tag="4")]
+    pub r#type: i32,
+    //// Dummy Member
+    #[prost(bool, tag="5")]
+    pub dummy: bool,
+    //// The load score of the member.
+    #[prost(float, tag="6")]
+    pub load_score: f32,
+    //// The number of shards in the node.
+    #[prost(uint32, tag="7")]
+    pub shard_count: u32,
+}
+/// Nested message and enum types in `Member`.
+pub mod member {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Type {
+        Io = 0,
+        Search = 1,
+        Ingest = 2,
+        Train = 3,
+        Unknown = 4,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListMembersRequest {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ListMembersResponse {
+    #[prost(message, repeated, tag="1")]
+    pub members: ::prost::alloc::vec::Vec<Member>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShadowShard {
+    #[prost(message, optional, tag="1")]
+    pub shard: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(string, tag="2")]
+    pub node: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardReplica {
+    #[prost(message, optional, tag="1")]
+    pub shard: ::core::option::Option<super::noderesources::ShardCreated>,
+    #[prost(string, tag="2")]
+    pub node: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub shadow_replica: ::core::option::Option<ShadowShard>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShardObject {
+    #[prost(string, tag="1")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(message, repeated, tag="3")]
+    pub replicas: ::prost::alloc::vec::Vec<ShardReplica>,
+    #[prost(message, optional, tag="4")]
+    pub timestamp: ::core::option::Option<::prost_types::Timestamp>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Shards {
+    #[prost(message, repeated, tag="1")]
+    pub shards: ::prost::alloc::vec::Vec<ShardObject>,
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+    /// current shard that resources index to
+    #[prost(int32, tag="3")]
+    pub actual: i32,
+    #[prost(enumeration="super::utils::VectorSimilarity", tag="4")]
+    pub similarity: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceFieldId {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub rid: ::prost::alloc::string::String,
+    #[prost(enumeration="super::resources::FieldType", tag="3")]
+    pub field_type: i32,
+    #[prost(string, tag="4")]
+    pub field: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexResource {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub rid: ::prost::alloc::string::String,
+    #[prost(bool, tag="3")]
+    pub reindex_vectors: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexStatus {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceFieldExistsResponse {
+    #[prost(bool, tag="1")]
+    pub found: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceIdRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub slug: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResourceIdResponse {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ExportRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetVectorsRequest {
+    #[prost(message, optional, tag="1")]
+    pub vectors: ::core::option::Option<super::utils::VectorObject>,
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub rid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="4")]
+    pub field: ::core::option::Option<super::resources::FieldId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetVectorsResponse {
+    #[prost(bool, tag="1")]
+    pub found: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FileRequest {
+    #[prost(string, tag="1")]
+    pub bucket: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub key: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct BinaryData {
+    #[prost(bytes="vec", tag="1")]
+    pub data: ::prost::alloc::vec::Vec<u8>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct BinaryMetadata {
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub key: ::prost::alloc::string::String,
+    #[prost(int32, tag="4")]
+    pub size: i32,
+    #[prost(string, tag="5")]
+    pub filename: ::prost::alloc::string::String,
+    #[prost(string, tag="6")]
+    pub content_type: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UploadBinaryData {
+    #[prost(int32, tag="1")]
+    pub count: i32,
+    #[prost(oneof="upload_binary_data::Data", tags="2, 3")]
+    pub data: ::core::option::Option<upload_binary_data::Data>,
+}
+/// Nested message and enum types in `UploadBinaryData`.
+pub mod upload_binary_data {
+    #[derive(Clone, PartialEq, ::prost::Oneof)]
+    pub enum Data {
+        #[prost(message, tag="2")]
+        Metadata(super::BinaryMetadata),
+        #[prost(bytes, tag="3")]
+        Payload(::prost::alloc::vec::Vec<u8>),
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FileUploaded {
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct CreateShadowShardRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="2")]
+    pub replica: ::core::option::Option<super::noderesources::ShardId>,
+    /// node where the shadow shard is created
+    #[prost(string, tag="3")]
+    pub node: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DeleteShadowShardRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="2")]
+    pub replica: ::core::option::Option<super::noderesources::ShardId>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ShadowShardResponse {
+    #[prost(message, optional, tag="1")]
+    pub shadow_shard: ::core::option::Option<ShadowShard>,
+    #[prost(bool, tag="2")]
+    pub success: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SynonymsRequest {
+    #[prost(string, tag="1")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetSynonymsRequest {
+    #[prost(message, optional, tag="1")]
+    pub kbid: ::core::option::Option<super::knowledgebox::KnowledgeBoxId>,
+    #[prost(message, optional, tag="2")]
+    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetSynonymsResponse {
+    #[prost(message, optional, tag="1")]
+    pub status: ::core::option::Option<OpStatusWriter>,
+    #[prost(message, optional, tag="2")]
+    pub synonyms: ::core::option::Option<super::knowledgebox::Synonyms>,
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/lib.rs`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-#![allow(clippy::derive_partial_eq_without_eq)]
-
-pub mod fdbwriter;
-mod knowledgebox;
-mod nodereader;
-mod noderesources;
-mod nodewriter;
-mod resources;
-mod utils;
-
-extern crate serde;
-
-// writer should not serialized to rust as its only ingest and brings conflict
-// pub use fdbwriter::*;
-pub use knowledgebox::*;
-pub use nodereader::*;
-pub use noderesources::*;
-pub use nodewriter::*;
-pub use resources::*;
-pub use utils::*;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+#![allow(clippy::derive_partial_eq_without_eq)]
+
+pub mod fdbwriter;
+mod knowledgebox;
+mod nodereader;
+mod noderesources;
+mod nodewriter;
+mod resources;
+mod utils;
+
+extern crate serde;
+
+// writer should not serialized to rust as its only ingest and brings conflict
+// pub use fdbwriter::*;
+pub use knowledgebox::*;
+pub use nodereader::*;
+pub use noderesources::*;
+pub use nodewriter::*;
+pub use resources::*;
+pub use utils::*;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/nodereader.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/nodereader.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,1697 +1,1717 @@
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Filter {
-    #[prost(string, repeated, tag="1")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Faceted {
-    #[prost(string, repeated, tag="1")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct OrderBy {
-    #[deprecated]
-    #[prost(string, tag="1")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(enumeration="order_by::OrderType", tag="2")]
-    pub r#type: i32,
-    #[prost(enumeration="order_by::OrderField", tag="3")]
-    pub sort_by: i32,
-}
-/// Nested message and enum types in `OrderBy`.
-pub mod order_by {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum OrderType {
-        Desc = 0,
-        Asc = 1,
-    }
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum OrderField {
-        Created = 0,
-        Modified = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Timestamps {
-    #[prost(message, optional, tag="1")]
-    pub from_modified: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(message, optional, tag="2")]
-    pub to_modified: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(message, optional, tag="3")]
-    pub from_created: ::core::option::Option<::prost_types::Timestamp>,
-    #[prost(message, optional, tag="4")]
-    pub to_created: ::core::option::Option<::prost_types::Timestamp>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FacetResult {
-    #[prost(string, tag="1")]
-    pub tag: ::prost::alloc::string::String,
-    #[prost(int32, tag="2")]
-    pub total: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct FacetResults {
-    #[prost(message, repeated, tag="1")]
-    pub facetresults: ::prost::alloc::vec::Vec<FacetResult>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentSearchRequest {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="3")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(message, optional, tag="4")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="5")]
-    pub order: ::core::option::Option<OrderBy>,
-    #[prost(message, optional, tag="6")]
-    pub faceted: ::core::option::Option<Faceted>,
-    #[prost(int32, tag="7")]
-    pub page_number: i32,
-    #[prost(int32, tag="8")]
-    pub result_per_page: i32,
-    #[prost(message, optional, tag="9")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    #[prost(bool, tag="10")]
-    pub reload: bool,
-    #[prost(bool, tag="15")]
-    pub only_faceted: bool,
-    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="16")]
-    pub with_status: ::core::option::Option<i32>,
-    #[prost(string, optional, tag="17")]
-    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphSearchRequest {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="3")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// query this text in all the paragraphs
-    #[prost(string, tag="4")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="5")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="7")]
-    pub order: ::core::option::Option<OrderBy>,
-    /// Faceted{ tags: Vec<String>}
-    #[prost(message, optional, tag="8")]
-    pub faceted: ::core::option::Option<Faceted>,
-    #[prost(int32, tag="10")]
-    pub page_number: i32,
-    #[prost(int32, tag="11")]
-    pub result_per_page: i32,
-    #[prost(message, optional, tag="12")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    #[prost(bool, tag="13")]
-    pub reload: bool,
-    #[prost(bool, tag="14")]
-    pub with_duplicates: bool,
-    #[prost(bool, tag="15")]
-    pub only_faceted: bool,
-    #[prost(string, optional, tag="16")]
-    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ResultScore {
-    #[prost(float, tag="1")]
-    pub bm25: f32,
-    /// In the case of two equal bm25 scores, booster 
-    /// decides
-    #[prost(float, tag="2")]
-    pub booster: f32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentResult {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub score: ::core::option::Option<ResultScore>,
-    #[prost(string, tag="4")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="5")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentSearchResponse {
-    #[prost(int32, tag="1")]
-    pub total: i32,
-    #[prost(message, repeated, tag="2")]
-    pub results: ::prost::alloc::vec::Vec<DocumentResult>,
-    #[prost(map="string, message", tag="3")]
-    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-    /// The text that lead to this results
-    #[prost(string, tag="6")]
-    pub query: ::prost::alloc::string::String,
-    /// Is there a next page
-    #[prost(bool, tag="7")]
-    pub next_page: bool,
-    #[prost(bool, tag="8")]
-    pub bm25: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphResult {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="3")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(uint64, tag="4")]
-    pub start: u64,
-    #[prost(uint64, tag="5")]
-    pub end: u64,
-    #[prost(string, tag="6")]
-    pub paragraph: ::prost::alloc::string::String,
-    #[prost(string, tag="7")]
-    pub split: ::prost::alloc::string::String,
-    #[prost(uint64, tag="8")]
-    pub index: u64,
-    #[prost(message, optional, tag="9")]
-    pub score: ::core::option::Option<ResultScore>,
-    #[prost(string, repeated, tag="10")]
-    pub matches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// Metadata that can't be searched with but is returned on search results
-    #[prost(message, optional, tag="11")]
-    pub metadata: ::core::option::Option<super::noderesources::ParagraphMetadata>,
-    #[prost(string, repeated, tag="12")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphSearchResponse {
-    #[prost(int32, tag="10")]
-    pub fuzzy_distance: i32,
-    #[prost(int32, tag="1")]
-    pub total: i32,
-    /// 
-    #[prost(message, repeated, tag="2")]
-    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
-    /// For each field what facets are.
-    #[prost(map="string, message", tag="3")]
-    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
-    /// What page is the answer.
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    /// How many results are in this page.
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-    /// The text that lead to this results
-    #[prost(string, tag="6")]
-    pub query: ::prost::alloc::string::String,
-    /// Is there a next page
-    #[prost(bool, tag="7")]
-    pub next_page: bool,
-    #[prost(bool, tag="8")]
-    pub bm25: bool,
-    #[prost(string, repeated, tag="9")]
-    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSearchRequest {
-    ///Shard ID
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    /// ID for the vector set.
-    /// Empty for searching on the original index
-    #[prost(string, tag="15")]
-    pub vector_set: ::prost::alloc::string::String,
-    /// Embedded vector search.
-    #[prost(float, repeated, tag="2")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    /// tags to filter
-    #[prost(string, repeated, tag="3")]
-    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// What page is the answer.
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    /// How many results are in this page.
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-    #[prost(bool, tag="14")]
-    pub with_duplicates: bool,
-    #[prost(bool, tag="13")]
-    pub reload: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentVectorIdentifier {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentScored {
-    #[prost(message, optional, tag="1")]
-    pub doc_id: ::core::option::Option<DocumentVectorIdentifier>,
-    #[prost(float, tag="2")]
-    pub score: f32,
-    #[prost(message, optional, tag="3")]
-    pub metadata: ::core::option::Option<super::noderesources::SentenceMetadata>,
-    #[prost(string, repeated, tag="4")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorSearchResponse {
-    /// List of docs closer to the asked one.
-    #[prost(message, repeated, tag="1")]
-    pub documents: ::prost::alloc::vec::Vec<DocumentScored>,
-    /// What page is the answer.
-    #[prost(int32, tag="4")]
-    pub page_number: i32,
-    /// How many results are in this page.
-    #[prost(int32, tag="5")]
-    pub result_per_page: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationNodeFilter {
-    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
-    pub node_type: i32,
-    #[prost(string, optional, tag="2")]
-    pub node_subtype: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationEdgeFilter {
-    /// Will filter the search to edges of type ntype.
-    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
-    pub relation_type: i32,
-    #[prost(string, optional, tag="2")]
-    pub relation_subtype: ::core::option::Option<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationPrefixSearchRequest {
-    #[prost(string, tag="1")]
-    pub prefix: ::prost::alloc::string::String,
-    #[prost(message, repeated, tag="2")]
-    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationPrefixSearchResponse {
-    #[prost(message, repeated, tag="1")]
-    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EntitiesSubgraphRequest {
-    /// List of vertices where search will trigger
-    #[prost(message, repeated, tag="1")]
-    pub entry_points: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
-    /// Filters to apply while searching. It's an OR filtering: any
-    /// node (vertex) satisfying one condition will be returned
-    #[prost(message, repeated, tag="2")]
-    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
-    /// Filters to apply while searching. It's an OR filtering: any
-    /// edge satisfying one condition will be returned
-    #[prost(message, repeated, tag="4")]
-    pub edge_filters: ::prost::alloc::vec::Vec<RelationEdgeFilter>,
-    #[prost(int32, optional, tag="3")]
-    pub depth: ::core::option::Option<i32>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EntitiesSubgraphResponse {
-    #[prost(message, repeated, tag="1")]
-    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
-}
-// TODO: uncomment and implement (next iteration)
-// message RelationPathsSearchRequest {
-//     message PathEndpoints {
-//         utils.RelationNode origin = 1;
-//         utils.RelationNode destination = 2;
-//     }
-//     repeated PathEndpoints paths = 1;
-// }
-
-/// Query relation index to obtain different information about the
-/// knowledge graph. It can be queried using the following strategies:
-///
-/// - prefix search over vertex (node) names
-/// - graph search:
-///   - given some entry vertices, get the filtered subgraph around them
-///   - (TODO) given some vertices, get paths between them
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationSearchRequest {
-    #[prost(string, tag="1")]
-    pub shard_id: ::prost::alloc::string::String,
-    #[prost(bool, tag="5")]
-    pub reload: bool,
-    #[prost(message, optional, tag="11")]
-    pub prefix: ::core::option::Option<RelationPrefixSearchRequest>,
-    /// TODO: uncomment and implement (next iteration)
-    /// RelationPathsSearchRequest paths = 13;
-    #[prost(message, optional, tag="12")]
-    pub subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationSearchResponse {
-    #[prost(message, optional, tag="11")]
-    pub prefix: ::core::option::Option<RelationPrefixSearchResponse>,
-    /// TODO: uncomment and implement (next iteration)
-    /// repeated utils.RelationPath paths = 13;
-    #[prost(message, optional, tag="12")]
-    pub subgraph: ::core::option::Option<EntitiesSubgraphResponse>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SearchRequest {
-    #[prost(string, tag="1")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="2")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// query this text in all the paragraphs
-    #[prost(string, tag="3")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="4")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="5")]
-    pub order: ::core::option::Option<OrderBy>,
-    /// Faceted{ tags: Vec<String>}
-    #[prost(message, optional, tag="6")]
-    pub faceted: ::core::option::Option<Faceted>,
-    #[prost(int32, tag="7")]
-    pub page_number: i32,
-    #[prost(int32, tag="8")]
-    pub result_per_page: i32,
-    #[prost(message, optional, tag="9")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    /// Embedded vector search.
-    #[prost(float, repeated, tag="10")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    #[prost(string, tag="15")]
-    pub vectorset: ::prost::alloc::string::String,
-    #[prost(bool, tag="11")]
-    pub reload: bool,
-    #[prost(bool, tag="12")]
-    pub paragraph: bool,
-    #[prost(bool, tag="13")]
-    pub document: bool,
-    #[prost(bool, tag="14")]
-    pub with_duplicates: bool,
-    #[prost(bool, tag="16")]
-    pub only_faceted: bool,
-    #[prost(string, optional, tag="18")]
-    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
-    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="17")]
-    pub with_status: ::core::option::Option<i32>,
-    /// if provided, search metadata for this nodes (nodes at distance
-    /// one) and get the shortest path between nodes
-    #[deprecated]
-    #[prost(message, optional, tag="19")]
-    pub relations: ::core::option::Option<RelationSearchRequest>,
-    #[prost(message, optional, tag="20")]
-    pub relation_prefix: ::core::option::Option<RelationPrefixSearchRequest>,
-    #[prost(message, optional, tag="21")]
-    pub relation_subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SuggestRequest {
-    #[prost(string, tag="1")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub body: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="3")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(message, optional, tag="4")]
-    pub timestamps: ::core::option::Option<Timestamps>,
-    #[prost(string, repeated, tag="5")]
-    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelatedEntities {
-    #[prost(string, repeated, tag="1")]
-    pub entities: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(uint32, tag="2")]
-    pub total: u32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SuggestResponse {
-    #[prost(int32, tag="1")]
-    pub total: i32,
-    #[prost(message, repeated, tag="2")]
-    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
-    /// The text that lead to this results
-    #[prost(string, tag="3")]
-    pub query: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="4")]
-    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    /// Entities related with the query
-    #[prost(message, optional, tag="5")]
-    pub entities: ::core::option::Option<RelatedEntities>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SearchResponse {
-    #[prost(message, optional, tag="1")]
-    pub document: ::core::option::Option<DocumentSearchResponse>,
-    #[prost(message, optional, tag="2")]
-    pub paragraph: ::core::option::Option<ParagraphSearchResponse>,
-    #[prost(message, optional, tag="3")]
-    pub vector: ::core::option::Option<VectorSearchResponse>,
-    #[prost(message, optional, tag="4")]
-    pub relation: ::core::option::Option<RelationSearchResponse>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IdCollection {
-    #[prost(string, repeated, tag="1")]
-    pub ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationEdge {
-    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
-    pub edge_type: i32,
-    #[prost(string, tag="2")]
-    pub property: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct EdgeList {
-    #[prost(message, repeated, tag="1")]
-    pub list: ::prost::alloc::vec::Vec<RelationEdge>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationTypeListMember {
-    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
-    pub with_type: i32,
-    #[prost(string, tag="2")]
-    pub with_subtype: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct TypeList {
-    #[prost(message, repeated, tag="1")]
-    pub list: ::prost::alloc::vec::Vec<RelationTypeListMember>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct GetShardRequest {
-    #[prost(message, optional, tag="1")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(string, tag="2")]
-    pub vectorset: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ParagraphItem {
-    #[prost(string, tag="1")]
-    pub id: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="2")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DocumentItem {
-    #[prost(string, tag="1")]
-    pub uuid: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub field: ::prost::alloc::string::String,
-    #[prost(string, repeated, tag="3")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct StreamRequest {
-    #[prost(message, optional, tag="1")]
-    pub filter: ::core::option::Option<Filter>,
-    #[prost(bool, tag="2")]
-    pub reload: bool,
-    #[prost(message, optional, tag="3")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-}
-/// Generated client implementations.
-pub mod node_reader_client {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    #[derive(Debug, Clone)]
-    pub struct NodeReaderClient<T> {
-        inner: tonic::client::Grpc<T>,
-    }
-    impl NodeReaderClient<tonic::transport::Channel> {
-        /// Attempt to create a new client by connecting to a given endpoint.
-        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
-        where
-            D: std::convert::TryInto<tonic::transport::Endpoint>,
-            D::Error: Into<StdError>,
-        {
-            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
-            Ok(Self::new(conn))
-        }
-    }
-    impl<T> NodeReaderClient<T>
-    where
-        T: tonic::client::GrpcService<tonic::body::BoxBody>,
-        T::Error: Into<StdError>,
-        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
-        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
-    {
-        pub fn new(inner: T) -> Self {
-            let inner = tonic::client::Grpc::new(inner);
-            Self { inner }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> NodeReaderClient<InterceptedService<T, F>>
-        where
-            F: tonic::service::Interceptor,
-            T::ResponseBody: Default,
-            T: tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-                Response = http::Response<
-                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
-                >,
-            >,
-            <T as tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-            >>::Error: Into<StdError> + Send + Sync,
-        {
-            NodeReaderClient::new(InterceptedService::new(inner, interceptor))
-        }
-        /// Compress requests with `gzip`.
-        ///
-        /// This requires the server to support it otherwise it might respond with an
-        /// error.
-        #[must_use]
-        pub fn send_gzip(mut self) -> Self {
-            self.inner = self.inner.send_gzip();
-            self
-        }
-        /// Enable decompressing responses with `gzip`.
-        #[must_use]
-        pub fn accept_gzip(mut self) -> Self {
-            self.inner = self.inner.accept_gzip();
-            self
-        }
-        pub async fn get_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::GetShardRequest>,
-        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/GetShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn get_shards(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardList>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/GetShards",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn document_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::DocumentSearchRequest>,
-        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/DocumentSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn paragraph_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::ParagraphSearchRequest>,
-        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/ParagraphSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn vector_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::VectorSearchRequest>,
-        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/VectorSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::RelationSearchRequest>,
-        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationSearch",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn document_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/DocumentIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn paragraph_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/ParagraphIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn vector_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/VectorIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_ids(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationIds",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_edges(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationEdges",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn relation_types(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::TypeList>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/RelationTypes",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn search(
-            &mut self,
-            request: impl tonic::IntoRequest<super::SearchRequest>,
-        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Search",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn suggest(
-            &mut self,
-            request: impl tonic::IntoRequest<super::SuggestRequest>,
-        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Suggest",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        /// Streams
-        pub async fn paragraphs(
-            &mut self,
-            request: impl tonic::IntoRequest<super::StreamRequest>,
-        ) -> Result<
-            tonic::Response<tonic::codec::Streaming<super::ParagraphItem>>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Paragraphs",
-            );
-            self.inner.server_streaming(request.into_request(), path, codec).await
-        }
-        pub async fn documents(
-            &mut self,
-            request: impl tonic::IntoRequest<super::StreamRequest>,
-        ) -> Result<
-            tonic::Response<tonic::codec::Streaming<super::DocumentItem>>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodereader.NodeReader/Documents",
-            );
-            self.inner.server_streaming(request.into_request(), path, codec).await
-        }
-    }
-}
-/// Generated server implementations.
-pub mod node_reader_server {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    ///Generated trait containing gRPC methods that should be implemented for use with NodeReaderServer.
-    #[async_trait]
-    pub trait NodeReader: Send + Sync + 'static {
-        async fn get_shard(
-            &self,
-            request: tonic::Request<super::GetShardRequest>,
-        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status>;
-        async fn get_shards(
-            &self,
-            request: tonic::Request<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardList>,
-            tonic::Status,
-        >;
-        async fn document_search(
-            &self,
-            request: tonic::Request<super::DocumentSearchRequest>,
-        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status>;
-        async fn paragraph_search(
-            &self,
-            request: tonic::Request<super::ParagraphSearchRequest>,
-        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status>;
-        async fn vector_search(
-            &self,
-            request: tonic::Request<super::VectorSearchRequest>,
-        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status>;
-        async fn relation_search(
-            &self,
-            request: tonic::Request<super::RelationSearchRequest>,
-        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status>;
-        async fn document_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn paragraph_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn vector_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn relation_ids(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
-        async fn relation_edges(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status>;
-        async fn relation_types(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<tonic::Response<super::TypeList>, tonic::Status>;
-        async fn search(
-            &self,
-            request: tonic::Request<super::SearchRequest>,
-        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status>;
-        async fn suggest(
-            &self,
-            request: tonic::Request<super::SuggestRequest>,
-        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status>;
-        ///Server streaming response type for the Paragraphs method.
-        type ParagraphsStream: futures_core::Stream<
-                Item = Result<super::ParagraphItem, tonic::Status>,
-            >
-            + Send
-            + 'static;
-        /// Streams
-        async fn paragraphs(
-            &self,
-            request: tonic::Request<super::StreamRequest>,
-        ) -> Result<tonic::Response<Self::ParagraphsStream>, tonic::Status>;
-        ///Server streaming response type for the Documents method.
-        type DocumentsStream: futures_core::Stream<
-                Item = Result<super::DocumentItem, tonic::Status>,
-            >
-            + Send
-            + 'static;
-        async fn documents(
-            &self,
-            request: tonic::Request<super::StreamRequest>,
-        ) -> Result<tonic::Response<Self::DocumentsStream>, tonic::Status>;
-    }
-    #[derive(Debug)]
-    pub struct NodeReaderServer<T: NodeReader> {
-        inner: _Inner<T>,
-        accept_compression_encodings: (),
-        send_compression_encodings: (),
-    }
-    struct _Inner<T>(Arc<T>);
-    impl<T: NodeReader> NodeReaderServer<T> {
-        pub fn new(inner: T) -> Self {
-            Self::from_arc(Arc::new(inner))
-        }
-        pub fn from_arc(inner: Arc<T>) -> Self {
-            let inner = _Inner(inner);
-            Self {
-                inner,
-                accept_compression_encodings: Default::default(),
-                send_compression_encodings: Default::default(),
-            }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> InterceptedService<Self, F>
-        where
-            F: tonic::service::Interceptor,
-        {
-            InterceptedService::new(Self::new(inner), interceptor)
-        }
-    }
-    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeReaderServer<T>
-    where
-        T: NodeReader,
-        B: Body + Send + 'static,
-        B::Error: Into<StdError> + Send + 'static,
-    {
-        type Response = http::Response<tonic::body::BoxBody>;
-        type Error = std::convert::Infallible;
-        type Future = BoxFuture<Self::Response, Self::Error>;
-        fn poll_ready(
-            &mut self,
-            _cx: &mut Context<'_>,
-        ) -> Poll<Result<(), Self::Error>> {
-            Poll::Ready(Ok(()))
-        }
-        fn call(&mut self, req: http::Request<B>) -> Self::Future {
-            let inner = self.inner.clone();
-            match req.uri().path() {
-                "/nodereader.NodeReader/GetShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct GetShardSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::GetShardRequest>
-                    for GetShardSvc<T> {
-                        type Response = super::super::noderesources::Shard;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::GetShardRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).get_shard(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GetShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/GetShards" => {
-                    #[allow(non_camel_case_types)]
-                    struct GetShardsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::EmptyQuery,
-                    > for GetShardsSvc<T> {
-                        type Response = super::super::noderesources::ShardList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::EmptyQuery,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).get_shards(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GetShardsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/DocumentSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct DocumentSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::DocumentSearchRequest>
-                    for DocumentSearchSvc<T> {
-                        type Response = super::DocumentSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::DocumentSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).document_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DocumentSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/ParagraphSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct ParagraphSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::ParagraphSearchRequest>
-                    for ParagraphSearchSvc<T> {
-                        type Response = super::ParagraphSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::ParagraphSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).paragraph_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ParagraphSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/VectorSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct VectorSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::VectorSearchRequest>
-                    for VectorSearchSvc<T> {
-                        type Response = super::VectorSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::VectorSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).vector_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = VectorSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationSearch" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationSearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::RelationSearchRequest>
-                    for RelationSearchSvc<T> {
-                        type Response = super::RelationSearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::RelationSearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_search(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationSearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/DocumentIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct DocumentIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for DocumentIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).document_ids(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DocumentIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/ParagraphIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct ParagraphIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for ParagraphIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).paragraph_ids(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ParagraphIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/VectorIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct VectorIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for VectorIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).vector_ids(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = VectorIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationIds" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationIdsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for RelationIdsSvc<T> {
-                        type Response = super::IdCollection;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_ids(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationIdsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationEdges" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationEdgesSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for RelationEdgesSvc<T> {
-                        type Response = super::EdgeList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_edges(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationEdgesSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/RelationTypes" => {
-                    #[allow(non_camel_case_types)]
-                    struct RelationTypesSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for RelationTypesSvc<T> {
-                        type Response = super::TypeList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).relation_types(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RelationTypesSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Search" => {
-                    #[allow(non_camel_case_types)]
-                    struct SearchSvc<T: NodeReader>(pub Arc<T>);
-                    impl<T: NodeReader> tonic::server::UnaryService<super::SearchRequest>
-                    for SearchSvc<T> {
-                        type Response = super::SearchResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::SearchRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).search(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = SearchSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Suggest" => {
-                    #[allow(non_camel_case_types)]
-                    struct SuggestSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::UnaryService<super::SuggestRequest>
-                    for SuggestSvc<T> {
-                        type Response = super::SuggestResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::SuggestRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).suggest(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = SuggestSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Paragraphs" => {
-                    #[allow(non_camel_case_types)]
-                    struct ParagraphsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::ServerStreamingService<super::StreamRequest>
-                    for ParagraphsSvc<T> {
-                        type Response = super::ParagraphItem;
-                        type ResponseStream = T::ParagraphsStream;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::ResponseStream>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::StreamRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).paragraphs(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ParagraphsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.server_streaming(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodereader.NodeReader/Documents" => {
-                    #[allow(non_camel_case_types)]
-                    struct DocumentsSvc<T: NodeReader>(pub Arc<T>);
-                    impl<
-                        T: NodeReader,
-                    > tonic::server::ServerStreamingService<super::StreamRequest>
-                    for DocumentsSvc<T> {
-                        type Response = super::DocumentItem;
-                        type ResponseStream = T::DocumentsStream;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::ResponseStream>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::StreamRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).documents(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DocumentsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.server_streaming(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                _ => {
-                    Box::pin(async move {
-                        Ok(
-                            http::Response::builder()
-                                .status(200)
-                                .header("grpc-status", "12")
-                                .header("content-type", "application/grpc")
-                                .body(empty_body())
-                                .unwrap(),
-                        )
-                    })
-                }
-            }
-        }
-    }
-    impl<T: NodeReader> Clone for NodeReaderServer<T> {
-        fn clone(&self) -> Self {
-            let inner = self.inner.clone();
-            Self {
-                inner,
-                accept_compression_encodings: self.accept_compression_encodings,
-                send_compression_encodings: self.send_compression_encodings,
-            }
-        }
-    }
-    impl<T: NodeReader> Clone for _Inner<T> {
-        fn clone(&self) -> Self {
-            Self(self.0.clone())
-        }
-    }
-    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
-        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-            write!(f, "{:?}", self.0)
-        }
-    }
-    impl<T: NodeReader> tonic::transport::NamedService for NodeReaderServer<T> {
-        const NAME: &'static str = "nodereader.NodeReader";
-    }
-}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Filter {
+    #[prost(string, repeated, tag="1")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct StreamFilter {
+    #[prost(enumeration="stream_filter::Conjunction", tag="1")]
+    pub conjunction: i32,
+    #[prost(string, repeated, tag="2")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+/// Nested message and enum types in `StreamFilter`.
+pub mod stream_filter {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Conjunction {
+        And = 0,
+        Or = 1,
+        Not = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Faceted {
+    #[prost(string, repeated, tag="1")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct OrderBy {
+    #[deprecated]
+    #[prost(string, tag="1")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(enumeration="order_by::OrderType", tag="2")]
+    pub r#type: i32,
+    #[prost(enumeration="order_by::OrderField", tag="3")]
+    pub sort_by: i32,
+}
+/// Nested message and enum types in `OrderBy`.
+pub mod order_by {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum OrderType {
+        Desc = 0,
+        Asc = 1,
+    }
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum OrderField {
+        Created = 0,
+        Modified = 1,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Timestamps {
+    #[prost(message, optional, tag="1")]
+    pub from_modified: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(message, optional, tag="2")]
+    pub to_modified: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(message, optional, tag="3")]
+    pub from_created: ::core::option::Option<::prost_types::Timestamp>,
+    #[prost(message, optional, tag="4")]
+    pub to_created: ::core::option::Option<::prost_types::Timestamp>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FacetResult {
+    #[prost(string, tag="1")]
+    pub tag: ::prost::alloc::string::String,
+    #[prost(int32, tag="2")]
+    pub total: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct FacetResults {
+    #[prost(message, repeated, tag="1")]
+    pub facetresults: ::prost::alloc::vec::Vec<FacetResult>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentSearchRequest {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="3")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(message, optional, tag="4")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="5")]
+    pub order: ::core::option::Option<OrderBy>,
+    #[prost(message, optional, tag="6")]
+    pub faceted: ::core::option::Option<Faceted>,
+    #[prost(int32, tag="7")]
+    pub page_number: i32,
+    #[prost(int32, tag="8")]
+    pub result_per_page: i32,
+    #[prost(message, optional, tag="9")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    #[prost(bool, tag="10")]
+    pub reload: bool,
+    #[prost(bool, tag="15")]
+    pub only_faceted: bool,
+    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="16")]
+    pub with_status: ::core::option::Option<i32>,
+    #[prost(string, optional, tag="17")]
+    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphSearchRequest {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="3")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// query this text in all the paragraphs
+    #[prost(string, tag="4")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="5")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="7")]
+    pub order: ::core::option::Option<OrderBy>,
+    /// Faceted{ tags: Vec<String>}
+    #[prost(message, optional, tag="8")]
+    pub faceted: ::core::option::Option<Faceted>,
+    #[prost(int32, tag="10")]
+    pub page_number: i32,
+    #[prost(int32, tag="11")]
+    pub result_per_page: i32,
+    #[prost(message, optional, tag="12")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    #[prost(bool, tag="13")]
+    pub reload: bool,
+    #[prost(bool, tag="14")]
+    pub with_duplicates: bool,
+    #[prost(bool, tag="15")]
+    pub only_faceted: bool,
+    #[prost(string, optional, tag="16")]
+    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ResultScore {
+    #[prost(float, tag="1")]
+    pub bm25: f32,
+    /// In the case of two equal bm25 scores, booster 
+    /// decides
+    #[prost(float, tag="2")]
+    pub booster: f32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentResult {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub score: ::core::option::Option<ResultScore>,
+    #[prost(string, tag="4")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="5")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentSearchResponse {
+    #[prost(int32, tag="1")]
+    pub total: i32,
+    #[prost(message, repeated, tag="2")]
+    pub results: ::prost::alloc::vec::Vec<DocumentResult>,
+    #[prost(map="string, message", tag="3")]
+    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+    /// The text that lead to this results
+    #[prost(string, tag="6")]
+    pub query: ::prost::alloc::string::String,
+    /// Is there a next page
+    #[prost(bool, tag="7")]
+    pub next_page: bool,
+    #[prost(bool, tag="8")]
+    pub bm25: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphResult {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="3")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(uint64, tag="4")]
+    pub start: u64,
+    #[prost(uint64, tag="5")]
+    pub end: u64,
+    #[prost(string, tag="6")]
+    pub paragraph: ::prost::alloc::string::String,
+    #[prost(string, tag="7")]
+    pub split: ::prost::alloc::string::String,
+    #[prost(uint64, tag="8")]
+    pub index: u64,
+    #[prost(message, optional, tag="9")]
+    pub score: ::core::option::Option<ResultScore>,
+    #[prost(string, repeated, tag="10")]
+    pub matches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// Metadata that can't be searched with but is returned on search results
+    #[prost(message, optional, tag="11")]
+    pub metadata: ::core::option::Option<super::noderesources::ParagraphMetadata>,
+    #[prost(string, repeated, tag="12")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphSearchResponse {
+    #[prost(int32, tag="10")]
+    pub fuzzy_distance: i32,
+    #[prost(int32, tag="1")]
+    pub total: i32,
+    /// 
+    #[prost(message, repeated, tag="2")]
+    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
+    /// For each field what facets are.
+    #[prost(map="string, message", tag="3")]
+    pub facets: ::std::collections::HashMap<::prost::alloc::string::String, FacetResults>,
+    /// What page is the answer.
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    /// How many results are in this page.
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+    /// The text that lead to this results
+    #[prost(string, tag="6")]
+    pub query: ::prost::alloc::string::String,
+    /// Is there a next page
+    #[prost(bool, tag="7")]
+    pub next_page: bool,
+    #[prost(bool, tag="8")]
+    pub bm25: bool,
+    #[prost(string, repeated, tag="9")]
+    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSearchRequest {
+    ///Shard ID
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    /// ID for the vector set.
+    /// Empty for searching on the original index
+    #[prost(string, tag="15")]
+    pub vector_set: ::prost::alloc::string::String,
+    /// Embedded vector search.
+    #[prost(float, repeated, tag="2")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    /// tags to filter
+    #[prost(string, repeated, tag="3")]
+    pub tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// What page is the answer.
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    /// How many results are in this page.
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+    #[prost(bool, tag="14")]
+    pub with_duplicates: bool,
+    #[prost(bool, tag="13")]
+    pub reload: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentVectorIdentifier {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentScored {
+    #[prost(message, optional, tag="1")]
+    pub doc_id: ::core::option::Option<DocumentVectorIdentifier>,
+    #[prost(float, tag="2")]
+    pub score: f32,
+    #[prost(message, optional, tag="3")]
+    pub metadata: ::core::option::Option<super::noderesources::SentenceMetadata>,
+    #[prost(string, repeated, tag="4")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorSearchResponse {
+    /// List of docs closer to the asked one.
+    #[prost(message, repeated, tag="1")]
+    pub documents: ::prost::alloc::vec::Vec<DocumentScored>,
+    /// What page is the answer.
+    #[prost(int32, tag="4")]
+    pub page_number: i32,
+    /// How many results are in this page.
+    #[prost(int32, tag="5")]
+    pub result_per_page: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationNodeFilter {
+    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
+    pub node_type: i32,
+    #[prost(string, optional, tag="2")]
+    pub node_subtype: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationEdgeFilter {
+    /// Will filter the search to edges of type ntype.
+    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
+    pub relation_type: i32,
+    #[prost(string, optional, tag="2")]
+    pub relation_subtype: ::core::option::Option<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationPrefixSearchRequest {
+    #[prost(string, tag="1")]
+    pub prefix: ::prost::alloc::string::String,
+    #[prost(message, repeated, tag="2")]
+    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationPrefixSearchResponse {
+    #[prost(message, repeated, tag="1")]
+    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EntitiesSubgraphRequest {
+    /// List of vertices where search will trigger
+    #[prost(message, repeated, tag="1")]
+    pub entry_points: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
+    /// Filters to apply while searching. It's an OR filtering: any
+    /// node (vertex) satisfying one condition will be returned
+    #[prost(message, repeated, tag="2")]
+    pub node_filters: ::prost::alloc::vec::Vec<RelationNodeFilter>,
+    /// Filters to apply while searching. It's an OR filtering: any
+    /// edge satisfying one condition will be returned
+    #[prost(message, repeated, tag="4")]
+    pub edge_filters: ::prost::alloc::vec::Vec<RelationEdgeFilter>,
+    #[prost(int32, optional, tag="3")]
+    pub depth: ::core::option::Option<i32>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EntitiesSubgraphResponse {
+    #[prost(message, repeated, tag="1")]
+    pub relations: ::prost::alloc::vec::Vec<super::utils::Relation>,
+}
+// TODO: uncomment and implement (next iteration)
+// message RelationPathsSearchRequest {
+//     message PathEndpoints {
+//         utils.RelationNode origin = 1;
+//         utils.RelationNode destination = 2;
+//     }
+//     repeated PathEndpoints paths = 1;
+// }
+
+/// Query relation index to obtain different information about the
+/// knowledge graph. It can be queried using the following strategies:
+///
+/// - prefix search over vertex (node) names
+/// - graph search:
+///   - given some entry vertices, get the filtered subgraph around them
+///   - (TODO) given some vertices, get paths between them
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationSearchRequest {
+    #[prost(string, tag="1")]
+    pub shard_id: ::prost::alloc::string::String,
+    #[prost(bool, tag="5")]
+    pub reload: bool,
+    #[prost(message, optional, tag="11")]
+    pub prefix: ::core::option::Option<RelationPrefixSearchRequest>,
+    /// TODO: uncomment and implement (next iteration)
+    /// RelationPathsSearchRequest paths = 13;
+    #[prost(message, optional, tag="12")]
+    pub subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationSearchResponse {
+    #[prost(message, optional, tag="11")]
+    pub prefix: ::core::option::Option<RelationPrefixSearchResponse>,
+    /// TODO: uncomment and implement (next iteration)
+    /// repeated utils.RelationPath paths = 13;
+    #[prost(message, optional, tag="12")]
+    pub subgraph: ::core::option::Option<EntitiesSubgraphResponse>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SearchRequest {
+    #[prost(string, tag="1")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="2")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// query this text in all the paragraphs
+    #[prost(string, tag="3")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="4")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="5")]
+    pub order: ::core::option::Option<OrderBy>,
+    /// Faceted{ tags: Vec<String>}
+    #[prost(message, optional, tag="6")]
+    pub faceted: ::core::option::Option<Faceted>,
+    #[prost(int32, tag="7")]
+    pub page_number: i32,
+    #[prost(int32, tag="8")]
+    pub result_per_page: i32,
+    #[prost(message, optional, tag="9")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    /// Embedded vector search.
+    #[prost(float, repeated, tag="10")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    #[prost(string, tag="15")]
+    pub vectorset: ::prost::alloc::string::String,
+    #[prost(bool, tag="11")]
+    pub reload: bool,
+    #[prost(bool, tag="12")]
+    pub paragraph: bool,
+    #[prost(bool, tag="13")]
+    pub document: bool,
+    #[prost(bool, tag="14")]
+    pub with_duplicates: bool,
+    #[prost(bool, tag="16")]
+    pub only_faceted: bool,
+    #[prost(string, optional, tag="18")]
+    pub advanced_query: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(enumeration="super::noderesources::resource::ResourceStatus", optional, tag="17")]
+    pub with_status: ::core::option::Option<i32>,
+    /// if provided, search metadata for this nodes (nodes at distance
+    /// one) and get the shortest path between nodes
+    #[deprecated]
+    #[prost(message, optional, tag="19")]
+    pub relations: ::core::option::Option<RelationSearchRequest>,
+    #[prost(message, optional, tag="20")]
+    pub relation_prefix: ::core::option::Option<RelationPrefixSearchRequest>,
+    #[prost(message, optional, tag="21")]
+    pub relation_subgraph: ::core::option::Option<EntitiesSubgraphRequest>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SuggestRequest {
+    #[prost(string, tag="1")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub body: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="3")]
+    pub filter: ::core::option::Option<Filter>,
+    #[prost(message, optional, tag="4")]
+    pub timestamps: ::core::option::Option<Timestamps>,
+    #[prost(string, repeated, tag="5")]
+    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelatedEntities {
+    #[prost(string, repeated, tag="1")]
+    pub entities: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(uint32, tag="2")]
+    pub total: u32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SuggestResponse {
+    #[prost(int32, tag="1")]
+    pub total: i32,
+    #[prost(message, repeated, tag="2")]
+    pub results: ::prost::alloc::vec::Vec<ParagraphResult>,
+    /// The text that lead to this results
+    #[prost(string, tag="3")]
+    pub query: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="4")]
+    pub ematches: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    /// Entities related with the query
+    #[prost(message, optional, tag="5")]
+    pub entities: ::core::option::Option<RelatedEntities>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SearchResponse {
+    #[prost(message, optional, tag="1")]
+    pub document: ::core::option::Option<DocumentSearchResponse>,
+    #[prost(message, optional, tag="2")]
+    pub paragraph: ::core::option::Option<ParagraphSearchResponse>,
+    #[prost(message, optional, tag="3")]
+    pub vector: ::core::option::Option<VectorSearchResponse>,
+    #[prost(message, optional, tag="4")]
+    pub relation: ::core::option::Option<RelationSearchResponse>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IdCollection {
+    #[prost(string, repeated, tag="1")]
+    pub ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationEdge {
+    #[prost(enumeration="super::utils::relation::RelationType", tag="1")]
+    pub edge_type: i32,
+    #[prost(string, tag="2")]
+    pub property: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct EdgeList {
+    #[prost(message, repeated, tag="1")]
+    pub list: ::prost::alloc::vec::Vec<RelationEdge>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationTypeListMember {
+    #[prost(enumeration="super::utils::relation_node::NodeType", tag="1")]
+    pub with_type: i32,
+    #[prost(string, tag="2")]
+    pub with_subtype: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct TypeList {
+    #[prost(message, repeated, tag="1")]
+    pub list: ::prost::alloc::vec::Vec<RelationTypeListMember>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct GetShardRequest {
+    #[prost(message, optional, tag="1")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(string, tag="2")]
+    pub vectorset: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ParagraphItem {
+    #[prost(string, tag="1")]
+    pub id: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="2")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DocumentItem {
+    #[prost(string, tag="1")]
+    pub uuid: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub field: ::prost::alloc::string::String,
+    #[prost(string, repeated, tag="3")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct StreamRequest {
+    #[deprecated]
+    #[prost(message, optional, tag="1")]
+    pub filter_deprecated: ::core::option::Option<Filter>,
+    #[prost(bool, tag="2")]
+    pub reload: bool,
+    #[prost(message, optional, tag="3")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(message, optional, tag="4")]
+    pub filter: ::core::option::Option<StreamFilter>,
+}
+/// Generated client implementations.
+pub mod node_reader_client {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    #[derive(Debug, Clone)]
+    pub struct NodeReaderClient<T> {
+        inner: tonic::client::Grpc<T>,
+    }
+    impl NodeReaderClient<tonic::transport::Channel> {
+        /// Attempt to create a new client by connecting to a given endpoint.
+        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
+        where
+            D: std::convert::TryInto<tonic::transport::Endpoint>,
+            D::Error: Into<StdError>,
+        {
+            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
+            Ok(Self::new(conn))
+        }
+    }
+    impl<T> NodeReaderClient<T>
+    where
+        T: tonic::client::GrpcService<tonic::body::BoxBody>,
+        T::Error: Into<StdError>,
+        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
+        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
+    {
+        pub fn new(inner: T) -> Self {
+            let inner = tonic::client::Grpc::new(inner);
+            Self { inner }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> NodeReaderClient<InterceptedService<T, F>>
+        where
+            F: tonic::service::Interceptor,
+            T::ResponseBody: Default,
+            T: tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+                Response = http::Response<
+                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
+                >,
+            >,
+            <T as tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+            >>::Error: Into<StdError> + Send + Sync,
+        {
+            NodeReaderClient::new(InterceptedService::new(inner, interceptor))
+        }
+        /// Compress requests with `gzip`.
+        ///
+        /// This requires the server to support it otherwise it might respond with an
+        /// error.
+        #[must_use]
+        pub fn send_gzip(mut self) -> Self {
+            self.inner = self.inner.send_gzip();
+            self
+        }
+        /// Enable decompressing responses with `gzip`.
+        #[must_use]
+        pub fn accept_gzip(mut self) -> Self {
+            self.inner = self.inner.accept_gzip();
+            self
+        }
+        pub async fn get_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::GetShardRequest>,
+        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/GetShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn get_shards(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardList>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/GetShards",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn document_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::DocumentSearchRequest>,
+        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/DocumentSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn paragraph_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::ParagraphSearchRequest>,
+        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/ParagraphSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn vector_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::VectorSearchRequest>,
+        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/VectorSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::RelationSearchRequest>,
+        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationSearch",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn document_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/DocumentIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn paragraph_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/ParagraphIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn vector_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/VectorIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_ids(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationIds",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_edges(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationEdges",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn relation_types(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::TypeList>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/RelationTypes",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn search(
+            &mut self,
+            request: impl tonic::IntoRequest<super::SearchRequest>,
+        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Search",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn suggest(
+            &mut self,
+            request: impl tonic::IntoRequest<super::SuggestRequest>,
+        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Suggest",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        /// Streams
+        pub async fn paragraphs(
+            &mut self,
+            request: impl tonic::IntoRequest<super::StreamRequest>,
+        ) -> Result<
+            tonic::Response<tonic::codec::Streaming<super::ParagraphItem>>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Paragraphs",
+            );
+            self.inner.server_streaming(request.into_request(), path, codec).await
+        }
+        pub async fn documents(
+            &mut self,
+            request: impl tonic::IntoRequest<super::StreamRequest>,
+        ) -> Result<
+            tonic::Response<tonic::codec::Streaming<super::DocumentItem>>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodereader.NodeReader/Documents",
+            );
+            self.inner.server_streaming(request.into_request(), path, codec).await
+        }
+    }
+}
+/// Generated server implementations.
+pub mod node_reader_server {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    ///Generated trait containing gRPC methods that should be implemented for use with NodeReaderServer.
+    #[async_trait]
+    pub trait NodeReader: Send + Sync + 'static {
+        async fn get_shard(
+            &self,
+            request: tonic::Request<super::GetShardRequest>,
+        ) -> Result<tonic::Response<super::super::noderesources::Shard>, tonic::Status>;
+        async fn get_shards(
+            &self,
+            request: tonic::Request<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardList>,
+            tonic::Status,
+        >;
+        async fn document_search(
+            &self,
+            request: tonic::Request<super::DocumentSearchRequest>,
+        ) -> Result<tonic::Response<super::DocumentSearchResponse>, tonic::Status>;
+        async fn paragraph_search(
+            &self,
+            request: tonic::Request<super::ParagraphSearchRequest>,
+        ) -> Result<tonic::Response<super::ParagraphSearchResponse>, tonic::Status>;
+        async fn vector_search(
+            &self,
+            request: tonic::Request<super::VectorSearchRequest>,
+        ) -> Result<tonic::Response<super::VectorSearchResponse>, tonic::Status>;
+        async fn relation_search(
+            &self,
+            request: tonic::Request<super::RelationSearchRequest>,
+        ) -> Result<tonic::Response<super::RelationSearchResponse>, tonic::Status>;
+        async fn document_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn paragraph_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn vector_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn relation_ids(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::IdCollection>, tonic::Status>;
+        async fn relation_edges(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::EdgeList>, tonic::Status>;
+        async fn relation_types(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<tonic::Response<super::TypeList>, tonic::Status>;
+        async fn search(
+            &self,
+            request: tonic::Request<super::SearchRequest>,
+        ) -> Result<tonic::Response<super::SearchResponse>, tonic::Status>;
+        async fn suggest(
+            &self,
+            request: tonic::Request<super::SuggestRequest>,
+        ) -> Result<tonic::Response<super::SuggestResponse>, tonic::Status>;
+        ///Server streaming response type for the Paragraphs method.
+        type ParagraphsStream: futures_core::Stream<
+                Item = Result<super::ParagraphItem, tonic::Status>,
+            >
+            + Send
+            + 'static;
+        /// Streams
+        async fn paragraphs(
+            &self,
+            request: tonic::Request<super::StreamRequest>,
+        ) -> Result<tonic::Response<Self::ParagraphsStream>, tonic::Status>;
+        ///Server streaming response type for the Documents method.
+        type DocumentsStream: futures_core::Stream<
+                Item = Result<super::DocumentItem, tonic::Status>,
+            >
+            + Send
+            + 'static;
+        async fn documents(
+            &self,
+            request: tonic::Request<super::StreamRequest>,
+        ) -> Result<tonic::Response<Self::DocumentsStream>, tonic::Status>;
+    }
+    #[derive(Debug)]
+    pub struct NodeReaderServer<T: NodeReader> {
+        inner: _Inner<T>,
+        accept_compression_encodings: (),
+        send_compression_encodings: (),
+    }
+    struct _Inner<T>(Arc<T>);
+    impl<T: NodeReader> NodeReaderServer<T> {
+        pub fn new(inner: T) -> Self {
+            Self::from_arc(Arc::new(inner))
+        }
+        pub fn from_arc(inner: Arc<T>) -> Self {
+            let inner = _Inner(inner);
+            Self {
+                inner,
+                accept_compression_encodings: Default::default(),
+                send_compression_encodings: Default::default(),
+            }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> InterceptedService<Self, F>
+        where
+            F: tonic::service::Interceptor,
+        {
+            InterceptedService::new(Self::new(inner), interceptor)
+        }
+    }
+    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeReaderServer<T>
+    where
+        T: NodeReader,
+        B: Body + Send + 'static,
+        B::Error: Into<StdError> + Send + 'static,
+    {
+        type Response = http::Response<tonic::body::BoxBody>;
+        type Error = std::convert::Infallible;
+        type Future = BoxFuture<Self::Response, Self::Error>;
+        fn poll_ready(
+            &mut self,
+            _cx: &mut Context<'_>,
+        ) -> Poll<Result<(), Self::Error>> {
+            Poll::Ready(Ok(()))
+        }
+        fn call(&mut self, req: http::Request<B>) -> Self::Future {
+            let inner = self.inner.clone();
+            match req.uri().path() {
+                "/nodereader.NodeReader/GetShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct GetShardSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::GetShardRequest>
+                    for GetShardSvc<T> {
+                        type Response = super::super::noderesources::Shard;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::GetShardRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).get_shard(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GetShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/GetShards" => {
+                    #[allow(non_camel_case_types)]
+                    struct GetShardsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::EmptyQuery,
+                    > for GetShardsSvc<T> {
+                        type Response = super::super::noderesources::ShardList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::EmptyQuery,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).get_shards(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GetShardsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/DocumentSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct DocumentSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::DocumentSearchRequest>
+                    for DocumentSearchSvc<T> {
+                        type Response = super::DocumentSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::DocumentSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).document_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DocumentSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/ParagraphSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct ParagraphSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::ParagraphSearchRequest>
+                    for ParagraphSearchSvc<T> {
+                        type Response = super::ParagraphSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::ParagraphSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).paragraph_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ParagraphSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/VectorSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct VectorSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::VectorSearchRequest>
+                    for VectorSearchSvc<T> {
+                        type Response = super::VectorSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::VectorSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).vector_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = VectorSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationSearch" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationSearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::RelationSearchRequest>
+                    for RelationSearchSvc<T> {
+                        type Response = super::RelationSearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::RelationSearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_search(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationSearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/DocumentIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct DocumentIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for DocumentIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).document_ids(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DocumentIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/ParagraphIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct ParagraphIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for ParagraphIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).paragraph_ids(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ParagraphIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/VectorIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct VectorIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for VectorIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).vector_ids(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = VectorIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationIds" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationIdsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for RelationIdsSvc<T> {
+                        type Response = super::IdCollection;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_ids(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationIdsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationEdges" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationEdgesSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for RelationEdgesSvc<T> {
+                        type Response = super::EdgeList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_edges(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationEdgesSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/RelationTypes" => {
+                    #[allow(non_camel_case_types)]
+                    struct RelationTypesSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for RelationTypesSvc<T> {
+                        type Response = super::TypeList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).relation_types(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RelationTypesSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Search" => {
+                    #[allow(non_camel_case_types)]
+                    struct SearchSvc<T: NodeReader>(pub Arc<T>);
+                    impl<T: NodeReader> tonic::server::UnaryService<super::SearchRequest>
+                    for SearchSvc<T> {
+                        type Response = super::SearchResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::SearchRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).search(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = SearchSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Suggest" => {
+                    #[allow(non_camel_case_types)]
+                    struct SuggestSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::UnaryService<super::SuggestRequest>
+                    for SuggestSvc<T> {
+                        type Response = super::SuggestResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::SuggestRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).suggest(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = SuggestSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Paragraphs" => {
+                    #[allow(non_camel_case_types)]
+                    struct ParagraphsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::ServerStreamingService<super::StreamRequest>
+                    for ParagraphsSvc<T> {
+                        type Response = super::ParagraphItem;
+                        type ResponseStream = T::ParagraphsStream;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::ResponseStream>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::StreamRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).paragraphs(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ParagraphsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.server_streaming(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodereader.NodeReader/Documents" => {
+                    #[allow(non_camel_case_types)]
+                    struct DocumentsSvc<T: NodeReader>(pub Arc<T>);
+                    impl<
+                        T: NodeReader,
+                    > tonic::server::ServerStreamingService<super::StreamRequest>
+                    for DocumentsSvc<T> {
+                        type Response = super::DocumentItem;
+                        type ResponseStream = T::DocumentsStream;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::ResponseStream>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::StreamRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).documents(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DocumentsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.server_streaming(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                _ => {
+                    Box::pin(async move {
+                        Ok(
+                            http::Response::builder()
+                                .status(200)
+                                .header("grpc-status", "12")
+                                .header("content-type", "application/grpc")
+                                .body(empty_body())
+                                .unwrap(),
+                        )
+                    })
+                }
+            }
+        }
+    }
+    impl<T: NodeReader> Clone for NodeReaderServer<T> {
+        fn clone(&self) -> Self {
+            let inner = self.inner.clone();
+            Self {
+                inner,
+                accept_compression_encodings: self.accept_compression_encodings,
+                send_compression_encodings: self.send_compression_encodings,
+            }
+        }
+    }
+    impl<T: NodeReader> Clone for _Inner<T> {
+        fn clone(&self) -> Self {
+            Self(self.0.clone())
+        }
+    }
+    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
+        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+            write!(f, "{:?}", self.0)
+        }
+    }
+    impl<T: NodeReader> tonic::transport::NamedService for NodeReaderServer<T> {
+        const NAME: &'static str = "nodereader.NodeReader";
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/nodewriter.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/nodewriter.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,1321 +1,1338 @@
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct OpStatus {
-    #[prost(enumeration="op_status::Status", tag="1")]
-    pub status: i32,
-    #[prost(string, tag="2")]
-    pub detail: ::prost::alloc::string::String,
-    #[prost(uint64, tag="3")]
-    pub count: u64,
-    #[prost(uint64, tag="5")]
-    pub count_paragraphs: u64,
-    #[prost(uint64, tag="6")]
-    pub count_sentences: u64,
-    #[prost(string, tag="4")]
-    pub shard_id: ::prost::alloc::string::String,
-}
-/// Nested message and enum types in `OpStatus`.
-pub mod op_status {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum Status {
-        Ok = 0,
-        Warning = 1,
-        Error = 2,
-    }
-}
-// Implemented at nucliadb_object_storage
-
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct IndexMessage {
-    #[prost(string, tag="1")]
-    pub node: ::prost::alloc::string::String,
-    #[prost(string, tag="2")]
-    pub shard: ::prost::alloc::string::String,
-    #[prost(uint64, tag="3")]
-    pub txid: u64,
-    #[prost(string, tag="4")]
-    pub resource: ::prost::alloc::string::String,
-    #[prost(enumeration="index_message::TypeMessage", tag="5")]
-    pub typemessage: i32,
-    #[prost(string, tag="6")]
-    pub reindex_id: ::prost::alloc::string::String,
-}
-/// Nested message and enum types in `IndexMessage`.
-pub mod index_message {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum TypeMessage {
-        Creation = 0,
-        Deletion = 1,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct SetGraph {
-    #[prost(message, optional, tag="1")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(message, optional, tag="2")]
-    pub graph: ::core::option::Option<super::utils::JoinGraph>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct DeleteGraphNodes {
-    #[prost(message, optional, tag="2")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(message, repeated, tag="1")]
-    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct MoveShardRequest {
-    #[prost(message, optional, tag="1")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(string, tag="2")]
-    pub address: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct AcceptShardRequest {
-    #[prost(message, optional, tag="1")]
-    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
-    #[prost(uint32, tag="2")]
-    pub port: u32,
-    #[prost(bool, tag="3")]
-    pub override_shard: bool,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NewShardRequest {
-    #[prost(enumeration="super::utils::VectorSimilarity", tag="1")]
-    pub similarity: i32,
-    #[prost(string, tag="2")]
-    pub kbid: ::prost::alloc::string::String,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct NewVectorSetRequest {
-    #[prost(message, optional, tag="1")]
-    pub id: ::core::option::Option<super::noderesources::VectorSetId>,
-    #[prost(enumeration="super::utils::VectorSimilarity", tag="2")]
-    pub similarity: i32,
-}
-/// Generated client implementations.
-pub mod node_writer_client {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    #[derive(Debug, Clone)]
-    pub struct NodeWriterClient<T> {
-        inner: tonic::client::Grpc<T>,
-    }
-    impl NodeWriterClient<tonic::transport::Channel> {
-        /// Attempt to create a new client by connecting to a given endpoint.
-        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
-        where
-            D: std::convert::TryInto<tonic::transport::Endpoint>,
-            D::Error: Into<StdError>,
-        {
-            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
-            Ok(Self::new(conn))
-        }
-    }
-    impl<T> NodeWriterClient<T>
-    where
-        T: tonic::client::GrpcService<tonic::body::BoxBody>,
-        T::Error: Into<StdError>,
-        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
-        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
-    {
-        pub fn new(inner: T) -> Self {
-            let inner = tonic::client::Grpc::new(inner);
-            Self { inner }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> NodeWriterClient<InterceptedService<T, F>>
-        where
-            F: tonic::service::Interceptor,
-            T::ResponseBody: Default,
-            T: tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-                Response = http::Response<
-                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
-                >,
-            >,
-            <T as tonic::codegen::Service<
-                http::Request<tonic::body::BoxBody>,
-            >>::Error: Into<StdError> + Send + Sync,
-        {
-            NodeWriterClient::new(InterceptedService::new(inner, interceptor))
-        }
-        /// Compress requests with `gzip`.
-        ///
-        /// This requires the server to support it otherwise it might respond with an
-        /// error.
-        #[must_use]
-        pub fn send_gzip(mut self) -> Self {
-            self.inner = self.inner.send_gzip();
-            self
-        }
-        /// Enable decompressing responses with `gzip`.
-        #[must_use]
-        pub fn accept_gzip(mut self) -> Self {
-            self.inner = self.inner.accept_gzip();
-            self
-        }
-        pub async fn get_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardId>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/GetShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn new_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::NewShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCreated>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/NewShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn clean_and_upgrade_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCleaned>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/CleanAndUpgradeShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn delete_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardId>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/DeleteShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn list_shards(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardIds>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/ListShards",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn gc(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static("/nodewriter.NodeWriter/GC");
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn set_resource(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::Resource>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/SetResource",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn delete_relation_nodes(
-            &mut self,
-            request: impl tonic::IntoRequest<super::DeleteGraphNodes>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/DeleteRelationNodes",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn join_graph(
-            &mut self,
-            request: impl tonic::IntoRequest<super::SetGraph>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/JoinGraph",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn remove_resource(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ResourceId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/RemoveResource",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn add_vector_set(
-            &mut self,
-            request: impl tonic::IntoRequest<super::NewVectorSetRequest>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/AddVectorSet",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn remove_vector_set(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::VectorSetId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/RemoveVectorSet",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn list_vector_sets(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::VectorSetList>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/ListVectorSets",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn move_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::MoveShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/MoveShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn accept_shard(
-            &mut self,
-            request: impl tonic::IntoRequest<super::AcceptShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/AcceptShard",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-        pub async fn get_metadata(
-            &mut self,
-            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::NodeMetadata>,
-            tonic::Status,
-        > {
-            self.inner
-                .ready()
-                .await
-                .map_err(|e| {
-                    tonic::Status::new(
-                        tonic::Code::Unknown,
-                        format!("Service was not ready: {}", e.into()),
-                    )
-                })?;
-            let codec = tonic::codec::ProstCodec::default();
-            let path = http::uri::PathAndQuery::from_static(
-                "/nodewriter.NodeWriter/GetMetadata",
-            );
-            self.inner.unary(request.into_request(), path, codec).await
-        }
-    }
-}
-/// Generated server implementations.
-pub mod node_writer_server {
-    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
-    use tonic::codegen::*;
-    ///Generated trait containing gRPC methods that should be implemented for use with NodeWriterServer.
-    #[async_trait]
-    pub trait NodeWriter: Send + Sync + 'static {
-        async fn get_shard(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardId>,
-            tonic::Status,
-        >;
-        async fn new_shard(
-            &self,
-            request: tonic::Request<super::NewShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCreated>,
-            tonic::Status,
-        >;
-        async fn clean_and_upgrade_shard(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardCleaned>,
-            tonic::Status,
-        >;
-        async fn delete_shard(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardId>,
-            tonic::Status,
-        >;
-        async fn list_shards(
-            &self,
-            request: tonic::Request<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::ShardIds>,
-            tonic::Status,
-        >;
-        async fn gc(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        >;
-        async fn set_resource(
-            &self,
-            request: tonic::Request<super::super::noderesources::Resource>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn delete_relation_nodes(
-            &self,
-            request: tonic::Request<super::DeleteGraphNodes>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn join_graph(
-            &self,
-            request: tonic::Request<super::SetGraph>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn remove_resource(
-            &self,
-            request: tonic::Request<super::super::noderesources::ResourceId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn add_vector_set(
-            &self,
-            request: tonic::Request<super::NewVectorSetRequest>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn remove_vector_set(
-            &self,
-            request: tonic::Request<super::super::noderesources::VectorSetId>,
-        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
-        async fn list_vector_sets(
-            &self,
-            request: tonic::Request<super::super::noderesources::ShardId>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::VectorSetList>,
-            tonic::Status,
-        >;
-        async fn move_shard(
-            &self,
-            request: tonic::Request<super::MoveShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        >;
-        async fn accept_shard(
-            &self,
-            request: tonic::Request<super::AcceptShardRequest>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::EmptyResponse>,
-            tonic::Status,
-        >;
-        async fn get_metadata(
-            &self,
-            request: tonic::Request<super::super::noderesources::EmptyQuery>,
-        ) -> Result<
-            tonic::Response<super::super::noderesources::NodeMetadata>,
-            tonic::Status,
-        >;
-    }
-    #[derive(Debug)]
-    pub struct NodeWriterServer<T: NodeWriter> {
-        inner: _Inner<T>,
-        accept_compression_encodings: (),
-        send_compression_encodings: (),
-    }
-    struct _Inner<T>(Arc<T>);
-    impl<T: NodeWriter> NodeWriterServer<T> {
-        pub fn new(inner: T) -> Self {
-            Self::from_arc(Arc::new(inner))
-        }
-        pub fn from_arc(inner: Arc<T>) -> Self {
-            let inner = _Inner(inner);
-            Self {
-                inner,
-                accept_compression_encodings: Default::default(),
-                send_compression_encodings: Default::default(),
-            }
-        }
-        pub fn with_interceptor<F>(
-            inner: T,
-            interceptor: F,
-        ) -> InterceptedService<Self, F>
-        where
-            F: tonic::service::Interceptor,
-        {
-            InterceptedService::new(Self::new(inner), interceptor)
-        }
-    }
-    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeWriterServer<T>
-    where
-        T: NodeWriter,
-        B: Body + Send + 'static,
-        B::Error: Into<StdError> + Send + 'static,
-    {
-        type Response = http::Response<tonic::body::BoxBody>;
-        type Error = std::convert::Infallible;
-        type Future = BoxFuture<Self::Response, Self::Error>;
-        fn poll_ready(
-            &mut self,
-            _cx: &mut Context<'_>,
-        ) -> Poll<Result<(), Self::Error>> {
-            Poll::Ready(Ok(()))
-        }
-        fn call(&mut self, req: http::Request<B>) -> Self::Future {
-            let inner = self.inner.clone();
-            match req.uri().path() {
-                "/nodewriter.NodeWriter/GetShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct GetShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for GetShardSvc<T> {
-                        type Response = super::super::noderesources::ShardId;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).get_shard(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GetShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/NewShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct NewShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::NewShardRequest>
-                    for NewShardSvc<T> {
-                        type Response = super::super::noderesources::ShardCreated;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::NewShardRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).new_shard(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = NewShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/CleanAndUpgradeShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct CleanAndUpgradeShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for CleanAndUpgradeShardSvc<T> {
-                        type Response = super::super::noderesources::ShardCleaned;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).clean_and_upgrade_shard(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = CleanAndUpgradeShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/DeleteShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct DeleteShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for DeleteShardSvc<T> {
-                        type Response = super::super::noderesources::ShardId;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).delete_shard(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DeleteShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/ListShards" => {
-                    #[allow(non_camel_case_types)]
-                    struct ListShardsSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::EmptyQuery,
-                    > for ListShardsSvc<T> {
-                        type Response = super::super::noderesources::ShardIds;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::EmptyQuery,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).list_shards(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ListShardsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/GC" => {
-                    #[allow(non_camel_case_types)]
-                    struct GCSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for GCSvc<T> {
-                        type Response = super::super::noderesources::EmptyResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).gc(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GCSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/SetResource" => {
-                    #[allow(non_camel_case_types)]
-                    struct SetResourceSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::Resource>
-                    for SetResourceSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::Resource,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).set_resource(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = SetResourceSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/DeleteRelationNodes" => {
-                    #[allow(non_camel_case_types)]
-                    struct DeleteRelationNodesSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::DeleteGraphNodes>
-                    for DeleteRelationNodesSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::DeleteGraphNodes>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).delete_relation_nodes(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = DeleteRelationNodesSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/JoinGraph" => {
-                    #[allow(non_camel_case_types)]
-                    struct JoinGraphSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<T: NodeWriter> tonic::server::UnaryService<super::SetGraph>
-                    for JoinGraphSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::SetGraph>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).join_graph(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = JoinGraphSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/RemoveResource" => {
-                    #[allow(non_camel_case_types)]
-                    struct RemoveResourceSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::ResourceId,
-                    > for RemoveResourceSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::ResourceId,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).remove_resource(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RemoveResourceSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/AddVectorSet" => {
-                    #[allow(non_camel_case_types)]
-                    struct AddVectorSetSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::NewVectorSetRequest>
-                    for AddVectorSetSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::NewVectorSetRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).add_vector_set(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = AddVectorSetSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/RemoveVectorSet" => {
-                    #[allow(non_camel_case_types)]
-                    struct RemoveVectorSetSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::VectorSetId,
-                    > for RemoveVectorSetSvc<T> {
-                        type Response = super::OpStatus;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::VectorSetId,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).remove_vector_set(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = RemoveVectorSetSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/ListVectorSets" => {
-                    #[allow(non_camel_case_types)]
-                    struct ListVectorSetsSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
-                    for ListVectorSetsSvc<T> {
-                        type Response = super::super::noderesources::VectorSetList;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::super::noderesources::ShardId>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).list_vector_sets(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = ListVectorSetsSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/MoveShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct MoveShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::MoveShardRequest>
-                    for MoveShardSvc<T> {
-                        type Response = super::super::noderesources::EmptyResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::MoveShardRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move { (*inner).move_shard(request).await };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = MoveShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/AcceptShard" => {
-                    #[allow(non_camel_case_types)]
-                    struct AcceptShardSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<super::AcceptShardRequest>
-                    for AcceptShardSvc<T> {
-                        type Response = super::super::noderesources::EmptyResponse;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<super::AcceptShardRequest>,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).accept_shard(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = AcceptShardSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                "/nodewriter.NodeWriter/GetMetadata" => {
-                    #[allow(non_camel_case_types)]
-                    struct GetMetadataSvc<T: NodeWriter>(pub Arc<T>);
-                    impl<
-                        T: NodeWriter,
-                    > tonic::server::UnaryService<
-                        super::super::noderesources::EmptyQuery,
-                    > for GetMetadataSvc<T> {
-                        type Response = super::super::noderesources::NodeMetadata;
-                        type Future = BoxFuture<
-                            tonic::Response<Self::Response>,
-                            tonic::Status,
-                        >;
-                        fn call(
-                            &mut self,
-                            request: tonic::Request<
-                                super::super::noderesources::EmptyQuery,
-                            >,
-                        ) -> Self::Future {
-                            let inner = self.0.clone();
-                            let fut = async move {
-                                (*inner).get_metadata(request).await
-                            };
-                            Box::pin(fut)
-                        }
-                    }
-                    let accept_compression_encodings = self.accept_compression_encodings;
-                    let send_compression_encodings = self.send_compression_encodings;
-                    let inner = self.inner.clone();
-                    let fut = async move {
-                        let inner = inner.0;
-                        let method = GetMetadataSvc(inner);
-                        let codec = tonic::codec::ProstCodec::default();
-                        let mut grpc = tonic::server::Grpc::new(codec)
-                            .apply_compression_config(
-                                accept_compression_encodings,
-                                send_compression_encodings,
-                            );
-                        let res = grpc.unary(method, req).await;
-                        Ok(res)
-                    };
-                    Box::pin(fut)
-                }
-                _ => {
-                    Box::pin(async move {
-                        Ok(
-                            http::Response::builder()
-                                .status(200)
-                                .header("grpc-status", "12")
-                                .header("content-type", "application/grpc")
-                                .body(empty_body())
-                                .unwrap(),
-                        )
-                    })
-                }
-            }
-        }
-    }
-    impl<T: NodeWriter> Clone for NodeWriterServer<T> {
-        fn clone(&self) -> Self {
-            let inner = self.inner.clone();
-            Self {
-                inner,
-                accept_compression_encodings: self.accept_compression_encodings,
-                send_compression_encodings: self.send_compression_encodings,
-            }
-        }
-    }
-    impl<T: NodeWriter> Clone for _Inner<T> {
-        fn clone(&self) -> Self {
-            Self(self.0.clone())
-        }
-    }
-    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
-        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-            write!(f, "{:?}", self.0)
-        }
-    }
-    impl<T: NodeWriter> tonic::transport::NamedService for NodeWriterServer<T> {
-        const NAME: &'static str = "nodewriter.NodeWriter";
-    }
-}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct OpStatus {
+    #[prost(enumeration="op_status::Status", tag="1")]
+    pub status: i32,
+    #[prost(string, tag="2")]
+    pub detail: ::prost::alloc::string::String,
+    #[prost(uint64, tag="3")]
+    pub count: u64,
+    #[prost(uint64, tag="5")]
+    pub count_paragraphs: u64,
+    #[prost(uint64, tag="6")]
+    pub count_sentences: u64,
+    #[prost(string, tag="4")]
+    pub shard_id: ::prost::alloc::string::String,
+}
+/// Nested message and enum types in `OpStatus`.
+pub mod op_status {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum Status {
+        Ok = 0,
+        Warning = 1,
+        Error = 2,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexMessage {
+    #[prost(string, tag="1")]
+    pub node: ::prost::alloc::string::String,
+    /// physical shard message is for
+    #[prost(string, tag="2")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(uint64, tag="3")]
+    pub txid: u64,
+    #[prost(string, tag="4")]
+    pub resource: ::prost::alloc::string::String,
+    #[prost(enumeration="TypeMessage", tag="5")]
+    pub typemessage: i32,
+    #[prost(string, tag="6")]
+    pub reindex_id: ::prost::alloc::string::String,
+    #[prost(string, optional, tag="7")]
+    pub partition: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(string, tag="8")]
+    pub storage_key: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct IndexedMessage {
+    #[prost(string, tag="1")]
+    pub node: ::prost::alloc::string::String,
+    #[prost(string, tag="2")]
+    pub shard: ::prost::alloc::string::String,
+    #[prost(uint64, tag="3")]
+    pub txid: u64,
+    #[prost(string, tag="4")]
+    pub resource: ::prost::alloc::string::String,
+    #[prost(enumeration="TypeMessage", tag="5")]
+    pub typemessage: i32,
+    #[prost(string, tag="6")]
+    pub reindex_id: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct SetGraph {
+    #[prost(message, optional, tag="1")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(message, optional, tag="2")]
+    pub graph: ::core::option::Option<super::utils::JoinGraph>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct DeleteGraphNodes {
+    #[prost(message, optional, tag="2")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(message, repeated, tag="1")]
+    pub nodes: ::prost::alloc::vec::Vec<super::utils::RelationNode>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct MoveShardRequest {
+    #[prost(message, optional, tag="1")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(string, tag="2")]
+    pub address: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct AcceptShardRequest {
+    #[prost(message, optional, tag="1")]
+    pub shard_id: ::core::option::Option<super::noderesources::ShardId>,
+    #[prost(uint32, tag="2")]
+    pub port: u32,
+    #[prost(bool, tag="3")]
+    pub override_shard: bool,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewShardRequest {
+    #[prost(enumeration="super::utils::VectorSimilarity", tag="1")]
+    pub similarity: i32,
+    #[prost(string, tag="2")]
+    pub kbid: ::prost::alloc::string::String,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct NewVectorSetRequest {
+    #[prost(message, optional, tag="1")]
+    pub id: ::core::option::Option<super::noderesources::VectorSetId>,
+    #[prost(enumeration="super::utils::VectorSimilarity", tag="2")]
+    pub similarity: i32,
+}
+// Implemented at nucliadb_object_storage
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+#[repr(i32)]
+pub enum TypeMessage {
+    Creation = 0,
+    Deletion = 1,
+}
+/// Generated client implementations.
+pub mod node_writer_client {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    #[derive(Debug, Clone)]
+    pub struct NodeWriterClient<T> {
+        inner: tonic::client::Grpc<T>,
+    }
+    impl NodeWriterClient<tonic::transport::Channel> {
+        /// Attempt to create a new client by connecting to a given endpoint.
+        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
+        where
+            D: std::convert::TryInto<tonic::transport::Endpoint>,
+            D::Error: Into<StdError>,
+        {
+            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
+            Ok(Self::new(conn))
+        }
+    }
+    impl<T> NodeWriterClient<T>
+    where
+        T: tonic::client::GrpcService<tonic::body::BoxBody>,
+        T::Error: Into<StdError>,
+        T::ResponseBody: Body<Data = Bytes> + Send + 'static,
+        <T::ResponseBody as Body>::Error: Into<StdError> + Send,
+    {
+        pub fn new(inner: T) -> Self {
+            let inner = tonic::client::Grpc::new(inner);
+            Self { inner }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> NodeWriterClient<InterceptedService<T, F>>
+        where
+            F: tonic::service::Interceptor,
+            T::ResponseBody: Default,
+            T: tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+                Response = http::Response<
+                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
+                >,
+            >,
+            <T as tonic::codegen::Service<
+                http::Request<tonic::body::BoxBody>,
+            >>::Error: Into<StdError> + Send + Sync,
+        {
+            NodeWriterClient::new(InterceptedService::new(inner, interceptor))
+        }
+        /// Compress requests with `gzip`.
+        ///
+        /// This requires the server to support it otherwise it might respond with an
+        /// error.
+        #[must_use]
+        pub fn send_gzip(mut self) -> Self {
+            self.inner = self.inner.send_gzip();
+            self
+        }
+        /// Enable decompressing responses with `gzip`.
+        #[must_use]
+        pub fn accept_gzip(mut self) -> Self {
+            self.inner = self.inner.accept_gzip();
+            self
+        }
+        pub async fn get_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardId>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/GetShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn new_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::NewShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCreated>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/NewShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn clean_and_upgrade_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCleaned>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/CleanAndUpgradeShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn delete_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardId>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/DeleteShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn list_shards(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardIds>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/ListShards",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn gc(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static("/nodewriter.NodeWriter/GC");
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn set_resource(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::Resource>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/SetResource",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn delete_relation_nodes(
+            &mut self,
+            request: impl tonic::IntoRequest<super::DeleteGraphNodes>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/DeleteRelationNodes",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn join_graph(
+            &mut self,
+            request: impl tonic::IntoRequest<super::SetGraph>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/JoinGraph",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn remove_resource(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ResourceId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/RemoveResource",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn add_vector_set(
+            &mut self,
+            request: impl tonic::IntoRequest<super::NewVectorSetRequest>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/AddVectorSet",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn remove_vector_set(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::VectorSetId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status> {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/RemoveVectorSet",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn list_vector_sets(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::VectorSetList>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/ListVectorSets",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn move_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::MoveShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/MoveShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn accept_shard(
+            &mut self,
+            request: impl tonic::IntoRequest<super::AcceptShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/AcceptShard",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+        pub async fn get_metadata(
+            &mut self,
+            request: impl tonic::IntoRequest<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::NodeMetadata>,
+            tonic::Status,
+        > {
+            self.inner
+                .ready()
+                .await
+                .map_err(|e| {
+                    tonic::Status::new(
+                        tonic::Code::Unknown,
+                        format!("Service was not ready: {}", e.into()),
+                    )
+                })?;
+            let codec = tonic::codec::ProstCodec::default();
+            let path = http::uri::PathAndQuery::from_static(
+                "/nodewriter.NodeWriter/GetMetadata",
+            );
+            self.inner.unary(request.into_request(), path, codec).await
+        }
+    }
+}
+/// Generated server implementations.
+pub mod node_writer_server {
+    #![allow(unused_variables, dead_code, missing_docs, clippy::let_unit_value)]
+    use tonic::codegen::*;
+    ///Generated trait containing gRPC methods that should be implemented for use with NodeWriterServer.
+    #[async_trait]
+    pub trait NodeWriter: Send + Sync + 'static {
+        async fn get_shard(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardId>,
+            tonic::Status,
+        >;
+        async fn new_shard(
+            &self,
+            request: tonic::Request<super::NewShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCreated>,
+            tonic::Status,
+        >;
+        async fn clean_and_upgrade_shard(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardCleaned>,
+            tonic::Status,
+        >;
+        async fn delete_shard(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardId>,
+            tonic::Status,
+        >;
+        async fn list_shards(
+            &self,
+            request: tonic::Request<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::ShardIds>,
+            tonic::Status,
+        >;
+        async fn gc(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        >;
+        async fn set_resource(
+            &self,
+            request: tonic::Request<super::super::noderesources::Resource>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn delete_relation_nodes(
+            &self,
+            request: tonic::Request<super::DeleteGraphNodes>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn join_graph(
+            &self,
+            request: tonic::Request<super::SetGraph>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn remove_resource(
+            &self,
+            request: tonic::Request<super::super::noderesources::ResourceId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn add_vector_set(
+            &self,
+            request: tonic::Request<super::NewVectorSetRequest>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn remove_vector_set(
+            &self,
+            request: tonic::Request<super::super::noderesources::VectorSetId>,
+        ) -> Result<tonic::Response<super::OpStatus>, tonic::Status>;
+        async fn list_vector_sets(
+            &self,
+            request: tonic::Request<super::super::noderesources::ShardId>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::VectorSetList>,
+            tonic::Status,
+        >;
+        async fn move_shard(
+            &self,
+            request: tonic::Request<super::MoveShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        >;
+        async fn accept_shard(
+            &self,
+            request: tonic::Request<super::AcceptShardRequest>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::EmptyResponse>,
+            tonic::Status,
+        >;
+        async fn get_metadata(
+            &self,
+            request: tonic::Request<super::super::noderesources::EmptyQuery>,
+        ) -> Result<
+            tonic::Response<super::super::noderesources::NodeMetadata>,
+            tonic::Status,
+        >;
+    }
+    #[derive(Debug)]
+    pub struct NodeWriterServer<T: NodeWriter> {
+        inner: _Inner<T>,
+        accept_compression_encodings: (),
+        send_compression_encodings: (),
+    }
+    struct _Inner<T>(Arc<T>);
+    impl<T: NodeWriter> NodeWriterServer<T> {
+        pub fn new(inner: T) -> Self {
+            Self::from_arc(Arc::new(inner))
+        }
+        pub fn from_arc(inner: Arc<T>) -> Self {
+            let inner = _Inner(inner);
+            Self {
+                inner,
+                accept_compression_encodings: Default::default(),
+                send_compression_encodings: Default::default(),
+            }
+        }
+        pub fn with_interceptor<F>(
+            inner: T,
+            interceptor: F,
+        ) -> InterceptedService<Self, F>
+        where
+            F: tonic::service::Interceptor,
+        {
+            InterceptedService::new(Self::new(inner), interceptor)
+        }
+    }
+    impl<T, B> tonic::codegen::Service<http::Request<B>> for NodeWriterServer<T>
+    where
+        T: NodeWriter,
+        B: Body + Send + 'static,
+        B::Error: Into<StdError> + Send + 'static,
+    {
+        type Response = http::Response<tonic::body::BoxBody>;
+        type Error = std::convert::Infallible;
+        type Future = BoxFuture<Self::Response, Self::Error>;
+        fn poll_ready(
+            &mut self,
+            _cx: &mut Context<'_>,
+        ) -> Poll<Result<(), Self::Error>> {
+            Poll::Ready(Ok(()))
+        }
+        fn call(&mut self, req: http::Request<B>) -> Self::Future {
+            let inner = self.inner.clone();
+            match req.uri().path() {
+                "/nodewriter.NodeWriter/GetShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct GetShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for GetShardSvc<T> {
+                        type Response = super::super::noderesources::ShardId;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).get_shard(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GetShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/NewShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct NewShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::NewShardRequest>
+                    for NewShardSvc<T> {
+                        type Response = super::super::noderesources::ShardCreated;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::NewShardRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).new_shard(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = NewShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/CleanAndUpgradeShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct CleanAndUpgradeShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for CleanAndUpgradeShardSvc<T> {
+                        type Response = super::super::noderesources::ShardCleaned;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).clean_and_upgrade_shard(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = CleanAndUpgradeShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/DeleteShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct DeleteShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for DeleteShardSvc<T> {
+                        type Response = super::super::noderesources::ShardId;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).delete_shard(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DeleteShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/ListShards" => {
+                    #[allow(non_camel_case_types)]
+                    struct ListShardsSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::EmptyQuery,
+                    > for ListShardsSvc<T> {
+                        type Response = super::super::noderesources::ShardIds;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::EmptyQuery,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).list_shards(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ListShardsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/GC" => {
+                    #[allow(non_camel_case_types)]
+                    struct GCSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for GCSvc<T> {
+                        type Response = super::super::noderesources::EmptyResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).gc(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GCSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/SetResource" => {
+                    #[allow(non_camel_case_types)]
+                    struct SetResourceSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::Resource>
+                    for SetResourceSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::Resource,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).set_resource(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = SetResourceSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/DeleteRelationNodes" => {
+                    #[allow(non_camel_case_types)]
+                    struct DeleteRelationNodesSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::DeleteGraphNodes>
+                    for DeleteRelationNodesSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::DeleteGraphNodes>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).delete_relation_nodes(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = DeleteRelationNodesSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/JoinGraph" => {
+                    #[allow(non_camel_case_types)]
+                    struct JoinGraphSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<T: NodeWriter> tonic::server::UnaryService<super::SetGraph>
+                    for JoinGraphSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::SetGraph>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).join_graph(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = JoinGraphSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/RemoveResource" => {
+                    #[allow(non_camel_case_types)]
+                    struct RemoveResourceSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::ResourceId,
+                    > for RemoveResourceSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::ResourceId,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).remove_resource(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RemoveResourceSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/AddVectorSet" => {
+                    #[allow(non_camel_case_types)]
+                    struct AddVectorSetSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::NewVectorSetRequest>
+                    for AddVectorSetSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::NewVectorSetRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).add_vector_set(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = AddVectorSetSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/RemoveVectorSet" => {
+                    #[allow(non_camel_case_types)]
+                    struct RemoveVectorSetSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::VectorSetId,
+                    > for RemoveVectorSetSvc<T> {
+                        type Response = super::OpStatus;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::VectorSetId,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).remove_vector_set(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = RemoveVectorSetSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/ListVectorSets" => {
+                    #[allow(non_camel_case_types)]
+                    struct ListVectorSetsSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::super::noderesources::ShardId>
+                    for ListVectorSetsSvc<T> {
+                        type Response = super::super::noderesources::VectorSetList;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::super::noderesources::ShardId>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).list_vector_sets(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = ListVectorSetsSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/MoveShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct MoveShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::MoveShardRequest>
+                    for MoveShardSvc<T> {
+                        type Response = super::super::noderesources::EmptyResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::MoveShardRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move { (*inner).move_shard(request).await };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = MoveShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/AcceptShard" => {
+                    #[allow(non_camel_case_types)]
+                    struct AcceptShardSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<super::AcceptShardRequest>
+                    for AcceptShardSvc<T> {
+                        type Response = super::super::noderesources::EmptyResponse;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<super::AcceptShardRequest>,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).accept_shard(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = AcceptShardSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                "/nodewriter.NodeWriter/GetMetadata" => {
+                    #[allow(non_camel_case_types)]
+                    struct GetMetadataSvc<T: NodeWriter>(pub Arc<T>);
+                    impl<
+                        T: NodeWriter,
+                    > tonic::server::UnaryService<
+                        super::super::noderesources::EmptyQuery,
+                    > for GetMetadataSvc<T> {
+                        type Response = super::super::noderesources::NodeMetadata;
+                        type Future = BoxFuture<
+                            tonic::Response<Self::Response>,
+                            tonic::Status,
+                        >;
+                        fn call(
+                            &mut self,
+                            request: tonic::Request<
+                                super::super::noderesources::EmptyQuery,
+                            >,
+                        ) -> Self::Future {
+                            let inner = self.0.clone();
+                            let fut = async move {
+                                (*inner).get_metadata(request).await
+                            };
+                            Box::pin(fut)
+                        }
+                    }
+                    let accept_compression_encodings = self.accept_compression_encodings;
+                    let send_compression_encodings = self.send_compression_encodings;
+                    let inner = self.inner.clone();
+                    let fut = async move {
+                        let inner = inner.0;
+                        let method = GetMetadataSvc(inner);
+                        let codec = tonic::codec::ProstCodec::default();
+                        let mut grpc = tonic::server::Grpc::new(codec)
+                            .apply_compression_config(
+                                accept_compression_encodings,
+                                send_compression_encodings,
+                            );
+                        let res = grpc.unary(method, req).await;
+                        Ok(res)
+                    };
+                    Box::pin(fut)
+                }
+                _ => {
+                    Box::pin(async move {
+                        Ok(
+                            http::Response::builder()
+                                .status(200)
+                                .header("grpc-status", "12")
+                                .header("content-type", "application/grpc")
+                                .body(empty_body())
+                                .unwrap(),
+                        )
+                    })
+                }
+            }
+        }
+    }
+    impl<T: NodeWriter> Clone for NodeWriterServer<T> {
+        fn clone(&self) -> Self {
+            let inner = self.inner.clone();
+            Self {
+                inner,
+                accept_compression_encodings: self.accept_compression_encodings,
+                send_compression_encodings: self.send_compression_encodings,
+            }
+        }
+    }
+    impl<T: NodeWriter> Clone for _Inner<T> {
+        fn clone(&self) -> Self {
+            Self(self.0.clone())
+        }
+    }
+    impl<T: std::fmt::Debug> std::fmt::Debug for _Inner<T> {
+        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+            write!(f, "{:?}", self.0)
+        }
+    }
+    impl<T: NodeWriter> tonic::transport::NamedService for NodeWriterServer<T> {
+        const NAME: &'static str = "nodewriter.NodeWriter";
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_protos/src/utils.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_protos/src/utils.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,161 +1,161 @@
-/// Relations are connexions between nodes in the relation index.
-/// They are tuplets (Source, Relation Type, Relation Label, To).
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Relation {
-    #[prost(message, optional, tag="6")]
-    pub source: ::core::option::Option<RelationNode>,
-    #[prost(message, optional, tag="7")]
-    pub to: ::core::option::Option<RelationNode>,
-    #[prost(enumeration="relation::RelationType", tag="5")]
-    pub relation: i32,
-    #[prost(string, tag="8")]
-    pub relation_label: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="9")]
-    pub metadata: ::core::option::Option<RelationMetadata>,
-}
-/// Nested message and enum types in `Relation`.
-pub mod relation {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum RelationType {
-        /// Child resource
-        Child = 0,
-        /// related with label (GENERATED)
-        About = 1,
-        /// related with an entity (GENERATED)
-        Entity = 2,
-        /// related with user (GENERATED)
-        Colab = 3,
-        /// Synonym relation
-        Synonym = 4,
-        /// related with something
-        Other = 5,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationMetadata {
-    #[prost(string, optional, tag="1")]
-    pub paragraph_id: ::core::option::Option<::prost::alloc::string::String>,
-    #[prost(int32, optional, tag="2")]
-    pub source_start: ::core::option::Option<i32>,
-    #[prost(int32, optional, tag="3")]
-    pub source_end: ::core::option::Option<i32>,
-    #[prost(int32, optional, tag="4")]
-    pub to_start: ::core::option::Option<i32>,
-    #[prost(int32, optional, tag="5")]
-    pub to_end: ::core::option::Option<i32>,
-}
-/// Nodes are tuplets (Value, Type, Subtype) and they are the main element in the relation index.
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct RelationNode {
-    /// Value of the node.
-    #[prost(string, tag="4")]
-    pub value: ::prost::alloc::string::String,
-    /// The type of the node.
-    #[prost(enumeration="relation_node::NodeType", tag="5")]
-    pub ntype: i32,
-    /// A node may have a subtype (the string should be empty in case it does not).
-    #[prost(string, tag="6")]
-    pub subtype: ::prost::alloc::string::String,
-}
-/// Nested message and enum types in `RelationNode`.
-pub mod relation_node {
-    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-    #[repr(i32)]
-    pub enum NodeType {
-        Entity = 0,
-        Label = 1,
-        Resource = 2,
-        User = 3,
-    }
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct JoinGraphEdge {
-    #[prost(int32, tag="4")]
-    pub source: i32,
-    #[prost(int32, tag="1")]
-    pub target: i32,
-    #[prost(enumeration="relation::RelationType", tag="2")]
-    pub rtype: i32,
-    #[prost(string, tag="3")]
-    pub rsubtype: ::prost::alloc::string::String,
-    #[prost(message, optional, tag="5")]
-    pub metadata: ::core::option::Option<RelationMetadata>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct JoinGraph {
-    #[prost(map="int32, message", tag="1")]
-    pub nodes: ::std::collections::HashMap<i32, RelationNode>,
-    #[prost(message, repeated, tag="2")]
-    pub edges: ::prost::alloc::vec::Vec<JoinGraphEdge>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct ExtractedText {
-    #[prost(string, tag="1")]
-    pub text: ::prost::alloc::string::String,
-    #[prost(map="string, string", tag="2")]
-    pub split_text: ::std::collections::HashMap<::prost::alloc::string::String, ::prost::alloc::string::String>,
-    #[prost(string, repeated, tag="3")]
-    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Vector {
-    #[prost(int32, tag="1")]
-    pub start: i32,
-    #[prost(int32, tag="2")]
-    pub end: i32,
-    #[prost(int32, tag="3")]
-    pub start_paragraph: i32,
-    #[prost(int32, tag="4")]
-    pub end_paragraph: i32,
-    #[prost(float, repeated, tag="5")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct Vectors {
-    #[prost(message, repeated, tag="1")]
-    pub vectors: ::prost::alloc::vec::Vec<Vector>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct VectorObject {
-    #[prost(message, optional, tag="1")]
-    pub vectors: ::core::option::Option<Vectors>,
-    #[prost(map="string, message", tag="2")]
-    pub split_vectors: ::std::collections::HashMap<::prost::alloc::string::String, Vectors>,
-    #[prost(string, repeated, tag="3")]
-    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVector {
-    #[prost(float, repeated, tag="1")]
-    pub vector: ::prost::alloc::vec::Vec<f32>,
-    #[prost(string, repeated, tag="2")]
-    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-    #[prost(int32, tag="3")]
-    pub start: i32,
-    #[prost(int32, tag="4")]
-    pub end: i32,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVectors {
-    /// vector's id
-    #[prost(map="string, message", tag="1")]
-    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVector>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVectorSet {
-    /// vectorsets
-    #[prost(map="string, message", tag="1")]
-    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVectors>,
-}
-#[derive(Clone, PartialEq, ::prost::Message)]
-pub struct UserVectorsList {
-    #[prost(string, repeated, tag="1")]
-    pub vectors: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
-}
-#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
-#[repr(i32)]
-pub enum VectorSimilarity {
-    Cosine = 0,
-    Dot = 1,
-}
+/// Relations are connexions between nodes in the relation index.
+/// They are tuplets (Source, Relation Type, Relation Label, To).
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Relation {
+    #[prost(message, optional, tag="6")]
+    pub source: ::core::option::Option<RelationNode>,
+    #[prost(message, optional, tag="7")]
+    pub to: ::core::option::Option<RelationNode>,
+    #[prost(enumeration="relation::RelationType", tag="5")]
+    pub relation: i32,
+    #[prost(string, tag="8")]
+    pub relation_label: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="9")]
+    pub metadata: ::core::option::Option<RelationMetadata>,
+}
+/// Nested message and enum types in `Relation`.
+pub mod relation {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum RelationType {
+        /// Child resource
+        Child = 0,
+        /// related with label (GENERATED)
+        About = 1,
+        /// related with an entity (GENERATED)
+        Entity = 2,
+        /// related with user (GENERATED)
+        Colab = 3,
+        /// Synonym relation
+        Synonym = 4,
+        /// related with something
+        Other = 5,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationMetadata {
+    #[prost(string, optional, tag="1")]
+    pub paragraph_id: ::core::option::Option<::prost::alloc::string::String>,
+    #[prost(int32, optional, tag="2")]
+    pub source_start: ::core::option::Option<i32>,
+    #[prost(int32, optional, tag="3")]
+    pub source_end: ::core::option::Option<i32>,
+    #[prost(int32, optional, tag="4")]
+    pub to_start: ::core::option::Option<i32>,
+    #[prost(int32, optional, tag="5")]
+    pub to_end: ::core::option::Option<i32>,
+}
+/// Nodes are tuplets (Value, Type, Subtype) and they are the main element in the relation index.
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct RelationNode {
+    /// Value of the node.
+    #[prost(string, tag="4")]
+    pub value: ::prost::alloc::string::String,
+    /// The type of the node.
+    #[prost(enumeration="relation_node::NodeType", tag="5")]
+    pub ntype: i32,
+    /// A node may have a subtype (the string should be empty in case it does not).
+    #[prost(string, tag="6")]
+    pub subtype: ::prost::alloc::string::String,
+}
+/// Nested message and enum types in `RelationNode`.
+pub mod relation_node {
+    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+    #[repr(i32)]
+    pub enum NodeType {
+        Entity = 0,
+        Label = 1,
+        Resource = 2,
+        User = 3,
+    }
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct JoinGraphEdge {
+    #[prost(int32, tag="4")]
+    pub source: i32,
+    #[prost(int32, tag="1")]
+    pub target: i32,
+    #[prost(enumeration="relation::RelationType", tag="2")]
+    pub rtype: i32,
+    #[prost(string, tag="3")]
+    pub rsubtype: ::prost::alloc::string::String,
+    #[prost(message, optional, tag="5")]
+    pub metadata: ::core::option::Option<RelationMetadata>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct JoinGraph {
+    #[prost(map="int32, message", tag="1")]
+    pub nodes: ::std::collections::HashMap<i32, RelationNode>,
+    #[prost(message, repeated, tag="2")]
+    pub edges: ::prost::alloc::vec::Vec<JoinGraphEdge>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct ExtractedText {
+    #[prost(string, tag="1")]
+    pub text: ::prost::alloc::string::String,
+    #[prost(map="string, string", tag="2")]
+    pub split_text: ::std::collections::HashMap<::prost::alloc::string::String, ::prost::alloc::string::String>,
+    #[prost(string, repeated, tag="3")]
+    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Vector {
+    #[prost(int32, tag="1")]
+    pub start: i32,
+    #[prost(int32, tag="2")]
+    pub end: i32,
+    #[prost(int32, tag="3")]
+    pub start_paragraph: i32,
+    #[prost(int32, tag="4")]
+    pub end_paragraph: i32,
+    #[prost(float, repeated, tag="5")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct Vectors {
+    #[prost(message, repeated, tag="1")]
+    pub vectors: ::prost::alloc::vec::Vec<Vector>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct VectorObject {
+    #[prost(message, optional, tag="1")]
+    pub vectors: ::core::option::Option<Vectors>,
+    #[prost(map="string, message", tag="2")]
+    pub split_vectors: ::std::collections::HashMap<::prost::alloc::string::String, Vectors>,
+    #[prost(string, repeated, tag="3")]
+    pub deleted_splits: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVector {
+    #[prost(float, repeated, tag="1")]
+    pub vector: ::prost::alloc::vec::Vec<f32>,
+    #[prost(string, repeated, tag="2")]
+    pub labels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+    #[prost(int32, tag="3")]
+    pub start: i32,
+    #[prost(int32, tag="4")]
+    pub end: i32,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVectors {
+    /// vector's id
+    #[prost(map="string, message", tag="1")]
+    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVector>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVectorSet {
+    /// vectorsets
+    #[prost(map="string, message", tag="1")]
+    pub vectors: ::std::collections::HashMap<::prost::alloc::string::String, UserVectors>,
+}
+#[derive(Clone, PartialEq, ::prost::Message)]
+pub struct UserVectorsList {
+    #[prost(string, repeated, tag="1")]
+    pub vectors: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
+}
+#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
+#[repr(i32)]
+pub enum VectorSimilarity {
+    Cosine = 0,
+    Dot = 1,
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/Cargo.toml` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/Cargo.toml`

 * *Files 25% similar despite different names*

```diff
@@ -1,41 +1,43 @@
-00000000: 5b70 6163 6b61 6765 5d0a 6e61 6d65 203d  [package].name =
-00000010: 2022 6e75 636c 6961 6462 5f66 7470 220a   "nucliadb_ftp".
-00000020: 7665 7273 696f 6e20 3d20 2230 2e31 2e30  version = "0.1.0
-00000030: 220a 6564 6974 696f 6e20 3d20 2232 3032  ".edition = "202
-00000040: 3122 0a6c 6963 656e 7365 203d 2022 4147  1".license = "AG
-00000050: 504c 2d33 2e30 2d6f 722d 6c61 7465 7222  PL-3.0-or-later"
-00000060: 0a0a 5b64 6570 656e 6465 6e63 6965 735d  ..[dependencies]
-00000070: 0a65 7972 6520 3d20 2230 2e36 2e38 220a  .eyre = "0.6.8".
-00000080: 636f 6c6f 722d 6579 7265 203d 2022 302e  color-eyre = "0.
-00000090: 362e 3222 0a64 6572 6976 655f 6275 696c  6.2".derive_buil
-000000a0: 6465 7220 3d20 2230 2e31 322e 3022 0a74  der = "0.12.0".t
-000000b0: 6869 7365 7272 6f72 203d 2022 312e 302e  hiserror = "1.0.
-000000c0: 3338 220a 746f 6b69 6f20 3d20 7b20 7665  38".tokio = { ve
-000000d0: 7273 696f 6e20 3d20 2231 2e32 332e 3022  rsion = "1.23.0"
-000000e0: 2c20 6665 6174 7572 6573 203d 205b 226e  , features = ["n
-000000f0: 6574 225d 207d 0a74 6f6b 696f 2d74 6172  et"] }.tokio-tar
-00000100: 203d 207b 2067 6974 203d 2022 6874 7470   = { git = "http
-00000110: 733a 2f2f 6769 7468 7562 2e63 6f6d 2f6e  s://github.com/n
-00000120: 7563 6c69 612f 746f 6b69 6f2d 7461 722e  uclia/tokio-tar.
-00000130: 6769 7422 207d 0a63 6c61 7020 3d20 7b20  git" }.clap = { 
-00000140: 7665 7273 696f 6e20 3d20 2234 2e30 2e33  version = "4.0.3
-00000150: 3222 2c20 6665 6174 7572 6573 203d 205b  2", features = [
-00000160: 2264 6572 6976 6522 5d20 7d0a 6261 636b  "derive"] }.back
-00000170: 6f66 6620 3d20 7b20 7665 7273 696f 6e20  off = { version 
-00000180: 3d20 2230 2e34 2e30 222c 2066 6561 7475  = "0.4.0", featu
-00000190: 7265 7320 3d20 5b22 746f 6b69 6f22 5d20  res = ["tokio"] 
-000001a0: 7d0a 7472 6163 696e 6720 3d20 2230 2e31  }.tracing = "0.1
-000001b0: 2e33 3722 0a74 7261 6369 6e67 2d73 7562  .37".tracing-sub
-000001c0: 7363 7269 6265 7220 3d20 7b20 7665 7273  scriber = { vers
-000001d0: 696f 6e20 3d20 2230 2e33 2e31 3622 2c20  ion = "0.3.16", 
-000001e0: 6665 6174 7572 6573 203d 205b 2265 6e76  features = ["env
-000001f0: 2d66 696c 7465 7222 2c20 2273 6d61 6c6c  -filter", "small
-00000200: 7665 6322 5d20 7d0a 0a5b 6465 762d 6465  vec"] }..[dev-de
-00000210: 7065 6e64 656e 6369 6573 5d0a 7465 6d70  pendencies].temp
-00000220: 6669 6c65 203d 2022 332e 332e 3022 0a74  file = "3.3.0".t
-00000230: 6f6b 696f 203d 207b 2076 6572 7369 6f6e  okio = { version
-00000240: 203d 2022 312e 3233 2e30 222c 2066 6561   = "1.23.0", fea
-00000250: 7475 7265 7320 3d20 5b22 6d61 6372 6f73  tures = ["macros
-00000260: 222c 2022 7274 2d6d 756c 7469 2d74 6872  ", "rt-multi-thr
-00000270: 6561 6422 5d20 7d0a 746f 6b69 6f2d 7465  ead"] }.tokio-te
-00000280: 7374 203d 2022 302e 342e 3222 0a         st = "0.4.2".
+00000000: 5b70 6163 6b61 6765 5d0d 0a6e 616d 6520  [package]..name 
+00000010: 3d20 226e 7563 6c69 6164 625f 6674 7022  = "nucliadb_ftp"
+00000020: 0d0a 7665 7273 696f 6e20 3d20 2230 2e31  ..version = "0.1
+00000030: 2e30 220d 0a65 6469 7469 6f6e 203d 2022  .0"..edition = "
+00000040: 3230 3231 220d 0a6c 6963 656e 7365 203d  2021"..license =
+00000050: 2022 4147 504c 2d33 2e30 2d6f 722d 6c61   "AGPL-3.0-or-la
+00000060: 7465 7222 0d0a 0d0a 5b64 6570 656e 6465  ter"....[depende
+00000070: 6e63 6965 735d 0d0a 6579 7265 203d 2022  ncies]..eyre = "
+00000080: 302e 362e 3822 0d0a 636f 6c6f 722d 6579  0.6.8"..color-ey
+00000090: 7265 203d 2022 302e 362e 3222 0d0a 6465  re = "0.6.2"..de
+000000a0: 7269 7665 5f62 7569 6c64 6572 203d 2022  rive_builder = "
+000000b0: 302e 3132 2e30 220d 0a74 6869 7365 7272  0.12.0"..thiserr
+000000c0: 6f72 203d 2022 312e 302e 3338 220d 0a74  or = "1.0.38"..t
+000000d0: 6f6b 696f 203d 207b 2076 6572 7369 6f6e  okio = { version
+000000e0: 203d 2022 312e 3233 2e30 222c 2066 6561   = "1.23.0", fea
+000000f0: 7475 7265 7320 3d20 5b22 6e65 7422 5d20  tures = ["net"] 
+00000100: 7d0d 0a74 6f6b 696f 2d74 6172 203d 207b  }..tokio-tar = {
+00000110: 2067 6974 203d 2022 6874 7470 733a 2f2f   git = "https://
+00000120: 6769 7468 7562 2e63 6f6d 2f6e 7563 6c69  github.com/nucli
+00000130: 612f 746f 6b69 6f2d 7461 722e 6769 7422  a/tokio-tar.git"
+00000140: 207d 0d0a 636c 6170 203d 207b 2076 6572   }..clap = { ver
+00000150: 7369 6f6e 203d 2022 342e 302e 3332 222c  sion = "4.0.32",
+00000160: 2066 6561 7475 7265 7320 3d20 5b22 6465   features = ["de
+00000170: 7269 7665 225d 207d 0d0a 6261 636b 6f66  rive"] }..backof
+00000180: 6620 3d20 7b20 7665 7273 696f 6e20 3d20  f = { version = 
+00000190: 2230 2e34 2e30 222c 2066 6561 7475 7265  "0.4.0", feature
+000001a0: 7320 3d20 5b22 746f 6b69 6f22 5d20 7d0d  s = ["tokio"] }.
+000001b0: 0a74 7261 6369 6e67 203d 2022 302e 312e  .tracing = "0.1.
+000001c0: 3337 220d 0a74 7261 6369 6e67 2d73 7562  37"..tracing-sub
+000001d0: 7363 7269 6265 7220 3d20 7b20 7665 7273  scriber = { vers
+000001e0: 696f 6e20 3d20 2230 2e33 2e31 3622 2c20  ion = "0.3.16", 
+000001f0: 6665 6174 7572 6573 203d 205b 2265 6e76  features = ["env
+00000200: 2d66 696c 7465 7222 2c20 2273 6d61 6c6c  -filter", "small
+00000210: 7665 6322 5d20 7d0d 0a0d 0a5b 6465 762d  vec"] }....[dev-
+00000220: 6465 7065 6e64 656e 6369 6573 5d0d 0a74  dependencies]..t
+00000230: 656d 7066 696c 6520 3d20 2233 2e33 2e30  empfile = "3.3.0
+00000240: 220d 0a74 6f6b 696f 203d 207b 2076 6572  "..tokio = { ver
+00000250: 7369 6f6e 203d 2022 312e 3233 2e30 222c  sion = "1.23.0",
+00000260: 2066 6561 7475 7265 7320 3d20 5b22 6d61   features = ["ma
+00000270: 6372 6f73 222c 2022 7274 2d6d 756c 7469  cros", "rt-multi
+00000280: 2d74 6872 6561 6422 5d20 7d0d 0a74 6f6b  -thread"] }..tok
+00000290: 696f 2d74 6573 7420 3d20 2230 2e34 2e32  io-test = "0.4.2
+000002a0: 220d 0a                                  "..
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/error.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,32 +1,43 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io;
-
-use thiserror::Error;
-
-/// The error that may occur when publishing/receiving files/directories.
-#[derive(Debug, Error)]
-pub enum Error {
-    #[error("invalid path '{0}': {1}")]
-    InvalidPath(String, String),
-    #[error(transparent)]
-    IoError(#[from] io::Error),
-}
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import logging
+
+SERVICE_NAME = "nucliadb_node"
+
+logger = logging.getLogger(SERVICE_NAME)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/lib.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,214 +1,213 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-#![warn(clippy::pedantic)]
-
-//! # `nucliadb_ftp`
-//!
-//! The `nucliadb_ftp` crate aims to transfer files/directories asynchronously over the network
-//! (using a TCP/IP connection), based on a patched version of [`tokio-tar`](https://github.com/alekece/tokio-tar) crate.
-//!
-//! To do so, `nucliadb_ftp` provides two simple and easy-to-use types:
-//! - [`Publisher`] that helps appending files/directories before publishing them.
-//! - [`Listener`] that helps listening (once or multiple time) incoming files/directories.
-//!
-//! ## Examples
-//!
-//! ```no_run
-//! # tokio_test::block_on(async {
-//! use nucliadb_ftp::{Listener, Publisher, RetryPolicy};
-//!
-//! let listener_task = tokio::spawn(async {
-//!     Listener::default()
-//!         .save_at("my_dir")
-//!         // Uncomment this line if you want to preserve metadata of receveived files/directories.
-//!         // .preserve_metadata()
-//!         .listen_once(4242)
-//!         // Uncomment this line if you want to keep the listener active.
-//!         // .listen(4242)
-//!         .await
-//!         .unwrap();
-//! });
-//!
-//! let publisher_task = tokio::spawn(async {
-//!     Publisher::default()
-//!         // Uncomment this line if you want to publish files/directories with their metadata
-//!         //.preserve_metadata()
-//!         // Uncomment this line if you want to follow symlinks in appended directories.
-//!         // .follow_symlink()
-//!         .append("my_dir").unwrap()
-//!         .append("path/to/my_file").unwrap()
-//!         .retry_on_failure(RetryPolicy::Always)
-//!         .send_to_localhost(4242)
-//!         // Or
-//!         // .sent_to("x.x.x.x:4242")
-//!         .await
-//!         .unwrap();
-//! });
-//!
-//! publisher_task.await.unwrap();
-//! listener_task.await.unwrap();
-//!
-//! # });
-//! ```
-
-mod error;
-mod listener;
-mod publisher;
-
-pub use error::Error;
-pub use listener::Listener;
-pub use publisher::{Publisher, RetryPolicy};
-
-#[cfg(test)]
-mod tests {
-    use std::ffi::OsStr;
-    use std::fs::{self, File};
-    use std::io::Write;
-
-    use eyre::Result;
-    use tokio::time::Duration;
-
-    use super::*;
-
-    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
-    async fn it_sends_file_on_localhost() -> Result<()> {
-        let port = 4242;
-        let file_name = "dummy.txt";
-        let file_content = "Time spent with cats is never wasted";
-
-        let listener_task = tokio::spawn({
-            let file_content = file_content;
-
-            async move {
-                let destination_dir = tempfile::tempdir()?;
-
-                Listener::default()
-                    .save_at(destination_dir.path())
-                    .listen_once(port)
-                    .await?;
-
-                let destination = destination_dir.path().join(file_name);
-
-                assert_eq!(file_content, &fs::read_to_string(destination)?);
-
-                Ok(()) as Result<()>
-            }
-        });
-
-        let publisher_task = tokio::spawn({
-            async move {
-                let source_dir = tempfile::tempdir()?;
-                let source = source_dir.path().join(file_name);
-
-                {
-                    let mut file = File::create(&source)?;
-
-                    write!(file, "{file_content}")?;
-                }
-
-                Publisher::default()
-                    .retry_on_failure(RetryPolicy::MaxDuration(Duration::from_secs(30)))
-                    .append(source)?
-                    .send_to_localhost(port)
-                    .await?;
-
-                Ok(()) as Result<()>
-            }
-        });
-
-        publisher_task.await??;
-        listener_task.await??;
-
-        Ok(())
-    }
-
-    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
-    async fn it_sends_directory_on_localhost() -> Result<()> {
-        let port = 4243;
-        let files = &[
-            ("dummy.txt", "Time spent with cats is never waster"),
-            ("file1.org", "hello world"),
-            ("README.md", include_str!("../README.md")),
-        ];
-        let dir_name = "my_dir";
-
-        let listener_task = tokio::spawn(async move {
-            let destination_dir = tempfile::tempdir()?;
-
-            Listener::default()
-                .save_at(destination_dir.path())
-                .listen_once(port)
-                .await?;
-
-            let received_files = fs::read_dir(destination_dir.path().join(dir_name))?
-                .into_iter()
-                .filter_map(|entry| entry.ok().map(|e| e.path()))
-                .collect::<Vec<_>>();
-
-            assert_eq!(files.len(), received_files.len());
-
-            for received_file in received_files {
-                let (_, file_content) = files
-                    .iter()
-                    .find(|(name, _)| received_file.file_name() == Some(OsStr::new(name)))
-                    .unwrap_or_else(|| {
-                        panic!(
-                            "'{}' not found in destination directory",
-                            received_file.display()
-                        )
-                    });
-
-                assert_eq!(file_content, &fs::read_to_string(received_file)?);
-            }
-
-            Ok(()) as Result<()>
-        });
-
-        let publisher_task = tokio::spawn(async move {
-            let source_dir = tempfile::tempdir()?;
-            let source_dir = source_dir.path().join(dir_name);
-
-            fs::create_dir(&source_dir)?;
-
-            {
-                for (file_name, file_content) in files {
-                    let source = source_dir.join(file_name);
-                    let mut file = File::create(source)?;
-
-                    write!(file, "{file_content}")?;
-                }
-            }
-
-            Publisher::default()
-                .retry_on_failure(RetryPolicy::MaxDuration(Duration::from_secs(30)))
-                .append(source_dir)?
-                .send_to_localhost(port)
-                .await?;
-
-            Ok(()) as Result<()>
-        });
-
-        publisher_task.await??;
-        listener_task.await??;
-
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+#![warn(clippy::pedantic)]
+
+//! # `nucliadb_ftp`
+//!
+//! The `nucliadb_ftp` crate aims to transfer files/directories asynchronously over the network
+//! (using a TCP/IP connection), based on a patched version of [`tokio-tar`](https://github.com/alekece/tokio-tar) crate.
+//!
+//! To do so, `nucliadb_ftp` provides two simple and easy-to-use types:
+//! - [`Publisher`] that helps appending files/directories before publishing them.
+//! - [`Listener`] that helps listening (once or multiple time) incoming files/directories.
+//!
+//! ## Examples
+//!
+//! ```no_run
+//! # tokio_test::block_on(async {
+//! use nucliadb_ftp::{Listener, Publisher, RetryPolicy};
+//!
+//! let listener_task = tokio::spawn(async {
+//!     Listener::default()
+//!         .save_at("my_dir")
+//!         // Uncomment this line if you want to preserve metadata of receveived files/directories.
+//!         // .preserve_metadata()
+//!         .listen_once(4242)
+//!         // Uncomment this line if you want to keep the listener active.
+//!         // .listen(4242)
+//!         .await
+//!         .unwrap();
+//! });
+//!
+//! let publisher_task = tokio::spawn(async {
+//!     Publisher::default()
+//!         // Uncomment this line if you want to publish files/directories with their metadata
+//!         //.preserve_metadata()
+//!         // Uncomment this line if you want to follow symlinks in appended directories.
+//!         // .follow_symlink()
+//!         .append("my_dir").unwrap()
+//!         .append("path/to/my_file").unwrap()
+//!         .retry_on_failure(RetryPolicy::Always)
+//!         .send_to_localhost(4242)
+//!         // Or
+//!         // .sent_to("x.x.x.x:4242")
+//!         .await
+//!         .unwrap();
+//! });
+//!
+//! publisher_task.await.unwrap();
+//! listener_task.await.unwrap();
+//!
+//! # });
+//! ```
+
+mod error;
+mod listener;
+mod publisher;
+
+pub use error::Error;
+pub use listener::Listener;
+pub use publisher::{Publisher, RetryPolicy};
+
+#[cfg(test)]
+mod tests {
+    use std::ffi::OsStr;
+    use std::fs::{self, File};
+    use std::io::Write;
+
+    use eyre::Result;
+    use tokio::time::Duration;
+
+    use super::*;
+
+    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+    async fn it_sends_file_on_localhost() -> Result<()> {
+        let port = 4242;
+        let file_name = "dummy.txt";
+        let file_content = "Time spent with cats is never wasted";
+
+        let listener_task = tokio::spawn({
+            let file_content = file_content;
+
+            async move {
+                let destination_dir = tempfile::tempdir()?;
+
+                Listener::default()
+                    .save_at(destination_dir.path())
+                    .listen_once(port)
+                    .await?;
+
+                let destination = destination_dir.path().join(file_name);
+
+                assert_eq!(file_content, &fs::read_to_string(destination)?);
+
+                Ok(()) as Result<()>
+            }
+        });
+
+        let publisher_task = tokio::spawn({
+            async move {
+                let source_dir = tempfile::tempdir()?;
+                let source = source_dir.path().join(file_name);
+
+                {
+                    let mut file = File::create(&source)?;
+
+                    write!(file, "{file_content}")?;
+                }
+
+                Publisher::default()
+                    .retry_on_failure(RetryPolicy::MaxDuration(Duration::from_secs(30)))
+                    .append(source)?
+                    .send_to_localhost(port)
+                    .await?;
+
+                Ok(()) as Result<()>
+            }
+        });
+
+        publisher_task.await??;
+        listener_task.await??;
+
+        Ok(())
+    }
+
+    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+    async fn it_sends_directory_on_localhost() -> Result<()> {
+        let port = 4243;
+        let files = &[
+            ("dummy.txt", "Time spent with cats is never waster"),
+            ("file1.org", "hello world"),
+            ("README.md", include_str!("../README.md")),
+        ];
+        let dir_name = "my_dir";
+
+        let listener_task = tokio::spawn(async move {
+            let destination_dir = tempfile::tempdir()?;
+
+            Listener::default()
+                .save_at(destination_dir.path())
+                .listen_once(port)
+                .await?;
+
+            let received_files = fs::read_dir(destination_dir.path().join(dir_name))?
+                .filter_map(|entry| entry.ok().map(|e| e.path()))
+                .collect::<Vec<_>>();
+
+            assert_eq!(files.len(), received_files.len());
+
+            for received_file in received_files {
+                let (_, file_content) = files
+                    .iter()
+                    .find(|(name, _)| received_file.file_name() == Some(OsStr::new(name)))
+                    .unwrap_or_else(|| {
+                        panic!(
+                            "'{}' not found in destination directory",
+                            received_file.display()
+                        )
+                    });
+
+                assert_eq!(file_content, &fs::read_to_string(received_file)?);
+            }
+
+            Ok(()) as Result<()>
+        });
+
+        let publisher_task = tokio::spawn(async move {
+            let source_dir = tempfile::tempdir()?;
+            let source_dir = source_dir.path().join(dir_name);
+
+            fs::create_dir(&source_dir)?;
+
+            {
+                for (file_name, file_content) in files {
+                    let source = source_dir.join(file_name);
+                    let mut file = File::create(source)?;
+
+                    write!(file, "{file_content}")?;
+                }
+            }
+
+            Publisher::default()
+                .retry_on_failure(RetryPolicy::MaxDuration(Duration::from_secs(30)))
+                .append(source_dir)?
+                .send_to_localhost(port)
+                .await?;
+
+            Ok(()) as Result<()>
+        });
+
+        publisher_task.await??;
+        listener_task.await??;
+
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/listener.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/listener.rs`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::path::PathBuf;
-
-use tokio::net::TcpListener;
-use tokio_tar::ArchiveBuilder;
-
-use super::error::Error;
-
-/// A structure for receiving files/directories over TCP/IP connection.
-///
-/// # Examples
-/// ```no_run
-/// # tokio_test::block_on(async {
-/// use nucliadb_ftp::Listener;
-///
-/// Listener::default()
-///     .save_at("my_path")
-///     .preserve_metadata()
-///     .listen_once(4242)
-///     // Uncomment this line if you want to keep the listener active.
-///     // .listen(4242)
-///     .await
-///     .unwrap();
-/// # });
-/// ```
-#[must_use]
-pub struct Listener {
-    /// Preserves received file/directory metadata.
-    /// Defaulted to `false`.
-    preserve_metadata: bool,
-    /// Indicates the location of the received files/directories on the file system.
-    path: PathBuf,
-}
-
-impl Default for Listener {
-    fn default() -> Self {
-        Self {
-            preserve_metadata: false,
-            path: PathBuf::from("."),
-        }
-    }
-}
-
-impl Listener {
-    /// Set the location of the received files/directories on the file system.
-    ///
-    /// Note that if the given location does not exist, it will be created on the fly
-    /// on file/directory reception.
-    pub fn save_at(mut self, path: impl Into<PathBuf>) -> Self {
-        self.path = path.into();
-
-        self
-    }
-
-    /// Preserves received file/directory metadata.
-    ///
-    /// Note that [`Publisher`](crate.publisher.Publisher.struct) needs to publish file/directory
-    /// with metadata preservation.
-    pub fn preserve_metadata(mut self) -> Self {
-        self.preserve_metadata = true;
-
-        self
-    }
-
-    /// Listen only once for receiving files/directories.
-    ///
-    /// # Errors
-    /// This method can fail if:
-    /// - The received files/directories are ill-formed.
-    /// - The TCP/IP connection cannot be opened (port already in use).
-    pub async fn listen_once(&self, port: u16) -> Result<(), Error> {
-        self.listen_nth(port, Some(1)).await
-    }
-
-    /// Listen for receiving files/directories.
-    ///
-    /// Note that this method only returns on failure.
-    ///
-    /// # Errors
-    /// This method can fail if:
-    /// - The received files/directories are ill-formed.
-    /// - The TCP/IP connection cannot be opened (port already in use).
-    pub async fn listen(&self, port: u16) -> Result<(), Error> {
-        self.listen_nth(port, None).await
-    }
-
-    async fn listen_nth(&self, port: u16, mut limit: Option<usize>) -> Result<(), Error> {
-        let listener = TcpListener::bind(format!("0.0.0.0:{port}")).await?;
-
-        while limit.map_or(true, |limit| limit > 0) {
-            let (socket, _) = listener.accept().await?;
-
-            let mut archive = ArchiveBuilder::new(socket)
-                .set_preserve_mtime(self.preserve_metadata)
-                .set_preserve_permissions(self.preserve_metadata)
-                .set_unpack_xattrs(self.preserve_metadata)
-                .build();
-
-            archive.unpack(&self.path).await?;
-
-            if let Some(limit) = limit.as_mut() {
-                *limit -= 1;
-            }
-        }
-
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::path::PathBuf;
+
+use tokio::net::TcpListener;
+use tokio_tar::ArchiveBuilder;
+
+use super::error::Error;
+
+/// A structure for receiving files/directories over TCP/IP connection.
+///
+/// # Examples
+/// ```no_run
+/// # tokio_test::block_on(async {
+/// use nucliadb_ftp::Listener;
+///
+/// Listener::default()
+///     .save_at("my_path")
+///     .preserve_metadata()
+///     .listen_once(4242)
+///     // Uncomment this line if you want to keep the listener active.
+///     // .listen(4242)
+///     .await
+///     .unwrap();
+/// # });
+/// ```
+#[must_use]
+pub struct Listener {
+    /// Preserves received file/directory metadata.
+    /// Defaulted to `false`.
+    preserve_metadata: bool,
+    /// Indicates the location of the received files/directories on the file system.
+    path: PathBuf,
+}
+
+impl Default for Listener {
+    fn default() -> Self {
+        Self {
+            preserve_metadata: false,
+            path: PathBuf::from("."),
+        }
+    }
+}
+
+impl Listener {
+    /// Set the location of the received files/directories on the file system.
+    ///
+    /// Note that if the given location does not exist, it will be created on the fly
+    /// on file/directory reception.
+    pub fn save_at(mut self, path: impl Into<PathBuf>) -> Self {
+        self.path = path.into();
+
+        self
+    }
+
+    /// Preserves received file/directory metadata.
+    ///
+    /// Note that [`Publisher`](crate.publisher.Publisher.struct) needs to publish file/directory
+    /// with metadata preservation.
+    pub fn preserve_metadata(mut self) -> Self {
+        self.preserve_metadata = true;
+
+        self
+    }
+
+    /// Listen only once for receiving files/directories.
+    ///
+    /// # Errors
+    /// This method can fail if:
+    /// - The received files/directories are ill-formed.
+    /// - The TCP/IP connection cannot be opened (port already in use).
+    pub async fn listen_once(&self, port: u16) -> Result<(), Error> {
+        self.listen_nth(port, Some(1)).await
+    }
+
+    /// Listen for receiving files/directories.
+    ///
+    /// Note that this method only returns on failure.
+    ///
+    /// # Errors
+    /// This method can fail if:
+    /// - The received files/directories are ill-formed.
+    /// - The TCP/IP connection cannot be opened (port already in use).
+    pub async fn listen(&self, port: u16) -> Result<(), Error> {
+        self.listen_nth(port, None).await
+    }
+
+    async fn listen_nth(&self, port: u16, mut limit: Option<usize>) -> Result<(), Error> {
+        let listener = TcpListener::bind(format!("0.0.0.0:{port}")).await?;
+
+        while limit.map_or(true, |limit| limit > 0) {
+            let (socket, _) = listener.accept().await?;
+
+            let mut archive = ArchiveBuilder::new(socket)
+                .set_preserve_mtime(self.preserve_metadata)
+                .set_preserve_permissions(self.preserve_metadata)
+                .set_unpack_xattrs(self.preserve_metadata)
+                .build();
+
+            archive.unpack(&self.path).await?;
+
+            if let Some(limit) = limit.as_mut() {
+                *limit -= 1;
+            }
+        }
+
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_ftp/src/publisher.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/publisher.rs`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,203 +1,203 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::ffi::{OsStr, OsString};
-use std::path::PathBuf;
-use std::time::Duration;
-
-use backoff::backoff::{Backoff, Stop};
-use backoff::{ExponentialBackoff, ExponentialBackoffBuilder};
-use tokio::net::{TcpStream, ToSocketAddrs};
-use tokio_tar::{Builder, HeaderMode};
-
-use super::error::Error;
-
-/// A structure for representing all the retry policy on publication failure.
-#[non_exhaustive]
-#[derive(Debug, Copy, Clone, PartialEq, Eq)]
-pub enum RetryPolicy {
-    /// Indicates to always retry the publication after a failure.
-    Always,
-    /// Indicates to never retry the publication after a failure.
-    Never,
-    /// Indicates to retry the publication during at most the given duration after a failure.
-    MaxDuration(Duration),
-}
-
-/// A structure for publishing files/directories over TCP/IP connection.
-///
-/// # Examples
-/// ```no_run
-/// # tokio_test::block_on(async {
-/// use nucliadb_ftp::Publisher;
-///
-/// Publisher::default()
-///     .follow_symlinks()
-///     .preserve_metadata()
-///     .append("my_dir").unwrap()
-///     .append("path/to/my_file").unwrap()
-///     .send_to("0.0.0.0:4040")
-///     .await
-///     .unwrap();
-/// # })
-/// ```
-#[must_use]
-#[derive(Default)]
-pub struct Publisher {
-    /// Indicates to follow symlinks if any directory has been appended to the current `Publisher`.
-    /// Defaulted to `false`.
-    follow_symlinks: bool,
-    /// Preserves file/directory metadata.
-    /// Defaulted to `false`.
-    preserve_metadata: bool,
-    /// The list of files/directories published on [`Publisher::send_to`] call.
-    /// Note that calling [`Publisher::send_to`] method with an empty list of paths results to a
-    /// no-op.
-    paths: Vec<(PathBuf, OsString)>,
-    /// The backoff policy on publication failure.
-    ///
-    /// Note that if the publication fails because of invalid paths, the backoff policy will be
-    /// ignored.
-    backoff: Option<ExponentialBackoff>,
-}
-
-impl Publisher {
-    /// Follows symlinks on publishing.
-    pub fn follow_symlinks(mut self) -> Self {
-        self.follow_symlinks = true;
-
-        self
-    }
-
-    /// Preserves file/directory metadata on publishing.
-    pub fn preserve_metadata(mut self) -> Self {
-        self.preserve_metadata = true;
-
-        self
-    }
-
-    /// Appends the given path to the current publisher.
-    ///
-    /// Note that if the path point to a directory all files and the directory itself will be
-    /// published.
-    ///
-    /// # Errors
-    /// This methods can fails if:
-    /// - The given path cannot be canonicalized (path does not exists).
-    /// - The given path does not contain file name (e.g "/" or "").
-    pub fn append(mut self, path: impl Into<PathBuf>) -> Result<Self, Error> {
-        let path = path.into();
-
-        let name = path
-            .canonicalize()
-            .map_err(|e| Error::InvalidPath(path.display().to_string(), e.to_string()))?;
-
-        let name = name.file_name().map(OsStr::to_os_string).ok_or_else(|| {
-            Error::InvalidPath(path.display().to_string(), "missing file name".to_string())
-        })?;
-
-        self.paths.push((path, name));
-
-        Ok(self)
-    }
-
-    /// Indicates to retry the files/directories publication on failure.
-    ///
-    /// Note that if the given `retry_policy` is set to [`RetryPolicy::Always`], the current
-    /// publisher will always retry to send files/directories until success. To put it simply,
-    /// the publication could end up as a blocking operation.
-    pub fn retry_on_failure(mut self, retry_policy: RetryPolicy) -> Self {
-        let backoff = match retry_policy {
-            RetryPolicy::Always => Some(None),
-            RetryPolicy::MaxDuration(duration) => Some(Some(duration)),
-            RetryPolicy::Never => None,
-        };
-
-        self.backoff = backoff.map(|max_elapsed_time| {
-            ExponentialBackoffBuilder::new()
-                .with_max_elapsed_time(max_elapsed_time)
-                .with_initial_interval(Duration::from_secs(1))
-                .with_multiplier(2.0)
-                .build()
-        });
-
-        self
-    }
-
-    /// Publishs all the appended files/directories to localhost.
-    ///
-    /// # Errors
-    /// This method can fails if the connection with the remote address is refused/closed or
-    /// timeout.
-    pub async fn send_to_localhost(&self, port: u16) -> Result<(), Error> {
-        self.send_to(format!("0.0.0.0:{port}")).await
-    }
-
-    /// Publishs all the appended files/directories to the given IP address.
-    ///
-    /// # Errors
-    /// This method can fails if the connection with the remote address is refused/closed or
-    /// timeout.
-    pub async fn send_to(&self, address: impl ToSocketAddrs + Clone) -> Result<(), Error> {
-        let backoff = self.backoff.clone().map_or_else(
-            || Box::new(Stop {}) as Box<dyn Backoff + Send>,
-            |backoff| Box::new(backoff) as Box<dyn Backoff + Send>,
-        );
-
-        backoff::future::retry_notify(
-            backoff,
-            || {
-                let address = address.clone();
-
-                async move {
-                    let socket = TcpStream::connect(address).await?;
-                    let mut archive = Builder::new(socket);
-
-                    archive.mode(if self.preserve_metadata {
-                        HeaderMode::Complete
-                    } else {
-                        HeaderMode::Deterministic
-                    });
-
-                    archive.follow_symlinks(self.follow_symlinks);
-
-                    for (path, name) in &self.paths {
-                        if path.is_dir() {
-                            archive.append_dir_all(name, path).await?;
-                        } else {
-                            archive.append_path_with_name(path, name).await?;
-                        }
-                    }
-
-                    // NOTE: this line is mandatory and SHOULD NOT be removed.
-                    // Increment that counter if you tried and broke the codebase: 1
-                    let _writer = archive.into_inner().await?;
-
-                    Ok(())
-                }
-            },
-            |e, duration| {
-                tracing::warn!("Error happened at {duration:?}: {e}");
-            },
-        )
-        .await
-        .map_err(Into::into)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::ffi::{OsStr, OsString};
+use std::path::PathBuf;
+use std::time::Duration;
+
+use backoff::backoff::{Backoff, Stop};
+use backoff::{ExponentialBackoff, ExponentialBackoffBuilder};
+use tokio::net::{TcpStream, ToSocketAddrs};
+use tokio_tar::{Builder, HeaderMode};
+
+use super::error::Error;
+
+/// A structure for representing all the retry policy on publication failure.
+#[non_exhaustive]
+#[derive(Debug, Copy, Clone, PartialEq, Eq)]
+pub enum RetryPolicy {
+    /// Indicates to always retry the publication after a failure.
+    Always,
+    /// Indicates to never retry the publication after a failure.
+    Never,
+    /// Indicates to retry the publication during at most the given duration after a failure.
+    MaxDuration(Duration),
+}
+
+/// A structure for publishing files/directories over TCP/IP connection.
+///
+/// # Examples
+/// ```no_run
+/// # tokio_test::block_on(async {
+/// use nucliadb_ftp::Publisher;
+///
+/// Publisher::default()
+///     .follow_symlinks()
+///     .preserve_metadata()
+///     .append("my_dir").unwrap()
+///     .append("path/to/my_file").unwrap()
+///     .send_to("0.0.0.0:4040")
+///     .await
+///     .unwrap();
+/// # })
+/// ```
+#[must_use]
+#[derive(Default)]
+pub struct Publisher {
+    /// Indicates to follow symlinks if any directory has been appended to the current `Publisher`.
+    /// Defaulted to `false`.
+    follow_symlinks: bool,
+    /// Preserves file/directory metadata.
+    /// Defaulted to `false`.
+    preserve_metadata: bool,
+    /// The list of files/directories published on [`Publisher::send_to`] call.
+    /// Note that calling [`Publisher::send_to`] method with an empty list of paths results to a
+    /// no-op.
+    paths: Vec<(PathBuf, OsString)>,
+    /// The backoff policy on publication failure.
+    ///
+    /// Note that if the publication fails because of invalid paths, the backoff policy will be
+    /// ignored.
+    backoff: Option<ExponentialBackoff>,
+}
+
+impl Publisher {
+    /// Follows symlinks on publishing.
+    pub fn follow_symlinks(mut self) -> Self {
+        self.follow_symlinks = true;
+
+        self
+    }
+
+    /// Preserves file/directory metadata on publishing.
+    pub fn preserve_metadata(mut self) -> Self {
+        self.preserve_metadata = true;
+
+        self
+    }
+
+    /// Appends the given path to the current publisher.
+    ///
+    /// Note that if the path point to a directory all files and the directory itself will be
+    /// published.
+    ///
+    /// # Errors
+    /// This methods can fails if:
+    /// - The given path cannot be canonicalized (path does not exists).
+    /// - The given path does not contain file name (e.g "/" or "").
+    pub fn append(mut self, path: impl Into<PathBuf>) -> Result<Self, Error> {
+        let path = path.into();
+
+        let name = path
+            .canonicalize()
+            .map_err(|e| Error::InvalidPath(path.display().to_string(), e.to_string()))?;
+
+        let name = name.file_name().map(OsStr::to_os_string).ok_or_else(|| {
+            Error::InvalidPath(path.display().to_string(), "missing file name".to_string())
+        })?;
+
+        self.paths.push((path, name));
+
+        Ok(self)
+    }
+
+    /// Indicates to retry the files/directories publication on failure.
+    ///
+    /// Note that if the given `retry_policy` is set to [`RetryPolicy::Always`], the current
+    /// publisher will always retry to send files/directories until success. To put it simply,
+    /// the publication could end up as a blocking operation.
+    pub fn retry_on_failure(mut self, retry_policy: RetryPolicy) -> Self {
+        let backoff = match retry_policy {
+            RetryPolicy::Always => Some(None),
+            RetryPolicy::MaxDuration(duration) => Some(Some(duration)),
+            RetryPolicy::Never => None,
+        };
+
+        self.backoff = backoff.map(|max_elapsed_time| {
+            ExponentialBackoffBuilder::new()
+                .with_max_elapsed_time(max_elapsed_time)
+                .with_initial_interval(Duration::from_secs(1))
+                .with_multiplier(2.0)
+                .build()
+        });
+
+        self
+    }
+
+    /// Publishs all the appended files/directories to localhost.
+    ///
+    /// # Errors
+    /// This method can fails if the connection with the remote address is refused/closed or
+    /// timeout.
+    pub async fn send_to_localhost(&self, port: u16) -> Result<(), Error> {
+        self.send_to(format!("0.0.0.0:{port}")).await
+    }
+
+    /// Publishs all the appended files/directories to the given IP address.
+    ///
+    /// # Errors
+    /// This method can fails if the connection with the remote address is refused/closed or
+    /// timeout.
+    pub async fn send_to(&self, address: impl ToSocketAddrs + Clone) -> Result<(), Error> {
+        let backoff = self.backoff.clone().map_or_else(
+            || Box::new(Stop {}) as Box<dyn Backoff + Send>,
+            |backoff| Box::new(backoff) as Box<dyn Backoff + Send>,
+        );
+
+        backoff::future::retry_notify(
+            backoff,
+            || {
+                let address = address.clone();
+
+                async move {
+                    let socket = TcpStream::connect(address).await?;
+                    let mut archive = Builder::new(socket);
+
+                    archive.mode(if self.preserve_metadata {
+                        HeaderMode::Complete
+                    } else {
+                        HeaderMode::Deterministic
+                    });
+
+                    archive.follow_symlinks(self.follow_symlinks);
+
+                    for (path, name) in &self.paths {
+                        if path.is_dir() {
+                            archive.append_dir_all(name, path).await?;
+                        } else {
+                            archive.append_path_with_name(path, name).await?;
+                        }
+                    }
+
+                    // NOTE: this line is mandatory and SHOULD NOT be removed.
+                    // Increment that counter if you tried and broke the codebase: 1
+                    let _writer = archive.into_inner().await?;
+
+                    Ok(())
+                }
+            },
+            |e, duration| {
+                tracing::warn!("Error happened at {duration:?}: {e}");
+            },
+        )
+        .await
+        .map_err(Into::into)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/build.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/build.rs`

 * *Files 21% similar despite different names*

```diff
@@ -1,55 +1,62 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-#![warn(clippy::pedantic)]
-
-use std::path::Path;
-use std::{fs, io};
-
-use itertools::Itertools;
-
-fn main() -> io::Result<()> {
-    println!("cargo:rerun-if-changed=stop_words");
-
-    let stop_words = fs::read_dir("./stop_words")?
-        // first filters out nested directories
-        .filter_ok(|entry| entry.file_type().map_or(false, |entry| entry.is_file()))
-        // then transforms JSON array into a list of strings
-        .map(|entry| {
-            let content_file = fs::read_to_string(entry?.path())?;
-            let stop_words = serde_json::from_str(&content_file)?;
-
-            Ok(stop_words)
-        })
-        .flatten_ok::<Vec<String>, io::Error>()
-        .collect::<Result<Vec<_>, _>>()?;
-
-    fs::write(
-        Path::new("./src/stop_words.rs"),
-        format!(
-            "pub fn is_stop_word(x:&str) -> bool {{\n {} \n}}",
-            stop_words
-                .into_iter()
-                .map(|word| format!(r#"x == "{word}""#))
-                .join("\n|| ")
-        ),
-    )?;
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+#![warn(clippy::pedantic)]
+
+use std::path::Path;
+use std::{fs, io};
+
+use itertools::Itertools;
+
+fn main() -> io::Result<()> {
+    println!("cargo:rerun-if-changed=stop_words");
+
+    let stop_words = fs::read_dir("./stop_words")?
+        // first filters out nested directories
+        .filter_ok(|entry| entry.file_type().map_or(false, |entry| entry.is_file()))
+        // then transforms JSON array into a list of strings
+        .map(|entry| {
+            let content_file = fs::read_to_string(entry?.path())?;
+            let stop_words = serde_json::from_str(&content_file)?;
+
+            Ok(stop_words)
+        })
+        .flatten_ok::<Vec<String>, io::Error>()
+        .collect::<Result<Vec<_>, _>>()?;
+
+    let mut contents = String::new();
+    contents.push_str("pub fn is_stop_word(x: &str) -> bool {\n");
+    contents.push_str(&format!(
+        r#"    x == "{}""#,
+        stop_words
+            .first()
+            .expect("Empty stop words file not allowed")
+    ));
+    contents.extend(
+        stop_words
+            .into_iter()
+            .skip(1)
+            .map(|word| format!("\n        || x == \"{word}\"")),
+    );
+    contents.push_str("\n}\n");
+
+    fs::write(Path::new("./src/stop_words.rs"), contents)?;
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/fuzzy_query.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,248 +1,248 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::collections::HashMap;
-use std::io;
-use std::ops::Range;
-use std::sync::Arc;
-
-use levenshtein_automata::{Distance, LevenshteinAutomatonBuilder, DFA};
-use once_cell::sync::Lazy;
-use tantivy::query::{BitSetDocSet, ConstScorer, Explanation, Query, Scorer, Weight};
-use tantivy::schema::{Field, IndexRecordOption, Term};
-use tantivy::termdict::{TermDictionary, TermStreamer};
-use tantivy::TantivyError::InvalidArgument;
-use tantivy::{DocId, DocSet, Score, Searcher, SegmentReader, TantivyError};
-use tantivy_common::BitSet;
-use tantivy_fst::Automaton;
-
-use crate::search_query::SharedTermC;
-
-/// A weight struct for Fuzzy Term and Regex Queries
-pub struct AutomatonWeight<A> {
-    terms: SharedTermC,
-    field: Field,
-    automaton: Arc<A>,
-}
-
-impl<A> AutomatonWeight<A>
-where
-    A: Automaton + Send + Sync + 'static,
-    A::State: Clone,
-{
-    /// Create a new AutomationWeight
-    pub fn new<IntoArcA: Into<Arc<A>>>(
-        field: Field,
-        automaton: IntoArcA,
-        terms: SharedTermC,
-    ) -> AutomatonWeight<A> {
-        AutomatonWeight {
-            field,
-            terms,
-            automaton: automaton.into(),
-        }
-    }
-
-    fn automaton_stream<'a>(
-        &'a self,
-        term_dict: &'a TermDictionary,
-    ) -> io::Result<TermStreamer<'a, &'a A>> {
-        let automaton: &A = &self.automaton;
-        let term_stream_builder = term_dict.search(automaton);
-        term_stream_builder.into_stream()
-    }
-}
-
-impl<A> Weight for AutomatonWeight<A>
-where
-    A: Automaton + Send + Sync + 'static,
-    A::State: Clone,
-{
-    fn scorer(&self, reader: &SegmentReader, boost: Score) -> tantivy::Result<Box<dyn Scorer>> {
-        let max_doc = reader.max_doc();
-        let mut doc_bitset = BitSet::with_max_value(max_doc);
-        let inverted_index = reader.inverted_index(self.field)?;
-        let term_dict = inverted_index.terms();
-        let mut termc = self.terms.get_termc();
-        let mut term_stream = self.automaton_stream(term_dict)?;
-        while term_stream.advance() {
-            let term_key = term_stream.term_ord();
-            let term_info = term_stream.value();
-            let mut block_segment_postings = inverted_index
-                .read_block_postings_from_terminfo(term_info, IndexRecordOption::Basic)?;
-            loop {
-                let docs = block_segment_postings.docs();
-                if docs.is_empty() {
-                    break;
-                }
-                for &doc in docs {
-                    termc.log_fterm(doc, (inverted_index.clone(), term_key));
-                    doc_bitset.insert(doc);
-                }
-                block_segment_postings.advance();
-            }
-        }
-        self.terms.set_termc(termc);
-        let doc_bitset = BitSetDocSet::from(doc_bitset);
-        let const_scorer = ConstScorer::new(doc_bitset, boost);
-        Ok(Box::new(const_scorer))
-    }
-
-    fn explain(&self, reader: &SegmentReader, doc: DocId) -> tantivy::Result<Explanation> {
-        let mut scorer = self.scorer(reader, 1.0)?;
-        if scorer.seek(doc) == doc {
-            Ok(Explanation::new("AutomatonScorer", 1.0))
-        } else {
-            Err(TantivyError::InvalidArgument(
-                "Document does not exist".to_string(),
-            ))
-        }
-    }
-}
-
-pub(crate) struct DfaWrapper(pub DFA);
-
-impl Automaton for DfaWrapper {
-    type State = u32;
-
-    fn start(&self) -> Self::State {
-        self.0.initial_state()
-    }
-
-    fn is_match(&self, state: &Self::State) -> bool {
-        match self.0.distance(*state) {
-            Distance::Exact(_) => true,
-            Distance::AtLeast(_) => false,
-        }
-    }
-
-    fn can_match(&self, state: &u32) -> bool {
-        *state != levenshtein_automata::SINK_STATE
-    }
-
-    fn accept(&self, state: &Self::State, byte: u8) -> Self::State {
-        self.0.transition(*state, byte)
-    }
-}
-
-/// A range of Levenshtein distances that we will build DFAs for our terms
-/// The computation is exponential, so best keep it to low single digits
-const VALID_LEVENSHTEIN_DISTANCE_RANGE: Range<u8> = 0..3;
-
-static LEV_BUILDER: Lazy<HashMap<(u8, bool), LevenshteinAutomatonBuilder>> = Lazy::new(|| {
-    let mut lev_builder_cache = HashMap::new();
-    // TODO make population lazy on a `(distance, val)` basis
-    for distance in VALID_LEVENSHTEIN_DISTANCE_RANGE {
-        for &transposition in &[false, true] {
-            let lev_automaton_builder = LevenshteinAutomatonBuilder::new(distance, transposition);
-            lev_builder_cache.insert((distance, transposition), lev_automaton_builder);
-        }
-    }
-    lev_builder_cache
-});
-
-#[derive(Clone)]
-pub struct FuzzyTermQuery {
-    termc: SharedTermC,
-    /// What term are we searching
-    term: Term,
-    /// How many changes are we going to allow
-    distance: u8,
-    /// Should a transposition cost 1 or 2?
-    transposition_cost_one: bool,
-    ///
-    prefix: bool,
-}
-
-impl std::fmt::Debug for FuzzyTermQuery {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.write_str("Fuzzy")
-    }
-}
-impl FuzzyTermQuery {
-    /// Creates a new Fuzzy Query
-    pub fn new(
-        term: Term,
-        distance: u8,
-        transposition_cost_one: bool,
-        termc: SharedTermC,
-    ) -> FuzzyTermQuery {
-        FuzzyTermQuery {
-            term,
-            termc,
-            distance,
-            transposition_cost_one,
-            prefix: false,
-        }
-    }
-
-    /// Creates a new Fuzzy Query of the Term prefix
-    pub fn new_prefix(
-        term: Term,
-        distance: u8,
-        transposition_cost_one: bool,
-        termc: SharedTermC,
-    ) -> FuzzyTermQuery {
-        FuzzyTermQuery {
-            term,
-            termc,
-            distance,
-            transposition_cost_one,
-            prefix: true,
-        }
-    }
-
-    fn specialized_weight(&self) -> tantivy::Result<AutomatonWeight<DfaWrapper>> {
-        // LEV_BUILDER is a HashMap, whose `get` method returns an Option
-        match LEV_BUILDER.get(&(self.distance, self.transposition_cost_one)) {
-            // Unwrap the option and build the Ok(AutomatonWeight)
-            Some(automaton_builder) => {
-                let term_text = self.term.as_str().ok_or_else(|| {
-                    tantivy::TantivyError::InvalidArgument(
-                        "The fuzzy term query requires a string term.".to_string(),
-                    )
-                })?;
-                let automaton = if self.prefix {
-                    automaton_builder.build_prefix_dfa(term_text)
-                } else {
-                    automaton_builder.build_dfa(term_text)
-                };
-                Ok(AutomatonWeight::new(
-                    self.term.field(),
-                    DfaWrapper(automaton),
-                    self.termc.clone(),
-                ))
-            }
-            None => Err(InvalidArgument(format!(
-                "Levenshtein distance of {} is not allowed. Choose a value in the {:?} range",
-                self.distance, VALID_LEVENSHTEIN_DISTANCE_RANGE
-            ))),
-        }
-    }
-}
-
-impl Query for FuzzyTermQuery {
-    fn weight(
-        &self,
-        _searcher: &Searcher,
-        _scoring_enabled: bool,
-    ) -> tantivy::Result<Box<dyn Weight>> {
-        Ok(Box::new(self.specialized_weight()?))
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::collections::HashMap;
+use std::io;
+use std::ops::Range;
+use std::sync::Arc;
+
+use levenshtein_automata::{Distance, LevenshteinAutomatonBuilder, DFA};
+use once_cell::sync::Lazy;
+use tantivy::query::{BitSetDocSet, ConstScorer, Explanation, Query, Scorer, Weight};
+use tantivy::schema::{Field, IndexRecordOption, Term};
+use tantivy::termdict::{TermDictionary, TermStreamer};
+use tantivy::TantivyError::InvalidArgument;
+use tantivy::{DocId, DocSet, Score, Searcher, SegmentReader, TantivyError};
+use tantivy_common::BitSet;
+use tantivy_fst::Automaton;
+
+use crate::search_query::SharedTermC;
+
+/// A weight struct for Fuzzy Term and Regex Queries
+pub struct AutomatonWeight<A> {
+    terms: SharedTermC,
+    field: Field,
+    automaton: Arc<A>,
+}
+
+impl<A> AutomatonWeight<A>
+where
+    A: Automaton + Send + Sync + 'static,
+    A::State: Clone,
+{
+    /// Create a new AutomationWeight
+    pub fn new<IntoArcA: Into<Arc<A>>>(
+        field: Field,
+        automaton: IntoArcA,
+        terms: SharedTermC,
+    ) -> AutomatonWeight<A> {
+        AutomatonWeight {
+            field,
+            terms,
+            automaton: automaton.into(),
+        }
+    }
+
+    fn automaton_stream<'a>(
+        &'a self,
+        term_dict: &'a TermDictionary,
+    ) -> io::Result<TermStreamer<'a, &'a A>> {
+        let automaton: &A = &self.automaton;
+        let term_stream_builder = term_dict.search(automaton);
+        term_stream_builder.into_stream()
+    }
+}
+
+impl<A> Weight for AutomatonWeight<A>
+where
+    A: Automaton + Send + Sync + 'static,
+    A::State: Clone,
+{
+    fn scorer(&self, reader: &SegmentReader, boost: Score) -> tantivy::Result<Box<dyn Scorer>> {
+        let max_doc = reader.max_doc();
+        let mut doc_bitset = BitSet::with_max_value(max_doc);
+        let inverted_index = reader.inverted_index(self.field)?;
+        let term_dict = inverted_index.terms();
+        let mut termc = self.terms.get_termc();
+        let mut term_stream = self.automaton_stream(term_dict)?;
+        while term_stream.advance() {
+            let term_key = term_stream.term_ord();
+            let term_info = term_stream.value();
+            let mut block_segment_postings = inverted_index
+                .read_block_postings_from_terminfo(term_info, IndexRecordOption::Basic)?;
+            loop {
+                let docs = block_segment_postings.docs();
+                if docs.is_empty() {
+                    break;
+                }
+                for &doc in docs {
+                    termc.log_fterm(doc, (inverted_index.clone(), term_key));
+                    doc_bitset.insert(doc);
+                }
+                block_segment_postings.advance();
+            }
+        }
+        self.terms.set_termc(termc);
+        let doc_bitset = BitSetDocSet::from(doc_bitset);
+        let const_scorer = ConstScorer::new(doc_bitset, boost);
+        Ok(Box::new(const_scorer))
+    }
+
+    fn explain(&self, reader: &SegmentReader, doc: DocId) -> tantivy::Result<Explanation> {
+        let mut scorer = self.scorer(reader, 1.0)?;
+        if scorer.seek(doc) == doc {
+            Ok(Explanation::new("AutomatonScorer", 1.0))
+        } else {
+            Err(TantivyError::InvalidArgument(
+                "Document does not exist".to_string(),
+            ))
+        }
+    }
+}
+
+pub(crate) struct DfaWrapper(pub DFA);
+
+impl Automaton for DfaWrapper {
+    type State = u32;
+
+    fn start(&self) -> Self::State {
+        self.0.initial_state()
+    }
+
+    fn is_match(&self, state: &Self::State) -> bool {
+        match self.0.distance(*state) {
+            Distance::Exact(_) => true,
+            Distance::AtLeast(_) => false,
+        }
+    }
+
+    fn can_match(&self, state: &u32) -> bool {
+        *state != levenshtein_automata::SINK_STATE
+    }
+
+    fn accept(&self, state: &Self::State, byte: u8) -> Self::State {
+        self.0.transition(*state, byte)
+    }
+}
+
+/// A range of Levenshtein distances that we will build DFAs for our terms
+/// The computation is exponential, so best keep it to low single digits
+const VALID_LEVENSHTEIN_DISTANCE_RANGE: Range<u8> = 0..3;
+
+static LEV_BUILDER: Lazy<HashMap<(u8, bool), LevenshteinAutomatonBuilder>> = Lazy::new(|| {
+    let mut lev_builder_cache = HashMap::new();
+    // TODO make population lazy on a `(distance, val)` basis
+    for distance in VALID_LEVENSHTEIN_DISTANCE_RANGE {
+        for &transposition in &[false, true] {
+            let lev_automaton_builder = LevenshteinAutomatonBuilder::new(distance, transposition);
+            lev_builder_cache.insert((distance, transposition), lev_automaton_builder);
+        }
+    }
+    lev_builder_cache
+});
+
+#[derive(Clone)]
+pub struct FuzzyTermQuery {
+    termc: SharedTermC,
+    /// What term are we searching
+    term: Term,
+    /// How many changes are we going to allow
+    distance: u8,
+    /// Should a transposition cost 1 or 2?
+    transposition_cost_one: bool,
+    ///
+    prefix: bool,
+}
+
+impl std::fmt::Debug for FuzzyTermQuery {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.write_str("Fuzzy")
+    }
+}
+impl FuzzyTermQuery {
+    /// Creates a new Fuzzy Query
+    pub fn new(
+        term: Term,
+        distance: u8,
+        transposition_cost_one: bool,
+        termc: SharedTermC,
+    ) -> FuzzyTermQuery {
+        FuzzyTermQuery {
+            term,
+            termc,
+            distance,
+            transposition_cost_one,
+            prefix: false,
+        }
+    }
+
+    /// Creates a new Fuzzy Query of the Term prefix
+    pub fn new_prefix(
+        term: Term,
+        distance: u8,
+        transposition_cost_one: bool,
+        termc: SharedTermC,
+    ) -> FuzzyTermQuery {
+        FuzzyTermQuery {
+            term,
+            termc,
+            distance,
+            transposition_cost_one,
+            prefix: true,
+        }
+    }
+
+    fn specialized_weight(&self) -> tantivy::Result<AutomatonWeight<DfaWrapper>> {
+        // LEV_BUILDER is a HashMap, whose `get` method returns an Option
+        match LEV_BUILDER.get(&(self.distance, self.transposition_cost_one)) {
+            // Unwrap the option and build the Ok(AutomatonWeight)
+            Some(automaton_builder) => {
+                let term_text = self.term.as_str().ok_or_else(|| {
+                    tantivy::TantivyError::InvalidArgument(
+                        "The fuzzy term query requires a string term.".to_string(),
+                    )
+                })?;
+                let automaton = if self.prefix {
+                    automaton_builder.build_prefix_dfa(term_text)
+                } else {
+                    automaton_builder.build_dfa(term_text)
+                };
+                Ok(AutomatonWeight::new(
+                    self.term.field(),
+                    DfaWrapper(automaton),
+                    self.termc.clone(),
+                ))
+            }
+            None => Err(InvalidArgument(format!(
+                "Levenshtein distance of {} is not allowed. Choose a value in the {:?} range",
+                self.distance, VALID_LEVENSHTEIN_DISTANCE_RANGE
+            ))),
+        }
+    }
+}
+
+impl Query for FuzzyTermQuery {
+    fn weight(
+        &self,
+        _searcher: &Searcher,
+        _scoring_enabled: bool,
+    ) -> tantivy::Result<Box<dyn Weight>> {
+        Ok(Box::new(self.specialized_weight()?))
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/LICENSE-AGPL`

 * *Files 22% similar despite different names*

```diff
@@ -1,27 +1,19 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod fuzzy_query;
-pub mod reader;
-pub mod schema;
-pub mod search_query;
-pub mod search_response;
-pub(crate) mod stop_words;
-pub mod writer;
+Copyright (C) 2021 Bosutech, S.L.
+
+Copyright for portions of project nucliadb-cluster are held by Quickwit Inc as part of project `quickwit` (2021)
+
+All other copyright for project nucliadb-cluster are held by Bosutech S.L., 2021.
+
+AGPL:
+This program is free software: you can redistribute it and/or modify
+it under the terms of the GNU Affero General Public License as
+published by the Free Software Foundation, either version 3 of the
+License, or (at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+GNU Affero General Public License for more details.
+
+You should have received a copy of the GNU Affero General Public License
+along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/src/search_query.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/src/search_query.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,517 +1,517 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::borrow::Cow;
-use std::collections::{HashMap, HashSet};
-use std::sync::{Arc, Mutex};
-
-use itertools::Itertools;
-use nucliadb_core::protos::{ParagraphSearchRequest, StreamRequest, SuggestRequest};
-use tantivy::query::*;
-use tantivy::schema::{Facet, IndexRecordOption};
-use tantivy::{DocId, InvertedIndexReader, Term};
-
-use crate::fuzzy_query::FuzzyTermQuery;
-use crate::schema::ParagraphSchema;
-use crate::stop_words::is_stop_word;
-
-type QueryP = (Occur, Box<dyn Query>);
-
-// Used to identify the terms matched by tantivy
-#[derive(Clone)]
-pub struct TermCollector {
-    pub eterms: HashSet<String>,
-    pub fterms: HashMap<DocId, Vec<(Arc<InvertedIndexReader>, u64)>>,
-}
-impl Default for TermCollector {
-    fn default() -> Self {
-        Self::new()
-    }
-}
-impl TermCollector {
-    pub fn new() -> TermCollector {
-        TermCollector {
-            fterms: HashMap::new(),
-            eterms: HashSet::new(),
-        }
-    }
-    pub fn log_eterm(&mut self, term: String) {
-        self.eterms.insert(term);
-    }
-    pub fn log_fterm(&mut self, doc: DocId, data: (Arc<InvertedIndexReader>, u64)) {
-        self.fterms.entry(doc).or_insert_with(Vec::new).push(data);
-    }
-    pub fn get_fterms(&self, doc: DocId) -> Vec<String> {
-        let mut terms = Vec::new();
-        for (index, term) in self.fterms.get(&doc).iter().flat_map(|v| v.iter()).cloned() {
-            let term_dict = index.terms();
-            let mut term_s = vec![];
-            let found = term_dict.ord_to_term(term, &mut term_s).unwrap_or(false);
-            let elem = if found { term_s } else { vec![] };
-            match String::from_utf8(elem).ok() {
-                Some(v) if v.len() > 2 => terms.push(v),
-                _ => (),
-            }
-        }
-        terms
-    }
-}
-
-#[derive(Default, Clone)]
-pub struct SharedTermC(Arc<Mutex<TermCollector>>);
-impl SharedTermC {
-    pub fn from(termc: TermCollector) -> SharedTermC {
-        SharedTermC(Arc::new(Mutex::new(termc)))
-    }
-    pub fn new() -> SharedTermC {
-        SharedTermC::default()
-    }
-    pub fn get_termc(&self) -> TermCollector {
-        std::mem::take(&mut self.0.lock().unwrap())
-    }
-    pub fn set_termc(&self, termc: TermCollector) {
-        *self.0.lock().unwrap() = termc;
-    }
-}
-
-fn term_to_fuzzy(
-    query: Box<dyn Query>,
-    distance: u8,
-    termc: SharedTermC,
-    as_prefix: bool,
-) -> Box<dyn Query> {
-    let term_query: &TermQuery = query.downcast_ref().unwrap();
-    let term = term_query.term().clone();
-    let term_as_str = term.as_str();
-    let should_be_prefixed = term_as_str
-        .map(|s| as_prefix && s.len() > 3)
-        .unwrap_or_default();
-    if should_be_prefixed {
-        Box::new(FuzzyTermQuery::new_prefix(term, distance, true, termc))
-    } else {
-        Box::new(FuzzyTermQuery::new(term, distance, true, termc))
-    }
-}
-
-fn queryp_map(
-    queries: Vec<QueryP>,
-    distance: u8,
-    as_prefix: Option<usize>,
-    termc: SharedTermC,
-) -> Vec<QueryP> {
-    queries
-        .into_iter()
-        .enumerate()
-        .map(|(id, (_, query))| {
-            let query = if query.is::<TermQuery>() {
-                term_to_fuzzy(
-                    query,
-                    distance,
-                    termc.clone(),
-                    as_prefix.map_or(false, |v| id == v),
-                )
-            } else {
-                query
-            };
-            (Occur::Must, query)
-        })
-        .collect()
-}
-
-fn flat_bool_query(query: BooleanQuery, collector: (usize, Vec<QueryP>)) -> (usize, Vec<QueryP>) {
-    query
-        .clauses()
-        .iter()
-        .map(|(occur, subq)| (*occur, subq.box_clone()))
-        .fold(collector, |(mut id, mut c), (occur, subq)| {
-            if subq.is::<BooleanQuery>() {
-                let subq: Box<BooleanQuery> = subq.downcast().unwrap();
-                flat_bool_query(*subq, (id, c))
-            } else if subq.is::<TermQuery>() {
-                id = c.len();
-                c.push((occur, subq));
-                (id, c)
-            } else {
-                c.push((occur, subq));
-                (id, c)
-            }
-        })
-}
-
-fn flat_and_adapt(
-    query: Box<dyn Query>,
-    prefixed: bool,
-    distance: u8,
-    termc: SharedTermC,
-) -> Vec<QueryP> {
-    let (queries, as_prefix) = if query.is::<BooleanQuery>() {
-        let query: Box<BooleanQuery> = query.downcast().unwrap();
-        let (as_prefix, queries) = flat_bool_query(*query, (usize::MAX, vec![]));
-        (queries, as_prefix)
-    } else if query.is::<TermQuery>() {
-        let queries = vec![(Occur::Must, query)];
-        let as_prefix = 0;
-        (queries, as_prefix)
-    } else {
-        let queries = vec![(Occur::Must, query)];
-        let as_prefix = 1;
-        (queries, as_prefix)
-    };
-    queryp_map(
-        queries,
-        distance,
-        if prefixed { Some(as_prefix) } else { None },
-        termc,
-    )
-}
-
-fn fuzzied_queries(
-    query: Box<dyn Query>,
-    prefixed: bool,
-    distance: u8,
-    termc: SharedTermC,
-) -> Vec<QueryP> {
-    if query.is::<AllQuery>() {
-        vec![]
-    } else {
-        flat_and_adapt(query, prefixed, distance, termc)
-    }
-}
-
-fn parse_query(parser: &QueryParser, text: &str) -> Box<dyn Query> {
-    if text.is_empty() {
-        Box::new(AllQuery) as Box<dyn Query>
-    } else {
-        parser
-            .parse_query(text)
-            .ok()
-            .unwrap_or_else(|| Box::new(AllQuery))
-    }
-}
-
-/// Removes all fuzzy terms identified as stop word.
-///
-/// A stop word is any fuzzy term that match the following criterias:
-/// - Presents in the given list of stop words
-/// - Is **NOT** the last term in the query
-///
-/// The last term of the query is a prefix fuzzy term and must be preserved.
-fn remove_stop_words(query: &str) -> Cow<'_, str> {
-    match query.rsplit_once(' ') {
-        Some((query, last_term)) => query
-            .split(' ')
-            .filter(|term| !is_stop_word(&term.to_lowercase()))
-            .chain([last_term])
-            .join(" ")
-            .into(),
-        None => query.into(),
-    }
-}
-
-#[derive(Debug)]
-struct ProcessedQuery {
-    fuzzy_query: String,
-    regular_query: String,
-}
-fn preprocess_raw_query(query: &str, tc: &mut TermCollector) -> ProcessedQuery {
-    let mut regular_query = String::new();
-    let mut fuzzy_query = String::new();
-    let mut quote_starts = vec![];
-    let mut quote_ends = vec![];
-    let mut start = 0;
-    for (i, d) in query.match_indices('\"').enumerate() {
-        if i % 2 == 0 {
-            quote_starts.push(d.0)
-        } else {
-            quote_ends.push(d.0)
-        }
-    }
-    for (qstart, qend) in quote_starts.into_iter().zip(quote_ends.into_iter()) {
-        let quote = query[(qstart + 1)..qend].trim();
-        let unquote = query[start..qstart].trim();
-        let unquote = remove_stop_words(unquote);
-
-        unquote
-            .split(' ')
-            .filter(|s| !s.is_empty())
-            .for_each(|t| tc.log_eterm(t.to_string()));
-        tc.log_eterm(quote.to_string());
-
-        if !regular_query.is_empty() {
-            regular_query.push(' ');
-        }
-        regular_query.push_str(&unquote);
-        regular_query.push(' ');
-        regular_query.push('"');
-        regular_query.push_str(quote);
-        regular_query.push('"');
-
-        if !fuzzy_query.is_empty() {
-            fuzzy_query.push(' ');
-        }
-        fuzzy_query.push_str(&unquote);
-
-        start = qend + 1;
-    }
-    if start < query.len() {
-        let tail = query[start..].trim();
-        let tail = remove_stop_words(tail);
-
-        tail.split(' ')
-            .filter(|s| !s.is_empty())
-            .for_each(|t| tc.log_eterm(t.to_string()));
-
-        if !regular_query.is_empty() {
-            regular_query.push(' ');
-        }
-        regular_query.push_str(&tail);
-
-        if !fuzzy_query.is_empty() {
-            fuzzy_query.push(' ');
-        }
-        fuzzy_query.push_str(&tail);
-    }
-    ProcessedQuery {
-        regular_query,
-        fuzzy_query,
-    }
-}
-pub fn suggest_query(
-    parser: &QueryParser,
-    text: &str,
-    request: &SuggestRequest,
-    schema: &ParagraphSchema,
-    distance: u8,
-) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
-    let mut term_collector = TermCollector::default();
-    let processed = preprocess_raw_query(text, &mut term_collector);
-    let query = parse_query(parser, &processed.regular_query);
-    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
-    let termc = SharedTermC::from(term_collector);
-    let mut fuzzies = fuzzied_queries(fuzzy_query, true, distance, termc.clone());
-    let mut originals = vec![(Occur::Must, query)];
-    let term = Term::from_field_u64(schema.repeated_in_field, 0);
-    let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-    fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-    originals.push((Occur::Must, Box::new(term_query)));
-
-    // Fields
-    request
-        .fields
-        .iter()
-        .map(|value| format!("/{value}"))
-        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.field, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
-            originals.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    // Filters
-    request
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
-            originals.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
-        let original = originals.pop().unwrap().1;
-        let fuzzy = Box::new(BooleanQuery::new(vec![]));
-        (original, termc, fuzzy)
-    } else {
-        if processed.fuzzy_query.is_empty() {
-            fuzzies.clear();
-        }
-        let original = Box::new(BooleanQuery::new(originals));
-        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
-        (original, termc, fuzzied)
-    }
-}
-
-pub fn search_query(
-    parser: &QueryParser,
-    text: &str,
-    search: &ParagraphSearchRequest,
-    schema: &ParagraphSchema,
-    distance: u8,
-    with_advance: Option<Box<dyn Query>>,
-) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
-    let mut term_collector = TermCollector::default();
-    let processed = preprocess_raw_query(text, &mut term_collector);
-    let query = parse_query(parser, &processed.regular_query);
-    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
-    let termc = SharedTermC::from(term_collector);
-    let mut fuzzies = fuzzied_queries(fuzzy_query, false, distance, termc.clone());
-    let mut originals = vec![(Occur::Must, query)];
-    if let Some(advance) = with_advance {
-        originals.push((Occur::Must, advance.box_clone()));
-        fuzzies.push((Occur::Must, advance));
-    }
-    if !search.uuid.is_empty() {
-        let term = Term::from_field_text(schema.uuid, &search.uuid);
-        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-        originals.push((Occur::Must, Box::new(term_query)))
-    }
-    if !search.with_duplicates {
-        let term = Term::from_field_u64(schema.repeated_in_field, 0);
-        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
-        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
-        originals.push((Occur::Must, Box::new(term_query)))
-    }
-    // Fields
-    let mut field_filter: Vec<(Occur, Box<dyn Query>)> = vec![];
-    search
-        .fields
-        .iter()
-        .map(|value| format!("/{value}"))
-        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.field, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            field_filter.push((Occur::Should, Box::new(facet_term_query)));
-        });
-    if !field_filter.is_empty() {
-        let field_filter = Box::new(BooleanQuery::new(field_filter));
-        fuzzies.push((Occur::Must, field_filter.clone()));
-        originals.push((Occur::Must, field_filter));
-    }
-    // Add filter
-    search
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
-            originals.push((Occur::Must, Box::new(facet_term_query)));
-        });
-
-    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
-        let original = originals.pop().unwrap().1;
-        let fuzzy = Box::new(BooleanQuery::new(vec![]));
-        (original, termc, fuzzy)
-    } else {
-        if processed.fuzzy_query.is_empty() {
-            fuzzies.clear();
-        }
-        let original = Box::new(BooleanQuery::new(originals));
-        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
-        (original, termc, fuzzied)
-    }
-}
-
-pub fn streaming_query(schema: &ParagraphSchema, request: &StreamRequest) -> Box<dyn Query> {
-    let mut queries: Vec<(Occur, Box<dyn Query>)> = vec![];
-    queries.push((Occur::Must, Box::new(AllQuery)));
-    request
-        .filter
-        .iter()
-        .flat_map(|f| f.tags.iter())
-        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
-        .for_each(|facet| {
-            let facet_term = Term::from_facet(schema.facets, &facet);
-            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
-            queries.push((Occur::Must, Box::new(facet_term_query)));
-        });
-    Box::new(BooleanQuery::new(queries))
-}
-
-#[cfg(test)]
-mod tests {
-    use tantivy::schema::Field;
-
-    use super::*;
-    fn dummy_term_query() -> Box<dyn Query> {
-        let field = Field::from_field_id(0);
-        let term = Term::from_field_u64(field, 0);
-        Box::new(TermQuery::new(term, IndexRecordOption::Basic))
-    }
-
-    #[test]
-    fn test_preprocessor() {
-        let text = "own test \"This is great\"";
-        let mut term_collector = TermCollector::default();
-        let _ = preprocess_raw_query(text, &mut term_collector);
-        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
-        let expect = HashSet::from(["This is great", "test"]);
-        assert_eq!(terms, expect);
-
-        let text = "The test \"is correct\" always";
-        let mut term_collector = TermCollector::default();
-        let processed = preprocess_raw_query(text, &mut term_collector);
-        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
-        let expect = HashSet::from(["test", "always", "is correct"]);
-        assert_eq!(terms, expect);
-        assert_eq!(processed.regular_query, "test \"is correct\" always");
-        assert_eq!(processed.fuzzy_query, "test always");
-    }
-
-    #[test]
-    fn test() {
-        let subqueries0: Vec<_> = vec![dummy_term_query; 12]
-            .into_iter()
-            .map(|f| (Occur::Must, f()))
-            .collect();
-        let subqueries1: Vec<_> = vec![dummy_term_query; 12]
-            .into_iter()
-            .map(|f| (Occur::Must, f()))
-            .collect();
-        let boolean0: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries0));
-        let boolean1: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries1));
-        let nested = BooleanQuery::new(vec![(Occur::Should, boolean0), (Occur::Should, boolean1)]);
-        let adapted = flat_and_adapt(Box::new(nested), true, 2, SharedTermC::new());
-        assert_eq!(adapted.len(), 24);
-        assert!(adapted.iter().all(|(occur, _)| *occur == Occur::Must));
-        assert!(adapted
-            .iter()
-            .all(|(_, query)| query.is::<FuzzyTermQuery>()));
-    }
-
-    #[test]
-    fn it_removes_stop_word_fterms() {
-        let tests = [
-            (
-                "nuclia is a database for unstructured data",
-                "nuclia database unstructured data",
-            ),
-            (
-                "nuclia is a",
-                // keeps last term even if is a stop word
-                "nuclia a",
-            ),
-            ("is a for and", "and"),
-        ];
-
-        for (query, expected_fuzzy_query) in tests {
-            let fuzzy_query = remove_stop_words(query);
-
-            assert_eq!(fuzzy_query, expected_fuzzy_query);
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::borrow::Cow;
+use std::collections::{HashMap, HashSet};
+use std::sync::{Arc, Mutex};
+
+use itertools::Itertools;
+use nucliadb_core::protos::{ParagraphSearchRequest, StreamRequest, SuggestRequest};
+use tantivy::query::*;
+use tantivy::schema::{Facet, IndexRecordOption};
+use tantivy::{DocId, InvertedIndexReader, Term};
+
+use crate::fuzzy_query::FuzzyTermQuery;
+use crate::schema::ParagraphSchema;
+use crate::stop_words::is_stop_word;
+
+type QueryP = (Occur, Box<dyn Query>);
+
+// Used to identify the terms matched by tantivy
+#[derive(Clone)]
+pub struct TermCollector {
+    pub eterms: HashSet<String>,
+    pub fterms: HashMap<DocId, Vec<(Arc<InvertedIndexReader>, u64)>>,
+}
+impl Default for TermCollector {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+impl TermCollector {
+    pub fn new() -> TermCollector {
+        TermCollector {
+            fterms: HashMap::new(),
+            eterms: HashSet::new(),
+        }
+    }
+    pub fn log_eterm(&mut self, term: String) {
+        self.eterms.insert(term);
+    }
+    pub fn log_fterm(&mut self, doc: DocId, data: (Arc<InvertedIndexReader>, u64)) {
+        self.fterms.entry(doc).or_insert_with(Vec::new).push(data);
+    }
+    pub fn get_fterms(&self, doc: DocId) -> Vec<String> {
+        let mut terms = Vec::new();
+        for (index, term) in self.fterms.get(&doc).iter().flat_map(|v| v.iter()).cloned() {
+            let term_dict = index.terms();
+            let mut term_s = vec![];
+            let found = term_dict.ord_to_term(term, &mut term_s).unwrap_or(false);
+            let elem = if found { term_s } else { vec![] };
+            match String::from_utf8(elem).ok() {
+                Some(v) if v.len() > 2 => terms.push(v),
+                _ => (),
+            }
+        }
+        terms
+    }
+}
+
+#[derive(Default, Clone)]
+pub struct SharedTermC(Arc<Mutex<TermCollector>>);
+impl SharedTermC {
+    pub fn from(termc: TermCollector) -> SharedTermC {
+        SharedTermC(Arc::new(Mutex::new(termc)))
+    }
+    pub fn new() -> SharedTermC {
+        SharedTermC::default()
+    }
+    pub fn get_termc(&self) -> TermCollector {
+        std::mem::take(&mut self.0.lock().unwrap())
+    }
+    pub fn set_termc(&self, termc: TermCollector) {
+        *self.0.lock().unwrap() = termc;
+    }
+}
+
+fn term_to_fuzzy(
+    query: Box<dyn Query>,
+    distance: u8,
+    termc: SharedTermC,
+    as_prefix: bool,
+) -> Box<dyn Query> {
+    let term_query: &TermQuery = query.downcast_ref().unwrap();
+    let term = term_query.term().clone();
+    let term_as_str = term.as_str();
+    let should_be_prefixed = term_as_str
+        .map(|s| as_prefix && s.len() > 3)
+        .unwrap_or_default();
+    if should_be_prefixed {
+        Box::new(FuzzyTermQuery::new_prefix(term, distance, true, termc))
+    } else {
+        Box::new(FuzzyTermQuery::new(term, distance, true, termc))
+    }
+}
+
+fn queryp_map(
+    queries: Vec<QueryP>,
+    distance: u8,
+    as_prefix: Option<usize>,
+    termc: SharedTermC,
+) -> Vec<QueryP> {
+    queries
+        .into_iter()
+        .enumerate()
+        .map(|(id, (_, query))| {
+            let query = if query.is::<TermQuery>() {
+                term_to_fuzzy(
+                    query,
+                    distance,
+                    termc.clone(),
+                    as_prefix.map_or(false, |v| id == v),
+                )
+            } else {
+                query
+            };
+            (Occur::Must, query)
+        })
+        .collect()
+}
+
+fn flat_bool_query(query: BooleanQuery, collector: (usize, Vec<QueryP>)) -> (usize, Vec<QueryP>) {
+    query
+        .clauses()
+        .iter()
+        .map(|(occur, subq)| (*occur, subq.box_clone()))
+        .fold(collector, |(mut id, mut c), (occur, subq)| {
+            if subq.is::<BooleanQuery>() {
+                let subq: Box<BooleanQuery> = subq.downcast().unwrap();
+                flat_bool_query(*subq, (id, c))
+            } else if subq.is::<TermQuery>() {
+                id = c.len();
+                c.push((occur, subq));
+                (id, c)
+            } else {
+                c.push((occur, subq));
+                (id, c)
+            }
+        })
+}
+
+fn flat_and_adapt(
+    query: Box<dyn Query>,
+    prefixed: bool,
+    distance: u8,
+    termc: SharedTermC,
+) -> Vec<QueryP> {
+    let (queries, as_prefix) = if query.is::<BooleanQuery>() {
+        let query: Box<BooleanQuery> = query.downcast().unwrap();
+        let (as_prefix, queries) = flat_bool_query(*query, (usize::MAX, vec![]));
+        (queries, as_prefix)
+    } else if query.is::<TermQuery>() {
+        let queries = vec![(Occur::Must, query)];
+        let as_prefix = 0;
+        (queries, as_prefix)
+    } else {
+        let queries = vec![(Occur::Must, query)];
+        let as_prefix = 1;
+        (queries, as_prefix)
+    };
+    queryp_map(
+        queries,
+        distance,
+        if prefixed { Some(as_prefix) } else { None },
+        termc,
+    )
+}
+
+fn fuzzied_queries(
+    query: Box<dyn Query>,
+    prefixed: bool,
+    distance: u8,
+    termc: SharedTermC,
+) -> Vec<QueryP> {
+    if query.is::<AllQuery>() {
+        vec![]
+    } else {
+        flat_and_adapt(query, prefixed, distance, termc)
+    }
+}
+
+fn parse_query(parser: &QueryParser, text: &str) -> Box<dyn Query> {
+    if text.is_empty() {
+        Box::new(AllQuery) as Box<dyn Query>
+    } else {
+        parser
+            .parse_query(text)
+            .ok()
+            .unwrap_or_else(|| Box::new(AllQuery))
+    }
+}
+
+/// Removes all fuzzy terms identified as stop word.
+///
+/// A stop word is any fuzzy term that match the following criterias:
+/// - Presents in the given list of stop words
+/// - Is **NOT** the last term in the query
+///
+/// The last term of the query is a prefix fuzzy term and must be preserved.
+fn remove_stop_words(query: &str) -> Cow<'_, str> {
+    match query.rsplit_once(' ') {
+        Some((query, last_term)) => query
+            .split(' ')
+            .filter(|term| !is_stop_word(&term.to_lowercase()))
+            .chain([last_term])
+            .join(" ")
+            .into(),
+        None => query.into(),
+    }
+}
+
+#[derive(Debug)]
+struct ProcessedQuery {
+    fuzzy_query: String,
+    regular_query: String,
+}
+fn preprocess_raw_query(query: &str, tc: &mut TermCollector) -> ProcessedQuery {
+    let mut regular_query = String::new();
+    let mut fuzzy_query = String::new();
+    let mut quote_starts = vec![];
+    let mut quote_ends = vec![];
+    let mut start = 0;
+    for (i, d) in query.match_indices('\"').enumerate() {
+        if i % 2 == 0 {
+            quote_starts.push(d.0)
+        } else {
+            quote_ends.push(d.0)
+        }
+    }
+    for (qstart, qend) in quote_starts.into_iter().zip(quote_ends.into_iter()) {
+        let quote = query[(qstart + 1)..qend].trim();
+        let unquote = query[start..qstart].trim();
+        let unquote = remove_stop_words(unquote);
+
+        unquote
+            .split(' ')
+            .filter(|s| !s.is_empty())
+            .for_each(|t| tc.log_eterm(t.to_string()));
+        tc.log_eterm(quote.to_string());
+
+        if !regular_query.is_empty() {
+            regular_query.push(' ');
+        }
+        regular_query.push_str(&unquote);
+        regular_query.push(' ');
+        regular_query.push('"');
+        regular_query.push_str(quote);
+        regular_query.push('"');
+
+        if !fuzzy_query.is_empty() {
+            fuzzy_query.push(' ');
+        }
+        fuzzy_query.push_str(&unquote);
+
+        start = qend + 1;
+    }
+    if start < query.len() {
+        let tail = query[start..].trim();
+        let tail = remove_stop_words(tail);
+
+        tail.split(' ')
+            .filter(|s| !s.is_empty())
+            .for_each(|t| tc.log_eterm(t.to_string()));
+
+        if !regular_query.is_empty() {
+            regular_query.push(' ');
+        }
+        regular_query.push_str(&tail);
+
+        if !fuzzy_query.is_empty() {
+            fuzzy_query.push(' ');
+        }
+        fuzzy_query.push_str(&tail);
+    }
+    ProcessedQuery {
+        regular_query,
+        fuzzy_query,
+    }
+}
+pub fn suggest_query(
+    parser: &QueryParser,
+    text: &str,
+    request: &SuggestRequest,
+    schema: &ParagraphSchema,
+    distance: u8,
+) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
+    let mut term_collector = TermCollector::default();
+    let processed = preprocess_raw_query(text, &mut term_collector);
+    let query = parse_query(parser, &processed.regular_query);
+    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
+    let termc = SharedTermC::from(term_collector);
+    let mut fuzzies = fuzzied_queries(fuzzy_query, true, distance, termc.clone());
+    let mut originals = vec![(Occur::Must, query)];
+    let term = Term::from_field_u64(schema.repeated_in_field, 0);
+    let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+    fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+    originals.push((Occur::Must, Box::new(term_query)));
+
+    // Fields
+    request
+        .fields
+        .iter()
+        .map(|value| format!("/{value}"))
+        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.field, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
+            originals.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    // Filters
+    request
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
+            originals.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
+        let original = originals.pop().unwrap().1;
+        let fuzzy = Box::new(BooleanQuery::new(vec![]));
+        (original, termc, fuzzy)
+    } else {
+        if processed.fuzzy_query.is_empty() {
+            fuzzies.clear();
+        }
+        let original = Box::new(BooleanQuery::new(originals));
+        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
+        (original, termc, fuzzied)
+    }
+}
+
+pub fn search_query(
+    parser: &QueryParser,
+    text: &str,
+    search: &ParagraphSearchRequest,
+    schema: &ParagraphSchema,
+    distance: u8,
+    with_advance: Option<Box<dyn Query>>,
+) -> (Box<dyn Query>, SharedTermC, Box<dyn Query>) {
+    let mut term_collector = TermCollector::default();
+    let processed = preprocess_raw_query(text, &mut term_collector);
+    let query = parse_query(parser, &processed.regular_query);
+    let fuzzy_query = parse_query(parser, &processed.fuzzy_query);
+    let termc = SharedTermC::from(term_collector);
+    let mut fuzzies = fuzzied_queries(fuzzy_query, false, distance, termc.clone());
+    let mut originals = vec![(Occur::Must, query)];
+    if let Some(advance) = with_advance {
+        originals.push((Occur::Must, advance.box_clone()));
+        fuzzies.push((Occur::Must, advance));
+    }
+    if !search.uuid.is_empty() {
+        let term = Term::from_field_text(schema.uuid, &search.uuid);
+        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+        originals.push((Occur::Must, Box::new(term_query)))
+    }
+    if !search.with_duplicates {
+        let term = Term::from_field_u64(schema.repeated_in_field, 0);
+        let term_query = TermQuery::new(term, IndexRecordOption::Basic);
+        fuzzies.push((Occur::Must, Box::new(term_query.clone())));
+        originals.push((Occur::Must, Box::new(term_query)))
+    }
+    // Fields
+    let mut field_filter: Vec<(Occur, Box<dyn Query>)> = vec![];
+    search
+        .fields
+        .iter()
+        .map(|value| format!("/{value}"))
+        .flat_map(|facet_key| Facet::from_text(&facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.field, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            field_filter.push((Occur::Should, Box::new(facet_term_query)));
+        });
+    if !field_filter.is_empty() {
+        let field_filter = Box::new(BooleanQuery::new(field_filter));
+        fuzzies.push((Occur::Must, field_filter.clone()));
+        originals.push((Occur::Must, field_filter));
+    }
+    // Add filter
+    search
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            fuzzies.push((Occur::Must, Box::new(facet_term_query.clone())));
+            originals.push((Occur::Must, Box::new(facet_term_query)));
+        });
+
+    if originals.len() == 1 && originals[0].1.is::<AllQuery>() {
+        let original = originals.pop().unwrap().1;
+        let fuzzy = Box::new(BooleanQuery::new(vec![]));
+        (original, termc, fuzzy)
+    } else {
+        if processed.fuzzy_query.is_empty() {
+            fuzzies.clear();
+        }
+        let original = Box::new(BooleanQuery::new(originals));
+        let fuzzied = Box::new(BoostQuery::new(Box::new(BooleanQuery::new(fuzzies)), 0.5));
+        (original, termc, fuzzied)
+    }
+}
+
+pub fn streaming_query(schema: &ParagraphSchema, request: &StreamRequest) -> Box<dyn Query> {
+    let mut queries: Vec<(Occur, Box<dyn Query>)> = vec![];
+    queries.push((Occur::Must, Box::new(AllQuery)));
+    request
+        .filter
+        .iter()
+        .flat_map(|f| f.tags.iter())
+        .flat_map(|facet_key| Facet::from_text(facet_key).ok().into_iter())
+        .for_each(|facet| {
+            let facet_term = Term::from_facet(schema.facets, &facet);
+            let facet_term_query = TermQuery::new(facet_term, IndexRecordOption::Basic);
+            queries.push((Occur::Must, Box::new(facet_term_query)));
+        });
+    Box::new(BooleanQuery::new(queries))
+}
+
+#[cfg(test)]
+mod tests {
+    use tantivy::schema::Field;
+
+    use super::*;
+    fn dummy_term_query() -> Box<dyn Query> {
+        let field = Field::from_field_id(0);
+        let term = Term::from_field_u64(field, 0);
+        Box::new(TermQuery::new(term, IndexRecordOption::Basic))
+    }
+
+    #[test]
+    fn test_preprocessor() {
+        let text = "own test \"This is great\"";
+        let mut term_collector = TermCollector::default();
+        let _ = preprocess_raw_query(text, &mut term_collector);
+        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
+        let expect = HashSet::from(["This is great", "test"]);
+        assert_eq!(terms, expect);
+
+        let text = "The test \"is correct\" always";
+        let mut term_collector = TermCollector::default();
+        let processed = preprocess_raw_query(text, &mut term_collector);
+        let terms: HashSet<_> = term_collector.eterms.iter().map(|s| s.as_str()).collect();
+        let expect = HashSet::from(["test", "always", "is correct"]);
+        assert_eq!(terms, expect);
+        assert_eq!(processed.regular_query, "test \"is correct\" always");
+        assert_eq!(processed.fuzzy_query, "test always");
+    }
+
+    #[test]
+    fn test() {
+        let subqueries0: Vec<_> = vec![dummy_term_query; 12]
+            .into_iter()
+            .map(|f| (Occur::Must, f()))
+            .collect();
+        let subqueries1: Vec<_> = vec![dummy_term_query; 12]
+            .into_iter()
+            .map(|f| (Occur::Must, f()))
+            .collect();
+        let boolean0: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries0));
+        let boolean1: Box<dyn Query> = Box::new(BooleanQuery::new(subqueries1));
+        let nested = BooleanQuery::new(vec![(Occur::Should, boolean0), (Occur::Should, boolean1)]);
+        let adapted = flat_and_adapt(Box::new(nested), true, 2, SharedTermC::new());
+        assert_eq!(adapted.len(), 24);
+        assert!(adapted.iter().all(|(occur, _)| *occur == Occur::Must));
+        assert!(adapted
+            .iter()
+            .all(|(_, query)| query.is::<FuzzyTermQuery>()));
+    }
+
+    #[test]
+    fn it_removes_stop_word_fterms() {
+        let tests = [
+            (
+                "nuclia is a database for unstructured data",
+                "nuclia database unstructured data",
+            ),
+            (
+                "nuclia is a",
+                // keeps last term even if is a stop word
+                "nuclia a",
+            ),
+            ("is a for and", "and"),
+        ];
+
+        for (query, expected_fuzzy_query) in tests {
+            let fuzzy_query = remove_stop_words(query);
+
+            assert_eq!(fuzzy_query, expected_fuzzy_query);
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/ca.json` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/ca.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 0% similar despite different names*

```diff
@@ -87,8 +87,8 @@
 00000560: 222c 2275 6e65 7322 2c22 756e 7322 2c22  ","unes","uns","
 00000570: 7573 222c 2276 6122 2c22 7661 6967 222c  us","va","vaig",
 00000580: 2276 616d 222c 2276 616e 222c 2276 6173  "vam","van","vas
 00000590: 222c 2276 6575 222c 2276 6f73 616c 7472  ","veu","vosaltr
 000005a0: 6573 222c 2276 6f73 7472 6122 2c22 766f  es","vostra","vo
 000005b0: 7374 7265 222c 2276 6f73 7472 6573 222c  stre","vostres",
 000005c0: 22c3 a972 656d 222c 22c3 a972 6575 222c  "..rem","..reu",
-000005d0: 22c3 a973 225d 0a                        "..s"].
+000005d0: 22c3 a973 225d 0d0a                      "..s"]..
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/en.json` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/en.json`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/es.json` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/es.json`

 * *Format-specific differences are supported for JSON files but no file-specific differences were detected; falling back to a binary diff. file(1) reports: JSON text data*

 * *Files 0% similar despite different names*

```diff
@@ -276,8 +276,8 @@
 00001130: 7a22 2c22 c3a9 6c22 2c22 c3a9 7361 222c  z","..l","..sa",
 00001140: 22c3 a973 6173 222c 22c3 a973 6522 2c22  "..sas","..se","
 00001150: c3a9 736f 7322 2c22 c3a9 7374 6122 2c22  ..sos","..sta","
 00001160: c3a9 7374 6173 222c 22c3 a973 7465 222c  ..stas","..ste",
 00001170: 22c3 a973 746f 7322 2c22 c3ba 6c74 696d  "..stos","..ltim
 00001180: 6122 2c22 c3ba 6c74 696d 6173 222c 22c3  a","..ltimas",".
 00001190: ba6c 7469 6d6f 222c 22c3 ba6c 7469 6d6f  .ltimo","..ltimo
-000011a0: 7322 5d0a                                s"].
+000011a0: 7322 5d0d 0a                             s"]..
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_paragraphs/stop_words/fr.json` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_paragraphs/stop_words/fr.json`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/lib.rs`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod reader;
-mod schema;
-mod search_query;
-pub mod writer;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod reader;
+mod schema;
+mod search_query;
+pub mod writer;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_texts/src/schema.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_texts/src/schema.rs`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,92 +1,92 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use nucliadb_core::protos::*;
-use tantivy::chrono::{DateTime, NaiveDateTime, Utc};
-use tantivy::schema::{
-    Cardinality, FacetOptions, Field, NumericOptions, Schema, STORED, STRING, TEXT,
-};
-
-#[derive(Debug, Clone)]
-pub struct TextSchema {
-    pub schema: Schema,
-
-    pub uuid: Field,
-    pub text: Field,
-    pub created: Field,
-    pub modified: Field,
-    pub status: Field,
-    pub facets: Field,
-    pub field: Field,
-}
-
-pub fn timestamp_to_datetime_utc(timestamp: &prost_types::Timestamp) -> DateTime<Utc> {
-    let naive =
-        NaiveDateTime::from_timestamp_opt(timestamp.seconds, timestamp.nanos as u32).unwrap();
-    DateTime::from_utc(naive, tantivy::chrono::Utc)
-}
-
-impl TextSchema {
-    pub fn new() -> Self {
-        let mut sb = Schema::builder();
-        let num_options: NumericOptions = NumericOptions::default()
-            .set_indexed()
-            .set_fast(Cardinality::SingleValue);
-
-        let date_options = NumericOptions::default()
-            .set_indexed()
-            .set_fast(Cardinality::SingleValue);
-
-        let facet_options = FacetOptions::default().set_stored();
-
-        let uuid = sb.add_text_field("uuid", STRING | STORED);
-        let field = sb.add_facet_field("field", facet_options.clone());
-
-        let text = sb.add_text_field("text", TEXT);
-
-        // Date fields needs to be searched in order, order_by_u64_field seems to work in TopDocs.
-        let created = sb.add_date_field("created", date_options.clone());
-        let modified = sb.add_date_field("modified", date_options);
-
-        // Status
-        let status = sb.add_u64_field("status", num_options);
-
-        // Facets
-        let facets = sb.add_facet_field("facets", facet_options);
-
-        let schema = sb.build();
-
-        TextSchema {
-            schema,
-            uuid,
-            text,
-            created,
-            modified,
-            status,
-            facets,
-            field,
-        }
-    }
-}
-
-impl Default for TextSchema {
-    fn default() -> Self {
-        Self::new()
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use nucliadb_core::protos::*;
+use tantivy::chrono::{DateTime, NaiveDateTime, Utc};
+use tantivy::schema::{
+    Cardinality, FacetOptions, Field, NumericOptions, Schema, STORED, STRING, TEXT,
+};
+
+#[derive(Debug, Clone)]
+pub struct TextSchema {
+    pub schema: Schema,
+
+    pub uuid: Field,
+    pub text: Field,
+    pub created: Field,
+    pub modified: Field,
+    pub status: Field,
+    pub facets: Field,
+    pub field: Field,
+}
+
+pub fn timestamp_to_datetime_utc(timestamp: &prost_types::Timestamp) -> DateTime<Utc> {
+    let naive =
+        NaiveDateTime::from_timestamp_opt(timestamp.seconds, timestamp.nanos as u32).unwrap();
+    DateTime::from_utc(naive, tantivy::chrono::Utc)
+}
+
+impl TextSchema {
+    pub fn new() -> Self {
+        let mut sb = Schema::builder();
+        let num_options: NumericOptions = NumericOptions::default()
+            .set_indexed()
+            .set_fast(Cardinality::SingleValue);
+
+        let date_options = NumericOptions::default()
+            .set_indexed()
+            .set_fast(Cardinality::SingleValue);
+
+        let facet_options = FacetOptions::default().set_stored();
+
+        let uuid = sb.add_text_field("uuid", STRING | STORED);
+        let field = sb.add_facet_field("field", facet_options.clone());
+
+        let text = sb.add_text_field("text", TEXT);
+
+        // Date fields needs to be searched in order, order_by_u64_field seems to work in TopDocs.
+        let created = sb.add_date_field("created", date_options.clone());
+        let modified = sb.add_date_field("modified", date_options);
+
+        // Status
+        let status = sb.add_u64_field("status", num_options);
+
+        // Facets
+        let facets = sb.add_facet_field("facets", facet_options);
+
+        let schema = sb.build();
+
+        TextSchema {
+            schema,
+            uuid,
+            text,
+            created,
+            modified,
+            status,
+            facets,
+            field,
+        }
+    }
+}
+
+impl Default for TextSchema {
+    fn default() -> Self {
+        Self::new()
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/Cargo.toml` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/Cargo.toml`

 * *Files 10% similar despite different names*

```diff
@@ -20,27 +20,30 @@
 path = "src/bin/reader.rs"
 
 [[bin]]
 name = "node_writer"
 path = "src/bin/writer.rs"
 
 [dependencies]
+axum = "0.6.15"
+axum-server = "0.4.7"
+
 tonic = "0.7"
 tonic-health = "0.6"
 async-std = "1.10.0"
 futures-core = "0.3.17"
 futures-util = "0.3.17"
 futures = "0.3.17"
-tokio = { version = "1.12.0", features = [
-    "rt-multi-thread",
-    "macros",
-    "sync",
-    "time",
-    "signal",
-    "fs",
+tokio = { version = "1.12.0", features = [
+    "rt-multi-thread",
+    "macros",
+    "sync",
+    "time",
+    "signal",
+    "fs",
 ] }
 tokio-stream = "0.1.7"
 log = "0.4.14"
 serde_json = "1"
 serde = { version = "1.0", features = ["derive"] }
 uuid = { version = "1.1", features = ["serde", "v4"] }
 bincode = "1.3.3"
@@ -48,14 +51,16 @@
 time = "0.3.3"
 itertools = "0.10"
 anyhow = "1"
 http = "0.2"
 thiserror = "1"
 opentelemetry = { version = "0.17", features = ["rt-tokio", "trace"] }
 tracing-opentelemetry = "0.17.2"
+reqwest = "0.11.16"
+
 # Test dependencies
 tempfile = "3.2.0"
 regex = "1.5.5"
 lazy_static = "1.4.0"
 openssl = { version = "0.10", features = ["vendored"] }
 
 # Text Service
@@ -76,18 +81,18 @@
 # test
 tempdir = "0.3.7"
 portpicker = "0.1.1"
 
 # sentry sdk
 sentry = "0.26.0"
 opentelemetry-jaeger = { version = "0.16.0", features = ["rt-tokio"] }
-tracing-subscriber = { version = "0.3.11", features = [
-    "env-filter",
-    "registry",
-    "std",
+tracing-subscriber = { version = "0.3.11", features = [
+    "env-filter",
+    "registry",
+    "std",
 ] }
 dotenvy = "0.15.1"
 tracing-log = { version = "0.1.3", features = ["env_logger"] }
 opentelemetry-zipkin = "0.15.0"
 sentry-tracing = "0.27.0"
 
 parse_duration = "2.1.1"
@@ -106,26 +111,26 @@
 lto = true
 
 [dev-dependencies]
 backoff = { version = "0.4.0", features = ["tokio"] }
 
 once_cell = { version = "1.17" }
 
-tokio = { version = "1.12.0", features = [
-    "rt-multi-thread",
-    "macros",
-    "sync",
-    "time",
-    "signal",
-    "fs",
+tokio = { version = "1.12.0", features = [
+    "rt-multi-thread",
+    "macros",
+    "sync",
+    "time",
+    "signal",
+    "fs",
 ] }
 
 tracing = { version = "0.1.29" }
 tracing-log = { version = "0.1.3", features = ["env_logger"] }
-tracing-subscriber = { version = "0.3.11", features = [
-    "env-filter",
-    "registry",
-    "std",
+tracing-subscriber = { version = "0.3.11", features = [
+    "env-filter",
+    "registry",
+    "std",
 ] }
 uuid = { version = "1.1", features = ["v4", "fast-rng", "macro-diagnostics"] }
 
 nucliadb_protos = { path = "../nucliadb_protos" }
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/.rustc_info.json` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/.rustc_info.json`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/README.md` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/README.md`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-# NucliaDB Node
-
-<p align="center">
-  <img src="../docs/assets/images/node_scheme.png" alt="nucliadb_node"  width="500px" style="background-color: #fff">
-</p>
-
-The node is nucliadb's own indexing system, used for storing and retrieving complex AI data like vector embeddings or relations.
-Currently the main crates involved in making this system possible are shown in the previous image, along with how they interact with each other.
-
-### nucliadb_core
-
-The node contains four different indexes that need to share certain functionality like error handling, tracing or handling protos types. `nucliadb_core` is the 
-crate where all this shared dependencies live.
-
-### nucliadb_node
-
-Interacting with nucliadb's four indexes requiers going through `nucliadb_node`. This crate provides a grpc interface for adding, removing and searching informacion stored in 
-the four indexes that live inside the node.
-
-### nucliadb_vectors
-
-Is possible to index vector embeddings using `nucliadb_node` thanks to this crate, nucliadb's own [HNSW](https://arxiv.org/abs/1603.09320) implementaion.
-
-### nucliadb_relations
-
-Is possible to index knowledge graphs using `nucliadb_node` thanks to this crate, nucliadb's own knowledge graph implementation built on top of [heed](https://github.com/meilisearch/heed).
-
-
-### nucliadb_paragraphs and nucliadb_fields
-
-`nucliadb_node` uses [Tantivy](https://github.com/quickwit-oss/tantivy) to obtain full-text search capabilities, we offer complex querying, BM25 and fuzzy search.
+# NucliaDB Node
+
+<p align="center">
+  <img src="../docs/assets/images/node_scheme.png" alt="nucliadb_node"  width="500px" style="background-color: #fff">
+</p>
+
+The node is nucliadb's own indexing system, used for storing and retrieving complex AI data like vector embeddings or relations.
+Currently the main crates involved in making this system possible are shown in the previous image, along with how they interact with each other.
+
+### nucliadb_core
+
+The node contains four different indexes that need to share certain functionality like error handling, tracing or handling protos types. `nucliadb_core` is the 
+crate where all this shared dependencies live.
+
+### nucliadb_node
+
+Interacting with nucliadb's four indexes requiers going through `nucliadb_node`. This crate provides a grpc interface for adding, removing and searching informacion stored in 
+the four indexes that live inside the node.
+
+### nucliadb_vectors
+
+Is possible to index vector embeddings using `nucliadb_node` thanks to this crate, nucliadb's own [HNSW](https://arxiv.org/abs/1603.09320) implementaion.
+
+### nucliadb_relations
+
+Is possible to index knowledge graphs using `nucliadb_node` thanks to this crate, nucliadb's own knowledge graph implementation built on top of [heed](https://github.com/meilisearch/heed).
+
+
+### nucliadb_paragraphs and nucliadb_fields
+
+`nucliadb_node` uses [Tantivy](https://github.com/quickwit-oss/tantivy) to obtain full-text search capabilities, we offer complex querying, BM25 and fuzzy search.
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/__init__.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/mod.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,43 +1,34 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import logging
-
-SERVICE_NAME = "nucliadb_node"
-
-logger = logging.getLogger(SERVICE_NAME)
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod versions;
+// Main services
+pub mod reader;
+pub mod writer;
+
+mod shard_disk_structure {
+    pub const VERSION_FILE: &str = "versions.json";
+    pub const VECTORS_DIR: &str = "vectors";
+    pub const VECTORSET_DIR: &str = "vectorset";
+    pub const TEXTS_DIR: &str = "text";
+    pub const PARAGRAPHS_DIR: &str = "paragraph";
+    pub const RELATIONS_DIR: &str = "relations";
+    pub const METADATA_FILE: &str = "metadata.json";
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/reader.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/reader.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,70 +1,64 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import Optional
-
-from grpc import aio  # type: ignore
-from lru import LRU  # type: ignore
-from nucliadb_protos.nodereader_pb2 import GetShardRequest  # type: ignore
-from nucliadb_protos.nodereader_pb2_grpc import NodeReaderStub
-from nucliadb_protos.noderesources_pb2 import Shard, ShardId
-from nucliadb_protos.nodewriter_pb2 import OpStatus
-
-from nucliadb_node import SERVICE_NAME  # type: ignore
-from nucliadb_telemetry.grpc import OpenTelemetryGRPC
-from nucliadb_telemetry.utils import get_telemetry
-
-CACHE = LRU(128)
-
-
-class Reader:
-    _stub: Optional[NodeReaderStub] = None
-    lock: asyncio.Lock
-
-    def __init__(self, grpc_reader_address: str):
-        self.lock = asyncio.Lock()
-        tracer_provider = get_telemetry(SERVICE_NAME)
-        if tracer_provider is not None:  # pragma: no cover
-            telemetry_grpc = OpenTelemetryGRPC(
-                f"{SERVICE_NAME}_grpc_reader", tracer_provider
-            )
-            self.channel = telemetry_grpc.init_client(grpc_reader_address)
-        else:
-            self.channel = aio.insecure_channel(grpc_reader_address)
-        self.stub = NodeReaderStub(self.channel)
-
-    async def get_shard(self, pb: ShardId) -> Optional[Shard]:
-        if pb.id not in CACHE:
-            req = GetShardRequest()
-            req.shard_id.id = pb.id
-            shard: Shard = await self.stub.GetShard(req)  # type: ignore
-            CACHE[pb.id] = shard
-            return CACHE[pb.id]
-        else:
-            return CACHE[pb.id]
-
-    def update(self, shard: str, status: OpStatus):
-        if status.status == 0:
-            cached_shard = Shard()
-            cached_shard.shard_id = shard
-            cached_shard.resources = status.count
-            cached_shard.paragraphs = status.count_paragraphs
-            cached_shard.sentences = status.count_sentences
-            CACHE[shard] = cached_shard
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import Optional
+
+from lru import LRU  # type: ignore
+from nucliadb_protos.nodereader_pb2 import GetShardRequest  # type: ignore
+from nucliadb_protos.nodereader_pb2_grpc import NodeReaderStub
+from nucliadb_protos.noderesources_pb2 import Shard, ShardId
+from nucliadb_protos.nodewriter_pb2 import OpStatus
+from nucliadb_utils.grpc import get_traced_grpc_channel
+
+from nucliadb_node import SERVICE_NAME  # type: ignore
+
+CACHE = LRU(128)
+
+
+class Reader:
+    _stub: Optional[NodeReaderStub] = None
+    lock: asyncio.Lock
+
+    def __init__(self, grpc_reader_address: str):
+        self.lock = asyncio.Lock()
+        self.channel = get_traced_grpc_channel(grpc_reader_address, SERVICE_NAME)
+        self.stub = NodeReaderStub(self.channel)
+
+    async def get_shard(self, pb: ShardId) -> Optional[Shard]:
+        if pb.id not in CACHE:
+            req = GetShardRequest()
+            req.shard_id.id = pb.id
+            shard: Shard = await self.stub.GetShard(req)  # type: ignore
+            CACHE[pb.id] = shard
+            return CACHE[pb.id]
+        else:
+            return CACHE[pb.id]
+
+    def update(self, shard: str, status: OpStatus):
+        if status.status == 0:
+            cached_shard = Shard()
+            cached_shard.shard_id = shard
+            cached_shard.resources = status.count
+            cached_shard.paragraphs = status.count_paragraphs
+            cached_shard.sentences = status.count_sentences
+            CACHE[shard] = cached_shard
+
+    async def close(self):
+        await self.channel.close()
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/service.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/service.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,59 +1,50 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-import asyncio
-
-from grpc import aio  # type: ignore
-from grpc_health.v1 import health, health_pb2_grpc
-
-from nucliadb_node import SERVICE_NAME
-from nucliadb_node.reader import Reader  # type: ignore
-from nucliadb_node.servicer import SidecarServicer
-from nucliadb_node.settings import settings
-from nucliadb_node.writer import Writer
-from nucliadb_protos import nodesidecar_pb2_grpc
-from nucliadb_telemetry.grpc import OpenTelemetryGRPC
-from nucliadb_telemetry.utils import get_telemetry
-
-
-async def start_grpc(writer: Writer, reader: Reader):
-    aio.init_grpc_aio()
-
-    tracer_provider = get_telemetry(SERVICE_NAME)
-    if tracer_provider is not None:  # pragma: no cover
-        telemetry_grpc = OpenTelemetryGRPC(
-            f"{SERVICE_NAME}_grpc_server", tracer_provider
-        )
-        server = telemetry_grpc.init_server()
-    else:
-        server = aio.server()
-    servicer = SidecarServicer(reader=reader, writer=writer)
-    await servicer.initialize()
-    health_servicer = health.aio.HealthServicer()  # type: ignore
-    server.add_insecure_port(settings.sidecar_listen_address)
-
-    nodesidecar_pb2_grpc.add_NodeSidecarServicer_to_server(servicer, server)
-    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
-    await server.start()
-
-    def finalizer():
-        asyncio.create_task(servicer.finalize())
-        asyncio.create_task(server.stop(grace=False))
-
-    return finalizer
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+
+from grpc import aio  # type: ignore
+from grpc_health.v1 import health, health_pb2_grpc
+from nucliadb_protos import nodesidecar_pb2_grpc
+from nucliadb_utils.grpc import get_traced_grpc_server
+
+from nucliadb_node import SERVICE_NAME
+from nucliadb_node.reader import Reader  # type: ignore
+from nucliadb_node.servicer import SidecarServicer
+from nucliadb_node.settings import settings
+from nucliadb_node.writer import Writer
+
+
+async def start_grpc(writer: Writer, reader: Reader):
+    aio.init_grpc_aio()
+
+    server = get_traced_grpc_server(SERVICE_NAME)
+    servicer = SidecarServicer(reader=reader, writer=writer)
+    await servicer.initialize()
+    health_servicer = health.aio.HealthServicer()  # type: ignore
+    server.add_insecure_port(settings.sidecar_listen_address)
+
+    nodesidecar_pb2_grpc.add_NodeSidecarServicer_to_server(servicer, server)
+    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
+    await server.start()
+
+    async def finalizer():
+        await servicer.finalize()
+        await server.stop(grace=False)
+
+    return finalizer
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/servicer.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/servicer.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,101 +1,101 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from nucliadb_protos.noderesources_pb2 import EmptyQuery, ShardId
-from nucliadb_protos.nodesidecar_pb2 import Counter, ShadowShardResponse
-
-from nucliadb_node import logger, shadow_shards
-from nucliadb_node.reader import Reader
-from nucliadb_node.shadow_shards import ShadowShardNotFound, ShadowShardsManager
-from nucliadb_node.writer import Writer
-from nucliadb_protos import nodesidecar_pb2_grpc
-from nucliadb_telemetry import errors
-
-
-class SidecarServicer(nodesidecar_pb2_grpc.NodeSidecarServicer):
-    def __init__(self, reader: Reader, writer: Writer):
-        self.reader = reader
-        self.writer = writer
-
-    async def initialize(self):
-        pass
-
-    async def finalize(self):
-        pass
-
-    async def GetCount(self, request: ShardId, context) -> Counter:  # type: ignore
-        response = Counter()
-        shard = await self.reader.get_shard(request)
-        if shard is not None:
-            response.resources = shard.resources
-            response.paragraphs = shard.paragraphs
-        return response
-
-    async def CreateShadowShard(self, request: EmptyQuery, context) -> ShadowShardResponse:  # type: ignore
-        ssm: ShadowShardsManager = shadow_shards.get_manager()
-        await ssm.load()
-        response = ShadowShardResponse()
-        try:
-            shard_id = await ssm.create()
-            response.success = True
-            response.shard.id = shard_id
-        except Exception as exc:
-            errors.capture_exception(exc)
-            logger.warning(f"Error creating shadow shard: {shard_id}")
-        finally:
-            return response
-
-    async def DeleteShadowShard(self, request: ShardId, context) -> ShadowShardResponse:  # type: ignore
-        ssm = shadow_shards.get_manager()
-        await ssm.load()
-        response = ShadowShardResponse()
-        shard_id = request.id
-        try:
-            await ssm.delete(shard_id=shard_id)
-            response.success = True
-            response.shard.id = shard_id
-        except ShadowShardNotFound:
-            logger.warning(
-                f"Attempting to delete a shadow shard that does not exist: {shard_id}"
-            )
-            response.success = True
-        except Exception as exc:
-            errors.capture_exception(exc)
-            logger.warning(f"Error deleting shadow shard: {shard_id}")
-        finally:
-            return response
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from nucliadb_protos import nodesidecar_pb2_grpc
+from nucliadb_protos.noderesources_pb2 import EmptyQuery, ShardId
+from nucliadb_protos.nodesidecar_pb2 import Counter, ShadowShardResponse
+from nucliadb_telemetry import errors
+
+from nucliadb_node import logger, shadow_shards
+from nucliadb_node.reader import Reader
+from nucliadb_node.shadow_shards import ShadowShardNotFound, ShadowShardsManager
+from nucliadb_node.writer import Writer
+
+
+class SidecarServicer(nodesidecar_pb2_grpc.NodeSidecarServicer):
+    def __init__(self, reader: Reader, writer: Writer):
+        self.reader = reader
+        self.writer = writer
+
+    async def initialize(self):
+        pass
+
+    async def finalize(self):
+        pass
+
+    async def GetCount(self, request: ShardId, context) -> Counter:  # type: ignore
+        response = Counter()
+        shard = await self.reader.get_shard(request)
+        if shard is not None:
+            response.resources = shard.resources
+            response.paragraphs = shard.paragraphs
+        return response
+
+    async def CreateShadowShard(self, request: EmptyQuery, context) -> ShadowShardResponse:  # type: ignore
+        ssm: ShadowShardsManager = shadow_shards.get_manager()
+        await ssm.load()
+        response = ShadowShardResponse()
+        try:
+            shard_id = await ssm.create()
+            response.success = True
+            response.shard.id = shard_id
+        except Exception as exc:
+            errors.capture_exception(exc)
+            logger.warning(f"Error creating shadow shard: {shard_id}")
+        finally:
+            return response
+
+    async def DeleteShadowShard(self, request: ShardId, context) -> ShadowShardResponse:  # type: ignore
+        ssm = shadow_shards.get_manager()
+        await ssm.load()
+        response = ShadowShardResponse()
+        shard_id = request.id
+        try:
+            await ssm.delete(shard_id=shard_id)
+            response.success = True
+            response.shard.id = shard_id
+        except ShadowShardNotFound:
+            logger.warning(
+                f"Attempting to delete a shadow shard that does not exist: {shard_id}"
+            )
+            response.success = True
+        except Exception as exc:
+            errors.capture_exception(exc)
+            logger.warning(f"Error deleting shadow shard: {shard_id}")
+        finally:
+            return response
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/settings.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/settings.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,36 +1,53 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-from typing import Optional
-
-from pydantic import BaseSettings
-
-
-class Settings(BaseSettings):
-    host_key_path: str = "node.key"
-    force_host_id: Optional[str] = None
-
-    writer_listen_address: str = "0.0.0.0:10000"
-    reader_listen_address: str = "0.0.0.0:10001"
-    sidecar_listen_address: str = "0.0.0.0:10002"
-
-    data_path: Optional[str] = None
-
-
-settings = Settings()
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+from typing import Optional
+
+from nucliadb_utils import settings as utils_settings
+from pydantic import BaseSettings
+
+
+class Settings(BaseSettings):
+    host_key_path: str = "node.key"
+    force_host_id: Optional[str] = None
+
+    writer_listen_address: str = "0.0.0.0:10000"
+    reader_listen_address: str = "0.0.0.0:10001"
+    sidecar_listen_address: str = "0.0.0.0:10002"
+
+    data_path: Optional[str] = None
+
+
+settings = Settings()
+
+
+class IndexingSettings(utils_settings.IndexingSettings):
+    indexed_jetstream_target: str = "indexed.{partition}"
+    indexed_jetstream_stream: str = "indexed"
+
+
+indexing_settings = IndexingSettings()
+
+
+class RunningSettings(BaseSettings):
+    debug: bool = True
+    log_level: str = "DEBUG"
+
+
+running_settings = RunningSettings()
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/shadow_shards.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/shadow_shards.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,255 +1,255 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-
-import glob
-import os
-import uuid
-from datetime import datetime
-from enum import Enum
-from typing import Any, AsyncIterator, Dict, Optional, Set, Tuple, Union
-
-import aiofiles
-from aiofiles import os as aos
-from nucliadb_protos.noderesources_pb2 import Resource
-from pydantic import BaseModel
-
-from nucliadb_node import logger
-
-SHADOW_SHARDS_FOLDER = "{data_path}/shadow_shards/"
-
-MAIN: Dict[str, Any] = {}
-
-
-class ShadowShardInfo(BaseModel):
-    shard_id: str
-    created_at: datetime = datetime.now()
-
-
-class ShadowMetadata(BaseModel):
-    file_id: str = "metadata.json"
-    shards: Dict[str, ShadowShardInfo] = {}
-
-    @classmethod
-    async def load(cls, file_path: str) -> "ShadowMetadata":
-        try:
-            async with aiofiles.open(file_path, mode="r") as f:
-                return cls.parse_raw(await f.read())
-        except FileNotFoundError:
-            return cls()
-
-    async def save(self, file_path: str):
-        async with aiofiles.open(file_path, mode="w") as f:
-            await f.write(self.json())
-
-    def get_info(self, shard_id: str) -> Optional[ShadowShardInfo]:
-        return self.shards.get(shard_id)
-
-
-class ShadowShardNotFound(Exception):
-    pass
-
-
-class ShadowShardsNotLoaded(Exception):
-    pass
-
-
-class OperationCode(str, Enum):
-    SET = "SET"
-    DELETE = "DEL"
-
-
-NodeOperation = Tuple[OperationCode, Union[Resource, str]]
-
-
-class ShadowShardsManager:
-    """
-    This class is responsible for managing the shadow shards of a particular node.
-    A shadow shard is essentially a temporary file where we store all the operations (set
-    or delete resources) for a shard that is being rebalanced or upgraded.
-    """
-
-    def __init__(self, folder: str):
-        self._folder: str = folder
-        self.shards: Set[str] = set()
-        self._loaded: bool = False
-        self._metadata_file: str = "metadata.json"
-        self._metadata: ShadowMetadata = ShadowMetadata()
-
-    @property
-    def folder(self) -> str:
-        if not self._folder.endswith("/"):
-            self._folder += "/"
-        return self._folder
-
-    async def load(self) -> None:
-        if self.loaded:
-            return
-
-        # Create shards folder if it doesn't exist
-        await aos.makedirs(self.folder, exist_ok=True)
-
-        await self.load_metadata()
-        await self.load_shards()
-        self._loaded = True
-
-    async def load_shards(self):
-        self.shards = set()
-        for shard_path in glob.glob(f"{self.folder}*"):
-            shard_id = shard_path.split(self._folder)[-1].lstrip("/")
-            if shard_id != self._metadata_file:
-                self.shards.add(shard_id)
-
-    async def load_metadata(self) -> None:
-        metadata_path = self.shard_path(self._metadata_file)
-        self._metadata = await ShadowMetadata.load(metadata_path)
-
-    async def save_metadata(self) -> None:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-        metadata_path = self.shard_path(self._metadata_file)
-        await self.metadata.save(metadata_path)
-
-    @property
-    def loaded(self) -> bool:
-        return self._loaded
-
-    @property
-    def metadata(self) -> ShadowMetadata:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-        return self._metadata
-
-    def shard_path(self, shard_id: str) -> str:
-        return f"{self.folder}{shard_id}"
-
-    async def create(self) -> str:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-
-        # Get a unique shard id
-        shard_id = uuid.uuid4().hex
-        while shard_id in self.shards:
-            shard_id = uuid.uuid4().hex
-
-        shard_path = self.shard_path(shard_id)
-        await aos.mkdir(shard_path)
-
-        self.shards.add(shard_id)
-        self.metadata.shards[shard_id] = ShadowShardInfo(shard_id=shard_id)
-        await self.save_metadata()
-        return shard_id
-
-    async def delete(self, shard_id: str) -> None:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-
-        if shard_id not in self.shards:
-            raise ShadowShardNotFound()
-
-        shard_path = self.shard_path(shard_id)
-        await aos.rmdir(shard_path)
-
-        self.shards.remove(shard_id)
-        self.metadata.shards.pop(shard_id)
-        await self.save_metadata()
-
-    def exists(self, shard_id: str) -> bool:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-        return shard_id in self.shards
-
-    def get_op_id(self, txid: int, opcode: OperationCode, uuid: str) -> str:
-        return f"{txid}_{opcode}_{uuid}"
-
-    def parse_op_id(self, op_id: str) -> Tuple[int, OperationCode, str]:
-        txid, opcode, uuid = op_id.split("_")
-        return int(txid), OperationCode(opcode), uuid
-
-    async def set_resource(self, brain: Resource, shard_id: str, txid: int) -> None:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-        if not self.exists(shard_id):
-            raise ShadowShardNotFound()
-
-        op_id = self.get_op_id(txid, OperationCode.SET, brain.resource.uuid)
-        shard_path = self.shard_path(shard_id)
-        async with aiofiles.open(f"{shard_path}/{op_id}", mode="wb") as f:
-            await f.write(brain.SerializeToString())
-
-    async def delete_resource(self, uuid: str, shard_id: str, txid: int) -> None:
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-        if not self.exists(shard_id):
-            raise ShadowShardNotFound()
-
-        op_id = self.get_op_id(txid, OperationCode.DELETE, uuid)
-        shard_path = self.shard_path(shard_id)
-        async with aiofiles.open(f"{shard_path}/{op_id}", mode="x"):
-            pass
-
-    async def iter_operations(self, shard_id: str) -> AsyncIterator[NodeOperation]:
-        """
-        Iterates the operations stored in the shard folder ordered by transaction id (txid)
-        """
-        if not self.loaded:
-            raise ShadowShardsNotLoaded()
-        if not self.exists(shard_id):
-            raise ShadowShardNotFound()
-
-        shard_path = self.shard_path(shard_id)
-
-        def get_filename(op_path: str) -> str:
-            return op_path.split("/")[-1]
-
-        def get_txid(op_path: str) -> int:
-            filename = get_filename(op_path)
-            return int(filename.split("_")[0])
-
-        op_paths = glob.glob(f"{shard_path}/*")
-        op_paths_sorted_by_txid = sorted(op_paths, key=lambda p: get_txid(p))
-        for path in op_paths_sorted_by_txid:
-            op_id = get_filename(path)
-            _, opcode, uuid = self.parse_op_id(op_id)
-            if opcode == OperationCode.SET:
-                async with aiofiles.open(path, mode="rb") as f:
-                    payload = await f.read()
-                    brain = Resource.FromString(payload)
-                    yield opcode, brain
-            elif opcode == OperationCode.DELETE:
-                yield opcode, uuid
-
-
-def get_data_path() -> str:
-    data_path = os.environ.get("DATA_PATH")
-    if not data_path:
-        logger.warning(f"DATA_PATH not configured. Defaulting to: /data")
-        data_path = "/data"
-    data_path.rstrip("/")
-    return data_path
-
-
-def get_manager() -> ShadowShardsManager:
-    if "manager" not in MAIN:
-        folder = SHADOW_SHARDS_FOLDER.format(data_path=get_data_path())
-        MAIN["manager"] = ShadowShardsManager(folder=folder)
-    manager = MAIN.get("manager")
-    if manager is None:
-        raise AttributeError()
-    return manager
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+
+import glob
+import os
+import uuid
+from datetime import datetime
+from enum import Enum
+from typing import Any, AsyncIterator, Dict, Optional, Set, Tuple, Union
+
+import aiofiles
+from aiofiles import os as aos
+from nucliadb_protos.noderesources_pb2 import Resource
+from pydantic import BaseModel
+
+from nucliadb_node import logger
+
+SHADOW_SHARDS_FOLDER = "{data_path}/shadow_shards/"
+
+MAIN: Dict[str, Any] = {}
+
+
+class ShadowShardInfo(BaseModel):
+    shard_id: str
+    created_at: datetime = datetime.now()
+
+
+class ShadowMetadata(BaseModel):
+    file_id: str = "metadata.json"
+    shards: Dict[str, ShadowShardInfo] = {}
+
+    @classmethod
+    async def load(cls, file_path: str) -> "ShadowMetadata":
+        try:
+            async with aiofiles.open(file_path, mode="r") as f:
+                return cls.parse_raw(await f.read())
+        except FileNotFoundError:
+            return cls()
+
+    async def save(self, file_path: str):
+        async with aiofiles.open(file_path, mode="w") as f:
+            await f.write(self.json())
+
+    def get_info(self, shard_id: str) -> Optional[ShadowShardInfo]:
+        return self.shards.get(shard_id)
+
+
+class ShadowShardNotFound(Exception):
+    pass
+
+
+class ShadowShardsNotLoaded(Exception):
+    pass
+
+
+class OperationCode(str, Enum):
+    SET = "SET"
+    DELETE = "DEL"
+
+
+NodeOperation = Tuple[OperationCode, Union[Resource, str]]
+
+
+class ShadowShardsManager:
+    """
+    This class is responsible for managing the shadow shards of a particular node.
+    A shadow shard is essentially a temporary file where we store all the operations (set
+    or delete resources) for a shard that is being rebalanced or upgraded.
+    """
+
+    def __init__(self, folder: str):
+        self._folder: str = folder
+        self.shards: Set[str] = set()
+        self._loaded: bool = False
+        self._metadata_file: str = "metadata.json"
+        self._metadata: ShadowMetadata = ShadowMetadata()
+
+    @property
+    def folder(self) -> str:
+        if not self._folder.endswith("/"):
+            self._folder += "/"
+        return self._folder
+
+    async def load(self) -> None:
+        if self.loaded:
+            return
+
+        # Create shards folder if it doesn't exist
+        await aos.makedirs(self.folder, exist_ok=True)
+
+        await self.load_metadata()
+        await self.load_shards()
+        self._loaded = True
+
+    async def load_shards(self):
+        self.shards = set()
+        for shard_path in glob.glob(f"{self.folder}*"):
+            shard_id = shard_path.split(self._folder)[-1].lstrip("/")
+            if shard_id != self._metadata_file:
+                self.shards.add(shard_id)
+
+    async def load_metadata(self) -> None:
+        metadata_path = self.shard_path(self._metadata_file)
+        self._metadata = await ShadowMetadata.load(metadata_path)
+
+    async def save_metadata(self) -> None:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+        metadata_path = self.shard_path(self._metadata_file)
+        await self.metadata.save(metadata_path)
+
+    @property
+    def loaded(self) -> bool:
+        return self._loaded
+
+    @property
+    def metadata(self) -> ShadowMetadata:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+        return self._metadata
+
+    def shard_path(self, shard_id: str) -> str:
+        return f"{self.folder}{shard_id}"
+
+    async def create(self) -> str:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+
+        # Get a unique shard id
+        shard_id = uuid.uuid4().hex
+        while shard_id in self.shards:
+            shard_id = uuid.uuid4().hex
+
+        shard_path = self.shard_path(shard_id)
+        await aos.mkdir(shard_path)
+
+        self.shards.add(shard_id)
+        self.metadata.shards[shard_id] = ShadowShardInfo(shard_id=shard_id)
+        await self.save_metadata()
+        return shard_id
+
+    async def delete(self, shard_id: str) -> None:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+
+        if shard_id not in self.shards:
+            raise ShadowShardNotFound()
+
+        shard_path = self.shard_path(shard_id)
+        await aos.rmdir(shard_path)
+
+        self.shards.remove(shard_id)
+        self.metadata.shards.pop(shard_id)
+        await self.save_metadata()
+
+    def exists(self, shard_id: str) -> bool:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+        return shard_id in self.shards
+
+    def get_op_id(self, txid: int, opcode: OperationCode, uuid: str) -> str:
+        return f"{txid}_{opcode}_{uuid}"
+
+    def parse_op_id(self, op_id: str) -> Tuple[int, OperationCode, str]:
+        txid, opcode, uuid = op_id.split("_")
+        return int(txid), OperationCode(opcode), uuid
+
+    async def set_resource(self, brain: Resource, shard_id: str, txid: int) -> None:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+        if not self.exists(shard_id):
+            raise ShadowShardNotFound()
+
+        op_id = self.get_op_id(txid, OperationCode.SET, brain.resource.uuid)
+        shard_path = self.shard_path(shard_id)
+        async with aiofiles.open(f"{shard_path}/{op_id}", mode="wb") as f:
+            await f.write(brain.SerializeToString())
+
+    async def delete_resource(self, uuid: str, shard_id: str, txid: int) -> None:
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+        if not self.exists(shard_id):
+            raise ShadowShardNotFound()
+
+        op_id = self.get_op_id(txid, OperationCode.DELETE, uuid)
+        shard_path = self.shard_path(shard_id)
+        async with aiofiles.open(f"{shard_path}/{op_id}", mode="x"):
+            pass
+
+    async def iter_operations(self, shard_id: str) -> AsyncIterator[NodeOperation]:
+        """
+        Iterates the operations stored in the shard folder ordered by transaction id (txid)
+        """
+        if not self.loaded:
+            raise ShadowShardsNotLoaded()
+        if not self.exists(shard_id):
+            raise ShadowShardNotFound()
+
+        shard_path = self.shard_path(shard_id)
+
+        def get_filename(op_path: str) -> str:
+            return op_path.split("/")[-1]
+
+        def get_txid(op_path: str) -> int:
+            filename = get_filename(op_path)
+            return int(filename.split("_")[0])
+
+        op_paths = glob.glob(f"{shard_path}/*")
+        op_paths_sorted_by_txid = sorted(op_paths, key=lambda p: get_txid(p))
+        for path in op_paths_sorted_by_txid:
+            op_id = get_filename(path)
+            _, opcode, uuid = self.parse_op_id(op_id)
+            if opcode == OperationCode.SET:
+                async with aiofiles.open(path, mode="rb") as f:
+                    payload = await f.read()
+                    brain = Resource.FromString(payload)
+                    yield opcode, brain
+            elif opcode == OperationCode.DELETE:
+                yield opcode, uuid
+
+
+def get_data_path() -> str:
+    data_path = os.environ.get("DATA_PATH")
+    if not data_path:
+        logger.warning(f"DATA_PATH not configured. Defaulting to: /data")
+        data_path = "/data"
+    data_path.rstrip("/")
+    return data_path
+
+
+def get_manager() -> ShadowShardsManager:
+    if "manager" not in MAIN:
+        folder = SHADOW_SHARDS_FOLDER.format(data_path=get_data_path())
+        MAIN["manager"] = ShadowShardsManager(folder=folder)
+    manager = MAIN.get("manager")
+    if manager is None:
+        raise AttributeError()
+    return manager
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,22 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import logging
+
+logger = logging.getLogger("nucliadb_telemetry")
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/service/mod.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,27 +1,37 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-pytest_plugins = [
-    "pytest_docker_fixtures",
-    "nucliadb_utils.tests.nats",
-    "nucliadb_utils.tests.gcs",
-    "nucliadb_utils.tests.s3",
-    "nucliadb_utils.tests.indexing",
-    "nucliadb_node.tests.fixtures",
-]
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod reader;
+pub mod writer;
+
+use nucliadb_core::protos::VectorSimilarity as GrpcSimilarity;
+pub use reader::*;
+pub use writer::*;
+
+use crate::data_point::Similarity;
+
+impl From<GrpcSimilarity> for Similarity {
+    fn from(value: GrpcSimilarity) -> Self {
+        match value {
+            GrpcSimilarity::Cosine => Similarity::Cosine,
+            GrpcSimilarity::Dot => Similarity::Dot,
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/test_shadow_shards.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_shadow_shards.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,170 +1,170 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import pytest
-from nucliadb_protos.noderesources_pb2 import Resource
-
-from nucliadb_node.shadow_shards import (
-    OperationCode,
-    ShadowShardNotFound,
-    ShadowShardsManager,
-    ShadowShardsNotLoaded,
-    get_manager,
-)
-
-
-@pytest.mark.asyncio
-async def test_get_manager(shadow_folder):
-    shadow_shards = get_manager()
-    assert isinstance(shadow_shards, ShadowShardsManager)
-    assert not shadow_shards._folder.endswith("{data_path}")
-    assert shadow_shards._folder.endswith("/shadow_shards/")
-
-
-@pytest.mark.asyncio
-async def test_load(shadow_folder):
-    ssm = ShadowShardsManager(shadow_folder)
-    assert not ssm.loaded
-    await ssm.load()
-    assert ssm.loaded
-    assert len(ssm.shards) == 0
-
-    # Create a new shadow shard and make sure it's loaded
-    shard_id = await ssm.create()
-    assert shard_id
-    await ssm.load()
-    assert len(ssm.shards) == 1
-
-
-@pytest.mark.asyncio
-async def test_create(shadow_folder):
-    ssm = ShadowShardsManager(shadow_folder)
-    with pytest.raises(ShadowShardsNotLoaded):
-        await ssm.create()
-
-    await ssm.load()
-    assert len(ssm.shards) == 0
-
-    shard_id = await ssm.create()
-    assert len(ssm.shards) == 1
-    assert ssm.exists(shard_id)
-
-
-@pytest.mark.asyncio
-async def test_delete(shadow_folder):
-    ssm = ShadowShardsManager(shadow_folder)
-    with pytest.raises(ShadowShardsNotLoaded):
-        await ssm.delete("foo")
-
-    await ssm.load()
-    with pytest.raises(ShadowShardNotFound):
-        await ssm.delete("not-there")
-
-    shard_id = await ssm.create()
-    assert len(ssm.shards) == 1
-    assert ssm.exists(shard_id)
-
-    await ssm.delete(shard_id)
-    assert len(ssm.shards) == 0
-    assert not ssm.exists(shard_id)
-
-
-@pytest.mark.asyncio
-async def test_exists(shadow_folder):
-    ssm = ShadowShardsManager(shadow_folder)
-    with pytest.raises(ShadowShardsNotLoaded):
-        ssm.exists("foo")
-    await ssm.load()
-    assert not ssm.exists("foo")
-    shard_id = await ssm.create()
-    assert ssm.exists(shard_id)
-
-
-def get_brain(uuid: str) -> Resource:
-    br = Resource()
-    br.resource.uuid = uuid
-    return br
-
-
-@pytest.mark.asyncio
-async def test_operations(shadow_folder):
-    ssm = ShadowShardsManager(shadow_folder)
-
-    await ssm.load()
-    shard_1 = await ssm.create()
-    shard_2 = await ssm.create()
-    shard_3 = await ssm.create()
-
-    brain_1 = get_brain("resource1")
-    brain_2 = get_brain("resource2")
-    brain_3 = get_brain("resource3")
-
-    await ssm.set_resource(brain_2, shard_1, 10)
-    await ssm.set_resource(brain_1, shard_1, 2)
-
-    await ssm.set_resource(brain_3, shard_2, 2)
-    await ssm.delete_resource("resource3", shard_2, 3)
-
-    shard1_ops = [op async for op in ssm.iter_operations(shard_1)]
-    shard2_ops = [op async for op in ssm.iter_operations(shard_2)]
-    shard3_ops = [op async for op in ssm.iter_operations(shard_3)]
-
-    assert len(shard1_ops) == 2
-    assert len(shard2_ops) == 2
-    assert len(shard3_ops) == 0
-
-    assert shard1_ops[0][0] == OperationCode.SET
-    assert shard1_ops[0][1] == brain_1
-    assert shard1_ops[1][0] == OperationCode.SET
-    assert shard1_ops[1][1] == brain_2
-
-    assert shard2_ops[0][0] == OperationCode.SET
-    assert shard2_ops[0][1] == brain_3
-    assert shard2_ops[1][0] == OperationCode.DELETE
-    assert shard2_ops[1][1] == "resource3"
-
-
-@pytest.mark.asyncio
-async def test_metadata(shadow_folder):
-    ssm = ShadowShardsManager(shadow_folder)
-
-    # Check that error is thrown if not loaded
-    with pytest.raises(ShadowShardsNotLoaded):
-        ssm.metadata
-
-    with pytest.raises(ShadowShardsNotLoaded):
-        await ssm.save_metadata()
-
-    # Check initial value
-    await ssm.load()
-    assert ssm.metadata.shards == {}
-
-    # Check initial shardinfo data
-    shard_1 = await ssm.create()
-
-    assert ssm.metadata.get_info(shard_1).created_at
-
-    # Check that it has been persisted in disk
-    ssm = ShadowShardsManager(shadow_folder)
-    await ssm.load()
-    assert ssm.metadata.get_info(shard_1).created_at
-
-    # Check that deleting the shard cleans up the metadata
-    await ssm.delete(shard_1)
-    assert ssm.metadata.get_info(shard_1) is None
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import pytest
+from nucliadb_protos.noderesources_pb2 import Resource
+
+from nucliadb_node.shadow_shards import (
+    OperationCode,
+    ShadowShardNotFound,
+    ShadowShardsManager,
+    ShadowShardsNotLoaded,
+    get_manager,
+)
+
+
+@pytest.mark.asyncio
+async def test_get_manager(shadow_folder):
+    shadow_shards = get_manager()
+    assert isinstance(shadow_shards, ShadowShardsManager)
+    assert not shadow_shards._folder.endswith("{data_path}")
+    assert shadow_shards._folder.endswith("/shadow_shards/")
+
+
+@pytest.mark.asyncio
+async def test_load(shadow_folder):
+    ssm = ShadowShardsManager(shadow_folder)
+    assert not ssm.loaded
+    await ssm.load()
+    assert ssm.loaded
+    assert len(ssm.shards) == 0
+
+    # Create a new shadow shard and make sure it's loaded
+    shard_id = await ssm.create()
+    assert shard_id
+    await ssm.load()
+    assert len(ssm.shards) == 1
+
+
+@pytest.mark.asyncio
+async def test_create(shadow_folder):
+    ssm = ShadowShardsManager(shadow_folder)
+    with pytest.raises(ShadowShardsNotLoaded):
+        await ssm.create()
+
+    await ssm.load()
+    assert len(ssm.shards) == 0
+
+    shard_id = await ssm.create()
+    assert len(ssm.shards) == 1
+    assert ssm.exists(shard_id)
+
+
+@pytest.mark.asyncio
+async def test_delete(shadow_folder):
+    ssm = ShadowShardsManager(shadow_folder)
+    with pytest.raises(ShadowShardsNotLoaded):
+        await ssm.delete("foo")
+
+    await ssm.load()
+    with pytest.raises(ShadowShardNotFound):
+        await ssm.delete("not-there")
+
+    shard_id = await ssm.create()
+    assert len(ssm.shards) == 1
+    assert ssm.exists(shard_id)
+
+    await ssm.delete(shard_id)
+    assert len(ssm.shards) == 0
+    assert not ssm.exists(shard_id)
+
+
+@pytest.mark.asyncio
+async def test_exists(shadow_folder):
+    ssm = ShadowShardsManager(shadow_folder)
+    with pytest.raises(ShadowShardsNotLoaded):
+        ssm.exists("foo")
+    await ssm.load()
+    assert not ssm.exists("foo")
+    shard_id = await ssm.create()
+    assert ssm.exists(shard_id)
+
+
+def get_brain(uuid: str) -> Resource:
+    br = Resource()
+    br.resource.uuid = uuid
+    return br
+
+
+@pytest.mark.asyncio
+async def test_operations(shadow_folder):
+    ssm = ShadowShardsManager(shadow_folder)
+
+    await ssm.load()
+    shard_1 = await ssm.create()
+    shard_2 = await ssm.create()
+    shard_3 = await ssm.create()
+
+    brain_1 = get_brain("resource1")
+    brain_2 = get_brain("resource2")
+    brain_3 = get_brain("resource3")
+
+    await ssm.set_resource(brain_2, shard_1, 10)
+    await ssm.set_resource(brain_1, shard_1, 2)
+
+    await ssm.set_resource(brain_3, shard_2, 2)
+    await ssm.delete_resource("resource3", shard_2, 3)
+
+    shard1_ops = [op async for op in ssm.iter_operations(shard_1)]
+    shard2_ops = [op async for op in ssm.iter_operations(shard_2)]
+    shard3_ops = [op async for op in ssm.iter_operations(shard_3)]
+
+    assert len(shard1_ops) == 2
+    assert len(shard2_ops) == 2
+    assert len(shard3_ops) == 0
+
+    assert shard1_ops[0][0] == OperationCode.SET
+    assert shard1_ops[0][1] == brain_1
+    assert shard1_ops[1][0] == OperationCode.SET
+    assert shard1_ops[1][1] == brain_2
+
+    assert shard2_ops[0][0] == OperationCode.SET
+    assert shard2_ops[0][1] == brain_3
+    assert shard2_ops[1][0] == OperationCode.DELETE
+    assert shard2_ops[1][1] == "resource3"
+
+
+@pytest.mark.asyncio
+async def test_metadata(shadow_folder):
+    ssm = ShadowShardsManager(shadow_folder)
+
+    # Check that error is thrown if not loaded
+    with pytest.raises(ShadowShardsNotLoaded):
+        ssm.metadata
+
+    with pytest.raises(ShadowShardsNotLoaded):
+        await ssm.save_metadata()
+
+    # Check initial value
+    await ssm.load()
+    assert ssm.metadata.shards == {}
+
+    # Check initial shardinfo data
+    shard_1 = await ssm.create()
+
+    assert ssm.metadata.get_info(shard_1).created_at
+
+    # Check that it has been persisted in disk
+    ssm = ShadowShardsManager(shadow_folder)
+    await ssm.load()
+    assert ssm.metadata.get_info(shard_1).created_at
+
+    # Check that deleting the shard cleans up the metadata
+    await ssm.delete(shard_1)
+    assert ssm.metadata.get_info(shard_1) is None
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/tests/test_sidecar_servicer.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/integration/test_sidecar_servicer.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import pytest
-from nucliadb_protos.noderesources_pb2 import EmptyQuery, ShardId
-from nucliadb_protos.nodesidecar_pb2_grpc import NodeSidecarStub
-
-
-@pytest.mark.asyncio
-async def test_create_delete_shadow_shards(
-    sidecar_stub: NodeSidecarStub, shadow_folder: str
-):
-    # Create a shadow shard
-    response = await sidecar_stub.CreateShadowShard(EmptyQuery())  # type: ignore
-    assert response.success
-
-    # Delete now
-    sipb = ShardId(id=response.shard.id)
-    response = await sidecar_stub.DeleteShadowShard(sipb)  # type: ignore
-    assert response.success
-
-    # Deleting again should succed (not found)
-    response = await sidecar_stub.DeleteShadowShard(sipb)  # type: ignore
-    assert response.success
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import pytest
+from nucliadb_protos.noderesources_pb2 import EmptyQuery, ShardId
+from nucliadb_protos.nodesidecar_pb2_grpc import NodeSidecarStub
+
+
+@pytest.mark.asyncio
+async def test_create_delete_shadow_shards(
+    sidecar_stub: NodeSidecarStub, shadow_folder: str
+):
+    # Create a shadow shard
+    response = await sidecar_stub.CreateShadowShard(EmptyQuery())  # type: ignore
+    assert response.success
+
+    # Delete now
+    sipb = ShardId(id=response.shard.id)
+    response = await sidecar_stub.DeleteShadowShard(sipb)  # type: ignore
+    assert response.success
+
+    # Deleting again should succed (not found)
+    response = await sidecar_stub.DeleteShadowShard(sipb)  # type: ignore
+    assert response.success
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/nucliadb_node/writer.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/writer.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,68 +1,62 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import Optional
-
-from grpc import aio
-from nucliadb_protos.noderesources_pb2 import (
-    EmptyQuery,
-    Resource,
-    ResourceID,
-    ShardId,
-    ShardIds,
-)
-from nucliadb_protos.nodewriter_pb2 import OpStatus
-from nucliadb_protos.nodewriter_pb2_grpc import NodeWriterStub
-
-from nucliadb_node import SERVICE_NAME  # type: ignore
-from nucliadb_telemetry.grpc import OpenTelemetryGRPC
-from nucliadb_telemetry.utils import get_telemetry
-
-
-class Writer:
-    _stub: Optional[NodeWriterStub] = None
-    lock: asyncio.Lock
-
-    def __init__(self, grpc_writer_address: str):
-        self.lock = asyncio.Lock()
-        tracer_provider = get_telemetry(SERVICE_NAME)
-        if tracer_provider is not None:  # pragma: no cover
-            telemetry_grpc = OpenTelemetryGRPC(
-                f"{SERVICE_NAME}_grpc_writer", tracer_provider
-            )
-            self.channel = telemetry_grpc.init_client(
-                grpc_writer_address, max_send_message=250
-            )
-        else:
-            self.channel = aio.insecure_channel(grpc_writer_address)
-        self.stub = NodeWriterStub(self.channel)
-
-    async def set_resource(self, pb: Resource) -> OpStatus:
-        return await self.stub.SetResource(pb)  # type: ignore
-
-    async def delete_resource(self, pb: ResourceID) -> OpStatus:
-        return await self.stub.RemoveResource(pb)  # type: ignore
-
-    async def garbage_collector(self, pb: ShardId):
-        await self.stub.GC(pb)  # type: ignore
-
-    async def shards(self) -> ShardIds:
-        pb = EmptyQuery()
-        return await self.stub.ListShards(pb)  # type: ignore
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import Optional
+
+from nucliadb_protos.noderesources_pb2 import (
+    EmptyQuery,
+    Resource,
+    ResourceID,
+    ShardId,
+    ShardIds,
+)
+from nucliadb_protos.nodewriter_pb2 import OpStatus
+from nucliadb_protos.nodewriter_pb2_grpc import NodeWriterStub
+from nucliadb_utils.grpc import get_traced_grpc_channel
+
+from nucliadb_node import SERVICE_NAME  # type: ignore
+
+
+class Writer:
+    _stub: Optional[NodeWriterStub] = None
+    lock: asyncio.Lock
+
+    def __init__(self, grpc_writer_address: str):
+        self.lock = asyncio.Lock()
+        self.channel = get_traced_grpc_channel(
+            grpc_writer_address, SERVICE_NAME, max_send_message=250
+        )
+        self.stub = NodeWriterStub(self.channel)
+
+    async def set_resource(self, pb: Resource) -> OpStatus:
+        return await self.stub.SetResource(pb)  # type: ignore
+
+    async def delete_resource(self, pb: ResourceID) -> OpStatus:
+        return await self.stub.RemoveResource(pb)  # type: ignore
+
+    async def garbage_collector(self, pb: ShardId):
+        await self.stub.GC(pb)  # type: ignore
+
+    async def shards(self) -> ShardIds:
+        pb = EmptyQuery()
+        return await self.stub.ListShards(pb)  # type: ignore
+
+    async def close(self):
+        await self.channel.close()
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/bin/payload_test.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/bin/payload_test.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,54 +1,52 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::io::Cursor;
-
-use nucliadb_core::protos::*;
-use nucliadb_node::reader::NodeReaderService;
-use nucliadb_node::writer::NodeWriterService;
-use prost::Message;
-
-fn main() -> anyhow::Result<()> {
-    let mut writer = NodeWriterService::new();
-    let reader = NodeReaderService::new();
-
-    let resources_dir = std::path::Path::new("/path/to/data");
-    let new_shard = writer.new_shard(&NewShardRequest::default())?;
-    let shard_id = ShardId { id: new_shard.id };
-    assert!(resources_dir.exists());
-    for file_path in std::fs::read_dir(resources_dir).unwrap() {
-        let file_path = file_path.unwrap().path();
-        println!("processing {file_path:?}");
-        let content = std::fs::read(&file_path).unwrap();
-        let resource = Resource::decode(&mut Cursor::new(content)).unwrap();
-        println!("Adding resource {}", file_path.display());
-        let res = writer.set_resource(&shard_id, &resource).unwrap();
-        assert!(res.is_some());
-        println!("Resource added: {:?}", res.unwrap());
-        let info = reader
-            .get_shard(&shard_id)
-            .unwrap()
-            .get_info(&GetShardRequest::default())
-            .unwrap();
-        println!("Sentences {}", info.sentences);
-        println!("Paragraphs {}", info.paragraphs);
-        println!("resources {}", info.resources);
-    }
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::io::Cursor;
+
+use nucliadb_core::protos::*;
+use nucliadb_node::reader::NodeReaderService;
+use nucliadb_node::writer::NodeWriterService;
+use prost::Message;
+
+fn main() -> anyhow::Result<()> {
+    let mut writer = NodeWriterService::new();
+    let reader = NodeReaderService::new();
+
+    let resources_dir = std::path::Path::new("/path/to/data");
+    let new_shard = writer.new_shard(&NewShardRequest::default())?;
+    let shard_id = ShardId { id: new_shard.id };
+    assert!(resources_dir.exists());
+    for file_path in std::fs::read_dir(resources_dir).unwrap() {
+        let file_path = file_path.unwrap().path();
+        println!("processing {file_path:?}");
+        let content = std::fs::read(&file_path).unwrap();
+        let resource = Resource::decode(&mut Cursor::new(content)).unwrap();
+        println!("Adding resource {}", file_path.display());
+        let res = writer.set_resource(&shard_id, &resource).unwrap();
+        assert!(res.is_some());
+        println!("Resource added: {:?}", res.unwrap());
+        let info = reader
+            .get_info(&shard_id, GetShardRequest::default())?
+            .unwrap();
+        println!("Sentences {}", info.sentences);
+        println!("Paragraphs {}", info.paragraphs);
+        println!("resources {}", info.resources);
+    }
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/bin/reader.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/bin/reader.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,93 +1,95 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::time::Instant;
-
-use nucliadb_core::protos::node_reader_server::NodeReaderServer;
-use nucliadb_core::tracing::*;
-use nucliadb_core::NodeResult;
-use nucliadb_node::env;
-use nucliadb_node::reader::grpc_driver::NodeReaderGRPCDriver;
-use nucliadb_node::reader::NodeReaderService;
-use nucliadb_node::telemetry::init_telemetry;
-use tokio::signal::unix::SignalKind;
-use tokio::signal::{ctrl_c, unix};
-use tonic::transport::Server;
-
-type GrpcServer = NodeReaderServer<NodeReaderGRPCDriver>;
-
-#[tokio::main]
-async fn main() -> Result<(), Box<dyn std::error::Error>> {
-    eprintln!("NucliaDB Reader Node starting...");
-    let _guard = init_telemetry()?;
-
-    let start_bootstrap = Instant::now();
-
-    let mut node_reader_service = NodeReaderService::new();
-
-    std::fs::create_dir_all(env::shards_path())?;
-    if !env::lazy_loading() {
-        node_reader_service.load_shards()?;
-    }
-
-    let grpc_driver = NodeReaderGRPCDriver::from(node_reader_service);
-
-    tokio::spawn(start_grpc_service(grpc_driver));
-
-    info!("Bootstrap complete in: {:?}", start_bootstrap.elapsed());
-    eprintln!("Running");
-
-    wait_for_sigkill().await?;
-
-    info!("Shutting down NucliaDB Reader Node...");
-    // wait some time to handle latest gRPC calls
-    tokio::time::sleep(env::shutdown_delay()).await;
-
-    Ok(())
-}
-
-async fn wait_for_sigkill() -> NodeResult<()> {
-    let mut sigterm = unix::signal(SignalKind::terminate())?;
-    let mut sigquit = unix::signal(SignalKind::quit())?;
-
-    tokio::select! {
-        _ = sigterm.recv() => println!("Terminating on SIGTERM"),
-        _ = sigquit.recv() => println!("Terminating on SIGQUIT"),
-        _ = ctrl_c() => println!("Terminating on ctrl-c"),
-    }
-
-    Ok(())
-}
-
-pub async fn start_grpc_service(grpc_driver: NodeReaderGRPCDriver) {
-    let addr = env::reader_listen_address();
-
-    info!("Reader listening for gRPC requests at: {:?}", addr);
-
-    let (mut health_reporter, health_service) = tonic_health::server::health_reporter();
-
-    health_reporter.set_serving::<GrpcServer>().await;
-
-    Server::builder()
-        .add_service(health_service)
-        .add_service(GrpcServer::new(grpc_driver))
-        .serve(addr)
-        .await
-        .expect("Error starting gRPC reader");
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::time::Instant;
+
+use nucliadb_core::protos::node_reader_server::NodeReaderServer;
+use nucliadb_core::tracing::*;
+use nucliadb_core::NodeResult;
+use nucliadb_node::env;
+use nucliadb_node::http_server::{run_http_metrics_server, MetricsServerOptions};
+use nucliadb_node::reader::grpc_driver::NodeReaderGRPCDriver;
+use nucliadb_node::reader::NodeReaderService;
+use nucliadb_node::telemetry::init_telemetry;
+use tokio::signal::unix::SignalKind;
+use tokio::signal::{ctrl_c, unix};
+use tonic::transport::Server;
+
+type GrpcServer = NodeReaderServer<NodeReaderGRPCDriver>;
+
+#[tokio::main]
+async fn main() -> Result<(), Box<dyn std::error::Error>> {
+    eprintln!("NucliaDB Reader Node starting...");
+    let _guard = init_telemetry()?;
+    let start_bootstrap = Instant::now();
+    let mut node_reader_service = NodeReaderService::new();
+
+    std::fs::create_dir_all(env::shards_path())?;
+    if !env::lazy_loading() {
+        node_reader_service.load_shards()?;
+    }
+
+    let grpc_driver = NodeReaderGRPCDriver::from(node_reader_service);
+    let _grpc_task = tokio::spawn(start_grpc_service(grpc_driver));
+    let metrics_task = tokio::spawn(run_http_metrics_server(MetricsServerOptions {
+        default_http_port: 3031,
+    }));
+
+    info!("Bootstrap complete in: {:?}", start_bootstrap.elapsed());
+    eprintln!("Running");
+
+    wait_for_sigkill().await?;
+    info!("Shutting down NucliaDB Reader Node...");
+    // wait some time to handle latest gRPC calls
+    tokio::time::sleep(env::shutdown_delay()).await;
+    metrics_task.abort();
+    let _ = metrics_task.await;
+
+    Ok(())
+}
+
+async fn wait_for_sigkill() -> NodeResult<()> {
+    let mut sigterm = unix::signal(SignalKind::terminate())?;
+    let mut sigquit = unix::signal(SignalKind::quit())?;
+
+    tokio::select! {
+        _ = sigterm.recv() => println!("Terminating on SIGTERM"),
+        _ = sigquit.recv() => println!("Terminating on SIGQUIT"),
+        _ = ctrl_c() => println!("Terminating on ctrl-c"),
+    }
+
+    Ok(())
+}
+
+pub async fn start_grpc_service(grpc_driver: NodeReaderGRPCDriver) {
+    let addr = env::reader_listen_address();
+
+    info!("Reader listening for gRPC requests at: {:?}", addr);
+
+    let (mut health_reporter, health_service) = tonic_health::server::health_reporter();
+
+    health_reporter.set_serving::<GrpcServer>().await;
+
+    Server::builder()
+        .add_service(health_service)
+        .add_service(GrpcServer::new(grpc_driver))
+        .serve(addr)
+        .await
+        .expect("Error starting gRPC reader");
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/bin/writer.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/bin/writer.rs`

 * *Files 18% similar despite different names*

```diff
@@ -1,272 +1,278 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::fs;
-use std::net::SocketAddr;
-use std::path::{Path, PathBuf};
-use std::str::FromStr;
-use std::time::Instant;
-
-use anyhow::{Context, Result};
-use futures::Stream;
-use nucliadb_cluster::{node, Key, Node, NodeHandle, NodeType};
-use nucliadb_core::protos::node_writer_server::NodeWriterServer;
-use nucliadb_core::tracing::*;
-use nucliadb_core::NodeResult;
-use nucliadb_node::env;
-use nucliadb_node::node_metadata::NodeMetadata;
-use nucliadb_node::telemetry::init_telemetry;
-use nucliadb_node::writer::grpc_driver::{NodeWriterEvent, NodeWriterGRPCDriver};
-use nucliadb_node::writer::NodeWriterService;
-use tokio::signal::unix::SignalKind;
-use tokio::signal::{ctrl_c, unix};
-use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
-use tokio_stream::StreamExt;
-use tonic::transport::Server;
-use uuid::Uuid;
-
-const LOAD_SCORE_KEY: Key<f32> = Key::new("load_score");
-const SHARD_COUNT_KEY: Key<u64> = Key::new("shard_count");
-
-type GrpcServer = NodeWriterServer<NodeWriterGRPCDriver>;
-
-#[derive(Debug)]
-pub enum NodeUpdate {
-    ShardCount(u64),
-    LoadScore(f32),
-}
-
-#[tokio::main]
-async fn main() -> NodeResult<()> {
-    eprintln!("NucliaDB Writer Node starting...");
-    let _guard = init_telemetry()?;
-
-    let start_bootstrap = Instant::now();
-
-    let metadata_path = env::metadata_path();
-    let node_metadata = NodeMetadata::load_or_create(&metadata_path).await?;
-    let mut node_writer_service = NodeWriterService::new();
-
-    std::fs::create_dir_all(env::shards_path())?;
-    if !env::lazy_loading() {
-        node_writer_service.load_shards()?;
-    }
-
-    let (metadata_sender, metadata_receiver) = tokio::sync::mpsc::unbounded_channel();
-    let (update_sender, update_receiver) = tokio::sync::mpsc::unbounded_channel();
-    let grpc_sender = metadata_sender.clone();
-    let grpc_driver = NodeWriterGRPCDriver::from(node_writer_service).with_sender(grpc_sender);
-    let host_key_path = env::host_key_path();
-    let public_ip = env::public_ip().await;
-    let chitchat_port = env::chitchat_port();
-    let seed_nodes = env::seed_nodes();
-
-    let chitchat_addr = SocketAddr::from_str(&format!("{}:{}", public_ip, chitchat_port))?;
-
-    // Cluster
-    let host_key = read_or_create_host_key(Path::new(&host_key_path))?;
-    let node = Node::builder()
-        .register_as(NodeType::Io)
-        .on_local_network(chitchat_addr)
-        .with_id(host_key.to_string())
-        .with_seed_nodes(seed_nodes)
-        .insert_to_initial_state(LOAD_SCORE_KEY, node_metadata.load_score())
-        .insert_to_initial_state(SHARD_COUNT_KEY, node_metadata.shard_count())
-        .build()?;
-    let node = node.start().await?;
-    let cluster_watcher = node.cluster_watcher().await;
-    let update_handle = node.clone();
-
-    nucliadb_telemetry::sync::start_telemetry_loop();
-
-    tokio::spawn(start_grpc_service(grpc_driver));
-    tokio::spawn(update_node_metadata(
-        update_sender,
-        metadata_receiver,
-        node_metadata,
-        metadata_path,
-    ));
-    let update_task = tokio::spawn(update_node_state(update_handle, update_receiver));
-    let monitor_task = tokio::spawn(monitor_cluster(cluster_watcher));
-
-    info!("Bootstrap complete in: {:?}", start_bootstrap.elapsed());
-    eprintln!("Running");
-
-    wait_for_sigkill().await?;
-
-    info!("Shutting down NucliaDB Writer Node...");
-    // abort all the tasks that hold a chitchat TCP/IP connection
-    monitor_task.abort();
-    update_task.abort();
-    let _ = monitor_task.await;
-    let _ = update_task.await;
-    // then close the chitchat TCP/IP connection
-    node.shutdown().await?;
-    // wait some time to handle latest gRPC calls
-    tokio::time::sleep(env::shutdown_delay()).await;
-
-    Ok(())
-}
-
-async fn wait_for_sigkill() -> NodeResult<()> {
-    let mut sigterm = unix::signal(SignalKind::terminate())?;
-    let mut sigquit = unix::signal(SignalKind::quit())?;
-
-    tokio::select! {
-        _ = sigterm.recv() => println!("Terminating on SIGTERM"),
-        _ = sigquit.recv() => println!("Terminating on SIGQUIT"),
-        _ = ctrl_c() => println!("Terminating on ctrl-c"),
-    }
-
-    Ok(())
-}
-
-pub async fn start_grpc_service(grpc_driver: NodeWriterGRPCDriver) {
-    let addr = env::writer_listen_address();
-
-    info!("Listening for gRPC requests at: {:?}", addr);
-
-    let (mut health_reporter, health_service) = tonic_health::server::health_reporter();
-
-    health_reporter.set_serving::<GrpcServer>().await;
-
-    Server::builder()
-        .add_service(health_service)
-        .add_service(GrpcServer::new(grpc_driver))
-        .serve(addr)
-        .await
-        .expect("Error starting gRPC service");
-}
-
-pub async fn monitor_cluster(cluster_watcher: impl Stream<Item = Vec<NodeHandle>>) {
-    info!("Start cluster monitoring");
-
-    let mut cluster_watcher = Box::pin(cluster_watcher);
-
-    loop {
-        debug!("Wait for cluster update");
-
-        if let Some(live_nodes) = cluster_watcher.next().await {
-            let cluster_snapshot = node::cluster_snapshot(live_nodes).await;
-
-            if let Ok(snapshot) = serde_json::to_string(&cluster_snapshot) {
-                info!("Cluster update: {snapshot}");
-            } else {
-                error!("Cluster snapshot cannot be serialized");
-            }
-        }
-    }
-}
-
-async fn update_node_state(node: NodeHandle, mut metadata_receiver: UnboundedReceiver<NodeUpdate>) {
-    info!("Start node update task");
-
-    while let Some(event) = metadata_receiver.recv().await {
-        info!("Receive node update event: {event:?}");
-
-        match event {
-            NodeUpdate::ShardCount(count) => node.update_state(SHARD_COUNT_KEY, count).await,
-            NodeUpdate::LoadScore(load_score) => {
-                node.update_state(LOAD_SCORE_KEY, load_score).await
-            }
-        }
-    }
-}
-
-pub async fn update_node_metadata(
-    update_sender: UnboundedSender<NodeUpdate>,
-    mut metadata_receiver: UnboundedReceiver<NodeWriterEvent>,
-    mut node_metadata: NodeMetadata,
-    path: PathBuf,
-) {
-    info!("Start node update task");
-
-    while let Some(event) = metadata_receiver.recv().await {
-        debug!("Receive metadata update event: {event:?}");
-
-        let result = match event {
-            NodeWriterEvent::ShardCreation(id, knowledge_box) => {
-                node_metadata.new_shard(id, knowledge_box, 0.0);
-                update_sender.send(NodeUpdate::ShardCount(node_metadata.shard_count()))
-            }
-            NodeWriterEvent::ShardDeletion(id) => {
-                node_metadata.delete_shard(id);
-                update_sender
-                    .send(NodeUpdate::ShardCount(node_metadata.shard_count()))
-                    .and_then(|_| {
-                        update_sender.send(NodeUpdate::LoadScore(node_metadata.load_score()))
-                    })
-            }
-            NodeWriterEvent::ParagraphCount(id, paragraph_count) => {
-                node_metadata.update_shard(id, paragraph_count);
-                update_sender.send(NodeUpdate::LoadScore(node_metadata.load_score()))
-            }
-        };
-
-        if let Err(e) = result {
-            warn!("Cannot send node update: {e:?}");
-        }
-
-        if let Err(e) = node_metadata.save(&path) {
-            error!("Node metadata update failed: {e}");
-        } else {
-            info!("Node metadata file updated successfully");
-        }
-    }
-
-    info!("Node update task stopped");
-}
-
-pub fn read_host_key(host_key_path: &Path) -> Result<Uuid> {
-    let host_key_contents = fs::read(host_key_path)
-        .with_context(|| format!("Failed to read host key from '{}'", host_key_path.display()))?;
-
-    let host_key = Uuid::from_slice(host_key_contents.as_slice())
-        .with_context(|| format!("Invalid host key from '{}'", host_key_path.display()))?;
-
-    Ok(host_key)
-}
-
-/// Reads the key that makes a node unique from the given file.
-/// If the file does not exist, it generates an ID and writes it to the file
-/// so that it can be reused on reboot.
-pub fn read_or_create_host_key(host_key_path: &Path) -> Result<Uuid> {
-    let host_key;
-
-    if host_key_path.exists() {
-        host_key = read_host_key(host_key_path)?;
-        info!(host_key=?host_key, host_key_path=?host_key_path, "Read existing host key.");
-    } else {
-        if let Some(dir) = host_key_path.parent() {
-            if !dir.exists() {
-                fs::create_dir_all(dir).with_context(|| {
-                    format!("Failed to create host key directory '{}'", dir.display())
-                })?;
-            }
-        }
-        host_key = Uuid::new_v4();
-        fs::write(host_key_path, host_key.as_bytes()).with_context(|| {
-            format!("Failed to write host key to '{}'", host_key_path.display())
-        })?;
-        info!(host_key=?host_key, host_key_path=?host_key_path, "Create new host key.");
-    }
-
-    Ok(host_key)
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fs;
+use std::net::SocketAddr;
+use std::path::{Path, PathBuf};
+use std::str::FromStr;
+use std::time::Instant;
+
+use anyhow::{Context, Result};
+use futures::Stream;
+use nucliadb_cluster::{node, Key, Node, NodeHandle, NodeType};
+use nucliadb_core::protos::node_writer_server::NodeWriterServer;
+use nucliadb_core::tracing::*;
+use nucliadb_core::NodeResult;
+use nucliadb_node::env;
+use nucliadb_node::http_server::{run_http_metrics_server, MetricsServerOptions};
+use nucliadb_node::node_metadata::NodeMetadata;
+use nucliadb_node::telemetry::init_telemetry;
+use nucliadb_node::writer::grpc_driver::{NodeWriterEvent, NodeWriterGRPCDriver};
+use nucliadb_node::writer::NodeWriterService;
+use tokio::signal::unix::SignalKind;
+use tokio::signal::{ctrl_c, unix};
+use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
+use tokio_stream::StreamExt;
+use tonic::transport::Server;
+use uuid::Uuid;
+
+const LOAD_SCORE_KEY: Key<f32> = Key::new("load_score");
+const SHARD_COUNT_KEY: Key<u64> = Key::new("shard_count");
+
+type GrpcServer = NodeWriterServer<NodeWriterGRPCDriver>;
+
+#[derive(Debug)]
+pub enum NodeUpdate {
+    ShardCount(u64),
+    LoadScore(f32),
+}
+
+#[tokio::main]
+async fn main() -> NodeResult<()> {
+    eprintln!("NucliaDB Writer Node starting...");
+    let _guard = init_telemetry()?;
+
+    let start_bootstrap = Instant::now();
+
+    let metadata_path = env::metadata_path();
+    let node_metadata = NodeMetadata::load_or_create(&metadata_path)?;
+    let mut node_writer_service = NodeWriterService::new();
+
+    std::fs::create_dir_all(env::shards_path())?;
+    if !env::lazy_loading() {
+        node_writer_service.load_shards()?;
+    }
+
+    let (metadata_sender, metadata_receiver) = tokio::sync::mpsc::unbounded_channel();
+    let (update_sender, update_receiver) = tokio::sync::mpsc::unbounded_channel();
+    let grpc_sender = metadata_sender.clone();
+    let grpc_driver = NodeWriterGRPCDriver::from(node_writer_service).with_sender(grpc_sender);
+    let host_key_path = env::host_key_path();
+    let public_ip = env::public_ip().await;
+    let chitchat_port = env::chitchat_port();
+    let seed_nodes = env::seed_nodes();
+
+    let chitchat_addr = SocketAddr::from_str(&format!("{}:{}", public_ip, chitchat_port))?;
+
+    // Cluster
+    let host_key = read_or_create_host_key(Path::new(&host_key_path))?;
+    let node = Node::builder()
+        .register_as(NodeType::Io)
+        .on_local_network(chitchat_addr)
+        .with_id(host_key.to_string())
+        .with_seed_nodes(seed_nodes)
+        .insert_to_initial_state(LOAD_SCORE_KEY, node_metadata.load_score())
+        .insert_to_initial_state(SHARD_COUNT_KEY, node_metadata.shard_count())
+        .build()?;
+    let node = node.start().await?;
+    let cluster_watcher = node.cluster_watcher().await;
+    let update_handle = node.clone();
+
+    nucliadb_telemetry::sync::start_telemetry_loop();
+
+    tokio::spawn(start_grpc_service(grpc_driver));
+    tokio::spawn(update_node_metadata(
+        update_sender,
+        metadata_receiver,
+        node_metadata,
+        metadata_path,
+    ));
+    let update_task = tokio::spawn(update_node_state(update_handle, update_receiver));
+    let monitor_task = tokio::spawn(monitor_cluster(cluster_watcher));
+    let metrics_task = tokio::spawn(run_http_metrics_server(MetricsServerOptions {
+        default_http_port: 3032,
+    }));
+
+    info!("Bootstrap complete in: {:?}", start_bootstrap.elapsed());
+    eprintln!("Running");
+
+    wait_for_sigkill().await?;
+
+    info!("Shutting down NucliaDB Writer Node...");
+    // abort all the tasks that hold a chitchat TCP/IP connection
+    monitor_task.abort();
+    update_task.abort();
+    metrics_task.abort();
+    let _ = monitor_task.await;
+    let _ = update_task.await;
+    let _ = metrics_task.await;
+    // then close the chitchat TCP/IP connection
+    node.shutdown().await?;
+    // wait some time to handle latest gRPC calls
+    tokio::time::sleep(env::shutdown_delay()).await;
+
+    Ok(())
+}
+
+async fn wait_for_sigkill() -> NodeResult<()> {
+    let mut sigterm = unix::signal(SignalKind::terminate())?;
+    let mut sigquit = unix::signal(SignalKind::quit())?;
+
+    tokio::select! {
+        _ = sigterm.recv() => println!("Terminating on SIGTERM"),
+        _ = sigquit.recv() => println!("Terminating on SIGQUIT"),
+        _ = ctrl_c() => println!("Terminating on ctrl-c"),
+    }
+
+    Ok(())
+}
+
+pub async fn start_grpc_service(grpc_driver: NodeWriterGRPCDriver) {
+    let addr = env::writer_listen_address();
+
+    info!("Listening for gRPC requests at: {:?}", addr);
+
+    let (mut health_reporter, health_service) = tonic_health::server::health_reporter();
+
+    health_reporter.set_serving::<GrpcServer>().await;
+
+    Server::builder()
+        .add_service(health_service)
+        .add_service(GrpcServer::new(grpc_driver))
+        .serve(addr)
+        .await
+        .expect("Error starting gRPC service");
+}
+
+pub async fn monitor_cluster(cluster_watcher: impl Stream<Item = Vec<NodeHandle>>) {
+    info!("Start cluster monitoring");
+
+    let mut cluster_watcher = Box::pin(cluster_watcher);
+
+    loop {
+        debug!("Wait for cluster update");
+
+        if let Some(live_nodes) = cluster_watcher.next().await {
+            let cluster_snapshot = node::cluster_snapshot(live_nodes).await;
+
+            if let Ok(snapshot) = serde_json::to_string(&cluster_snapshot) {
+                info!("Cluster update: {snapshot}");
+            } else {
+                error!("Cluster snapshot cannot be serialized");
+            }
+        }
+    }
+}
+
+async fn update_node_state(node: NodeHandle, mut metadata_receiver: UnboundedReceiver<NodeUpdate>) {
+    info!("Start node update task");
+
+    while let Some(event) = metadata_receiver.recv().await {
+        info!("Receive node update event: {event:?}");
+
+        match event {
+            NodeUpdate::ShardCount(count) => node.update_state(SHARD_COUNT_KEY, count).await,
+            NodeUpdate::LoadScore(load_score) => {
+                node.update_state(LOAD_SCORE_KEY, load_score).await
+            }
+        }
+    }
+}
+
+pub async fn update_node_metadata(
+    update_sender: UnboundedSender<NodeUpdate>,
+    mut metadata_receiver: UnboundedReceiver<NodeWriterEvent>,
+    mut node_metadata: NodeMetadata,
+    path: PathBuf,
+) {
+    info!("Start node update task");
+
+    while let Some(event) = metadata_receiver.recv().await {
+        debug!("Receive metadata update event: {event:?}");
+
+        let result = match event {
+            NodeWriterEvent::ShardCreation(id, knowledge_box) => {
+                node_metadata.new_shard(id, knowledge_box, 0.0);
+                update_sender.send(NodeUpdate::ShardCount(node_metadata.shard_count()))
+            }
+            NodeWriterEvent::ShardDeletion(id) => {
+                node_metadata.delete_shard(id);
+                update_sender
+                    .send(NodeUpdate::ShardCount(node_metadata.shard_count()))
+                    .and_then(|_| {
+                        update_sender.send(NodeUpdate::LoadScore(node_metadata.load_score()))
+                    })
+            }
+            NodeWriterEvent::ParagraphCount(id, paragraph_count) => {
+                node_metadata.update_shard(id, paragraph_count);
+                update_sender.send(NodeUpdate::LoadScore(node_metadata.load_score()))
+            }
+        };
+
+        if let Err(e) = result {
+            warn!("Cannot send node update: {e:?}");
+        }
+
+        if let Err(e) = node_metadata.save(&path) {
+            error!("Node metadata update failed: {e}");
+        } else {
+            info!("Node metadata file updated successfully");
+        }
+    }
+
+    info!("Node update task stopped");
+}
+
+pub fn read_host_key(host_key_path: &Path) -> Result<Uuid> {
+    let host_key_contents = fs::read(host_key_path)
+        .with_context(|| format!("Failed to read host key from '{}'", host_key_path.display()))?;
+
+    let host_key = Uuid::from_slice(host_key_contents.as_slice())
+        .with_context(|| format!("Invalid host key from '{}'", host_key_path.display()))?;
+
+    Ok(host_key)
+}
+
+/// Reads the key that makes a node unique from the given file.
+/// If the file does not exist, it generates an ID and writes it to the file
+/// so that it can be reused on reboot.
+pub fn read_or_create_host_key(host_key_path: &Path) -> Result<Uuid> {
+    let host_key;
+
+    if host_key_path.exists() {
+        host_key = read_host_key(host_key_path)?;
+        info!(host_key=?host_key, host_key_path=?host_key_path, "Read existing host key.");
+    } else {
+        if let Some(dir) = host_key_path.parent() {
+            if !dir.exists() {
+                fs::create_dir_all(dir).with_context(|| {
+                    format!("Failed to create host key directory '{}'", dir.display())
+                })?;
+            }
+        }
+        host_key = Uuid::new_v4();
+        fs::write(host_key_path, host_key.as_bytes()).with_context(|| {
+            format!("Failed to write host key to '{}'", host_key_path.display())
+        })?;
+        info!(host_key=?host_key, host_key_path=?host_key_path, "Create new host key.");
+    }
+
+    Ok(host_key)
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/env.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/env.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,281 +1,295 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::env;
-use std::net::{IpAddr, SocketAddr, ToSocketAddrs};
-use std::path::PathBuf;
-use std::str::FromStr;
-use std::time::Duration;
-
-use nucliadb_core::tracing::*;
-
-use crate::utils::{parse_log_level, reliable_lookup_host};
-
-const SENTRY_PROD: &str = "prod";
-const SENTRY_DEV: &str = "stage";
-
-/// Where data will be stored
-pub fn data_path() -> PathBuf {
-    match env::var("DATA_PATH") {
-        Ok(var) => PathBuf::from(var),
-        Err(_) => PathBuf::from("data"),
-    }
-}
-
-/// Path for metadata file inside data folder
-pub fn metadata_path() -> PathBuf {
-    data_path().join("metadata.json")
-}
-
-/// Path for shards information inside data folder
-pub fn shards_path() -> PathBuf {
-    data_path().join("shards")
-}
-
-pub fn shards_path_id(id: &str) -> PathBuf {
-    shards_path().join(id)
-}
-
-/// Reader GRPC service port
-pub fn reader_listen_address() -> SocketAddr {
-    let port = chitchat_port() + 2;
-
-    let default = SocketAddr::new(IpAddr::from_str("::1").unwrap(), port);
-
-    match env::var("READER_LISTEN_ADDRESS") {
-        Ok(var) => var
-            .to_socket_addrs()
-            .unwrap()
-            .next()
-            .expect("Error parsing Socket address for swim peers addrs"),
-        Err(_) => {
-            warn!(
-                "READER_LISTEN_ADDRESS not defined. Defaulting to: {}",
-                default
-            );
-            default
-        }
-    }
-}
-
-pub fn jaeger_agent_endp() -> String {
-    let default_host = "localhost".to_string();
-    let default_port = "6831".to_string();
-    let host = env::var("JAEGER_AGENT_HOST").unwrap_or_else(|_| {
-        warn!("JAEGER_AGENT_HOST not defined: Defaulting to {default_host}");
-        default_host
-    });
-    let port = env::var("JAEGER_AGENT_PORT").unwrap_or_else(|_| {
-        warn!("JAEGER_AGENT_PORT not defined: Defaulting to {default_port}");
-        default_port
-    });
-    format!("{host}:{port}")
-}
-
-pub fn jaeger_enabled() -> bool {
-    let default = false;
-    match env::var("JAEGER_ENABLED") {
-        Ok(v) => bool::from_str(&v).unwrap(),
-        Err(_) => {
-            warn!("JAEGER_ENABLED not defined: Defaulting to {}", default);
-            default
-        }
-    }
-}
-
-pub fn writer_listen_address() -> SocketAddr {
-    let port = chitchat_port() + 1;
-    let default = SocketAddr::new(IpAddr::from_str("::1").unwrap(), port);
-
-    match env::var("WRITER_LISTEN_ADDRESS") {
-        Ok(var) => var
-            .to_socket_addrs()
-            .unwrap()
-            .next()
-            .expect("Error parsing Socket address for swim peers addrs"),
-        Err(_) => {
-            warn!(
-                "WRITER_LISTEN_ADDRESS not defined. Defaulting to: {}",
-                default
-            );
-            default
-        }
-    }
-}
-
-pub fn lazy_loading() -> bool {
-    let default = true;
-    match env::var("LAZY_LOADING") {
-        Ok(var) => match var.parse() {
-            Ok(value) => value,
-            Err(_) => {
-                error!("Error parsing environment variable LAZY_LOADING");
-                default
-            }
-        },
-        Err(_) => {
-            warn!("LAZY_LOADING not defined. Defaulting to: {}", default);
-            default
-        }
-    }
-}
-
-pub fn host_key_path() -> String {
-    let default = String::from("host_key");
-    match env::var("HOST_KEY_PATH") {
-        Ok(var) => match var.parse() {
-            Ok(value) => value,
-            Err(_) => {
-                error!("Error parsing environment variable HOST_KEY_PATH");
-                default
-            }
-        },
-        Err(_) => default,
-    }
-}
-
-pub fn chitchat_port() -> u16 {
-    let default: u16 = 40100;
-    match env::var("CHITCHAT_PORT") {
-        Ok(var) => u16::from_str(&var).unwrap_or_else(|e| {
-            error!("Can't parse CHITCHAT_PORT variable: {e}");
-            default
-        }),
-        Err(_) => {
-            warn!("CHITCHAT_PORT not defined. Defaulting to: {}", default);
-            default
-        }
-    }
-}
-
-pub async fn public_ip() -> IpAddr {
-    let default = IpAddr::from_str("::1").unwrap();
-    match env::var("HOSTNAME") {
-        Ok(v) => {
-            let host = format!("{}:4444", &v);
-            reliable_lookup_host(&host).await
-        }
-        Err(e) => {
-            error!("HOSTNAME node defined. Defaulting to: {default}. Error details: {e}");
-            default
-        }
-    }
-}
-
-pub fn seed_nodes() -> Vec<String> {
-    let default = vec![];
-    match env::var("SEED_NODES") {
-        Ok(v) => v.split(';').map(|addr| addr.to_string()).collect(),
-        Err(e) => {
-            error!("Error parsing environment varialbe SEED_NODES: {e}");
-            default
-        }
-    }
-}
-
-pub fn sentry_url() -> String {
-    let default = String::from("");
-    match env::var("SENTRY_URL") {
-        Ok(var) => match var.parse() {
-            Ok(value) => value,
-            Err(_) => {
-                error!("Error parsing environment variable SENTRY_URL");
-                default
-            }
-        },
-        Err(_) => default,
-    }
-}
-
-pub fn log_level() -> Vec<(String, Level)> {
-    let default = "nucliadb_node=WARN,nucliadb_cluster=WARN,nucliadb_cluster=WARN".to_string();
-    match env::var("RUST_LOG") {
-        Ok(levels) => parse_log_level(&levels),
-        Err(_) => {
-            error!("RUST_LOG not defined. Defaulting to {default}");
-            parse_log_level(&default)
-        }
-    }
-}
-
-pub fn get_sentry_env() -> &'static str {
-    let default = SENTRY_DEV;
-    match env::var("RUNNING_ENVIRONMENT") {
-        Ok(sentry_env) if sentry_env.eq("prod") => SENTRY_PROD,
-        Ok(sentry_env) if sentry_env.eq("stage") => SENTRY_DEV,
-        Ok(_) => {
-            error!("RUNNING_ENVIRONMENT defined incorrectly. Defaulting to {default}");
-            default
-        }
-        Err(_) => {
-            error!("RUNNING_ENVIRONMENT not defined. Defaulting to {default}");
-            default
-        }
-    }
-}
-
-/// Retuns the liveliness interval update used by cluster node.
-pub fn get_cluster_liveliness_interval_update() -> Duration {
-    const DEFAULT_INTERVAL_UPDATE_PLACEHOLDER: &str = "500ms";
-    const DEFAULT_INTERVAL_UPDATE: Duration = Duration::from_millis(500);
-
-    match env::var("LIVELINESS_UPDATE") {
-        Ok(value) => {
-            if let Ok(duration) = parse_duration::parse(&value) {
-                duration
-            } else {
-                error!(
-                    "LIVELINESS_UPDATE defined incorrectly. Defaulting to \
-                     {DEFAULT_INTERVAL_UPDATE_PLACEHOLDER}"
-                );
-
-                DEFAULT_INTERVAL_UPDATE
-            }
-        }
-        Err(_) => {
-            warn!(
-                "LIVELINESS_UPDATE not defined. Defaulting to \
-                 {DEFAULT_INTERVAL_UPDATE_PLACEHOLDER}"
-            );
-
-            DEFAULT_INTERVAL_UPDATE
-        }
-    }
-}
-
-pub fn shutdown_delay() -> Duration {
-    const SHUTDOWN_DELAY_KEY: &str = "SHUTDOWN_DELAY";
-    const SHUTDOWN_DELAY_PLACEHOLDER: &str = "5s";
-    const DEFAULT_SHUTDOWN_DELAY: Duration = Duration::from_secs(5);
-
-    let Ok(value) = env::var(SHUTDOWN_DELAY_KEY) else {
-        warn!("{SHUTDOWN_DELAY_KEY} not defined. Defaulting to {SHUTDOWN_DELAY_PLACEHOLDER}");
-        return DEFAULT_SHUTDOWN_DELAY
-    };
-
-    let Ok(duration) = parse_duration::parse(&value) else {
-        error!(
-            "{SHUTDOWN_DELAY_KEY} defined incorrectly. Defaulting to \
-             {SHUTDOWN_DELAY_PLACEHOLDER}"
-        );
-        return DEFAULT_SHUTDOWN_DELAY
-    };
-
-    duration
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::env;
+use std::net::{IpAddr, SocketAddr, ToSocketAddrs};
+use std::path::PathBuf;
+use std::str::FromStr;
+use std::time::Duration;
+
+use nucliadb_core::tracing::*;
+
+use crate::utils::{parse_log_level, reliable_lookup_host};
+
+const SENTRY_PROD: &str = "prod";
+const SENTRY_DEV: &str = "stage";
+
+/// Where data will be stored
+pub fn data_path() -> PathBuf {
+    match env::var("DATA_PATH") {
+        Ok(var) => PathBuf::from(var),
+        Err(_) => PathBuf::from("data"),
+    }
+}
+
+/// Path for metadata file inside data folder
+pub fn metadata_path() -> PathBuf {
+    data_path().join("metadata.json")
+}
+
+/// Path for shards information inside data folder
+pub fn shards_path() -> PathBuf {
+    data_path().join("shards")
+}
+
+pub fn shards_path_id(id: &str) -> PathBuf {
+    shards_path().join(id)
+}
+
+/// Reader GRPC service port
+pub fn reader_listen_address() -> SocketAddr {
+    let port = chitchat_port() + 2;
+
+    let default = SocketAddr::new(IpAddr::from_str("::1").unwrap(), port);
+
+    match env::var("READER_LISTEN_ADDRESS") {
+        Ok(var) => var
+            .to_socket_addrs()
+            .unwrap()
+            .next()
+            .expect("Error parsing Socket address for swim peers addrs"),
+        Err(_) => {
+            warn!(
+                "READER_LISTEN_ADDRESS not defined. Defaulting to: {}",
+                default
+            );
+            default
+        }
+    }
+}
+
+pub fn jaeger_agent_endp() -> String {
+    let default_host = "localhost".to_string();
+    let default_port = "6831".to_string();
+    let host = env::var("JAEGER_AGENT_HOST").unwrap_or_else(|_| {
+        warn!("JAEGER_AGENT_HOST not defined: Defaulting to {default_host}");
+        default_host
+    });
+    let port = env::var("JAEGER_AGENT_PORT").unwrap_or_else(|_| {
+        warn!("JAEGER_AGENT_PORT not defined: Defaulting to {default_port}");
+        default_port
+    });
+    format!("{host}:{port}")
+}
+
+pub fn jaeger_enabled() -> bool {
+    let default = false;
+    match env::var("JAEGER_ENABLED") {
+        Ok(v) => bool::from_str(&v).unwrap(),
+        Err(_) => {
+            warn!("JAEGER_ENABLED not defined: Defaulting to {}", default);
+            default
+        }
+    }
+}
+
+pub fn writer_listen_address() -> SocketAddr {
+    let port = chitchat_port() + 1;
+    let default = SocketAddr::new(IpAddr::from_str("::1").unwrap(), port);
+
+    match env::var("WRITER_LISTEN_ADDRESS") {
+        Ok(var) => var
+            .to_socket_addrs()
+            .unwrap()
+            .next()
+            .expect("Error parsing Socket address for swim peers addrs"),
+        Err(_) => {
+            warn!(
+                "WRITER_LISTEN_ADDRESS not defined. Defaulting to: {}",
+                default
+            );
+            default
+        }
+    }
+}
+
+pub fn lazy_loading() -> bool {
+    let default = true;
+    match env::var("LAZY_LOADING") {
+        Ok(var) => match var.parse() {
+            Ok(value) => value,
+            Err(_) => {
+                error!("Error parsing environment variable LAZY_LOADING");
+                default
+            }
+        },
+        Err(_) => {
+            warn!("LAZY_LOADING not defined. Defaulting to: {}", default);
+            default
+        }
+    }
+}
+
+pub fn host_key_path() -> String {
+    let default = String::from("host_key");
+    match env::var("HOST_KEY_PATH") {
+        Ok(var) => match var.parse() {
+            Ok(value) => value,
+            Err(_) => {
+                error!("Error parsing environment variable HOST_KEY_PATH");
+                default
+            }
+        },
+        Err(_) => default,
+    }
+}
+
+pub fn chitchat_port() -> u16 {
+    let default: u16 = 40100;
+    match env::var("CHITCHAT_PORT") {
+        Ok(var) => u16::from_str(&var).unwrap_or_else(|e| {
+            error!("Can't parse CHITCHAT_PORT variable: {e}");
+            default
+        }),
+        Err(_) => {
+            warn!("CHITCHAT_PORT not defined. Defaulting to: {}", default);
+            default
+        }
+    }
+}
+
+pub async fn public_ip() -> IpAddr {
+    let default = IpAddr::from_str("::1").unwrap();
+    match env::var("HOSTNAME") {
+        Ok(v) => {
+            let host = format!("{}:4444", &v);
+            reliable_lookup_host(&host).await
+        }
+        Err(e) => {
+            error!("HOSTNAME node defined. Defaulting to: {default}. Error details: {e}");
+            default
+        }
+    }
+}
+
+pub fn seed_nodes() -> Vec<String> {
+    let default = vec![];
+    match env::var("SEED_NODES") {
+        Ok(v) => v.split(';').map(|addr| addr.to_string()).collect(),
+        Err(e) => {
+            error!("Error parsing environment varialbe SEED_NODES: {e}");
+            default
+        }
+    }
+}
+
+pub fn sentry_url() -> String {
+    let default = String::from("");
+    match env::var("SENTRY_URL") {
+        Ok(var) => match var.parse() {
+            Ok(value) => value,
+            Err(_) => {
+                error!("Error parsing environment variable SENTRY_URL");
+                default
+            }
+        },
+        Err(_) => default,
+    }
+}
+
+pub fn log_level() -> Vec<(String, Level)> {
+    let default = "nucliadb_node=WARN,nucliadb_cluster=WARN,nucliadb_cluster=WARN".to_string();
+    match env::var("RUST_LOG") {
+        Ok(levels) => parse_log_level(&levels),
+        Err(_) => {
+            error!("RUST_LOG not defined. Defaulting to {default}");
+            parse_log_level(&default)
+        }
+    }
+}
+
+pub fn get_sentry_env() -> &'static str {
+    let default = SENTRY_DEV;
+    match env::var("RUNNING_ENVIRONMENT") {
+        Ok(sentry_env) if sentry_env.eq("prod") => SENTRY_PROD,
+        Ok(sentry_env) if sentry_env.eq("stage") => SENTRY_DEV,
+        Ok(_) => {
+            error!("RUNNING_ENVIRONMENT defined incorrectly. Defaulting to {default}");
+            default
+        }
+        Err(_) => {
+            error!("RUNNING_ENVIRONMENT not defined. Defaulting to {default}");
+            default
+        }
+    }
+}
+
+/// Retuns the liveliness interval update used by cluster node.
+pub fn get_cluster_liveliness_interval_update() -> Duration {
+    const DEFAULT_INTERVAL_UPDATE_PLACEHOLDER: &str = "500ms";
+    const DEFAULT_INTERVAL_UPDATE: Duration = Duration::from_millis(500);
+
+    match env::var("LIVELINESS_UPDATE") {
+        Ok(value) => {
+            if let Ok(duration) = parse_duration::parse(&value) {
+                duration
+            } else {
+                error!(
+                    "LIVELINESS_UPDATE defined incorrectly. Defaulting to \
+                     {DEFAULT_INTERVAL_UPDATE_PLACEHOLDER}"
+                );
+
+                DEFAULT_INTERVAL_UPDATE
+            }
+        }
+        Err(_) => {
+            warn!(
+                "LIVELINESS_UPDATE not defined. Defaulting to \
+                 {DEFAULT_INTERVAL_UPDATE_PLACEHOLDER}"
+            );
+
+            DEFAULT_INTERVAL_UPDATE
+        }
+    }
+}
+
+pub fn shutdown_delay() -> Duration {
+    const SHUTDOWN_DELAY_KEY: &str = "SHUTDOWN_DELAY";
+    const SHUTDOWN_DELAY_PLACEHOLDER: &str = "5s";
+    const DEFAULT_SHUTDOWN_DELAY: Duration = Duration::from_secs(5);
+
+    let Ok(value) = env::var(SHUTDOWN_DELAY_KEY) else {
+        warn!("{SHUTDOWN_DELAY_KEY} not defined. Defaulting to {SHUTDOWN_DELAY_PLACEHOLDER}");
+        return DEFAULT_SHUTDOWN_DELAY
+    };
+
+    let Ok(duration) = parse_duration::parse(&value) else {
+        error!(
+            "{SHUTDOWN_DELAY_KEY} defined incorrectly. Defaulting to \
+             {SHUTDOWN_DELAY_PLACEHOLDER}"
+        );
+        return DEFAULT_SHUTDOWN_DELAY
+    };
+
+    duration
+}
+
+pub fn metrics_http_port(default: u16) -> u16 {
+    match env::var("METRICS_HTTP_PORT") {
+        Ok(http_port) => {
+            if let Ok(port) = http_port.parse() {
+                port
+            } else {
+                error!("METRICS_HTTP_PORT defined incorrectly. Defaulting to {default}");
+                default
+            }
+        }
+        Err(_) => default,
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/reader/grpc_driver.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/reader/grpc_driver.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,487 +1,493 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use async_std::sync::RwLock;
-use nucliadb_core::prelude::{DocumentIterator, ParagraphIterator};
-use nucliadb_core::protos::node_reader_server::NodeReader;
-use nucliadb_core::protos::*;
-use nucliadb_core::tracing::{self, *};
-use opentelemetry::global;
-use tracing_opentelemetry::OpenTelemetrySpanExt;
-use Shard as ShardPB;
-
-use crate::env;
-use crate::reader::NodeReaderService;
-use crate::utils::MetadataMap;
-
-pub struct NodeReaderGRPCDriver(RwLock<NodeReaderService>);
-impl From<NodeReaderService> for NodeReaderGRPCDriver {
-    fn from(node: NodeReaderService) -> NodeReaderGRPCDriver {
-        NodeReaderGRPCDriver(RwLock::new(node))
-    }
-}
-impl NodeReaderGRPCDriver {
-    // The GRPC reader will only request the reader to bring a shard
-    // to memory if lazy loading is enabled. Otherwise all the
-    // shards on disk would have been brought to memory before the driver is online.
-    #[tracing::instrument(skip_all)]
-    async fn shard_loading(&self, id: &ShardId) {
-        if env::lazy_loading() {
-            let mut writer = self.0.write().await;
-            writer.load_shard(id);
-        }
-    }
-
-    // Instrumentation utilities for telemetry
-    fn instrument<T>(&self, request: &tonic::Request<T>) {
-        let parent_cx =
-            global::get_text_map_propagator(|prop| prop.extract(&MetadataMap(request.metadata())));
-        Span::current().set_parent(parent_cx);
-    }
-}
-
-pub struct GrpcStreaming<T>(T);
-impl futures_core::Stream for GrpcStreaming<ParagraphIterator> {
-    type Item = Result<ParagraphItem, tonic::Status>;
-
-    fn poll_next(
-        mut self: std::pin::Pin<&mut Self>,
-        _: &mut std::task::Context<'_>,
-    ) -> std::task::Poll<Option<Self::Item>> {
-        std::task::Poll::Ready(self.0.next().map(Ok))
-    }
-}
-impl futures_core::Stream for GrpcStreaming<DocumentIterator> {
-    type Item = Result<DocumentItem, tonic::Status>;
-
-    fn poll_next(
-        mut self: std::pin::Pin<&mut Self>,
-        _: &mut std::task::Context<'_>,
-    ) -> std::task::Poll<Option<Self::Item>> {
-        std::task::Poll::Ready(self.0.next().map(Ok))
-    }
-}
-
-#[tonic::async_trait]
-impl NodeReader for NodeReaderGRPCDriver {
-    type ParagraphsStream = GrpcStreaming<ParagraphIterator>;
-    type DocumentsStream = GrpcStreaming<DocumentIterator>;
-    async fn paragraphs(
-        &self,
-        request: tonic::Request<StreamRequest>,
-    ) -> Result<tonic::Response<Self::ParagraphsStream>, tonic::Status> {
-        info!("Starting paragraph streaming");
-        self.instrument(&request);
-        let request = request.into_inner();
-        let Some(shard_id) = request.shard_id.clone() else {
-            return Err(tonic::Status::not_found("Shard ID not present"));
-        };
-        let shard_id = ShardId { id: shard_id.id };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.paragraph_iterator(&shard_id, request).transpose() {
-            Some(Ok(response)) => {
-                info!("Stream created correctly");
-                Ok(tonic::Response::new(GrpcStreaming(response)))
-            }
-            Some(Err(e)) => {
-                info!("Stream could not be created");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    async fn documents(
-        &self,
-        request: tonic::Request<StreamRequest>,
-    ) -> Result<tonic::Response<Self::DocumentsStream>, tonic::Status> {
-        info!("Starting document streaming");
-        self.instrument(&request);
-        let request = request.into_inner();
-        let Some(shard_id) = request.shard_id.clone() else {
-            return Err(tonic::Status::not_found("Shard ID not present"));
-        };
-        let shard_id = ShardId { id: shard_id.id };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.document_iterator(&shard_id, request).transpose() {
-            Some(Ok(response)) => {
-                info!("Document stream created correctly");
-                Ok(tonic::Response::new(GrpcStreaming(response)))
-            }
-            Some(Err(e)) => {
-                info!("Document stream could not be created");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn get_shard(
-        &self,
-        request: tonic::Request<GetShardRequest>,
-    ) -> Result<tonic::Response<ShardPB>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let request = request.into_inner();
-        let shard_id = request.shard_id.as_ref().unwrap();
-        self.shard_loading(shard_id).await;
-        let reader = self.0.read().await;
-        match reader.get_shard(shard_id).map(|s| s.get_info(&request)) {
-            Some(Ok(shard)) => {
-                info!("Get shard ends {}:{}", file!(), line!());
-                Ok(tonic::Response::new(shard))
-            }
-            Some(Err(e)) => {
-                info!("get_shard ended incorrectly");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn get_shards(
-        &self,
-        request: tonic::Request<EmptyQuery>,
-    ) -> Result<tonic::Response<ShardList>, tonic::Status> {
-        info!("Get shards starts");
-        self.instrument(&request);
-        self.0
-            .read()
-            .await
-            .get_shards()
-            .map(tonic::Response::new)
-            .map_err(|e| tonic::Status::internal(e.to_string()))
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn vector_search(
-        &self,
-        request: tonic::Request<VectorSearchRequest>,
-    ) -> Result<tonic::Response<VectorSearchResponse>, tonic::Status> {
-        info!("Vector search starts");
-        self.instrument(&request);
-        let vector_request = request.into_inner();
-        let shard_id = ShardId {
-            id: vector_request.id.clone(),
-        };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.vector_search(&shard_id, vector_request).transpose() {
-            Some(Ok(response)) => {
-                info!("Vector search ended correctly");
-                Ok(tonic::Response::new(response))
-            }
-            Some(Err(e)) => {
-                info!("Vector search ended incorrectly");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn relation_search(
-        &self,
-        request: tonic::Request<RelationSearchRequest>,
-    ) -> Result<tonic::Response<RelationSearchResponse>, tonic::Status> {
-        info!("Relation search starts");
-        self.instrument(&request);
-        let relation_request = request.into_inner();
-        let shard_id = ShardId {
-            id: relation_request.shard_id.clone(),
-        };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader
-            .relation_search(&shard_id, relation_request)
-            .transpose()
-        {
-            Some(Ok(response)) => {
-                info!("Relation search ended correctly");
-                Ok(tonic::Response::new(response))
-            }
-            Some(Err(e)) => {
-                info!("Relation search ended incorrectly");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn search(
-        &self,
-        request: tonic::Request<SearchRequest>,
-    ) -> Result<tonic::Response<SearchResponse>, tonic::Status> {
-        info!("Search starts");
-        self.instrument(&request);
-        let search_request = request.into_inner();
-        let shard_id = ShardId {
-            id: search_request.shard.clone(),
-        };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.search(&shard_id, search_request).transpose() {
-            Some(Ok(response)) => {
-                info!("Document search ended correctly");
-                Ok(tonic::Response::new(response))
-            }
-            Some(Err(e)) => {
-                info!("Document search ended incorrectly {:?}", e.to_string());
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn suggest(
-        &self,
-        request: tonic::Request<SuggestRequest>,
-    ) -> Result<tonic::Response<SuggestResponse>, tonic::Status> {
-        info!("Suggest starts");
-        self.instrument(&request);
-        let suggest_request = request.into_inner();
-        let shard_id = ShardId {
-            id: suggest_request.shard.clone(),
-        };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.suggest(&shard_id, suggest_request).transpose() {
-            Some(Ok(response)) => {
-                info!("Suggest ended correctly");
-                Ok(tonic::Response::new(response))
-            }
-            Some(Err(e)) => {
-                info!("Suggest ended incorrectly");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn document_search(
-        &self,
-        request: tonic::Request<DocumentSearchRequest>,
-    ) -> Result<tonic::Response<DocumentSearchResponse>, tonic::Status> {
-        info!("Document search starts");
-        self.instrument(&request);
-
-        let document_request = request.into_inner();
-        let shard_id = ShardId {
-            id: document_request.id.clone(),
-        };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader
-            .document_search(&shard_id, document_request)
-            .transpose()
-        {
-            Some(Ok(response)) => {
-                info!("Document search ended correctly");
-                Ok(tonic::Response::new(response))
-            }
-            Some(Err(e)) => {
-                info!("Document search ended incorrectly {:?}", e.to_string());
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn paragraph_search(
-        &self,
-        request: tonic::Request<ParagraphSearchRequest>,
-    ) -> Result<tonic::Response<ParagraphSearchResponse>, tonic::Status> {
-        info!("Paragraph search starts");
-        self.instrument(&request);
-        let paragraph_request = request.into_inner();
-        let shard_id = ShardId {
-            id: paragraph_request.id.clone(),
-        };
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader
-            .paragraph_search(&shard_id, paragraph_request)
-            .transpose()
-        {
-            Some(Ok(response)) => {
-                info!("Paragraph search ended correctly");
-                Ok(tonic::Response::new(response))
-            }
-            Some(Err(e)) => {
-                info!("Paragraph search ended incorrectly");
-                Err(tonic::Status::internal(e.to_string()))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn document_ids(
-        &self,
-        request: tonic::Request<ShardId>,
-    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.document_ids(&shard_id).transpose() {
-            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
-            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
-            None => Err(tonic::Status::not_found(format!(
-                "Shard not found {:?}",
-                shard_id
-            ))),
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn paragraph_ids(
-        &self,
-        request: tonic::Request<ShardId>,
-    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.paragraph_ids(&shard_id).transpose() {
-            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
-            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
-            None => Err(tonic::Status::not_found(format!(
-                "Shard not found {:?}",
-                shard_id
-            ))),
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn vector_ids(
-        &self,
-        request: tonic::Request<ShardId>,
-    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.vector_ids(&shard_id).transpose() {
-            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
-            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
-            None => Err(tonic::Status::not_found(format!(
-                "Shard not found {:?}",
-                shard_id
-            ))),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    async fn relation_ids(
-        &self,
-        request: tonic::Request<ShardId>,
-    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.relation_ids(&shard_id).transpose() {
-            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
-            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
-            None => Err(tonic::Status::not_found(format!(
-                "Shard not found {:?}",
-                shard_id
-            ))),
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn relation_edges(
-        &self,
-        request: tonic::Request<ShardId>,
-    ) -> Result<tonic::Response<EdgeList>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.relation_edges(&shard_id).transpose() {
-            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
-            Some(Err(e)) => Err(tonic::Status::not_found(format!("{e:?} in {shard_id:?}",))),
-            None => Err(tonic::Status::not_found(format!(
-                "Shard not found {:?}",
-                shard_id
-            ))),
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn relation_types(
-        &self,
-        request: tonic::Request<ShardId>,
-    ) -> Result<tonic::Response<TypeList>, tonic::Status> {
-        self.instrument(&request);
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.shard_loading(&shard_id).await;
-        let reader = self.0.read().await;
-        match reader.relation_types(&shard_id).transpose() {
-            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
-            Some(Err(e)) => Err(tonic::Status::not_found(format!("{e:?} in {shard_id:?}",))),
-            None => Err(tonic::Status::not_found(format!(
-                "Shard not found {shard_id:?}"
-            ))),
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use async_std::sync::RwLock;
+use nucliadb_core::prelude::{DocumentIterator, ParagraphIterator};
+use nucliadb_core::protos::node_reader_server::NodeReader;
+use nucliadb_core::protos::*;
+use nucliadb_core::tracing::{self, *};
+use opentelemetry::global;
+use tracing_opentelemetry::OpenTelemetrySpanExt;
+use Shard as ShardPB;
+
+use crate::env;
+use crate::reader::NodeReaderService;
+use crate::utils::MetadataMap;
+
+pub struct NodeReaderGRPCDriver {
+    lazy_loading: bool,
+    inner: RwLock<NodeReaderService>,
+}
+impl From<NodeReaderService> for NodeReaderGRPCDriver {
+    fn from(node: NodeReaderService) -> NodeReaderGRPCDriver {
+        NodeReaderGRPCDriver {
+            lazy_loading: env::lazy_loading(),
+            inner: RwLock::new(node),
+        }
+    }
+}
+impl NodeReaderGRPCDriver {
+    // The GRPC reader will only request the reader to bring a shard
+    // to memory if lazy loading is enabled. Otherwise all the
+    // shards on disk would have been brought to memory before the driver is online.
+    #[tracing::instrument(skip_all)]
+    async fn shard_loading(&self, id: &ShardId) {
+        if self.lazy_loading {
+            let mut writer = self.inner.write().await;
+            writer.load_shard(id);
+        }
+    }
+
+    // Instrumentation utilities for telemetry
+    fn instrument<T>(&self, request: &tonic::Request<T>) {
+        let parent_cx =
+            global::get_text_map_propagator(|prop| prop.extract(&MetadataMap(request.metadata())));
+        Span::current().set_parent(parent_cx);
+    }
+}
+
+pub struct GrpcStreaming<T>(T);
+impl futures_core::Stream for GrpcStreaming<ParagraphIterator> {
+    type Item = Result<ParagraphItem, tonic::Status>;
+
+    fn poll_next(
+        mut self: std::pin::Pin<&mut Self>,
+        _: &mut std::task::Context<'_>,
+    ) -> std::task::Poll<Option<Self::Item>> {
+        std::task::Poll::Ready(self.0.next().map(Ok))
+    }
+}
+impl futures_core::Stream for GrpcStreaming<DocumentIterator> {
+    type Item = Result<DocumentItem, tonic::Status>;
+
+    fn poll_next(
+        mut self: std::pin::Pin<&mut Self>,
+        _: &mut std::task::Context<'_>,
+    ) -> std::task::Poll<Option<Self::Item>> {
+        std::task::Poll::Ready(self.0.next().map(Ok))
+    }
+}
+
+#[tonic::async_trait]
+impl NodeReader for NodeReaderGRPCDriver {
+    type ParagraphsStream = GrpcStreaming<ParagraphIterator>;
+    type DocumentsStream = GrpcStreaming<DocumentIterator>;
+    async fn paragraphs(
+        &self,
+        request: tonic::Request<StreamRequest>,
+    ) -> Result<tonic::Response<Self::ParagraphsStream>, tonic::Status> {
+        debug!("Starting paragraph streaming");
+        self.instrument(&request);
+        let request = request.into_inner();
+        let Some(shard_id) = request.shard_id.clone() else {
+            return Err(tonic::Status::not_found("Shard ID not present"));
+        };
+        let shard_id = ShardId { id: shard_id.id };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.paragraph_iterator(&shard_id, request).transpose() {
+            Some(Ok(response)) => {
+                debug!("Stream created correctly");
+                Ok(tonic::Response::new(GrpcStreaming(response)))
+            }
+            Some(Err(e)) => {
+                debug!("Stream could not be created");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    async fn documents(
+        &self,
+        request: tonic::Request<StreamRequest>,
+    ) -> Result<tonic::Response<Self::DocumentsStream>, tonic::Status> {
+        debug!("Starting document streaming");
+        self.instrument(&request);
+        let request = request.into_inner();
+        let Some(shard_id) = request.shard_id.clone() else {
+            return Err(tonic::Status::not_found("Shard ID not present"));
+        };
+        let shard_id = ShardId { id: shard_id.id };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.document_iterator(&shard_id, request).transpose() {
+            Some(Ok(response)) => {
+                debug!("Document stream created correctly");
+                Ok(tonic::Response::new(GrpcStreaming(response)))
+            }
+            Some(Err(e)) => {
+                debug!("Document stream could not be created");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn get_shard(
+        &self,
+        request: tonic::Request<GetShardRequest>,
+    ) -> Result<tonic::Response<ShardPB>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let request = request.into_inner();
+        let shard_id = &request.shard_id.clone().unwrap();
+        self.shard_loading(shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.get_info(shard_id, request).transpose() {
+            Some(Ok(shard)) => {
+                debug!("Get shard ends {}:{}", file!(), line!());
+                Ok(tonic::Response::new(shard))
+            }
+            Some(Err(e)) => {
+                debug!("get_shard ended incorrectly");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn get_shards(
+        &self,
+        request: tonic::Request<EmptyQuery>,
+    ) -> Result<tonic::Response<ShardList>, tonic::Status> {
+        debug!("Get shards starts");
+        self.instrument(&request);
+        self.inner
+            .read()
+            .await
+            .get_shards()
+            .map(tonic::Response::new)
+            .map_err(|e| tonic::Status::internal(e.to_string()))
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn vector_search(
+        &self,
+        request: tonic::Request<VectorSearchRequest>,
+    ) -> Result<tonic::Response<VectorSearchResponse>, tonic::Status> {
+        debug!("Vector search starts");
+        self.instrument(&request);
+        let vector_request = request.into_inner();
+        let shard_id = ShardId {
+            id: vector_request.id.clone(),
+        };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.vector_search(&shard_id, vector_request).transpose() {
+            Some(Ok(response)) => {
+                debug!("Vector search ended correctly");
+                Ok(tonic::Response::new(response))
+            }
+            Some(Err(e)) => {
+                debug!("Vector search ended incorrectly");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn relation_search(
+        &self,
+        request: tonic::Request<RelationSearchRequest>,
+    ) -> Result<tonic::Response<RelationSearchResponse>, tonic::Status> {
+        debug!("Relation search starts");
+        self.instrument(&request);
+        let relation_request = request.into_inner();
+        let shard_id = ShardId {
+            id: relation_request.shard_id.clone(),
+        };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader
+            .relation_search(&shard_id, relation_request)
+            .transpose()
+        {
+            Some(Ok(response)) => {
+                debug!("Relation search ended correctly");
+                Ok(tonic::Response::new(response))
+            }
+            Some(Err(e)) => {
+                debug!("Relation search ended incorrectly");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn search(
+        &self,
+        request: tonic::Request<SearchRequest>,
+    ) -> Result<tonic::Response<SearchResponse>, tonic::Status> {
+        debug!("Search starts");
+        self.instrument(&request);
+        let search_request = request.into_inner();
+        let shard_id = ShardId {
+            id: search_request.shard.clone(),
+        };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.search(&shard_id, search_request).transpose() {
+            Some(Ok(response)) => {
+                debug!("Document search ended correctly");
+                Ok(tonic::Response::new(response))
+            }
+            Some(Err(e)) => {
+                debug!("Document search ended incorrectly {:?}", e.to_string());
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn suggest(
+        &self,
+        request: tonic::Request<SuggestRequest>,
+    ) -> Result<tonic::Response<SuggestResponse>, tonic::Status> {
+        debug!("Suggest starts");
+        self.instrument(&request);
+        let suggest_request = request.into_inner();
+        let shard_id = ShardId {
+            id: suggest_request.shard.clone(),
+        };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.suggest(&shard_id, suggest_request).transpose() {
+            Some(Ok(response)) => {
+                debug!("Suggest ended correctly");
+                Ok(tonic::Response::new(response))
+            }
+            Some(Err(e)) => {
+                debug!("Suggest ended incorrectly");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn document_search(
+        &self,
+        request: tonic::Request<DocumentSearchRequest>,
+    ) -> Result<tonic::Response<DocumentSearchResponse>, tonic::Status> {
+        debug!("Document search starts");
+        self.instrument(&request);
+
+        let document_request = request.into_inner();
+        let shard_id = ShardId {
+            id: document_request.id.clone(),
+        };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader
+            .document_search(&shard_id, document_request)
+            .transpose()
+        {
+            Some(Ok(response)) => {
+                debug!("Document search ended correctly");
+                Ok(tonic::Response::new(response))
+            }
+            Some(Err(e)) => {
+                debug!("Document search ended incorrectly {:?}", e.to_string());
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn paragraph_search(
+        &self,
+        request: tonic::Request<ParagraphSearchRequest>,
+    ) -> Result<tonic::Response<ParagraphSearchResponse>, tonic::Status> {
+        debug!("Paragraph search starts");
+        self.instrument(&request);
+        let paragraph_request = request.into_inner();
+        let shard_id = ShardId {
+            id: paragraph_request.id.clone(),
+        };
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader
+            .paragraph_search(&shard_id, paragraph_request)
+            .transpose()
+        {
+            Some(Ok(response)) => {
+                debug!("Paragraph search ended correctly");
+                Ok(tonic::Response::new(response))
+            }
+            Some(Err(e)) => {
+                debug!("Paragraph search ended incorrectly");
+                Err(tonic::Status::internal(e.to_string()))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn document_ids(
+        &self,
+        request: tonic::Request<ShardId>,
+    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.document_ids(&shard_id).transpose() {
+            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
+            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
+            None => Err(tonic::Status::not_found(format!(
+                "Shard not found {:?}",
+                shard_id
+            ))),
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn paragraph_ids(
+        &self,
+        request: tonic::Request<ShardId>,
+    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.paragraph_ids(&shard_id).transpose() {
+            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
+            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
+            None => Err(tonic::Status::not_found(format!(
+                "Shard not found {:?}",
+                shard_id
+            ))),
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn vector_ids(
+        &self,
+        request: tonic::Request<ShardId>,
+    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.vector_ids(&shard_id).transpose() {
+            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
+            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
+            None => Err(tonic::Status::not_found(format!(
+                "Shard not found {:?}",
+                shard_id
+            ))),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    async fn relation_ids(
+        &self,
+        request: tonic::Request<ShardId>,
+    ) -> Result<tonic::Response<IdCollection>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.relation_ids(&shard_id).transpose() {
+            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
+            Some(Err(e)) => Err(tonic::Status::internal(e.to_string())),
+            None => Err(tonic::Status::not_found(format!(
+                "Shard not found {:?}",
+                shard_id
+            ))),
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn relation_edges(
+        &self,
+        request: tonic::Request<ShardId>,
+    ) -> Result<tonic::Response<EdgeList>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.relation_edges(&shard_id).transpose() {
+            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
+            Some(Err(e)) => Err(tonic::Status::not_found(format!("{e:?} in {shard_id:?}",))),
+            None => Err(tonic::Status::not_found(format!(
+                "Shard not found {:?}",
+                shard_id
+            ))),
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn relation_types(
+        &self,
+        request: tonic::Request<ShardId>,
+    ) -> Result<tonic::Response<TypeList>, tonic::Status> {
+        self.instrument(&request);
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.shard_loading(&shard_id).await;
+        let reader = self.inner.read().await;
+        match reader.relation_types(&shard_id).transpose() {
+            Some(Ok(ids)) => Ok(tonic::Response::new(ids)),
+            Some(Err(e)) => Err(tonic::Status::not_found(format!("{e:?} in {shard_id:?}",))),
+            None => Err(tonic::Status::not_found(format!(
+                "Shard not found {shard_id:?}"
+            ))),
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/reader/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/reader/mod.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,286 +1,289 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod grpc_driver;
-use std::collections::HashMap;
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::{
-    DocumentSearchRequest, DocumentSearchResponse, EdgeList, IdCollection, ParagraphSearchRequest,
-    ParagraphSearchResponse, RelationSearchRequest, RelationSearchResponse, SearchRequest,
-    SearchResponse, Shard as ShardPB, ShardId, ShardList, StreamRequest, SuggestRequest,
-    SuggestResponse, TypeList, VectorSearchRequest, VectorSearchResponse,
-};
-use nucliadb_core::thread::*;
-use nucliadb_core::tracing::{self, *};
-
-use crate::env;
-use crate::services::reader::ShardReaderService;
-
-#[derive(Debug)]
-pub struct NodeReaderService {
-    pub cache: HashMap<String, ShardReaderService>,
-}
-
-impl Default for NodeReaderService {
-    fn default() -> Self {
-        Self::new()
-    }
-}
-
-impl NodeReaderService {
-    pub fn new() -> NodeReaderService {
-        // We shallow the error if the threadpool was already initialized
-        let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
-        Self {
-            cache: HashMap::new(),
-        }
-    }
-
-    /// Stop all shards on memory
-    #[tracing::instrument(skip_all)]
-    pub fn shutdown(&mut self) {
-        for (shard_id, shard) in &mut self.cache {
-            info!("Stopping shard {}", shard_id);
-            ShardReaderService::stop(shard);
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn iter_shards(
-        &mut self,
-    ) -> NodeResult<impl Iterator<Item = NodeResult<ShardReaderService>>> {
-        let shards_path = env::shards_path();
-        Ok(std::fs::read_dir(shards_path)?.flatten().map(|entry| {
-            let file_name = entry.file_name().to_str().unwrap().to_string();
-            let shard_path = entry.path();
-            info!("Opening {shard_path:?}");
-            ShardReaderService::new(file_name, &shard_path)
-        }))
-    }
-
-    /// Load all shards on the shards memory structure
-    #[tracing::instrument(skip_all)]
-    pub fn load_shards(&mut self) -> NodeResult<()> {
-        let shards_path = env::shards_path();
-        info!("Recovering shards from {shards_path:?}...");
-        for entry in std::fs::read_dir(&shards_path)? {
-            let entry = entry?;
-            let file_name = entry.file_name().to_str().unwrap().to_string();
-            let shard_path = entry.path();
-            match ShardReaderService::new(file_name.clone(), &shard_path) {
-                Err(err) => error!("Loading {shard_path:?} raised {err}"),
-                Ok(shard) => {
-                    info!("Shard loaded: {shard_path:?}");
-                    self.cache.insert(file_name, shard);
-                }
-            }
-        }
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn load_shard(&mut self, shard_id: &ShardId) {
-        let shard_name = shard_id.id.clone();
-        let shard_path = env::shards_path_id(&shard_id.id);
-
-        if self.cache.contains_key(&shard_id.id) {
-            info!("Shard {shard_path:?} is already on memory");
-            return;
-        }
-        if !shard_path.is_dir() {
-            error!("Shard {shard_path:?} is not on disk");
-            return;
-        }
-        let Ok(shard) = ShardReaderService::new(shard_name, &shard_path) else {
-            error!("Shard {shard_path:?} could not be loaded from disk");
-            return;
-        };
-        self.cache.insert(shard_id.id.clone(), shard);
-        info!("{shard_path:?}: Shard loaded");
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardReaderService> {
-        self.cache.get(&shard_id.id)
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_shards(&self) -> NodeResult<ShardList> {
-        use crate::telemetry::run_with_telemetry;
-        let span = tracing::Span::current();
-        let task = move |shard_id: &str, shard: &ShardReaderService| {
-            run_with_telemetry(info_span!(parent: &span, "get shards"), || {
-                shard.get_resources().map(|count| ShardPB {
-                    metadata: Some(shard.metadata.clone().into()),
-                    shard_id: shard_id.to_string(),
-                    resources: count as u64,
-                    paragraphs: 0_u64,
-                    sentences: 0_u64,
-                })
-            })
-        };
-        let shards = self
-            .cache
-            .par_iter()
-            .map(|(id, shard)| task(id, shard))
-            .collect::<Result<Vec<_>, _>>()?;
-        Ok(ShardList { shards })
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn suggest(
-        &self,
-        shard_id: &ShardId,
-        request: SuggestRequest,
-    ) -> NodeResult<Option<SuggestResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let suggest_response = shard.suggest(request)?;
-        Ok(Some(suggest_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn search(
-        &self,
-        shard_id: &ShardId,
-        request: SearchRequest,
-    ) -> NodeResult<Option<SearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_search(
-        &self,
-        shard_id: &ShardId,
-        request: RelationSearchRequest,
-    ) -> NodeResult<Option<RelationSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.relation_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_search(
-        &self,
-        shard_id: &ShardId,
-        request: VectorSearchRequest,
-    ) -> NodeResult<Option<VectorSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.vector_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_search(
-        &self,
-        shard_id: &ShardId,
-        request: ParagraphSearchRequest,
-    ) -> NodeResult<Option<ParagraphSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.paragraph_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_iterator(
-        &self,
-        shard_id: &ShardId,
-        request: StreamRequest,
-    ) -> NodeResult<Option<ParagraphIterator>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.paragraph_iterator(request)?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn document_iterator(
-        &self,
-        shard_id: &ShardId,
-        request: StreamRequest,
-    ) -> NodeResult<Option<DocumentIterator>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.document_iterator(request)?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn document_search(
-        &self,
-        shard_id: &ShardId,
-        request: DocumentSearchRequest,
-    ) -> NodeResult<Option<DocumentSearchResponse>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let search_response = shard.document_search(request)?;
-        Ok(Some(search_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn document_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_text_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_paragraphs_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_vectors_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let ids = shard.get_relations_keys()?;
-        Ok(Some(IdCollection { ids }))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_edges(&self, shard_id: &ShardId) -> NodeResult<Option<EdgeList>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.get_relations_edges()?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_types(&self, shard_id: &ShardId) -> NodeResult<Option<TypeList>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.get_relations_types()?))
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod grpc_driver;
+use std::collections::HashMap;
+
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::{
+    DocumentSearchRequest, DocumentSearchResponse, EdgeList, GetShardRequest, IdCollection,
+    ParagraphSearchRequest, ParagraphSearchResponse, RelationSearchRequest, RelationSearchResponse,
+    SearchRequest, SearchResponse, Shard as ShardPB, ShardId, ShardList, StreamRequest,
+    SuggestRequest, SuggestResponse, TypeList, VectorSearchRequest, VectorSearchResponse,
+};
+use nucliadb_core::thread::*;
+use nucliadb_core::tracing::{self, *};
+
+use crate::env;
+use crate::services::reader::ShardReaderService;
+
+#[derive(Default)]
+pub struct NodeReaderService {
+    pub cache: HashMap<String, ShardReaderService>,
+}
+
+impl NodeReaderService {
+    pub fn new() -> NodeReaderService {
+        // We shallow the error if the threadpool was already initialized
+        let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
+        Self::default()
+    }
+
+    /// Stop all shards on memory
+    #[tracing::instrument(skip_all)]
+    pub fn shutdown(&mut self) {
+        for (shard_id, shard) in &mut self.cache {
+            debug!("Stopping shard {}", shard_id);
+            ShardReaderService::stop(shard);
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn iter_shards(&self) -> NodeResult<impl Iterator<Item = NodeResult<ShardReaderService>>> {
+        let shards_path = env::shards_path();
+        Ok(std::fs::read_dir(shards_path)?.flatten().map(|entry| {
+            let file_name = entry.file_name().to_str().unwrap().to_string();
+            let shard_path = entry.path();
+            debug!("Opening {shard_path:?}");
+            ShardReaderService::new(file_name, &shard_path)
+        }))
+    }
+
+    /// Load all shards on the shards memory structure
+    #[tracing::instrument(skip_all)]
+    pub fn load_shards(&mut self) -> NodeResult<()> {
+        let shards_path = env::shards_path();
+        debug!("Recovering shards from {shards_path:?}...");
+        for entry in std::fs::read_dir(&shards_path)? {
+            let entry = entry?;
+            let file_name = entry.file_name().to_str().unwrap().to_string();
+            let shard_path = entry.path();
+            match ShardReaderService::new(file_name.clone(), &shard_path) {
+                Err(err) => error!("Loading {shard_path:?} raised {err}"),
+                Ok(shard) => {
+                    debug!("Shard loaded: {shard_path:?}");
+                    self.cache.insert(file_name, shard);
+                }
+            }
+        }
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn load_shard(&mut self, shard_id: &ShardId) {
+        let shard_name = shard_id.id.clone();
+        let shard_path = env::shards_path_id(&shard_id.id);
+
+        if self.cache.contains_key(&shard_id.id) {
+            debug!("Shard {shard_path:?} is already on memory");
+            return;
+        }
+        if !shard_path.is_dir() {
+            error!("Shard {shard_path:?} is not on disk");
+            return;
+        }
+        let Ok(shard) = ShardReaderService::new(shard_name, &shard_path) else {
+            error!("Shard {shard_path:?} could not be loaded from disk");
+            return;
+        };
+        self.cache.insert(shard_id.id.clone(), shard);
+        debug!("{shard_path:?}: Shard loaded");
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardReaderService> {
+        self.cache.get(&shard_id.id)
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_shards(&self) -> NodeResult<ShardList> {
+        use crate::telemetry::run_with_telemetry;
+        let span = tracing::Span::current();
+        let task = move |shard_id: &str, shard: &ShardReaderService| {
+            run_with_telemetry(info_span!(parent: &span, "get shards"), || {
+                shard.get_resources().map(|count| ShardPB {
+                    metadata: Some(shard.metadata.clone().into()),
+                    shard_id: shard_id.to_string(),
+                    resources: count as u64,
+                    paragraphs: 0_u64,
+                    sentences: 0_u64,
+                })
+            })
+        };
+        let shards = self
+            .cache
+            .par_iter()
+            .map(|(id, shard)| task(id, shard))
+            .collect::<Result<Vec<_>, _>>()?;
+        Ok(ShardList { shards })
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn suggest(
+        &self,
+        shard_id: &ShardId,
+        request: SuggestRequest,
+    ) -> NodeResult<Option<SuggestResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let suggest_response = shard.suggest(request)?;
+        Ok(Some(suggest_response))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn search(
+        &self,
+        shard_id: &ShardId,
+        request: SearchRequest,
+    ) -> NodeResult<Option<SearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_search(
+        &self,
+        shard_id: &ShardId,
+        request: RelationSearchRequest,
+    ) -> NodeResult<Option<RelationSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.relation_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_search(
+        &self,
+        shard_id: &ShardId,
+        request: VectorSearchRequest,
+    ) -> NodeResult<Option<VectorSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.vector_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_search(
+        &self,
+        shard_id: &ShardId,
+        request: ParagraphSearchRequest,
+    ) -> NodeResult<Option<ParagraphSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.paragraph_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_iterator(
+        &self,
+        shard_id: &ShardId,
+        request: StreamRequest,
+    ) -> NodeResult<Option<ParagraphIterator>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.paragraph_iterator(request)?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn document_iterator(
+        &self,
+        shard_id: &ShardId,
+        request: StreamRequest,
+    ) -> NodeResult<Option<DocumentIterator>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.document_iterator(request)?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn document_search(
+        &self,
+        shard_id: &ShardId,
+        request: DocumentSearchRequest,
+    ) -> NodeResult<Option<DocumentSearchResponse>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let search_response = shard.document_search(request)?;
+        Ok(Some(search_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn document_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_text_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_paragraphs_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_vectors_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_ids(&self, shard_id: &ShardId) -> NodeResult<Option<IdCollection>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let ids = shard.get_relations_keys()?;
+        Ok(Some(IdCollection { ids }))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_edges(&self, shard_id: &ShardId) -> NodeResult<Option<EdgeList>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.get_relations_edges()?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_types(&self, shard_id: &ShardId) -> NodeResult<Option<TypeList>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.get_relations_types()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn get_info(
+        &self,
+        shard_id: &ShardId,
+        request: GetShardRequest,
+    ) -> NodeResult<Option<ShardPB>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.get_info(&request).map(Some)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/http_server/metrics_service.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,34 +1,32 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod versions;
-// Main services
-pub mod reader;
-pub mod writer;
-
-mod shard_disk_structure {
-    pub const VERSION_FILE: &str = "versions.json";
-    pub const VECTORS_DIR: &str = "vectors";
-    pub const VECTORSET_DIR: &str = "vectorset";
-    pub const TEXTS_DIR: &str = "text";
-    pub const PARAGRAPHS_DIR: &str = "paragraph";
-    pub const RELATIONS_DIR: &str = "relations";
-    pub const METADATA_FILE: &str = "metadata.json";
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use nucliadb_core::{context, tracing};
+
+pub async fn metrics_service() -> String {
+    let metrics = context::get_metrics();
+    match metrics.collect() {
+        Ok(m) => m,
+        Err(err) => {
+            tracing::error!("Could not collect metrics due to {err:?}");
+            Default::default()
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/reader.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/reader.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,577 +1,630 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::path::Path;
-use std::sync::RwLock;
-use std::time::SystemTime;
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::shard_created::{
-    DocumentService, ParagraphService, RelationService, VectorService,
-};
-use nucliadb_core::protos::{
-    DocumentSearchRequest, DocumentSearchResponse, EdgeList, GetShardRequest,
-    ParagraphSearchRequest, ParagraphSearchResponse, RelatedEntities, RelationPrefixSearchRequest,
-    RelationSearchRequest, RelationSearchResponse, SearchRequest, SearchResponse, Shard,
-    StreamRequest, SuggestRequest, SuggestResponse, TypeList, VectorSearchRequest,
-    VectorSearchResponse,
-};
-use nucliadb_core::thread::{self, *};
-use nucliadb_core::tracing::{self, *};
-
-use super::shard_disk_structure::*;
-use super::versions::Versions;
-use crate::shard_metadata::ShardMetadata;
-use crate::telemetry::run_with_telemetry;
-
-const RELOAD_PERIOD: u128 = 5000;
-const FIXED_VECTORS_RESULTS: usize = 10;
-const MAX_SUGGEST_COMPOUND_WORDS: usize = 3;
-
-#[derive(Debug)]
-pub struct ShardReaderService {
-    pub id: String,
-    pub metadata: ShardMetadata,
-    creation_time: RwLock<SystemTime>,
-    text_reader: TextsReaderPointer,
-    paragraph_reader: ParagraphsReaderPointer,
-    vector_reader: VectorsReaderPointer,
-    relation_reader: RelationsReaderPointer,
-    document_service_version: i32,
-    paragraph_service_version: i32,
-    vector_service_version: i32,
-    relation_service_version: i32,
-}
-
-impl ShardReaderService {
-    #[tracing::instrument(skip_all)]
-    pub fn text_version(&self) -> DocumentService {
-        match self.document_service_version {
-            0 => DocumentService::DocumentV0,
-            1 => DocumentService::DocumentV1,
-            i => panic!("Unknown document version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_version(&self) -> ParagraphService {
-        match self.paragraph_service_version {
-            0 => ParagraphService::ParagraphV0,
-            1 => ParagraphService::ParagraphV1,
-            i => panic!("Unknown paragraph version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_version(&self) -> VectorService {
-        match self.vector_service_version {
-            0 => VectorService::VectorV0,
-            1 => VectorService::VectorV1,
-            i => panic!("Unknown vector version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_version(&self) -> RelationService {
-        match self.relation_service_version {
-            0 => RelationService::RelationV0,
-            1 => RelationService::RelationV1,
-            i => panic!("Unknown relation version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_info(&self, request: &GetShardRequest) -> NodeResult<Shard> {
-        self.reload_policy(true);
-        let paragraphs = self.paragraph_reader.clone();
-        let vectors = self.vector_reader.clone();
-        let texts = self.text_reader.clone();
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text count");
-        let text_task = || run_with_telemetry(info, move || texts.count());
-        let info = info_span!(parent: &span, "paragraph count");
-        let paragraph_task = || run_with_telemetry(info, move || paragraphs.count());
-        let info = info_span!(parent: &span, "vector count");
-        let vector_task = || run_with_telemetry(info, move || vectors.count(&request.vectorset));
-
-        let mut text_result = Ok(0);
-        let mut paragraph_result = Ok(0);
-        let mut vector_result = Ok(0);
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-        });
-
-        Ok(Shard {
-            metadata: Some(self.metadata.clone().into()),
-            shard_id: self.id.clone(),
-            resources: text_result? as u64,
-            paragraphs: paragraph_result? as u64,
-            sentences: vector_result? as u64,
-        })
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_text_keys(&self) -> NodeResult<Vec<String>> {
-        self.reload_policy(true);
-        self.text_reader.stored_ids()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_paragraphs_keys(&self) -> NodeResult<Vec<String>> {
-        self.reload_policy(true);
-        self.paragraph_reader.stored_ids()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_vectors_keys(&self) -> NodeResult<Vec<String>> {
-        self.reload_policy(true);
-        self.vector_reader.stored_ids()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_relations_keys(&self) -> NodeResult<Vec<String>> {
-        self.reload_policy(true);
-        self.relation_reader.stored_ids()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_relations_edges(&self) -> NodeResult<EdgeList> {
-        self.reload_policy(true);
-        self.relation_reader.get_edges()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_relations_types(&self) -> NodeResult<TypeList> {
-        self.reload_policy(true);
-        self.relation_reader.get_node_types()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_resources(&self) -> NodeResult<usize> {
-        self.text_reader.count()
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn new(id: String, shard_path: &Path) -> NodeResult<ShardReaderService> {
-        let metadata = ShardMetadata::open(&shard_path.join(METADATA_FILE))?;
-        let tsc = TextConfig {
-            path: shard_path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: shard_path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            similarity: None,
-            no_results: Some(FIXED_VECTORS_RESULTS),
-            path: shard_path.join(VECTORS_DIR),
-            vectorset: shard_path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: shard_path.join(RELATIONS_DIR),
-        };
-        let versions = Versions::load(&shard_path.join(VERSION_FILE))?;
-        let text_task = || Some(versions.get_texts_reader(&tsc));
-        let paragraph_task = || Some(versions.get_paragraphs_reader(&psc));
-        let vector_task = || Some(versions.get_vectors_reader(&vsc));
-        let relation_task = || Some(versions.get_relations_reader(&rsc));
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text open");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph open");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector open");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation open");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = None;
-        let mut paragraph_result = None;
-        let mut vector_result = None;
-        let mut relation_result = None;
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        let fields = text_result.transpose()?;
-        let paragraphs = paragraph_result.transpose()?;
-        let vectors = vector_result.transpose()?;
-        let relations = relation_result.transpose()?;
-
-        Ok(ShardReaderService {
-            id,
-            metadata,
-            text_reader: fields.unwrap(),
-            paragraph_reader: paragraphs.unwrap(),
-            vector_reader: vectors.unwrap(),
-            relation_reader: relations.unwrap(),
-            creation_time: RwLock::new(SystemTime::now()),
-            document_service_version: versions.version_texts() as i32,
-            paragraph_service_version: versions.version_paragraphs() as i32,
-            vector_service_version: versions.version_vectors() as i32,
-            relation_service_version: versions.version_relations() as i32,
-        })
-    }
-
-    /// Stop the service
-    #[tracing::instrument(skip_all)]
-    pub fn stop(&self) {
-        info!("Stopping shard {}...", { &self.id });
-        let fields = self.text_reader.clone();
-        let paragraphs = self.paragraph_reader.clone();
-        let vectors = self.vector_reader.clone();
-        let relations = self.relation_reader.clone();
-
-        let text_task = move || fields.stop();
-        let paragraph_task = move || paragraphs.stop();
-        let vector_task = move || vectors.stop();
-        let relation_task = move || relations.stop();
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text stop");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph stop");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector stop");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation stop");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        if let Err(e) = text_result {
-            error!("Error stopping the Field reader service: {}", e);
-        }
-        if let Err(e) = paragraph_result {
-            error!("Error stopping the Paragraph reader service: {}", e);
-        }
-        if let Err(e) = vector_result {
-            error!("Error stopping the Vector reader service: {}", e);
-        }
-        if let Err(e) = relation_result {
-            error!("Error stopping the Relation reader service: {}", e);
-        }
-        info!("Shard stopped {}...", { &self.id });
-    }
-
-    /// Return a list of queries to suggest from the original
-    /// query. The query with more words will come first. `max_group`
-    /// defines the limit of words a query can have.
-    #[tracing::instrument(skip_all)]
-    fn split_suggest_query(query: String, max_group: usize) -> Vec<String> {
-        let words = query.split(' ');
-        let mut i = 0;
-        let mut prefixes = vec![];
-        let mut prefix = String::new();
-
-        for word in words.rev() {
-            if prefix.is_empty() {
-                prefix = word.to_string();
-            } else {
-                prefix = format!("{word} {prefix}");
-            }
-            prefixes.push(prefix.clone());
-
-            i += 1;
-            if i == max_group {
-                break;
-            }
-        }
-
-        prefixes.into_iter().rev().collect()
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn suggest(&self, request: SuggestRequest) -> NodeResult<SuggestResponse> {
-        // Search for entities related to the query.
-
-        let relations_reader_service = self.relation_reader.clone();
-        let paragraph_reader_service = self.paragraph_reader.clone();
-
-        let prefixes = Self::split_suggest_query(request.body.clone(), MAX_SUGGEST_COMPOUND_WORDS);
-        let relations = prefixes.par_iter().map(|prefix| {
-            let request = RelationSearchRequest {
-                shard_id: String::default(),
-                prefix: Some(RelationPrefixSearchRequest {
-                    prefix: prefix.clone(),
-                    ..Default::default()
-                }),
-                ..Default::default()
-            };
-            relations_reader_service.search(&request)
-        });
-
-        let relation_task = move || relations.collect::<Vec<_>>();
-        let paragraph_task = move || paragraph_reader_service.suggest(&request);
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "relations suggest");
-        let relation_task = || run_with_telemetry(info, relation_task);
-        let info = info_span!(parent: &span, "paragraph suggest");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-
-        let tasks = thread::join(paragraph_task, relation_task);
-        let rparagraph = tasks.0.unwrap();
-        let entities = tasks
-            .1
-            .into_iter()
-            .flat_map(|relation| {
-                relation
-                    .unwrap()
-                    .prefix
-                    .expect("Prefix search request must return a prefix response")
-                    .nodes
-                    .iter()
-                    .map(|relation_node| relation_node.value.clone())
-                    .collect::<Vec<String>>()
-            })
-            .collect::<Vec<String>>();
-
-        Ok(SuggestResponse {
-            query: rparagraph.query,
-            total: rparagraph.total,
-            results: rparagraph.results,
-            ematches: rparagraph.ematches,
-            entities: Some(RelatedEntities {
-                total: entities.len() as u32,
-                entities,
-            }),
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn search(&self, search_request: SearchRequest) -> NodeResult<SearchResponse> {
-        self.reload_policy(search_request.reload);
-        let skip_paragraphs = !search_request.paragraph;
-        let skip_fields = !search_request.document;
-        let skip_vectors = search_request.result_per_page == 0 || search_request.vector.is_empty();
-
-        let field_request = DocumentSearchRequest {
-            id: "".to_string(),
-            body: search_request.body.clone(),
-            fields: search_request.fields.clone(),
-            filter: search_request.filter.clone(),
-            order: search_request.order.clone(),
-            faceted: search_request.faceted.clone(),
-            page_number: search_request.page_number,
-            result_per_page: search_request.result_per_page,
-            timestamps: search_request.timestamps.clone(),
-            reload: search_request.reload,
-            only_faceted: search_request.only_faceted,
-            advanced_query: search_request.advanced_query.clone(),
-            with_status: search_request.with_status,
-        };
-        let text_reader_service = self.text_reader.clone();
-        let text_task = move || {
-            if !skip_fields {
-                Some(text_reader_service.search(&field_request))
-            } else {
-                None
-            }
-        };
-
-        let paragraph_request = ParagraphSearchRequest {
-            id: "".to_string(),
-            uuid: "".to_string(),
-            with_duplicates: search_request.with_duplicates,
-            body: search_request.body.clone(),
-            fields: search_request.fields.clone(),
-            filter: search_request.filter.clone(),
-            order: search_request.order.clone(),
-            faceted: search_request.faceted.clone(),
-            page_number: search_request.page_number,
-            result_per_page: search_request.result_per_page,
-            timestamps: search_request.timestamps.clone(),
-            reload: search_request.reload,
-            only_faceted: search_request.only_faceted,
-            advanced_query: search_request.advanced_query.clone(),
-        };
-        let paragraph_reader_service = self.paragraph_reader.clone();
-        let paragraph_task = move || {
-            if !skip_paragraphs {
-                Some(paragraph_reader_service.search(&paragraph_request))
-            } else {
-                None
-            }
-        };
-
-        let vector_request = VectorSearchRequest {
-            id: "".to_string(),
-            vector_set: search_request.vectorset.clone(),
-            vector: search_request.vector.clone(),
-            reload: search_request.reload,
-            page_number: search_request.page_number,
-            result_per_page: search_request.result_per_page,
-            with_duplicates: true,
-            tags: search_request
-                .filter
-                .iter()
-                .flat_map(|f| f.tags.iter().cloned())
-                .chain(search_request.fields.iter().cloned())
-                .collect(),
-        };
-        let vector_reader_service = self.vector_reader.clone();
-        let vector_task = move || {
-            if !skip_vectors {
-                Some(vector_reader_service.search(&vector_request))
-            } else {
-                None
-            }
-        };
-
-        let relation_request = RelationSearchRequest {
-            shard_id: search_request.shard.clone(),
-            reload: search_request.reload,
-            prefix: search_request.relation_prefix.clone(),
-            subgraph: search_request.relation_subgraph,
-        };
-        let relation_reader_service = self.relation_reader.clone();
-        let relation_task = move || Some(relation_reader_service.search(&relation_request));
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text search");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph search");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector search");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relations search");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut rtext = None;
-        let mut rparagraph = None;
-        let mut rvector = None;
-        let mut rrelation = None;
-
-        thread::scope(|s| {
-            s.spawn(|_| rtext = text_task());
-            s.spawn(|_| rparagraph = paragraph_task());
-            s.spawn(|_| rvector = vector_task());
-            s.spawn(|_| rrelation = relation_task());
-        });
-        Ok(SearchResponse {
-            document: rtext.transpose()?,
-            paragraph: rparagraph.transpose()?,
-            vector: rvector.transpose()?,
-            relation: rrelation.transpose()?,
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_search(
-        &self,
-        search_request: ParagraphSearchRequest,
-    ) -> NodeResult<ParagraphSearchResponse> {
-        self.reload_policy(search_request.reload);
-        let span = tracing::Span::current();
-        run_with_telemetry(info_span!(parent: &span, "paragraph reader search"), || {
-            self.paragraph_reader.search(&search_request)
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_iterator(&self, request: StreamRequest) -> NodeResult<ParagraphIterator> {
-        self.reload_policy(request.reload);
-        let span = tracing::Span::current();
-        run_with_telemetry(info_span!(parent: &span, "paragraph iteration"), || {
-            self.paragraph_reader.iterator(&request)
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn document_iterator(&self, request: StreamRequest) -> NodeResult<DocumentIterator> {
-        self.reload_policy(request.reload);
-        let span = tracing::Span::current();
-        run_with_telemetry(info_span!(parent: &span, "field iteration"), || {
-            self.text_reader.iterator(&request)
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn document_search(
-        &self,
-        search_request: DocumentSearchRequest,
-    ) -> NodeResult<DocumentSearchResponse> {
-        self.reload_policy(search_request.reload);
-        let span = tracing::Span::current();
-        run_with_telemetry(info_span!(parent: &span, "field reader search"), || {
-            self.text_reader.search(&search_request)
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn vector_search(
-        &self,
-        search_request: VectorSearchRequest,
-    ) -> NodeResult<VectorSearchResponse> {
-        self.reload_policy(search_request.reload);
-        let span = tracing::Span::current();
-        run_with_telemetry(info_span!(parent: &span, "vector reader search"), || {
-            self.vector_reader.search(&search_request)
-        })
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_search(
-        &self,
-        search_request: RelationSearchRequest,
-    ) -> NodeResult<RelationSearchResponse> {
-        self.reload_policy(search_request.reload);
-        let span = tracing::Span::current();
-        run_with_telemetry(info_span!(parent: &span, "relation reader search"), || {
-            self.relation_reader.search(&search_request)
-        })
-    }
-
-    fn reload_policy(&self, trigger: bool) {
-        let elapsed = self
-            .creation_time
-            .read()
-            .unwrap()
-            .elapsed()
-            .unwrap()
-            .as_millis();
-        if trigger || elapsed >= RELOAD_PERIOD {
-            let mut creation_time = self.creation_time.write().unwrap();
-            *creation_time = SystemTime::now();
-            self.vector_reader.reload()
-        }
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-
-    #[test]
-    fn test_suggest_split() {
-        let query = "Some search with multiple words".to_string();
-
-        assert_eq!(
-            ShardReaderService::split_suggest_query(query.clone(), 3),
-            vec!["with multiple words", "multiple words", "words"]
-        );
-        assert_eq!(
-            ShardReaderService::split_suggest_query(query, 2),
-            vec!["multiple words", "words"]
-        );
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::path::Path;
+use std::sync::RwLock;
+use std::time::SystemTime;
+
+use nucliadb_core::context;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::shard_created::{
+    DocumentService, ParagraphService, RelationService, VectorService,
+};
+use nucliadb_core::protos::{
+    DocumentSearchRequest, DocumentSearchResponse, EdgeList, GetShardRequest,
+    ParagraphSearchRequest, ParagraphSearchResponse, RelatedEntities, RelationPrefixSearchRequest,
+    RelationSearchRequest, RelationSearchResponse, SearchRequest, SearchResponse, Shard,
+    StreamRequest, SuggestRequest, SuggestResponse, TypeList, VectorSearchRequest,
+    VectorSearchResponse,
+};
+use nucliadb_core::thread::{self, *};
+use nucliadb_core::tracing::{self, *};
+
+use super::shard_disk_structure::*;
+use super::versions::Versions;
+use crate::shard_metadata::ShardMetadata;
+use crate::telemetry::run_with_telemetry;
+
+const RELOAD_PERIOD: u128 = 5000;
+const FIXED_VECTORS_RESULTS: usize = 10;
+const MAX_SUGGEST_COMPOUND_WORDS: usize = 3;
+
+#[derive(Debug)]
+pub struct ShardReaderService {
+    pub id: String,
+    pub metadata: ShardMetadata,
+    creation_time: RwLock<SystemTime>,
+    text_reader: TextsReaderPointer,
+    paragraph_reader: ParagraphsReaderPointer,
+    vector_reader: VectorsReaderPointer,
+    relation_reader: RelationsReaderPointer,
+    document_service_version: i32,
+    paragraph_service_version: i32,
+    vector_service_version: i32,
+    relation_service_version: i32,
+}
+
+impl ShardReaderService {
+    #[tracing::instrument(skip_all)]
+    pub fn text_version(&self) -> DocumentService {
+        match self.document_service_version {
+            0 => DocumentService::DocumentV0,
+            1 => DocumentService::DocumentV1,
+            i => panic!("Unknown document version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_version(&self) -> ParagraphService {
+        match self.paragraph_service_version {
+            0 => ParagraphService::ParagraphV0,
+            1 => ParagraphService::ParagraphV1,
+            i => panic!("Unknown paragraph version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_version(&self) -> VectorService {
+        match self.vector_service_version {
+            0 => VectorService::VectorV0,
+            1 => VectorService::VectorV1,
+            i => panic!("Unknown vector version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_version(&self) -> RelationService {
+        match self.relation_service_version {
+            0 => RelationService::RelationV0,
+            1 => RelationService::RelationV1,
+            i => panic!("Unknown relation version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_info(&self, request: &GetShardRequest) -> NodeResult<Shard> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        self.reload_policy(true);
+        let paragraphs = self.paragraph_reader.clone();
+        let vectors = self.vector_reader.clone();
+        let texts = self.text_reader.clone();
+
+        let info = info_span!(parent: &span, "text count");
+        let text_task = || run_with_telemetry(info, move || texts.count());
+        let info = info_span!(parent: &span, "paragraph count");
+        let paragraph_task = || run_with_telemetry(info, move || paragraphs.count());
+        let info = info_span!(parent: &span, "vector count");
+        let vector_task = || run_with_telemetry(info, move || vectors.count(&request.vectorset));
+
+        let mut text_result = Ok(0);
+        let mut paragraph_result = Ok(0);
+        let mut vector_result = Ok(0);
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+        });
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("reader/get_info".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(Shard {
+            metadata: Some(self.metadata.clone().into()),
+            shard_id: self.id.clone(),
+            // naming issue here, this is not number of resource
+            // but more like number of fields
+            resources: text_result? as u64,
+            paragraphs: paragraph_result? as u64,
+            sentences: vector_result? as u64,
+        })
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_text_keys(&self) -> NodeResult<Vec<String>> {
+        self.reload_policy(true);
+        self.text_reader.stored_ids()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_paragraphs_keys(&self) -> NodeResult<Vec<String>> {
+        self.reload_policy(true);
+        self.paragraph_reader.stored_ids()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_vectors_keys(&self) -> NodeResult<Vec<String>> {
+        self.reload_policy(true);
+        self.vector_reader.stored_ids()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_relations_keys(&self) -> NodeResult<Vec<String>> {
+        self.reload_policy(true);
+        self.relation_reader.stored_ids()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_relations_edges(&self) -> NodeResult<EdgeList> {
+        self.reload_policy(true);
+        self.relation_reader.get_edges()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_relations_types(&self) -> NodeResult<TypeList> {
+        self.reload_policy(true);
+        self.relation_reader.get_node_types()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_resources(&self) -> NodeResult<usize> {
+        self.text_reader.count()
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn new(id: String, shard_path: &Path) -> NodeResult<ShardReaderService> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let metadata = ShardMetadata::open(&shard_path.join(METADATA_FILE))?;
+        let tsc = TextConfig {
+            path: shard_path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: shard_path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            similarity: None,
+            no_results: Some(FIXED_VECTORS_RESULTS),
+            path: shard_path.join(VECTORS_DIR),
+            vectorset: shard_path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: shard_path.join(RELATIONS_DIR),
+        };
+        let versions = Versions::load(&shard_path.join(VERSION_FILE))?;
+        let text_task = || Some(versions.get_texts_reader(&tsc));
+        let paragraph_task = || Some(versions.get_paragraphs_reader(&psc));
+        let vector_task = || Some(versions.get_vectors_reader(&vsc));
+        let relation_task = || Some(versions.get_relations_reader(&rsc));
+
+        let info = info_span!(parent: &span, "text open");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph open");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector open");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation open");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = None;
+        let mut paragraph_result = None;
+        let mut vector_result = None;
+        let mut relation_result = None;
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+        let fields = text_result.transpose()?;
+        let paragraphs = paragraph_result.transpose()?;
+        let vectors = vector_result.transpose()?;
+        let relations = relation_result.transpose()?;
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("reader/new".to_string());
+        metrics.record_request_time(metric, took);
+        Ok(ShardReaderService {
+            id,
+            metadata,
+            text_reader: fields.unwrap(),
+            paragraph_reader: paragraphs.unwrap(),
+            vector_reader: vectors.unwrap(),
+            relation_reader: relations.unwrap(),
+            creation_time: RwLock::new(SystemTime::now()),
+            document_service_version: versions.version_texts() as i32,
+            paragraph_service_version: versions.version_paragraphs() as i32,
+            vector_service_version: versions.version_vectors() as i32,
+            relation_service_version: versions.version_relations() as i32,
+        })
+    }
+
+    /// Stop the service
+    #[tracing::instrument(skip_all)]
+    pub fn stop(&self) {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        debug!("Stopping shard {}...", { &self.id });
+        let fields = self.text_reader.clone();
+        let paragraphs = self.paragraph_reader.clone();
+        let vectors = self.vector_reader.clone();
+        let relations = self.relation_reader.clone();
+
+        let text_task = move || fields.stop();
+        let paragraph_task = move || paragraphs.stop();
+        let vector_task = move || vectors.stop();
+        let relation_task = move || relations.stop();
+
+        let info = info_span!(parent: &span, "text stop");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph stop");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector stop");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation stop");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = Ok(());
+        let mut paragraph_result = Ok(());
+        let mut vector_result = Ok(());
+        let mut relation_result = Ok(());
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+        if let Err(e) = text_result {
+            error!("Error stopping the Field reader service: {}", e);
+        }
+        if let Err(e) = paragraph_result {
+            error!("Error stopping the Paragraph reader service: {}", e);
+        }
+        if let Err(e) = vector_result {
+            error!("Error stopping the Vector reader service: {}", e);
+        }
+        if let Err(e) = relation_result {
+            error!("Error stopping the Relation reader service: {}", e);
+        }
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("reader/stop".to_string());
+        metrics.record_request_time(metric, took);
+
+        debug!("Shard stopped {}...", { &self.id });
+    }
+
+    /// Return a list of queries to suggest from the original
+    /// query. The query with more words will come first. `max_group`
+    /// defines the limit of words a query can have.
+    #[tracing::instrument(skip_all)]
+    fn split_suggest_query(query: String, max_group: usize) -> Vec<String> {
+        let words = query.split(' ');
+        let mut i = 0;
+        let mut prefixes = vec![];
+        let mut prefix = String::new();
+
+        for word in words.rev() {
+            if prefix.is_empty() {
+                prefix = word.to_string();
+            } else {
+                prefix = format!("{word} {prefix}");
+            }
+            prefixes.push(prefix.clone());
+
+            i += 1;
+            if i == max_group {
+                break;
+            }
+        }
+
+        prefixes.into_iter().rev().collect()
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn suggest(&self, request: SuggestRequest) -> NodeResult<SuggestResponse> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let relations_reader_service = self.relation_reader.clone();
+        let paragraph_reader_service = self.paragraph_reader.clone();
+
+        let prefixes = Self::split_suggest_query(request.body.clone(), MAX_SUGGEST_COMPOUND_WORDS);
+        let relations = prefixes.par_iter().map(|prefix| {
+            let request = RelationSearchRequest {
+                shard_id: String::default(),
+                prefix: Some(RelationPrefixSearchRequest {
+                    prefix: prefix.clone(),
+                    ..Default::default()
+                }),
+                ..Default::default()
+            };
+            relations_reader_service.search(&request)
+        });
+
+        let relation_task = move || relations.collect::<Vec<_>>();
+        let paragraph_task = move || paragraph_reader_service.suggest(&request);
+
+        let info = info_span!(parent: &span, "relations suggest");
+        let relation_task = || run_with_telemetry(info, relation_task);
+        let info = info_span!(parent: &span, "paragraph suggest");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+
+        let tasks = thread::join(paragraph_task, relation_task);
+        let rparagraph = tasks.0.unwrap();
+        let entities = tasks
+            .1
+            .into_iter()
+            .flat_map(|relation| {
+                relation
+                    .unwrap()
+                    .prefix
+                    .expect("Prefix search request must return a prefix response")
+                    .nodes
+                    .iter()
+                    .map(|relation_node| relation_node.value.clone())
+                    .collect::<Vec<String>>()
+            })
+            .collect::<Vec<String>>();
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("reader/suggest".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(SuggestResponse {
+            query: rparagraph.query,
+            total: rparagraph.total,
+            results: rparagraph.results,
+            ematches: rparagraph.ematches,
+            entities: Some(RelatedEntities {
+                total: entities.len() as u32,
+                entities,
+            }),
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn search(&self, search_request: SearchRequest) -> NodeResult<SearchResponse> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        self.reload_policy(search_request.reload);
+        let skip_paragraphs = !search_request.paragraph;
+        let skip_fields = !search_request.document;
+        let skip_vectors = search_request.result_per_page == 0 || search_request.vector.is_empty();
+
+        let field_request = DocumentSearchRequest {
+            id: "".to_string(),
+            body: search_request.body.clone(),
+            fields: search_request.fields.clone(),
+            filter: search_request.filter.clone(),
+            order: search_request.order.clone(),
+            faceted: search_request.faceted.clone(),
+            page_number: search_request.page_number,
+            result_per_page: search_request.result_per_page,
+            timestamps: search_request.timestamps.clone(),
+            reload: search_request.reload,
+            only_faceted: search_request.only_faceted,
+            advanced_query: search_request.advanced_query.clone(),
+            with_status: search_request.with_status,
+        };
+        let text_reader_service = self.text_reader.clone();
+        let text_task = move || {
+            if !skip_fields {
+                Some(text_reader_service.search(&field_request))
+            } else {
+                None
+            }
+        };
+
+        let paragraph_request = ParagraphSearchRequest {
+            id: "".to_string(),
+            uuid: "".to_string(),
+            with_duplicates: search_request.with_duplicates,
+            body: search_request.body.clone(),
+            fields: search_request.fields.clone(),
+            filter: search_request.filter.clone(),
+            order: search_request.order.clone(),
+            faceted: search_request.faceted.clone(),
+            page_number: search_request.page_number,
+            result_per_page: search_request.result_per_page,
+            timestamps: search_request.timestamps.clone(),
+            reload: search_request.reload,
+            only_faceted: search_request.only_faceted,
+            advanced_query: search_request.advanced_query.clone(),
+        };
+        let paragraph_reader_service = self.paragraph_reader.clone();
+        let paragraph_task = move || {
+            if !skip_paragraphs {
+                Some(paragraph_reader_service.search(&paragraph_request))
+            } else {
+                None
+            }
+        };
+
+        let vector_request = VectorSearchRequest {
+            id: "".to_string(),
+            vector_set: search_request.vectorset.clone(),
+            vector: search_request.vector.clone(),
+            reload: search_request.reload,
+            page_number: search_request.page_number,
+            result_per_page: search_request.result_per_page,
+            with_duplicates: true,
+            tags: search_request
+                .filter
+                .iter()
+                .flat_map(|f| f.tags.iter().cloned())
+                .chain(search_request.fields.iter().cloned())
+                .collect(),
+        };
+        let vector_reader_service = self.vector_reader.clone();
+        let vector_task = move || {
+            if !skip_vectors {
+                Some(vector_reader_service.search(&vector_request))
+            } else {
+                None
+            }
+        };
+
+        let relation_request = RelationSearchRequest {
+            shard_id: search_request.shard.clone(),
+            reload: search_request.reload,
+            prefix: search_request.relation_prefix.clone(),
+            subgraph: search_request.relation_subgraph,
+        };
+        let relation_reader_service = self.relation_reader.clone();
+        let relation_task = move || Some(relation_reader_service.search(&relation_request));
+
+        let info = info_span!(parent: &span, "text search");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph search");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector search");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relations search");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut rtext = None;
+        let mut rparagraph = None;
+        let mut rvector = None;
+        let mut rrelation = None;
+
+        thread::scope(|s| {
+            s.spawn(|_| rtext = text_task());
+            s.spawn(|_| rparagraph = paragraph_task());
+            s.spawn(|_| rvector = vector_task());
+            s.spawn(|_| rrelation = relation_task());
+        });
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("reader/search".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(SearchResponse {
+            document: rtext.transpose()?,
+            paragraph: rparagraph.transpose()?,
+            vector: rvector.transpose()?,
+            relation: rrelation.transpose()?,
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_iterator(&self, request: StreamRequest) -> NodeResult<ParagraphIterator> {
+        self.reload_policy(request.reload);
+        let span = tracing::Span::current();
+        run_with_telemetry(info_span!(parent: &span, "paragraph iteration"), || {
+            self.paragraph_reader.iterator(&request)
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn document_iterator(&self, request: StreamRequest) -> NodeResult<DocumentIterator> {
+        self.reload_policy(request.reload);
+        let span = tracing::Span::current();
+        run_with_telemetry(info_span!(parent: &span, "field iteration"), || {
+            self.text_reader.iterator(&request)
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_search(
+        &self,
+        search_request: ParagraphSearchRequest,
+    ) -> NodeResult<ParagraphSearchResponse> {
+        let span = tracing::Span::current();
+        self.reload_policy(search_request.reload);
+        run_with_telemetry(info_span!(parent: &span, "paragraph reader search"), || {
+            self.paragraph_reader.search(&search_request)
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn document_search(
+        &self,
+        search_request: DocumentSearchRequest,
+    ) -> NodeResult<DocumentSearchResponse> {
+        let span = tracing::Span::current();
+        self.reload_policy(search_request.reload);
+        run_with_telemetry(info_span!(parent: &span, "field reader search"), || {
+            self.text_reader.search(&search_request)
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn vector_search(
+        &self,
+        search_request: VectorSearchRequest,
+    ) -> NodeResult<VectorSearchResponse> {
+        let span = tracing::Span::current();
+        self.reload_policy(search_request.reload);
+        run_with_telemetry(info_span!(parent: &span, "vector reader search"), || {
+            self.vector_reader.search(&search_request)
+        })
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_search(
+        &self,
+        search_request: RelationSearchRequest,
+    ) -> NodeResult<RelationSearchResponse> {
+        let span = tracing::Span::current();
+        self.reload_policy(search_request.reload);
+        run_with_telemetry(info_span!(parent: &span, "relation reader search"), || {
+            self.relation_reader.search(&search_request)
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_count(&self) -> NodeResult<usize> {
+        self.paragraph_reader.count()
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn vector_count(&self, vector_set: &str) -> NodeResult<usize> {
+        self.vector_reader.count(vector_set)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn text_count(&self) -> NodeResult<usize> {
+        self.text_reader.count()
+    }
+
+    fn reload_policy(&self, trigger: bool) {
+        let elapsed = self
+            .creation_time
+            .read()
+            .unwrap()
+            .elapsed()
+            .unwrap()
+            .as_millis();
+        if trigger || elapsed >= RELOAD_PERIOD {
+            let mut creation_time = self.creation_time.write().unwrap();
+            *creation_time = SystemTime::now();
+            self.vector_reader.reload()
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_suggest_split() {
+        let query = "Some search with multiple words".to_string();
+
+        assert_eq!(
+            ShardReaderService::split_suggest_query(query.clone(), 3),
+            vec!["with multiple words", "multiple words", "words"]
+        );
+        assert_eq!(
+            ShardReaderService::split_suggest_query(query, 2),
+            vec!["multiple words", "words"]
+        );
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/versions.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/versions.rs`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,220 +1,220 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::path::Path;
-
-use nucliadb_core::prelude::*;
-use serde::{Deserialize, Serialize};
-
-const VECTORS_VERSION: u32 = 1;
-const PARAGRAPHS_VERSION: u32 = 1;
-const RELATIONS_VERSION: u32 = 1;
-const TEXTS_VERSION: u32 = 1;
-const DEPRECATED_CONFIG: &str = "config.json";
-
-#[derive(Serialize, Deserialize)]
-pub struct Versions {
-    #[serde(default)]
-    version_paragraphs: Option<u32>,
-    #[serde(default)]
-    version_vectors: Option<u32>,
-    #[serde(default)]
-    version_texts: Option<u32>,
-    #[serde(default)]
-    version_relations: Option<u32>,
-}
-
-impl Versions {
-    fn deprecated_versions_exists(versions_file: &Path) -> bool {
-        versions_file
-            .parent()
-            .map(|v| v.join(DEPRECATED_CONFIG))
-            .map(|v| v.exists())
-            .unwrap_or_default()
-    }
-    fn new_from_deprecated() -> Versions {
-        Versions {
-            version_paragraphs: Some(1),
-            version_vectors: Some(1),
-            version_texts: Some(1),
-            version_relations: Some(1),
-        }
-    }
-    fn new() -> Versions {
-        Versions {
-            version_paragraphs: Some(PARAGRAPHS_VERSION),
-            version_vectors: Some(VECTORS_VERSION),
-            version_texts: Some(TEXTS_VERSION),
-            version_relations: Some(RELATIONS_VERSION),
-        }
-    }
-    fn fill_gaps(&mut self) -> bool {
-        let mut modified = false;
-        if self.version_paragraphs.is_none() {
-            self.version_paragraphs = Some(PARAGRAPHS_VERSION);
-            modified = true;
-        }
-        if self.version_relations.is_none() {
-            self.version_relations = Some(RELATIONS_VERSION);
-            modified = true;
-        }
-        if self.version_texts.is_none() {
-            self.version_texts = Some(TEXTS_VERSION);
-            modified = true;
-        }
-        if self.version_vectors.is_none() {
-            self.version_vectors = Some(VECTORS_VERSION);
-            modified = true;
-        }
-        modified
-    }
-    pub fn version_paragraphs(&self) -> u32 {
-        self.version_paragraphs.unwrap_or(PARAGRAPHS_VERSION)
-    }
-    pub fn version_vectors(&self) -> u32 {
-        self.version_vectors.unwrap_or(VECTORS_VERSION)
-    }
-    pub fn version_texts(&self) -> u32 {
-        self.version_texts.unwrap_or(TEXTS_VERSION)
-    }
-    pub fn version_relations(&self) -> u32 {
-        self.version_relations.unwrap_or(RELATIONS_VERSION)
-    }
-    pub fn get_vectors_reader(&self, config: &VectorConfig) -> NodeResult<VectorsReaderPointer> {
-        match self.version_vectors {
-            Some(1) => nucliadb_vectors::service::VectorReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as VectorsReaderPointer),
-            Some(v) => Err(node_error!("Invalid vectors  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-    pub fn get_paragraphs_reader(
-        &self,
-        config: &ParagraphConfig,
-    ) -> NodeResult<ParagraphsReaderPointer> {
-        match self.version_paragraphs {
-            Some(1) => nucliadb_paragraphs::reader::ParagraphReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as ParagraphsReaderPointer),
-            Some(v) => Err(node_error!("Invalid paragraphs  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_texts_reader(&self, config: &TextConfig) -> NodeResult<TextsReaderPointer> {
-        match self.version_texts {
-            Some(1) => nucliadb_texts::reader::TextReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as TextsReaderPointer),
-            Some(v) => Err(node_error!("Invalid text  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_relations_reader(
-        &self,
-        config: &RelationConfig,
-    ) -> NodeResult<RelationsReaderPointer> {
-        match self.version_relations {
-            Some(1) => nucliadb_relations::service::RelationsReaderService::start(config)
-                .map(|i| encapsulate_reader(i) as RelationsReaderPointer),
-            Some(v) => Err(node_error!("Invalid relations  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_vectors_writer(&self, config: &VectorConfig) -> NodeResult<VectorsWriterPointer> {
-        match self.version_vectors {
-            Some(1) => nucliadb_vectors::service::VectorWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as VectorsWriterPointer),
-            Some(v) => Err(node_error!("Invalid vectors  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-    pub fn get_paragraphs_writer(
-        &self,
-        config: &ParagraphConfig,
-    ) -> NodeResult<ParagraphsWriterPointer> {
-        match self.version_paragraphs {
-            Some(1) => nucliadb_paragraphs::writer::ParagraphWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as ParagraphsWriterPointer),
-            Some(v) => Err(node_error!("Invalid paragraphs  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_texts_writer(&self, config: &TextConfig) -> NodeResult<TextsWriterPointer> {
-        match self.version_texts {
-            Some(1) => nucliadb_texts::writer::TextWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as TextsWriterPointer),
-            Some(v) => Err(node_error!("Invalid text  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn get_relations_writer(
-        &self,
-        config: &RelationConfig,
-    ) -> NodeResult<RelationsWriterPointer> {
-        match self.version_relations {
-            Some(1) => nucliadb_relations::service::RelationsWriterService::start(config)
-                .map(|i| encapsulate_writer(i) as RelationsWriterPointer),
-            Some(v) => Err(node_error!("Invalid relations  version {v}")),
-            None => Err(node_error!("Corrupted version file")),
-        }
-    }
-
-    pub fn load(versions_file: &Path) -> NodeResult<Versions> {
-        if versions_file.exists() {
-            let versions_json = std::fs::read_to_string(versions_file)?;
-            let mut versions: Versions = serde_json::from_str(&versions_json)?;
-            versions.fill_gaps();
-            Ok(versions)
-        } else if Versions::deprecated_versions_exists(versions_file) {
-            // In this case is an old index, therefore we create the versions file
-            // with the index versions that where available that moment.
-            // The writer will create the file at some point
-            Ok(Versions::new_from_deprecated())
-        } else {
-            Err(node_error!("Versions not found"))
-        }
-    }
-    pub fn load_or_create(versions_file: &Path) -> NodeResult<Versions> {
-        if versions_file.exists() {
-            let versions_json = std::fs::read_to_string(versions_file)?;
-            let mut versions: Versions = serde_json::from_str(&versions_json)?;
-            if versions.fill_gaps() {
-                let serialized = serde_json::to_string(&versions)?;
-                std::fs::write(versions_file, serialized)?;
-            }
-            Ok(versions)
-        } else if Versions::deprecated_versions_exists(versions_file) {
-            // In this case is an old index, therefore we create the versions file
-            // with the index versions that where available that moment.
-            let versions = Versions::new_from_deprecated();
-            let serialized = serde_json::to_string(&versions)?;
-            std::fs::write(versions_file, serialized)?;
-            Ok(versions)
-        } else {
-            let versions = Versions::new();
-            let serialized = serde_json::to_string(&versions)?;
-            std::fs::write(versions_file, serialized)?;
-            Ok(versions)
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::path::Path;
+
+use nucliadb_core::prelude::*;
+use serde::{Deserialize, Serialize};
+
+const VECTORS_VERSION: u32 = 1;
+const PARAGRAPHS_VERSION: u32 = 1;
+const RELATIONS_VERSION: u32 = 1;
+const TEXTS_VERSION: u32 = 1;
+const DEPRECATED_CONFIG: &str = "config.json";
+
+#[derive(Serialize, Deserialize)]
+pub struct Versions {
+    #[serde(default)]
+    version_paragraphs: Option<u32>,
+    #[serde(default)]
+    version_vectors: Option<u32>,
+    #[serde(default)]
+    version_texts: Option<u32>,
+    #[serde(default)]
+    version_relations: Option<u32>,
+}
+
+impl Versions {
+    fn deprecated_versions_exists(versions_file: &Path) -> bool {
+        versions_file
+            .parent()
+            .map(|v| v.join(DEPRECATED_CONFIG))
+            .map(|v| v.exists())
+            .unwrap_or_default()
+    }
+    fn new_from_deprecated() -> Versions {
+        Versions {
+            version_paragraphs: Some(1),
+            version_vectors: Some(1),
+            version_texts: Some(1),
+            version_relations: Some(1),
+        }
+    }
+    fn new() -> Versions {
+        Versions {
+            version_paragraphs: Some(PARAGRAPHS_VERSION),
+            version_vectors: Some(VECTORS_VERSION),
+            version_texts: Some(TEXTS_VERSION),
+            version_relations: Some(RELATIONS_VERSION),
+        }
+    }
+    fn fill_gaps(&mut self) -> bool {
+        let mut modified = false;
+        if self.version_paragraphs.is_none() {
+            self.version_paragraphs = Some(PARAGRAPHS_VERSION);
+            modified = true;
+        }
+        if self.version_relations.is_none() {
+            self.version_relations = Some(RELATIONS_VERSION);
+            modified = true;
+        }
+        if self.version_texts.is_none() {
+            self.version_texts = Some(TEXTS_VERSION);
+            modified = true;
+        }
+        if self.version_vectors.is_none() {
+            self.version_vectors = Some(VECTORS_VERSION);
+            modified = true;
+        }
+        modified
+    }
+    pub fn version_paragraphs(&self) -> u32 {
+        self.version_paragraphs.unwrap_or(PARAGRAPHS_VERSION)
+    }
+    pub fn version_vectors(&self) -> u32 {
+        self.version_vectors.unwrap_or(VECTORS_VERSION)
+    }
+    pub fn version_texts(&self) -> u32 {
+        self.version_texts.unwrap_or(TEXTS_VERSION)
+    }
+    pub fn version_relations(&self) -> u32 {
+        self.version_relations.unwrap_or(RELATIONS_VERSION)
+    }
+    pub fn get_vectors_reader(&self, config: &VectorConfig) -> NodeResult<VectorsReaderPointer> {
+        match self.version_vectors {
+            Some(1) => nucliadb_vectors::service::VectorReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as VectorsReaderPointer),
+            Some(v) => Err(node_error!("Invalid vectors  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+    pub fn get_paragraphs_reader(
+        &self,
+        config: &ParagraphConfig,
+    ) -> NodeResult<ParagraphsReaderPointer> {
+        match self.version_paragraphs {
+            Some(1) => nucliadb_paragraphs::reader::ParagraphReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as ParagraphsReaderPointer),
+            Some(v) => Err(node_error!("Invalid paragraphs  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_texts_reader(&self, config: &TextConfig) -> NodeResult<TextsReaderPointer> {
+        match self.version_texts {
+            Some(1) => nucliadb_texts::reader::TextReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as TextsReaderPointer),
+            Some(v) => Err(node_error!("Invalid text  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_relations_reader(
+        &self,
+        config: &RelationConfig,
+    ) -> NodeResult<RelationsReaderPointer> {
+        match self.version_relations {
+            Some(1) => nucliadb_relations::service::RelationsReaderService::start(config)
+                .map(|i| encapsulate_reader(i) as RelationsReaderPointer),
+            Some(v) => Err(node_error!("Invalid relations  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_vectors_writer(&self, config: &VectorConfig) -> NodeResult<VectorsWriterPointer> {
+        match self.version_vectors {
+            Some(1) => nucliadb_vectors::service::VectorWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as VectorsWriterPointer),
+            Some(v) => Err(node_error!("Invalid vectors  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+    pub fn get_paragraphs_writer(
+        &self,
+        config: &ParagraphConfig,
+    ) -> NodeResult<ParagraphsWriterPointer> {
+        match self.version_paragraphs {
+            Some(1) => nucliadb_paragraphs::writer::ParagraphWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as ParagraphsWriterPointer),
+            Some(v) => Err(node_error!("Invalid paragraphs  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_texts_writer(&self, config: &TextConfig) -> NodeResult<TextsWriterPointer> {
+        match self.version_texts {
+            Some(1) => nucliadb_texts::writer::TextWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as TextsWriterPointer),
+            Some(v) => Err(node_error!("Invalid text  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn get_relations_writer(
+        &self,
+        config: &RelationConfig,
+    ) -> NodeResult<RelationsWriterPointer> {
+        match self.version_relations {
+            Some(1) => nucliadb_relations::service::RelationsWriterService::start(config)
+                .map(|i| encapsulate_writer(i) as RelationsWriterPointer),
+            Some(v) => Err(node_error!("Invalid relations  version {v}")),
+            None => Err(node_error!("Corrupted version file")),
+        }
+    }
+
+    pub fn load(versions_file: &Path) -> NodeResult<Versions> {
+        if versions_file.exists() {
+            let versions_json = std::fs::read_to_string(versions_file)?;
+            let mut versions: Versions = serde_json::from_str(&versions_json)?;
+            versions.fill_gaps();
+            Ok(versions)
+        } else if Versions::deprecated_versions_exists(versions_file) {
+            // In this case is an old index, therefore we create the versions file
+            // with the index versions that where available that moment.
+            // The writer will create the file at some point
+            Ok(Versions::new_from_deprecated())
+        } else {
+            Err(node_error!("Versions not found"))
+        }
+    }
+    pub fn load_or_create(versions_file: &Path) -> NodeResult<Versions> {
+        if versions_file.exists() {
+            let versions_json = std::fs::read_to_string(versions_file)?;
+            let mut versions: Versions = serde_json::from_str(&versions_json)?;
+            if versions.fill_gaps() {
+                let serialized = serde_json::to_string(&versions)?;
+                std::fs::write(versions_file, serialized)?;
+            }
+            Ok(versions)
+        } else if Versions::deprecated_versions_exists(versions_file) {
+            // In this case is an old index, therefore we create the versions file
+            // with the index versions that where available that moment.
+            let versions = Versions::new_from_deprecated();
+            let serialized = serde_json::to_string(&versions)?;
+            std::fs::write(versions_file, serialized)?;
+            Ok(versions)
+        } else {
+            let versions = Versions::new();
+            let serialized = serde_json::to_string(&versions)?;
+            std::fs::write(versions_file, serialized)?;
+            Ok(versions)
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/services/writer.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/services/writer.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,472 +1,521 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::path::{Path, PathBuf};
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::shard_created::{
-    DocumentService, ParagraphService, RelationService, VectorService,
-};
-use nucliadb_core::protos::{
-    DeleteGraphNodes, JoinGraph, NewShardRequest, OpStatus, Resource, ResourceId, VectorSetId,
-    VectorSimilarity,
-};
-use nucliadb_core::thread;
-use nucliadb_core::tracing::{self, *};
-
-use super::shard_disk_structure::*;
-use crate::services::versions::Versions;
-use crate::shard_metadata::ShardMetadata;
-use crate::telemetry::run_with_telemetry;
-
-#[derive(Debug)]
-pub struct ShardWriterService {
-    pub metadata: ShardMetadata,
-    pub id: String,
-    pub path: PathBuf,
-    text_writer: TextsWriterPointer,
-    paragraph_writer: ParagraphsWriterPointer,
-    vector_writer: VectorsWriterPointer,
-    relation_writer: RelationsWriterPointer,
-    document_service_version: i32,
-    paragraph_service_version: i32,
-    vector_service_version: i32,
-    relation_service_version: i32,
-}
-
-impl ShardWriterService {
-    #[tracing::instrument(skip_all)]
-    fn initialize(
-        id: String,
-        path: &Path,
-        metadata: ShardMetadata,
-        tsc: TextConfig,
-        psc: ParagraphConfig,
-        vsc: VectorConfig,
-        rsc: RelationConfig,
-    ) -> NodeResult<ShardWriterService> {
-        let versions = Versions::load_or_create(&path.join(VERSION_FILE))?;
-        let text_task = || Some(versions.get_texts_writer(&tsc));
-        let paragraph_task = || Some(versions.get_paragraphs_writer(&psc));
-        let vector_task = || Some(versions.get_vectors_writer(&vsc));
-        let relation_task = || Some(versions.get_relations_writer(&rsc));
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text start");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph start");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector start");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation start");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = None;
-        let mut paragraph_result = None;
-        let mut vector_result = None;
-        let mut relation_result = None;
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        let fields = text_result.transpose()?;
-        let paragraphs = paragraph_result.transpose()?;
-        let vectors = vector_result.transpose()?;
-        let relations = relation_result.transpose()?;
-
-        Ok(ShardWriterService {
-            id,
-            metadata,
-            path: path.to_path_buf(),
-            text_writer: fields.unwrap(),
-            paragraph_writer: paragraphs.unwrap(),
-            vector_writer: vectors.unwrap(),
-            relation_writer: relations.unwrap(),
-            document_service_version: versions.version_texts() as i32,
-            paragraph_service_version: versions.version_paragraphs() as i32,
-            vector_service_version: versions.version_vectors() as i32,
-            relation_service_version: versions.version_relations() as i32,
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn document_version(&self) -> DocumentService {
-        match self.document_service_version {
-            0 => DocumentService::DocumentV0,
-            1 => DocumentService::DocumentV1,
-            i => panic!("Unknown document version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_version(&self) -> ParagraphService {
-        match self.paragraph_service_version {
-            0 => ParagraphService::ParagraphV0,
-            1 => ParagraphService::ParagraphV1,
-            i => panic!("Unknown paragraph version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_version(&self) -> VectorService {
-        match self.vector_service_version {
-            0 => VectorService::VectorV0,
-            1 => VectorService::VectorV1,
-            i => panic!("Unknown vector version {i}"),
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn relation_version(&self) -> RelationService {
-        match self.relation_service_version {
-            0 => RelationService::RelationV0,
-            1 => RelationService::RelationV1,
-            i => panic!("Unknown relation version {i}"),
-        }
-    }
-    pub fn clean_and_create(id: String, path: &Path) -> NodeResult<ShardWriterService> {
-        let metadata = ShardMetadata::open(&path.join(METADATA_FILE))?;
-        std::fs::remove_dir_all(path)?;
-        std::fs::create_dir_all(path)?;
-        let tsc = TextConfig {
-            path: path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            no_results: None,
-            similarity: metadata.similarity(),
-            path: path.join(VECTORS_DIR),
-            vectorset: path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: path.join(RELATIONS_DIR),
-        };
-        ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc)
-    }
-    pub fn new(
-        id: String,
-        path: &Path,
-        request: &NewShardRequest,
-    ) -> NodeResult<ShardWriterService> {
-        std::fs::create_dir_all(path)?;
-        let metadata_path = path.join(METADATA_FILE);
-        let similarity = request.similarity();
-        let metadata = ShardMetadata::from(request.clone());
-        let tsc = TextConfig {
-            path: path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            no_results: None,
-            similarity: Some(similarity),
-            path: path.join(VECTORS_DIR),
-            vectorset: path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: path.join(RELATIONS_DIR),
-        };
-
-        metadata.serialize(&metadata_path)?;
-        ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc)
-    }
-
-    pub fn open(id: String, path: &Path) -> NodeResult<ShardWriterService> {
-        let metadata_path = path.join(METADATA_FILE);
-        let metadata = ShardMetadata::open(&metadata_path)?;
-        let tsc = TextConfig {
-            path: path.join(TEXTS_DIR),
-        };
-
-        let psc = ParagraphConfig {
-            path: path.join(PARAGRAPHS_DIR),
-        };
-
-        let vsc = VectorConfig {
-            no_results: None,
-            similarity: None,
-            path: path.join(VECTORS_DIR),
-            vectorset: path.join(VECTORSET_DIR),
-        };
-        let rsc = RelationConfig {
-            path: path.join(RELATIONS_DIR),
-        };
-        ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn stop(&mut self) {
-        info!("Stopping shard {}...", { &self.id });
-        let texts = self.text_writer.clone();
-        let paragraphs = self.paragraph_writer.clone();
-        let vectors = self.vector_writer.clone();
-        let relations = self.relation_writer.clone();
-
-        let text_task = move || text_write(&texts).stop();
-        let paragraph_task = move || paragraph_write(&paragraphs).stop();
-        let vector_task = move || vector_write(&vectors).stop();
-        let relation_task = move || relation_write(&relations).stop();
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text stop");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph stop");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector stop");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation stop");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        if let Err(e) = text_result {
-            error!("Error stopping the Field writer service: {}", e);
-        }
-        if let Err(e) = paragraph_result {
-            error!("Error stopping the Paragraph writer service: {}", e);
-        }
-        if let Err(e) = vector_result {
-            error!("Error stopping the Vector writer service: {}", e);
-        }
-        if let Err(e) = relation_result {
-            error!("Error stopping the Relation writer service: {}", e);
-        }
-        info!("Shard stopped {}...", { &self.id });
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn set_resource(&mut self, resource: &Resource) -> NodeResult<()> {
-        let text_writer_service = self.text_writer.clone();
-        let field_resource = resource.clone();
-        let text_task = move || {
-            info!("Field service starts set_resource");
-            let mut writer = text_write(&text_writer_service);
-            let result = writer.set_resource(&field_resource);
-            info!("Field service ends set_resource");
-            result
-        };
-
-        let paragraph_resource = resource.clone();
-        let paragraph_writer_service = self.paragraph_writer.clone();
-        let paragraph_task = move || {
-            info!("Paragraph service starts set_resource");
-            let mut writer = paragraph_write(&paragraph_writer_service);
-            let result = writer.set_resource(&paragraph_resource);
-            info!("Paragraph service ends set_resource");
-            result
-        };
-
-        let vector_writer_service = self.vector_writer.clone();
-        let vector_resource = resource.clone();
-        let vector_task = move || {
-            info!("Vector service starts set_resource");
-            let mut writer = vector_write(&vector_writer_service);
-            let result = writer.set_resource(&vector_resource);
-            info!("Vector service ends set_resource");
-            result
-        };
-
-        let relation_writer_service = self.relation_writer.clone();
-        let relation_resource = resource.clone();
-        let relation_task = move || {
-            info!("Relation service starts set_resource");
-            let mut writer = relation_write(&relation_writer_service);
-            let result = writer.set_resource(&relation_resource);
-            info!("Relation service ends set_resource");
-            result
-        };
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text set_resource");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph set_resource");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector set_resource");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation set_resource");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-
-        text_result?;
-        paragraph_result?;
-        vector_result?;
-        relation_result?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn remove_resource(&mut self, resource: &ResourceId) -> NodeResult<()> {
-        let text_writer_service = self.text_writer.clone();
-        let field_resource = resource.clone();
-        let text_task = move || {
-            let mut writer = text_write(&text_writer_service);
-            writer.delete_resource(&field_resource)
-        };
-        let paragraph_resource = resource.clone();
-        let paragraph_writer_service = self.paragraph_writer.clone();
-        let paragraph_task = move || {
-            let mut writer = paragraph_write(&paragraph_writer_service);
-            writer.delete_resource(&paragraph_resource)
-        };
-        let vector_writer_service = self.vector_writer.clone();
-        let vector_resource = resource.clone();
-        let vector_task = move || {
-            let mut writer = vector_write(&vector_writer_service);
-            writer.delete_resource(&vector_resource)
-        };
-        let relation_writer_service = self.relation_writer.clone();
-        let relation_resource = resource.clone();
-        let relation_task = move || {
-            let mut writer = relation_write(&relation_writer_service);
-            writer.delete_resource(&relation_resource)
-        };
-
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text remove");
-        let text_task = || run_with_telemetry(info, text_task);
-        let info = info_span!(parent: &span, "paragraph remove");
-        let paragraph_task = || run_with_telemetry(info, paragraph_task);
-        let info = info_span!(parent: &span, "vector remove");
-        let vector_task = || run_with_telemetry(info, vector_task);
-        let info = info_span!(parent: &span, "relation remove");
-        let relation_task = || run_with_telemetry(info, relation_task);
-
-        let mut text_result = Ok(());
-        let mut paragraph_result = Ok(());
-        let mut vector_result = Ok(());
-        let mut relation_result = Ok(());
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-            s.spawn(|_| relation_result = relation_task());
-        });
-        text_result?;
-        paragraph_result?;
-        vector_result?;
-        relation_result?;
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn get_opstatus(&self) -> NodeResult<OpStatus> {
-        let paragraphs = self.paragraph_writer.clone();
-        let vectors = self.vector_writer.clone();
-        let texts = self.text_writer.clone();
-        let span = tracing::Span::current();
-        let info = info_span!(parent: &span, "text count");
-        let text_task = || run_with_telemetry(info, move || text_read(&texts).count());
-        let info = info_span!(parent: &span, "paragraph count");
-        let paragraph_task =
-            || run_with_telemetry(info, move || paragraph_read(&paragraphs).count());
-        let info = info_span!(parent: &span, "vector count");
-        let vector_task = || run_with_telemetry(info, move || vector_read(&vectors).count());
-
-        let mut text_result = Ok(0);
-        let mut paragraph_result = Ok(0);
-        let mut vector_result = Ok(0);
-        thread::scope(|s| {
-            s.spawn(|_| text_result = text_task());
-            s.spawn(|_| paragraph_result = paragraph_task());
-            s.spawn(|_| vector_result = vector_task());
-        });
-        Ok(OpStatus {
-            shard_id: self.id.clone(),
-            count: text_result? as u64,
-            count_paragraphs: paragraph_result? as u64,
-            count_sentences: vector_result? as u64,
-            ..Default::default()
-        })
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn list_vectorsets(&self) -> NodeResult<Vec<String>> {
-        let reader = vector_read(&self.vector_writer);
-        let keys = reader.list_vectorsets()?;
-        Ok(keys)
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn add_vectorset(
-        &self,
-        setid: &VectorSetId,
-        similarity: VectorSimilarity,
-    ) -> NodeResult<()> {
-        let mut writer = vector_write(&self.vector_writer);
-        writer.add_vectorset(setid, similarity)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn remove_vectorset(&self, setid: &VectorSetId) -> NodeResult<()> {
-        let mut writer = vector_write(&self.vector_writer);
-        writer.remove_vectorset(setid)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn delete_relation_nodes(&self, nodes: &DeleteGraphNodes) -> NodeResult<()> {
-        let mut writer = relation_write(&self.relation_writer);
-        writer.delete_nodes(nodes)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn join_relations_graph(&self, graph: &JoinGraph) -> NodeResult<()> {
-        let mut writer = relation_write(&self.relation_writer);
-        writer.join_graph(graph)?;
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn paragraph_count(&self) -> NodeResult<usize> {
-        paragraph_read(&self.paragraph_writer).count()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn vector_count(&self) -> NodeResult<usize> {
-        vector_read(&self.vector_writer).count()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn text_count(&self) -> NodeResult<usize> {
-        text_read(&self.text_writer).count()
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn gc(&self) -> NodeResult<()> {
-        vector_write(&self.vector_writer).garbage_collection()
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::path::{Path, PathBuf};
+use std::time::SystemTime;
+
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::shard_created::{
+    DocumentService, ParagraphService, RelationService, VectorService,
+};
+use nucliadb_core::protos::{
+    DeleteGraphNodes, JoinGraph, NewShardRequest, OpStatus, Resource, ResourceId, VectorSetId,
+    VectorSimilarity,
+};
+use nucliadb_core::tracing::{self, *};
+use nucliadb_core::{context, thread};
+
+use super::shard_disk_structure::*;
+use crate::services::versions::Versions;
+use crate::shard_metadata::ShardMetadata;
+use crate::telemetry::run_with_telemetry;
+
+#[derive(Debug)]
+pub struct ShardWriterService {
+    pub metadata: ShardMetadata,
+    pub id: String,
+    pub path: PathBuf,
+    text_writer: TextsWriterPointer,
+    paragraph_writer: ParagraphsWriterPointer,
+    vector_writer: VectorsWriterPointer,
+    relation_writer: RelationsWriterPointer,
+    document_service_version: i32,
+    paragraph_service_version: i32,
+    vector_service_version: i32,
+    relation_service_version: i32,
+}
+
+impl ShardWriterService {
+    #[tracing::instrument(skip_all)]
+    fn initialize(
+        id: String,
+        path: &Path,
+        metadata: ShardMetadata,
+        tsc: TextConfig,
+        psc: ParagraphConfig,
+        vsc: VectorConfig,
+        rsc: RelationConfig,
+    ) -> NodeResult<ShardWriterService> {
+        let versions = Versions::load_or_create(&path.join(VERSION_FILE))?;
+        let text_task = || Some(versions.get_texts_writer(&tsc));
+        let paragraph_task = || Some(versions.get_paragraphs_writer(&psc));
+        let vector_task = || Some(versions.get_vectors_writer(&vsc));
+        let relation_task = || Some(versions.get_relations_writer(&rsc));
+
+        let span = tracing::Span::current();
+        let info = info_span!(parent: &span, "text start");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph start");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector start");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation start");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = None;
+        let mut paragraph_result = None;
+        let mut vector_result = None;
+        let mut relation_result = None;
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        let fields = text_result.transpose()?;
+        let paragraphs = paragraph_result.transpose()?;
+        let vectors = vector_result.transpose()?;
+        let relations = relation_result.transpose()?;
+
+        Ok(ShardWriterService {
+            id,
+            metadata,
+            path: path.to_path_buf(),
+            text_writer: fields.unwrap(),
+            paragraph_writer: paragraphs.unwrap(),
+            vector_writer: vectors.unwrap(),
+            relation_writer: relations.unwrap(),
+            document_service_version: versions.version_texts() as i32,
+            paragraph_service_version: versions.version_paragraphs() as i32,
+            vector_service_version: versions.version_vectors() as i32,
+            relation_service_version: versions.version_relations() as i32,
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn document_version(&self) -> DocumentService {
+        match self.document_service_version {
+            0 => DocumentService::DocumentV0,
+            1 => DocumentService::DocumentV1,
+            i => panic!("Unknown document version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_version(&self) -> ParagraphService {
+        match self.paragraph_service_version {
+            0 => ParagraphService::ParagraphV0,
+            1 => ParagraphService::ParagraphV1,
+            i => panic!("Unknown paragraph version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_version(&self) -> VectorService {
+        match self.vector_service_version {
+            0 => VectorService::VectorV0,
+            1 => VectorService::VectorV1,
+            i => panic!("Unknown vector version {i}"),
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn relation_version(&self) -> RelationService {
+        match self.relation_service_version {
+            0 => RelationService::RelationV0,
+            1 => RelationService::RelationV1,
+            i => panic!("Unknown relation version {i}"),
+        }
+    }
+    pub fn clean_and_create(id: String, path: &Path) -> NodeResult<ShardWriterService> {
+        let metadata = ShardMetadata::open(&path.join(METADATA_FILE))?;
+        std::fs::remove_dir_all(path)?;
+        std::fs::create_dir_all(path)?;
+        let tsc = TextConfig {
+            path: path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            no_results: None,
+            similarity: Some(metadata.similarity()),
+            path: path.join(VECTORS_DIR),
+            vectorset: path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: path.join(RELATIONS_DIR),
+        };
+        ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc)
+    }
+    pub fn new(
+        id: String,
+        path: &Path,
+        request: &NewShardRequest,
+    ) -> NodeResult<ShardWriterService> {
+        let time = SystemTime::now();
+
+        std::fs::create_dir_all(path)?;
+        let metadata_path = path.join(METADATA_FILE);
+        let similarity = request.similarity();
+        let metadata = ShardMetadata::from(request.clone());
+        let tsc = TextConfig {
+            path: path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            no_results: None,
+            similarity: Some(similarity),
+            path: path.join(VECTORS_DIR),
+            vectorset: path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: path.join(RELATIONS_DIR),
+        };
+
+        metadata.serialize(&metadata_path)?;
+        let result = ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc);
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/new".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+
+    pub fn open(id: String, path: &Path) -> NodeResult<ShardWriterService> {
+        let time = SystemTime::now();
+
+        let metadata_path = path.join(METADATA_FILE);
+        let metadata = ShardMetadata::open(&metadata_path)?;
+        let tsc = TextConfig {
+            path: path.join(TEXTS_DIR),
+        };
+
+        let psc = ParagraphConfig {
+            path: path.join(PARAGRAPHS_DIR),
+        };
+
+        let vsc = VectorConfig {
+            no_results: None,
+            similarity: None,
+            path: path.join(VECTORS_DIR),
+            vectorset: path.join(VECTORSET_DIR),
+        };
+        let rsc = RelationConfig {
+            path: path.join(RELATIONS_DIR),
+        };
+        let result = ShardWriterService::initialize(id, path, metadata, tsc, psc, vsc, rsc);
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/open".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn stop(&self) {
+        debug!("Stopping shard {}...", { &self.id });
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let texts = self.text_writer.clone();
+        let paragraphs = self.paragraph_writer.clone();
+        let vectors = self.vector_writer.clone();
+        let relations = self.relation_writer.clone();
+
+        let text_task = move || text_write(&texts).stop();
+        let paragraph_task = move || paragraph_write(&paragraphs).stop();
+        let vector_task = move || vector_write(&vectors).stop();
+        let relation_task = move || relation_write(&relations).stop();
+
+        let info = info_span!(parent: &span, "text stop");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph stop");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector stop");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation stop");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = Ok(());
+        let mut paragraph_result = Ok(());
+        let mut vector_result = Ok(());
+        let mut relation_result = Ok(());
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        if let Err(e) = text_result {
+            error!("Error stopping the Field writer service: {}", e);
+        }
+        if let Err(e) = paragraph_result {
+            error!("Error stopping the Paragraph writer service: {}", e);
+        }
+        if let Err(e) = vector_result {
+            error!("Error stopping the Vector writer service: {}", e);
+        }
+        if let Err(e) = relation_result {
+            error!("Error stopping the Relation writer service: {}", e);
+        }
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/stop".to_string());
+        metrics.record_request_time(metric, took);
+
+        debug!("Shard stopped {}...", { &self.id });
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn set_resource(&self, resource: &Resource) -> NodeResult<()> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let text_writer_service = self.text_writer.clone();
+        let field_resource = resource.clone();
+        let text_task = move || {
+            debug!("Field service starts set_resource");
+            let mut writer = text_write(&text_writer_service);
+            let result = writer.set_resource(&field_resource);
+            debug!("Field service ends set_resource");
+            result
+        };
+
+        let paragraph_resource = resource.clone();
+        let paragraph_writer_service = self.paragraph_writer.clone();
+        let paragraph_task = move || {
+            debug!("Paragraph service starts set_resource");
+            let mut writer = paragraph_write(&paragraph_writer_service);
+            let result = writer.set_resource(&paragraph_resource);
+            debug!("Paragraph service ends set_resource");
+            result
+        };
+
+        let vector_writer_service = self.vector_writer.clone();
+        let vector_resource = resource.clone();
+        let vector_task = move || {
+            debug!("Vector service starts set_resource");
+            let mut writer = vector_write(&vector_writer_service);
+            let result = writer.set_resource(&vector_resource);
+            debug!("Vector service ends set_resource");
+            result
+        };
+
+        let relation_writer_service = self.relation_writer.clone();
+        let relation_resource = resource.clone();
+        let relation_task = move || {
+            debug!("Relation service starts set_resource");
+            let mut writer = relation_write(&relation_writer_service);
+            let result = writer.set_resource(&relation_resource);
+            debug!("Relation service ends set_resource");
+            result
+        };
+
+        let info = info_span!(parent: &span, "text set_resource");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph set_resource");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector set_resource");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation set_resource");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = Ok(());
+        let mut paragraph_result = Ok(());
+        let mut vector_result = Ok(());
+        let mut relation_result = Ok(());
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/set_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        text_result?;
+        paragraph_result?;
+        vector_result?;
+        relation_result?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn remove_resource(&self, resource: &ResourceId) -> NodeResult<()> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let text_writer_service = self.text_writer.clone();
+        let field_resource = resource.clone();
+        let text_task = move || {
+            let mut writer = text_write(&text_writer_service);
+            writer.delete_resource(&field_resource)
+        };
+        let paragraph_resource = resource.clone();
+        let paragraph_writer_service = self.paragraph_writer.clone();
+        let paragraph_task = move || {
+            let mut writer = paragraph_write(&paragraph_writer_service);
+            writer.delete_resource(&paragraph_resource)
+        };
+        let vector_writer_service = self.vector_writer.clone();
+        let vector_resource = resource.clone();
+        let vector_task = move || {
+            let mut writer = vector_write(&vector_writer_service);
+            writer.delete_resource(&vector_resource)
+        };
+        let relation_writer_service = self.relation_writer.clone();
+        let relation_resource = resource.clone();
+        let relation_task = move || {
+            let mut writer = relation_write(&relation_writer_service);
+            writer.delete_resource(&relation_resource)
+        };
+
+        let info = info_span!(parent: &span, "text remove");
+        let text_task = || run_with_telemetry(info, text_task);
+        let info = info_span!(parent: &span, "paragraph remove");
+        let paragraph_task = || run_with_telemetry(info, paragraph_task);
+        let info = info_span!(parent: &span, "vector remove");
+        let vector_task = || run_with_telemetry(info, vector_task);
+        let info = info_span!(parent: &span, "relation remove");
+        let relation_task = || run_with_telemetry(info, relation_task);
+
+        let mut text_result = Ok(());
+        let mut paragraph_result = Ok(());
+        let mut vector_result = Ok(());
+        let mut relation_result = Ok(());
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+            s.spawn(|_| relation_result = relation_task());
+        });
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/remove_resource".to_string());
+        metrics.record_request_time(metric, took);
+
+        text_result?;
+        paragraph_result?;
+        vector_result?;
+        relation_result?;
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn get_opstatus(&self) -> NodeResult<OpStatus> {
+        let span = tracing::Span::current();
+        let time = SystemTime::now();
+
+        let paragraphs = self.paragraph_writer.clone();
+        let vectors = self.vector_writer.clone();
+        let texts = self.text_writer.clone();
+        let info = info_span!(parent: &span, "text count");
+        let text_task = || run_with_telemetry(info, move || text_read(&texts).count());
+        let info = info_span!(parent: &span, "paragraph count");
+        let paragraph_task =
+            || run_with_telemetry(info, move || paragraph_read(&paragraphs).count());
+        let info = info_span!(parent: &span, "vector count");
+        let vector_task = || run_with_telemetry(info, move || vector_read(&vectors).count());
+
+        let mut text_result = Ok(0);
+        let mut paragraph_result = Ok(0);
+        let mut vector_result = Ok(0);
+        thread::scope(|s| {
+            s.spawn(|_| text_result = text_task());
+            s.spawn(|_| paragraph_result = paragraph_task());
+            s.spawn(|_| vector_result = vector_task());
+        });
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::shard("writer/get_opstatus".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(OpStatus {
+            shard_id: self.id.clone(),
+            count: text_result? as u64,
+            count_paragraphs: paragraph_result? as u64,
+            count_sentences: vector_result? as u64,
+            ..Default::default()
+        })
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn list_vectorsets(&self) -> NodeResult<Vec<String>> {
+        let reader = vector_read(&self.vector_writer);
+        let keys = reader.list_vectorsets()?;
+        Ok(keys)
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn add_vectorset(
+        &self,
+        setid: &VectorSetId,
+        similarity: VectorSimilarity,
+    ) -> NodeResult<()> {
+        let mut writer = vector_write(&self.vector_writer);
+        writer.add_vectorset(setid, similarity)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn remove_vectorset(&self, setid: &VectorSetId) -> NodeResult<()> {
+        let mut writer = vector_write(&self.vector_writer);
+        writer.remove_vectorset(setid)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn delete_relation_nodes(&self, nodes: &DeleteGraphNodes) -> NodeResult<()> {
+        let mut writer = relation_write(&self.relation_writer);
+        writer.delete_nodes(nodes)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn join_relations_graph(&self, graph: &JoinGraph) -> NodeResult<()> {
+        let mut writer = relation_write(&self.relation_writer);
+        writer.join_graph(graph)?;
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn paragraph_count(&self) -> NodeResult<usize> {
+        paragraph_read(&self.paragraph_writer).count()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn vector_count(&self) -> NodeResult<usize> {
+        vector_read(&self.vector_writer).count()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn text_count(&self) -> NodeResult<usize> {
+        text_read(&self.text_writer).count()
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn gc(&self) -> NodeResult<()> {
+        vector_write(&self.vector_writer).garbage_collection()
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/shard_metadata.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/shard_metadata.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,126 +1,126 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::fs::File;
-use std::io::{BufReader, BufWriter, Write};
-use std::path::Path;
-
-use nucliadb_core::protos::{NewShardRequest, ShardMetadata as GrpcMetadata, VectorSimilarity};
-use nucliadb_core::{node_error, NodeResult};
-use serde::*;
-
-#[derive(Serialize, Deserialize, Default, Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
-enum Similarity {
-    #[default]
-    Cosine,
-    Dot,
-}
-
-impl From<VectorSimilarity> for Similarity {
-    fn from(value: VectorSimilarity) -> Self {
-        match value {
-            VectorSimilarity::Cosine => Similarity::Cosine,
-            VectorSimilarity::Dot => Similarity::Dot,
-        }
-    }
-}
-impl From<Similarity> for VectorSimilarity {
-    fn from(value: Similarity) -> Self {
-        match value {
-            Similarity::Cosine => VectorSimilarity::Cosine,
-            Similarity::Dot => VectorSimilarity::Dot,
-        }
-    }
-}
-
-#[derive(Serialize, Deserialize, Default, Clone, Debug)]
-pub struct ShardMetadata {
-    kbid: Option<String>,
-    similarity: Option<Similarity>,
-}
-
-impl From<ShardMetadata> for GrpcMetadata {
-    fn from(x: ShardMetadata) -> GrpcMetadata {
-        GrpcMetadata {
-            kbid: x.kbid.unwrap_or_default(),
-        }
-    }
-}
-impl From<NewShardRequest> for ShardMetadata {
-    fn from(value: NewShardRequest) -> Self {
-        ShardMetadata {
-            similarity: Some(value.similarity().into()),
-            kbid: Some(value.kbid).filter(|s| !s.is_empty()),
-        }
-    }
-}
-
-impl ShardMetadata {
-    pub fn open(metadata: &Path) -> NodeResult<ShardMetadata> {
-        if !metadata.exists() {
-            return Ok(ShardMetadata::default());
-        }
-
-        let mut reader = BufReader::new(File::open(metadata)?);
-        Ok(serde_json::from_reader(&mut reader)?)
-    }
-    pub fn serialize(&self, metadata: &Path) -> NodeResult<()> {
-        if metadata.exists() {
-            return Err(node_error!("Metadata file already exists at {metadata:?}"));
-        }
-
-        let mut writer = BufWriter::new(File::create(metadata)?);
-        serde_json::to_writer(&mut writer, self)?;
-        Ok(writer.flush()?)
-    }
-    pub fn kbid(&self) -> Option<&str> {
-        self.kbid.as_deref()
-    }
-    pub fn similarity(&self) -> Option<VectorSimilarity> {
-        self.similarity.map(VectorSimilarity::from)
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use tempfile::TempDir;
-
-    use super::*;
-    #[test]
-    fn create() {
-        let dir = TempDir::new().unwrap();
-        let metadata_path = dir.path().join("metadata.json");
-        let meta = ShardMetadata {
-            kbid: Some("KB".to_string()),
-            similarity: Some(Similarity::Cosine),
-        };
-        meta.serialize(&metadata_path).unwrap();
-        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
-        assert_eq!(meta.kbid, meta_disk.kbid);
-        assert_eq!(meta.similarity, meta_disk.similarity);
-    }
-    #[test]
-    fn open_empty() {
-        let dir = TempDir::new().unwrap();
-        let metadata_path = dir.path().join("metadata.json");
-        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
-        assert!(meta_disk.kbid.is_none());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fs::File;
+use std::io::{BufReader, BufWriter, Write};
+use std::path::Path;
+
+use nucliadb_core::protos::{NewShardRequest, ShardMetadata as GrpcMetadata, VectorSimilarity};
+use nucliadb_core::{node_error, NodeResult};
+use serde::*;
+
+#[derive(Serialize, Deserialize, Default, Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
+enum Similarity {
+    #[default]
+    Cosine,
+    Dot,
+}
+
+impl From<VectorSimilarity> for Similarity {
+    fn from(value: VectorSimilarity) -> Self {
+        match value {
+            VectorSimilarity::Cosine => Similarity::Cosine,
+            VectorSimilarity::Dot => Similarity::Dot,
+        }
+    }
+}
+impl From<Similarity> for VectorSimilarity {
+    fn from(value: Similarity) -> Self {
+        match value {
+            Similarity::Cosine => VectorSimilarity::Cosine,
+            Similarity::Dot => VectorSimilarity::Dot,
+        }
+    }
+}
+
+#[derive(Serialize, Deserialize, Default, Clone, Debug)]
+pub struct ShardMetadata {
+    kbid: Option<String>,
+    similarity: Option<Similarity>,
+}
+
+impl From<ShardMetadata> for GrpcMetadata {
+    fn from(x: ShardMetadata) -> GrpcMetadata {
+        GrpcMetadata {
+            kbid: x.kbid.unwrap_or_default(),
+        }
+    }
+}
+impl From<NewShardRequest> for ShardMetadata {
+    fn from(value: NewShardRequest) -> Self {
+        ShardMetadata {
+            similarity: Some(value.similarity().into()),
+            kbid: Some(value.kbid).filter(|s| !s.is_empty()),
+        }
+    }
+}
+
+impl ShardMetadata {
+    pub fn open(metadata: &Path) -> NodeResult<ShardMetadata> {
+        if !metadata.exists() {
+            return Ok(ShardMetadata::default());
+        }
+
+        let mut reader = BufReader::new(File::open(metadata)?);
+        Ok(serde_json::from_reader(&mut reader)?)
+    }
+    pub fn serialize(&self, metadata: &Path) -> NodeResult<()> {
+        if metadata.exists() {
+            return Err(node_error!("Metadata file already exists at {metadata:?}"));
+        }
+
+        let mut writer = BufWriter::new(File::create(metadata)?);
+        serde_json::to_writer(&mut writer, self)?;
+        Ok(writer.flush()?)
+    }
+    pub fn kbid(&self) -> Option<&str> {
+        self.kbid.as_deref()
+    }
+    pub fn similarity(&self) -> VectorSimilarity {
+        self.similarity.unwrap_or(Similarity::Cosine).into()
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use tempfile::TempDir;
+
+    use super::*;
+    #[test]
+    fn create() {
+        let dir = TempDir::new().unwrap();
+        let metadata_path = dir.path().join("metadata.json");
+        let meta = ShardMetadata {
+            kbid: Some("KB".to_string()),
+            similarity: Some(Similarity::Cosine),
+        };
+        meta.serialize(&metadata_path).unwrap();
+        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
+        assert_eq!(meta.kbid, meta_disk.kbid);
+        assert_eq!(meta.similarity, meta_disk.similarity);
+    }
+    #[test]
+    fn open_empty() {
+        let dir = TempDir::new().unwrap();
+        let metadata_path = dir.path().join("metadata.json");
+        let meta_disk = ShardMetadata::open(&metadata_path).unwrap();
+        assert!(meta_disk.kbid.is_none());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/telemetry.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/telemetry.rs`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,107 +1,107 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use nucliadb_core::tracing::{Level, Span};
-use nucliadb_core::{Context, NodeResult};
-use opentelemetry::global;
-use opentelemetry::trace::TraceContextExt;
-use sentry::ClientInitGuard;
-use tracing_opentelemetry::OpenTelemetrySpanExt;
-use tracing_subscriber::filter::{FilterFn, Targets};
-use tracing_subscriber::layer::SubscriberExt;
-use tracing_subscriber::util::SubscriberInitExt;
-use tracing_subscriber::{Layer, Registry};
-
-use crate::env;
-
-const TRACE_ID: &str = "trace-id";
-
-pub fn init_telemetry() -> NodeResult<ClientInitGuard> {
-    let log_levels = env::log_level();
-
-    let mut layers = Vec::new();
-
-    if env::jaeger_enabled() {
-        layers.push(init_jaeger(log_levels.clone())?);
-    }
-
-    let stdout_layer = tracing_subscriber::fmt::layer()
-        .with_level(true)
-        .with_filter(Targets::new().with_targets(log_levels))
-        .boxed();
-
-    layers.push(stdout_layer);
-
-    let sentry_env = env::get_sentry_env();
-    let guard = sentry::init((
-        env::sentry_url(),
-        sentry::ClientOptions {
-            release: sentry::release_name!(),
-            environment: Some(sentry_env.into()),
-            ..Default::default()
-        },
-    ));
-    layers.push(sentry_tracing::layer().boxed());
-
-    tracing_subscriber::registry()
-        .with(layers)
-        .try_init()
-        .with_context(|| "trying to init tracing")?;
-    Ok(guard)
-}
-
-pub(crate) fn run_with_telemetry<F, R>(current: Span, f: F) -> R
-where F: FnOnce() -> R {
-    let tid = current.context().span().span_context().trace_id();
-    sentry::with_scope(|scope| scope.set_tag(TRACE_ID, tid), || current.in_scope(f))
-}
-
-fn init_jaeger(
-    log_levels: Vec<(String, Level)>,
-) -> NodeResult<Box<dyn Layer<Registry> + Send + Sync>> {
-    let agent_endpoint = env::jaeger_agent_endp();
-    let tracer = opentelemetry_jaeger::new_pipeline()
-        .with_agent_endpoint(agent_endpoint)
-        .with_service_name("nucliadb_node")
-        .with_auto_split_batch(true)
-        .install_batch(opentelemetry::runtime::Tokio)?;
-
-    // This filter is needed because we want to keep logs in stdout and attach logs to jaeger
-    // spans in really rare cases So, basically it checks the source of event (allowed
-    // only from nucliadb_node crate) and filter out all events without special field
-    // For attaching log to jaeger span use this:
-    // tracing::event!(Level::INFO, trace_marker = true, "your logs for jaeger here: {}", foo =
-    // bar);
-    let filter = FilterFn::new(|metadata| {
-        metadata
-            .file()
-            .filter(|file| file.contains("nucliadb_node"))
-            .map(|_| metadata.is_event())
-            .map(|state| state && metadata.fields().field("trace_marker").is_none())
-            .map(|state| !state)
-            .unwrap_or_default()
-    });
-    global::set_text_map_propagator(opentelemetry_zipkin::Propagator::new());
-
-    Ok(tracing_opentelemetry::layer()
-        .with_tracer(tracer)
-        .with_filter(Targets::new().with_targets(log_levels))
-        .with_filter(filter)
-        .boxed())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use nucliadb_core::tracing::{Level, Span};
+use nucliadb_core::{Context, NodeResult};
+use opentelemetry::global;
+use opentelemetry::trace::TraceContextExt;
+use sentry::ClientInitGuard;
+use tracing_opentelemetry::OpenTelemetrySpanExt;
+use tracing_subscriber::filter::{FilterFn, Targets};
+use tracing_subscriber::layer::SubscriberExt;
+use tracing_subscriber::util::SubscriberInitExt;
+use tracing_subscriber::{Layer, Registry};
+
+use crate::env;
+
+const TRACE_ID: &str = "trace-id";
+
+pub fn init_telemetry() -> NodeResult<ClientInitGuard> {
+    let log_levels = env::log_level();
+
+    let mut layers = Vec::new();
+
+    if env::jaeger_enabled() {
+        layers.push(init_jaeger(log_levels.clone())?);
+    }
+
+    let stdout_layer = tracing_subscriber::fmt::layer()
+        .with_level(true)
+        .with_filter(Targets::new().with_targets(log_levels))
+        .boxed();
+
+    layers.push(stdout_layer);
+
+    let sentry_env = env::get_sentry_env();
+    let guard = sentry::init((
+        env::sentry_url(),
+        sentry::ClientOptions {
+            release: sentry::release_name!(),
+            environment: Some(sentry_env.into()),
+            ..Default::default()
+        },
+    ));
+    layers.push(sentry_tracing::layer().boxed());
+
+    tracing_subscriber::registry()
+        .with(layers)
+        .try_init()
+        .with_context(|| "trying to init tracing")?;
+    Ok(guard)
+}
+
+pub(crate) fn run_with_telemetry<F, R>(current: Span, f: F) -> R
+where F: FnOnce() -> R {
+    let tid = current.context().span().span_context().trace_id();
+    sentry::with_scope(|scope| scope.set_tag(TRACE_ID, tid), || current.in_scope(f))
+}
+
+fn init_jaeger(
+    log_levels: Vec<(String, Level)>,
+) -> NodeResult<Box<dyn Layer<Registry> + Send + Sync>> {
+    let agent_endpoint = env::jaeger_agent_endp();
+    let tracer = opentelemetry_jaeger::new_pipeline()
+        .with_agent_endpoint(agent_endpoint)
+        .with_service_name("nucliadb_node")
+        .with_auto_split_batch(true)
+        .install_batch(opentelemetry::runtime::Tokio)?;
+
+    // This filter is needed because we want to keep logs in stdout and attach logs to jaeger
+    // spans in really rare cases So, basically it checks the source of event (allowed
+    // only from nucliadb_node crate) and filter out all events without special field
+    // For attaching log to jaeger span use this:
+    // tracing::event!(Level::INFO, trace_marker = true, "your logs for jaeger here: {}", foo =
+    // bar);
+    let filter = FilterFn::new(|metadata| {
+        metadata
+            .file()
+            .filter(|file| file.contains("nucliadb_node"))
+            .map(|_| metadata.is_event())
+            .map(|state| state && metadata.fields().field("trace_marker").is_none())
+            .map(|state| !state)
+            .unwrap_or_default()
+    });
+    global::set_text_map_propagator(opentelemetry_zipkin::Propagator::new());
+
+    Ok(tracing_opentelemetry::layer()
+        .with_tracer(tracer)
+        .with_filter(Targets::new().with_targets(log_levels))
+        .with_filter(filter)
+        .boxed())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/utils.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/utils.rs`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,85 +1,85 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::net::{IpAddr, SocketAddr};
-use std::str::FromStr;
-use std::time::Duration;
-
-use http::Uri;
-use nucliadb_core::tracing::Level;
-use opentelemetry::propagation::Extractor;
-use tokio::net;
-use tokio::time::sleep;
-use tonic::transport::Endpoint;
-
-/// Prepares a socket addr for a grpc endpoint to connect to
-pub fn socket_to_endpoint(grpc_addr: SocketAddr) -> anyhow::Result<Endpoint> {
-    let uri = Uri::builder()
-        .scheme("http")
-        .authority(grpc_addr.to_string().as_str())
-        .path_and_query("/")
-        .build()?;
-    // Create a channel with connect_lazy to automatically reconnect to the node.
-    let channel = Endpoint::from(uri);
-    Ok(channel)
-}
-
-/// Metadata mapping
-pub struct MetadataMap<'a>(pub &'a tonic::metadata::MetadataMap);
-
-impl<'a> Extractor for MetadataMap<'a> {
-    /// Gets a value for a key from the MetadataMap.  If the value can't be converted to &str,
-    /// returns None
-    fn get(&self, key: &str) -> Option<&str> {
-        self.0.get(key).and_then(|metadata| metadata.to_str().ok())
-    }
-
-    /// Collect all the keys from the MetadataMap.
-    fn keys(&self) -> Vec<&str> {
-        self.0
-            .keys()
-            .map(|key| match key {
-                tonic::metadata::KeyRef::Ascii(v) => v.as_str(),
-                tonic::metadata::KeyRef::Binary(v) => v.as_str(),
-            })
-            .collect::<Vec<_>>()
-    }
-}
-
-pub async fn reliable_lookup_host(host: &str) -> IpAddr {
-    let mut tries = 5;
-    while tries != 0 {
-        if let Ok(mut addr_iter) = net::lookup_host(host).await {
-            if let Some(addr) = addr_iter.next() {
-                return addr.ip();
-            }
-        }
-        tries -= 1;
-        sleep(Duration::from_secs(1)).await;
-    }
-    IpAddr::from_str(host).unwrap()
-}
-
-pub fn parse_log_level(levels: &str) -> Vec<(String, Level)> {
-    levels
-        .split(',')
-        .map(|s| s.splitn(2, '=').collect::<Vec<_>>())
-        .map(|v| (v[0].to_string(), Level::from_str(v[1]).unwrap()))
-        .collect()
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::net::{IpAddr, SocketAddr};
+use std::str::FromStr;
+use std::time::Duration;
+
+use http::Uri;
+use nucliadb_core::tracing::Level;
+use opentelemetry::propagation::Extractor;
+use tokio::net;
+use tokio::time::sleep;
+use tonic::transport::Endpoint;
+
+/// Prepares a socket addr for a grpc endpoint to connect to
+pub fn socket_to_endpoint(grpc_addr: SocketAddr) -> anyhow::Result<Endpoint> {
+    let uri = Uri::builder()
+        .scheme("http")
+        .authority(grpc_addr.to_string().as_str())
+        .path_and_query("/")
+        .build()?;
+    // Create a channel with connect_lazy to automatically reconnect to the node.
+    let channel = Endpoint::from(uri);
+    Ok(channel)
+}
+
+/// Metadata mapping
+pub struct MetadataMap<'a>(pub &'a tonic::metadata::MetadataMap);
+
+impl<'a> Extractor for MetadataMap<'a> {
+    /// Gets a value for a key from the MetadataMap.  If the value can't be converted to &str,
+    /// returns None
+    fn get(&self, key: &str) -> Option<&str> {
+        self.0.get(key).and_then(|metadata| metadata.to_str().ok())
+    }
+
+    /// Collect all the keys from the MetadataMap.
+    fn keys(&self) -> Vec<&str> {
+        self.0
+            .keys()
+            .map(|key| match key {
+                tonic::metadata::KeyRef::Ascii(v) => v.as_str(),
+                tonic::metadata::KeyRef::Binary(v) => v.as_str(),
+            })
+            .collect::<Vec<_>>()
+    }
+}
+
+pub async fn reliable_lookup_host(host: &str) -> IpAddr {
+    let mut tries = 5;
+    while tries != 0 {
+        if let Ok(mut addr_iter) = net::lookup_host(host).await {
+            if let Some(addr) = addr_iter.next() {
+                return addr.ip();
+            }
+        }
+        tries -= 1;
+        sleep(Duration::from_secs(1)).await;
+    }
+    IpAddr::from_str(host).unwrap()
+}
+
+pub fn parse_log_level(levels: &str) -> Vec<(String, Level)> {
+    levels
+        .split(',')
+        .map(|s| s.splitn(2, '=').collect::<Vec<_>>())
+        .map(|v| (v[0].to_string(), Level::from_str(v[1]).unwrap()))
+        .collect()
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/writer/grpc_driver.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/writer/grpc_driver.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,712 +1,718 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::time::Duration;
-
-use async_std::sync::RwLock;
-use nucliadb_core::protos::node_writer_server::NodeWriter;
-use nucliadb_core::protos::{
-    op_status, AcceptShardRequest, DeleteGraphNodes, EmptyQuery, EmptyResponse, MoveShardRequest,
-    NewShardRequest, NewVectorSetRequest, NodeMetadata, OpStatus, Resource, ResourceId, SetGraph,
-    ShardCleaned, ShardCreated, ShardId, ShardIds, VectorSetId, VectorSetList,
-};
-use nucliadb_core::tracing::{self, *};
-use nucliadb_ftp::{Listener, Publisher, RetryPolicy};
-use nucliadb_telemetry::payload::TelemetryEvent;
-use nucliadb_telemetry::sync::send_telemetry_event;
-use opentelemetry::global;
-use tokio::sync::mpsc::UnboundedSender;
-use tonic::{Request, Response, Status};
-use tracing_opentelemetry::OpenTelemetrySpanExt;
-
-use crate::env;
-use crate::utils::MetadataMap;
-use crate::writer::NodeWriterService;
-
-/// Indicates the maximum duration used to move one shard from one node to another on failure only.
-const MAX_MOVE_SHARD_DURATION: Duration = Duration::from_secs(5 * 60);
-
-#[derive(Debug, Clone, PartialEq, Eq)]
-pub enum NodeWriterEvent {
-    ShardCreation(String, String),
-    ShardDeletion(String),
-    ParagraphCount(String, u64),
-}
-
-pub struct NodeWriterGRPCDriver {
-    inner: RwLock<NodeWriterService>,
-    sender: Option<UnboundedSender<NodeWriterEvent>>,
-}
-
-impl From<NodeWriterService> for NodeWriterGRPCDriver {
-    fn from(node: NodeWriterService) -> NodeWriterGRPCDriver {
-        NodeWriterGRPCDriver {
-            inner: RwLock::new(node),
-            sender: None,
-        }
-    }
-}
-
-impl NodeWriterGRPCDriver {
-    pub fn with_sender(self, sender: UnboundedSender<NodeWriterEvent>) -> Self {
-        Self {
-            sender: Some(sender),
-            ..self
-        }
-    }
-    // The GRPC writer will only request the writer to bring a shard
-    // to memory if lazy loading is enabled. Otherwise all the
-    // shards on disk would have been brought to memory before the driver is online.
-    #[tracing::instrument(skip_all)]
-    async fn load_shard(&self, id: &ShardId) {
-        if env::lazy_loading() {
-            let mut writer = self.inner.write().await;
-            writer.load_shard(id);
-        }
-    }
-
-    // Instrumentation utilities for telemetry
-    fn instrument<T>(&self, request: &tonic::Request<T>) {
-        let parent_cx =
-            global::get_text_map_propagator(|prop| prop.extract(&MetadataMap(request.metadata())));
-        Span::current().set_parent(parent_cx);
-    }
-
-    #[tracing::instrument(skip_all)]
-    fn emit_event(&self, event: NodeWriterEvent) {
-        if let Some(sender) = &self.sender {
-            let _ = sender.send(event);
-        }
-    }
-}
-
-#[tonic::async_trait]
-impl NodeWriter for NodeWriterGRPCDriver {
-    #[tracing::instrument(skip_all)]
-    async fn get_shard(&self, request: Request<ShardId>) -> Result<Response<ShardId>, Status> {
-        self.instrument(&request);
-
-        info!("{:?}: gRPC get_shard", request);
-        let shard_id = request.into_inner();
-        self.load_shard(&shard_id).await;
-        let reader = self.inner.read().await;
-        let result = reader.get_shard(&shard_id).is_some();
-        std::mem::drop(reader);
-        match result {
-            true => {
-                info!("{:?}: Ready readed", shard_id);
-                Ok(tonic::Response::new(shard_id))
-            }
-            false => {
-                let message = format!("Shard not found {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn new_shard(
-        &self,
-        request: Request<NewShardRequest>,
-    ) -> Result<Response<ShardCreated>, Status> {
-        self.instrument(&request);
-
-        info!("Creating new shard");
-        let request = request.into_inner();
-        send_telemetry_event(TelemetryEvent::Create).await;
-        let mut writer = self.inner.write().await;
-        let result = writer
-            .new_shard(&request)
-            .map_err(|e| tonic::Status::internal(e.to_string()))?;
-        std::mem::drop(writer);
-        self.emit_event(NodeWriterEvent::ShardCreation(
-            result.id.clone(),
-            request.kbid,
-        ));
-        Ok(tonic::Response::new(result))
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn delete_shard(&self, request: Request<ShardId>) -> Result<Response<ShardId>, Status> {
-        self.instrument(&request);
-
-        info!("gRPC delete_shard {:?}", request);
-        send_telemetry_event(TelemetryEvent::Delete).await;
-        // Deletion does not require for the shard
-        // to be loaded.
-        let shard_id = request.into_inner();
-        let mut writer = self.inner.write().await;
-        let result = writer.delete_shard(&shard_id);
-        std::mem::drop(writer);
-        match result {
-            Ok(_) => {
-                self.emit_event(NodeWriterEvent::ShardDeletion(shard_id.id.clone()));
-
-                Ok(tonic::Response::new(shard_id))
-            }
-            Err(e) => {
-                let error_msg = format!("Error deleting shard {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn clean_and_upgrade_shard(
-        &self,
-        request: Request<ShardId>,
-    ) -> Result<Response<ShardCleaned>, Status> {
-        self.instrument(&request);
-
-        info!("gRPC delete_shard {:?}", request);
-
-        // Deletion and upgrade do not require for the shard
-        // to be loaded.
-        let shard_id = request.into_inner();
-        let mut writer = self.inner.write().await;
-        let result = writer.clean_and_upgrade_shard(&shard_id);
-        std::mem::drop(writer);
-        match result {
-            Ok(updated) => Ok(tonic::Response::new(updated)),
-            Err(e) => {
-                let error_msg = format!("Error deleting shard {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn list_shards(
-        &self,
-        request: Request<EmptyQuery>,
-    ) -> Result<Response<ShardIds>, Status> {
-        self.instrument(&request);
-        let ids = self.inner.read().await.get_shard_ids();
-        Ok(tonic::Response::new(ids))
-    }
-
-    // Incremental call that can be call multiple times for the same resource
-    #[tracing::instrument(skip_all)]
-    async fn set_resource(&self, request: Request<Resource>) -> Result<Response<OpStatus>, Status> {
-        self.instrument(&request);
-        let resource = request.into_inner();
-        let shard_id = ShardId {
-            id: resource.shard_id.clone(),
-        };
-        self.load_shard(&shard_id).await;
-        let mut writer = self.inner.write().await;
-        let result = writer.set_resource(&shard_id, &resource);
-        match result.transpose() {
-            Some(Ok(mut status)) => {
-                info!("Set resource ends correctly");
-                status.status = 0;
-                status.detail = "Success!".to_string();
-
-                self.emit_event(NodeWriterEvent::ParagraphCount(
-                    shard_id.id.clone(),
-                    status.count_paragraphs,
-                ));
-
-                Ok(tonic::Response::new(status))
-            }
-            Some(Err(e)) => {
-                let status = op_status::Status::Error as i32;
-                let detail = format!("Error: {}", e);
-                let op_status = OpStatus {
-                    status,
-                    detail,
-                    count: 0_u64,
-                    shard_id: shard_id.id.clone(),
-                    ..Default::default()
-                };
-                Ok(tonic::Response::new(op_status))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn delete_relation_nodes(
-        &self,
-        request: Request<DeleteGraphNodes>,
-    ) -> Result<Response<OpStatus>, Status> {
-        self.instrument(&request);
-        let request = request.into_inner();
-        let shard_id = request.shard_id.as_ref().unwrap();
-        self.load_shard(shard_id).await;
-        let mut writer = self.inner.write().await;
-        match writer.delete_relation_nodes(shard_id, &request).transpose() {
-            Some(Ok(mut status)) => {
-                info!("Delete relations ends correctly");
-                status.status = 0;
-                status.detail = "Success!".to_string();
-                Ok(tonic::Response::new(status))
-            }
-            Some(Err(e)) => {
-                let error_msg = format!("Error {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-            None => {
-                let message = format!("Shard not found {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn join_graph(&self, request: Request<SetGraph>) -> Result<Response<OpStatus>, Status> {
-        self.instrument(&request);
-        let request = request.into_inner();
-        let shard_id = request.shard_id.unwrap();
-        let graph = request.graph.unwrap();
-        self.load_shard(&shard_id).await;
-        let mut writer = self.inner.write().await;
-        match writer.join_relations_graph(&shard_id, &graph).transpose() {
-            Some(Ok(mut status)) => {
-                info!("Join graph ends correctly");
-                status.status = 0;
-                status.detail = "Success!".to_string();
-                Ok(tonic::Response::new(status))
-            }
-            Some(Err(e)) => {
-                let error_msg = format!("Error {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-            None => {
-                let message = format!("Shard not found {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn remove_resource(
-        &self,
-        request: Request<ResourceId>,
-    ) -> Result<Response<OpStatus>, Status> {
-        self.instrument(&request);
-        let resource = request.into_inner();
-        let shard_id = ShardId {
-            id: resource.shard_id.clone(),
-        };
-
-        self.load_shard(&shard_id).await;
-        let mut writer = self.inner.write().await;
-        let result = writer.remove_resource(&shard_id, &resource);
-
-        match result.transpose() {
-            Some(Ok(mut status)) => {
-                info!("Remove resource ends correctly");
-                status.status = 0;
-                status.detail = "Success!".to_string();
-
-                self.emit_event(NodeWriterEvent::ParagraphCount(
-                    shard_id.id.clone(),
-                    status.count_paragraphs,
-                ));
-
-                Ok(tonic::Response::new(status))
-            }
-            Some(Err(e)) => {
-                let status = op_status::Status::Error as i32;
-                let detail = format!("Error: {}", e);
-                let op_status = OpStatus {
-                    status,
-                    detail,
-                    count: 0_u64,
-                    shard_id: shard_id.id.clone(),
-                    ..Default::default()
-                };
-                Ok(tonic::Response::new(op_status))
-            }
-            None => {
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    async fn add_vector_set(
-        &self,
-        request: Request<NewVectorSetRequest>,
-    ) -> Result<Response<OpStatus>, Status> {
-        self.instrument(&request);
-        let request = request.into_inner();
-        let shard_id = request.id.as_ref().and_then(|i| i.shard.clone()).unwrap();
-        self.load_shard(&shard_id).await;
-        let mut writer = self.inner.write().await;
-        match writer.add_vectorset(&request).transpose() {
-            Some(Ok(mut status)) => {
-                info!("add_vector_set ends correctly");
-                status.status = 0;
-                status.detail = "Success!".to_string();
-                Ok(tonic::Response::new(status))
-            }
-            Some(Err(e)) => {
-                let error_msg = format!("Error {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-            None => {
-                let message = format!("Shard not found {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    async fn remove_vector_set(
-        &self,
-        request: Request<VectorSetId>,
-    ) -> Result<Response<OpStatus>, Status> {
-        self.instrument(&request);
-        let request = request.into_inner();
-        let shard_id = request.shard.as_ref().unwrap();
-        self.load_shard(shard_id).await;
-        let mut writer = self.inner.write().await;
-        match writer.remove_vectorset(shard_id, &request).transpose() {
-            Some(Ok(mut status)) => {
-                info!("remove_vector_set ends correctly");
-                status.status = 0;
-                status.detail = "Success!".to_string();
-                Ok(tonic::Response::new(status))
-            }
-            Some(Err(e)) => {
-                let error_msg = format!("Error {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-            None => {
-                let message = format!("Shard not found {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    async fn list_vector_sets(
-        &self,
-        request: Request<ShardId>,
-    ) -> Result<Response<VectorSetList>, Status> {
-        self.instrument(&request);
-        let shard_id = request.into_inner();
-        let reader = self.inner.read().await;
-        match reader.list_vectorsets(&shard_id).transpose() {
-            Some(Ok(list)) => {
-                info!("list_vectorset ends correctly");
-                let list = VectorSetList {
-                    shard: Some(shard_id),
-                    vectorset: list,
-                };
-                Ok(tonic::Response::new(list))
-            }
-            Some(Err(e)) => {
-                let error_msg = format!("Error {:?}: {}", shard_id, e);
-                error!("{}", error_msg);
-                Err(tonic::Status::internal(error_msg))
-            }
-            None => {
-                let message = format!("Shard not found {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn move_shard(
-        &self,
-        request: Request<MoveShardRequest>,
-    ) -> Result<Response<EmptyResponse>, Status> {
-        self.instrument(&request);
-
-        let request = request.into_inner();
-        let shard_id = request.shard_id.unwrap();
-
-        let service = self.inner.read().await;
-
-        let Some(shard) = service.get_shard(&shard_id) else {
-            return Err(tonic::Status::not_found(format!(
-                "Shard {} not found",
-                shard_id.id
-            )));
-        };
-
-        match Publisher::default()
-            .append(&shard.path)
-            // `unwrap` call is safe since the shard path already terminate by a valid file name.
-            .unwrap()
-            .retry_on_failure(RetryPolicy::MaxDuration(MAX_MOVE_SHARD_DURATION))
-            .send_to(&request.address)
-            .await
-        {
-            Ok(_) => {
-                info!(
-                    "Shard {} moved to {} successfully",
-                    shard_id.id, request.address
-                );
-
-                Ok(tonic::Response::new(EmptyResponse {}))
-            }
-            Err(e) => {
-                let e = format!(
-                    "Error transfering shard {} to {}: {}",
-                    shard_id.id, request.address, e
-                );
-
-                error!("{}", e);
-
-                Err(tonic::Status::internal(e))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn accept_shard(
-        &self,
-        request: Request<AcceptShardRequest>,
-    ) -> Result<Response<EmptyResponse>, Status> {
-        self.instrument(&request);
-
-        let request = request.into_inner();
-        let shard_id = request.shard_id.unwrap();
-
-        if !request.override_shard && self.inner.read().await.get_shard(&shard_id).is_some() {
-            return Err(tonic::Status::already_exists(format!(
-                "Shard {} already exists",
-                shard_id.id
-            )));
-        }
-
-        match Listener::default()
-            .save_at(env::shards_path())
-            .listen_once(request.port as u16)
-            .await
-        {
-            Ok(_) => {
-                info!("Shard {} received successfully", shard_id.id);
-
-                Ok(tonic::Response::new(EmptyResponse {}))
-            }
-            Err(e) => {
-                let e = format!("Error receiving shard {}: {}", shard_id.id, e);
-
-                error!("{}", e);
-
-                Err(tonic::Status::internal(e))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn get_metadata(
-        &self,
-        request: Request<EmptyQuery>,
-    ) -> Result<Response<NodeMetadata>, Status> {
-        self.instrument(&request);
-
-        match crate::node_metadata::NodeMetadata::load(&env::metadata_path()) {
-            Ok(node_metadata) => Ok(tonic::Response::new(node_metadata.into())),
-            Err(e) => {
-                let e = format!("Cannot get node metadata: {e}");
-
-                error!("{e}");
-
-                Err(tonic::Status::internal(e))
-            }
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    async fn gc(&self, request: Request<ShardId>) -> Result<Response<EmptyResponse>, Status> {
-        self.instrument(&request);
-
-        send_telemetry_event(TelemetryEvent::GarbageCollect).await;
-        let shard_id = request.into_inner();
-        info!("Running garbage collection at {}", shard_id.id);
-        self.load_shard(&shard_id).await;
-        let mut writer = self.inner.write().await;
-        let result = writer.gc(&shard_id);
-        std::mem::drop(writer);
-        match result.transpose() {
-            Some(Ok(_)) => {
-                info!("Garbage collection at {} was successful", shard_id.id);
-                let resp = EmptyResponse {};
-                Ok(tonic::Response::new(resp))
-            }
-            Some(Err(_)) => {
-                info!("Garbage collection at {} raised an error", shard_id.id);
-                let resp = EmptyResponse {};
-                Ok(tonic::Response::new(resp))
-            }
-            None => {
-                info!("{} was not found", shard_id.id);
-                let message = format!("Error loading shard {:?}", shard_id);
-                Err(tonic::Status::not_found(message))
-            }
-        }
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::net::SocketAddr;
-
-    use nucliadb_core::protos::node_writer_client::NodeWriterClient;
-    use nucliadb_core::protos::node_writer_server::NodeWriterServer;
-    use portpicker::pick_unused_port;
-    use tonic::transport::Server;
-    use tonic::Request;
-
-    use super::*;
-    use crate::env;
-    use crate::utils::socket_to_endpoint;
-
-    async fn start_test_server(address: SocketAddr) -> anyhow::Result<()> {
-        let node_writer = NodeWriterGRPCDriver::from(NodeWriterService::new());
-        std::fs::create_dir_all(env::shards_path())?;
-
-        let _ = tokio::spawn(async move {
-            let node_writer_server = NodeWriterServer::new(node_writer);
-            Server::builder()
-                .add_service(node_writer_server)
-                .serve(address)
-                .await?;
-            Result::<_, anyhow::Error>::Ok(())
-        });
-        Ok(())
-    }
-
-    #[tokio::test]
-    async fn test_new_and_get_shard() -> anyhow::Result<()> {
-        let port: u16 = pick_unused_port().expect("No ports free");
-
-        let grpc_addr: SocketAddr = format!("127.0.0.1:{}", port).parse()?;
-        start_test_server(grpc_addr).await?;
-        let mut client = NodeWriterClient::new(socket_to_endpoint(grpc_addr)?.connect_lazy());
-
-        let response = client
-            .new_shard(Request::new(NewShardRequest::default()))
-            .await
-            .expect("Error in new_shard request");
-        let shard_id = &response.get_ref().id;
-
-        let response = client
-            .get_shard(Request::new(ShardId {
-                id: shard_id.clone(),
-            }))
-            .await
-            .expect("Error in get_shard request");
-        let response_id = &response.get_ref().id;
-
-        assert_eq!(shard_id, response_id);
-
-        Ok(())
-    }
-
-    #[tokio::test]
-    async fn test_list_shards() -> anyhow::Result<()> {
-        let port: u16 = pick_unused_port().expect("No ports free");
-        let grpc_addr: SocketAddr = format!("127.0.0.1:{}", port).parse()?;
-        start_test_server(grpc_addr).await?;
-        let mut request_ids: Vec<String> = Vec::new();
-
-        let mut client = NodeWriterClient::new(socket_to_endpoint(grpc_addr)?.connect_lazy());
-
-        for _ in 1..10 {
-            let response = client
-                .new_shard(Request::new(NewShardRequest::default()))
-                .await
-                .expect("Error in new_shard request");
-
-            request_ids.push(response.get_ref().id.clone());
-        }
-        let response = client
-            .list_shards(Request::new(EmptyQuery {}))
-            .await
-            .expect("Error in list_shards request");
-
-        let response_ids: Vec<String> = response
-            .get_ref()
-            .ids
-            .iter()
-            .map(|s| s.id.clone())
-            .collect();
-
-        assert!(request_ids.iter().all(|item| response_ids.contains(item)));
-
-        Ok(())
-    }
-
-    #[tokio::test]
-    async fn test_delete_shards() -> anyhow::Result<()> {
-        let port: u16 = pick_unused_port().expect("No ports free");
-        let grpc_addr: SocketAddr = format!("127.0.0.1:{}", port).parse()?;
-        start_test_server(grpc_addr).await?;
-        let mut request_ids: Vec<String> = Vec::new();
-
-        let mut client = NodeWriterClient::new(socket_to_endpoint(grpc_addr)?.connect_lazy());
-
-        let response = client
-            .list_shards(Request::new(EmptyQuery {}))
-            .await
-            .expect("Error in list_shards request");
-
-        assert_eq!(response.get_ref().ids.len(), 0);
-
-        for _ in 0..10 {
-            let response = client
-                .new_shard(Request::new(NewShardRequest::default()))
-                .await
-                .expect("Error in new_shard request");
-
-            request_ids.push(response.get_ref().id.clone());
-        }
-
-        for id in request_ids.iter().cloned() {
-            _ = client
-                .clean_and_upgrade_shard(Request::new(ShardId { id }))
-                .await
-                .expect("Error in new_shard request");
-        }
-
-        for (id, expected) in request_ids.iter().map(|v| (v.clone(), v.clone())) {
-            let response = client
-                .delete_shard(Request::new(ShardId { id }))
-                .await
-                .expect("Error in delete_shard request");
-            let deleted_id = response.get_ref().id.clone();
-            assert_eq!(deleted_id, expected);
-        }
-
-        let response = client
-            .list_shards(Request::new(EmptyQuery {}))
-            .await
-            .expect("Error in list_shards request");
-
-        assert_eq!(response.get_ref().ids.len(), 0);
-
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::time::Duration;
+
+use async_std::sync::RwLock;
+use nucliadb_core::protos::node_writer_server::NodeWriter;
+use nucliadb_core::protos::{
+    op_status, AcceptShardRequest, DeleteGraphNodes, EmptyQuery, EmptyResponse, MoveShardRequest,
+    NewShardRequest, NewVectorSetRequest, NodeMetadata, OpStatus, Resource, ResourceId, SetGraph,
+    ShardCleaned, ShardCreated, ShardId, ShardIds, VectorSetId, VectorSetList,
+};
+use nucliadb_core::tracing::{self, *};
+use nucliadb_ftp::{Listener, Publisher, RetryPolicy};
+use nucliadb_telemetry::payload::TelemetryEvent;
+use nucliadb_telemetry::sync::send_telemetry_event;
+use opentelemetry::global;
+use tokio::sync::mpsc::UnboundedSender;
+use tonic::{Request, Response, Status};
+use tracing_opentelemetry::OpenTelemetrySpanExt;
+
+use crate::env;
+use crate::utils::MetadataMap;
+use crate::writer::NodeWriterService;
+
+/// Indicates the maximum duration used to move one shard from one node to another on failure only.
+const MAX_MOVE_SHARD_DURATION: Duration = Duration::from_secs(5 * 60);
+
+#[derive(Debug, Clone, PartialEq, Eq)]
+pub enum NodeWriterEvent {
+    ShardCreation(String, String),
+    ShardDeletion(String),
+    ParagraphCount(String, u64),
+}
+
+pub struct NodeWriterGRPCDriver {
+    lazy_loading: bool,
+    inner: RwLock<NodeWriterService>,
+    sender: Option<UnboundedSender<NodeWriterEvent>>,
+}
+
+impl From<NodeWriterService> for NodeWriterGRPCDriver {
+    fn from(node: NodeWriterService) -> NodeWriterGRPCDriver {
+        NodeWriterGRPCDriver {
+            lazy_loading: env::lazy_loading(),
+            inner: RwLock::new(node),
+            sender: None,
+        }
+    }
+}
+
+impl NodeWriterGRPCDriver {
+    pub fn with_sender(self, sender: UnboundedSender<NodeWriterEvent>) -> Self {
+        Self {
+            sender: Some(sender),
+            ..self
+        }
+    }
+    // The GRPC writer will only request the writer to bring a shard
+    // to memory if lazy loading is enabled. Otherwise all the
+    // shards on disk would have been brought to memory before the driver is online.
+    #[tracing::instrument(skip_all)]
+    async fn load_shard(&self, id: &ShardId) {
+        if self.lazy_loading {
+            let mut writer = self.inner.write().await;
+            writer.load_shard(id);
+        }
+    }
+
+    // Instrumentation utilities for telemetry
+    fn instrument<T>(&self, request: &tonic::Request<T>) {
+        let parent_cx =
+            global::get_text_map_propagator(|prop| prop.extract(&MetadataMap(request.metadata())));
+        Span::current().set_parent(parent_cx);
+    }
+
+    #[tracing::instrument(skip_all)]
+    fn emit_event(&self, event: NodeWriterEvent) {
+        if let Some(sender) = &self.sender {
+            let _ = sender.send(event);
+        }
+    }
+}
+
+#[tonic::async_trait]
+impl NodeWriter for NodeWriterGRPCDriver {
+    #[tracing::instrument(skip_all)]
+    async fn get_shard(&self, request: Request<ShardId>) -> Result<Response<ShardId>, Status> {
+        self.instrument(&request);
+
+        debug!("{:?}: gRPC get_shard", request);
+        let shard_id = request.into_inner();
+        self.load_shard(&shard_id).await;
+        let reader = self.inner.read().await;
+        let result = reader.get_shard(&shard_id).is_some();
+        std::mem::drop(reader);
+        match result {
+            true => {
+                debug!("{:?}: Ready readed", shard_id);
+                Ok(tonic::Response::new(shard_id))
+            }
+            false => {
+                let message = format!("Shard not found {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn new_shard(
+        &self,
+        request: Request<NewShardRequest>,
+    ) -> Result<Response<ShardCreated>, Status> {
+        self.instrument(&request);
+
+        debug!("Creating new shard");
+        let request = request.into_inner();
+        send_telemetry_event(TelemetryEvent::Create).await;
+        let mut writer = self.inner.write().await;
+        let result = writer
+            .new_shard(&request)
+            .map_err(|e| tonic::Status::internal(e.to_string()))?;
+        std::mem::drop(writer);
+        self.emit_event(NodeWriterEvent::ShardCreation(
+            result.id.clone(),
+            request.kbid,
+        ));
+        Ok(tonic::Response::new(result))
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn delete_shard(&self, request: Request<ShardId>) -> Result<Response<ShardId>, Status> {
+        self.instrument(&request);
+
+        debug!("gRPC delete_shard {:?}", request);
+        send_telemetry_event(TelemetryEvent::Delete).await;
+        // Deletion does not require for the shard
+        // to be loaded.
+        let shard_id = request.into_inner();
+        let mut writer = self.inner.write().await;
+        let result = writer.delete_shard(&shard_id);
+        std::mem::drop(writer);
+        match result {
+            Ok(_) => {
+                self.emit_event(NodeWriterEvent::ShardDeletion(shard_id.id.clone()));
+
+                Ok(tonic::Response::new(shard_id))
+            }
+            Err(e) => {
+                let error_msg = format!("Error deleting shard {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn clean_and_upgrade_shard(
+        &self,
+        request: Request<ShardId>,
+    ) -> Result<Response<ShardCleaned>, Status> {
+        self.instrument(&request);
+
+        debug!("gRPC delete_shard {:?}", request);
+
+        // Deletion and upgrade do not require for the shard
+        // to be loaded.
+        let shard_id = request.into_inner();
+        let mut writer = self.inner.write().await;
+        let result = writer.clean_and_upgrade_shard(&shard_id);
+        std::mem::drop(writer);
+        match result {
+            Ok(updated) => Ok(tonic::Response::new(updated)),
+            Err(e) => {
+                let error_msg = format!("Error deleting shard {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn list_shards(
+        &self,
+        request: Request<EmptyQuery>,
+    ) -> Result<Response<ShardIds>, Status> {
+        self.instrument(&request);
+        let ids = self.inner.read().await.get_shard_ids();
+        Ok(tonic::Response::new(ids))
+    }
+
+    // Incremental call that can be call multiple times for the same resource
+    #[tracing::instrument(skip_all)]
+    async fn set_resource(&self, request: Request<Resource>) -> Result<Response<OpStatus>, Status> {
+        self.instrument(&request);
+        let resource = request.into_inner();
+        let shard_id = ShardId {
+            id: resource.shard_id.clone(),
+        };
+        self.load_shard(&shard_id).await;
+
+        let inner = self.inner.read().await;
+        let result = inner.set_resource(&shard_id, &resource);
+        match result.transpose() {
+            Some(Ok(mut status)) => {
+                debug!("Set resource ends correctly");
+                status.status = 0;
+                status.detail = "Success!".to_string();
+
+                self.emit_event(NodeWriterEvent::ParagraphCount(
+                    shard_id.id.clone(),
+                    status.count_paragraphs,
+                ));
+
+                Ok(tonic::Response::new(status))
+            }
+            Some(Err(e)) => {
+                let status = op_status::Status::Error as i32;
+                let detail = format!("Error: {}", e);
+                let op_status = OpStatus {
+                    status,
+                    detail,
+                    count: 0_u64,
+                    shard_id: shard_id.id.clone(),
+                    ..Default::default()
+                };
+                Ok(tonic::Response::new(op_status))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn delete_relation_nodes(
+        &self,
+        request: Request<DeleteGraphNodes>,
+    ) -> Result<Response<OpStatus>, Status> {
+        self.instrument(&request);
+        let request = request.into_inner();
+        let shard_id = request.shard_id.as_ref().unwrap();
+        self.load_shard(shard_id).await;
+
+        let inner = self.inner.read().await;
+        match inner.delete_relation_nodes(shard_id, &request).transpose() {
+            Some(Ok(mut status)) => {
+                debug!("Delete relations ends correctly");
+                status.status = 0;
+                status.detail = "Success!".to_string();
+                Ok(tonic::Response::new(status))
+            }
+            Some(Err(e)) => {
+                let error_msg = format!("Error {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+            None => {
+                let message = format!("Shard not found {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn join_graph(&self, request: Request<SetGraph>) -> Result<Response<OpStatus>, Status> {
+        self.instrument(&request);
+        let request = request.into_inner();
+        let shard_id = request.shard_id.unwrap();
+        let graph = request.graph.unwrap();
+        self.load_shard(&shard_id).await;
+
+        let inner = self.inner.read().await;
+        match inner.join_relations_graph(&shard_id, &graph).transpose() {
+            Some(Ok(mut status)) => {
+                debug!("Join graph ends correctly");
+                status.status = 0;
+                status.detail = "Success!".to_string();
+                Ok(tonic::Response::new(status))
+            }
+            Some(Err(e)) => {
+                let error_msg = format!("Error {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+            None => {
+                let message = format!("Shard not found {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn remove_resource(
+        &self,
+        request: Request<ResourceId>,
+    ) -> Result<Response<OpStatus>, Status> {
+        self.instrument(&request);
+        let resource = request.into_inner();
+        let shard_id = ShardId {
+            id: resource.shard_id.clone(),
+        };
+        self.load_shard(&shard_id).await;
+
+        let inner = self.inner.read().await;
+        let result = inner.remove_resource(&shard_id, &resource);
+        match result.transpose() {
+            Some(Ok(mut status)) => {
+                debug!("Remove resource ends correctly");
+                status.status = 0;
+                status.detail = "Success!".to_string();
+
+                self.emit_event(NodeWriterEvent::ParagraphCount(
+                    shard_id.id.clone(),
+                    status.count_paragraphs,
+                ));
+
+                Ok(tonic::Response::new(status))
+            }
+            Some(Err(e)) => {
+                let status = op_status::Status::Error as i32;
+                let detail = format!("Error: {}", e);
+                let op_status = OpStatus {
+                    status,
+                    detail,
+                    count: 0_u64,
+                    shard_id: shard_id.id.clone(),
+                    ..Default::default()
+                };
+                Ok(tonic::Response::new(op_status))
+            }
+            None => {
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    async fn add_vector_set(
+        &self,
+        request: Request<NewVectorSetRequest>,
+    ) -> Result<Response<OpStatus>, Status> {
+        self.instrument(&request);
+        let request = request.into_inner();
+        let shard_id = request.id.as_ref().and_then(|i| i.shard.clone()).unwrap();
+        self.load_shard(&shard_id).await;
+
+        let inner = self.inner.read().await;
+        match inner.add_vectorset(&request).transpose() {
+            Some(Ok(mut status)) => {
+                debug!("add_vector_set ends correctly");
+                status.status = 0;
+                status.detail = "Success!".to_string();
+                Ok(tonic::Response::new(status))
+            }
+            Some(Err(e)) => {
+                let error_msg = format!("Error {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+            None => {
+                let message = format!("Shard not found {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    async fn remove_vector_set(
+        &self,
+        request: Request<VectorSetId>,
+    ) -> Result<Response<OpStatus>, Status> {
+        self.instrument(&request);
+        let request = request.into_inner();
+        let shard_id = request.shard.as_ref().unwrap();
+        self.load_shard(shard_id).await;
+
+        let inner = self.inner.read().await;
+        match inner.remove_vectorset(shard_id, &request).transpose() {
+            Some(Ok(mut status)) => {
+                debug!("remove_vector_set ends correctly");
+                status.status = 0;
+                status.detail = "Success!".to_string();
+                Ok(tonic::Response::new(status))
+            }
+            Some(Err(e)) => {
+                let error_msg = format!("Error {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+            None => {
+                let message = format!("Shard not found {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    async fn list_vector_sets(
+        &self,
+        request: Request<ShardId>,
+    ) -> Result<Response<VectorSetList>, Status> {
+        self.instrument(&request);
+        let shard_id = request.into_inner();
+        let reader = self.inner.read().await;
+        match reader.list_vectorsets(&shard_id).transpose() {
+            Some(Ok(list)) => {
+                debug!("list_vectorset ends correctly");
+                let list = VectorSetList {
+                    shard: Some(shard_id),
+                    vectorset: list,
+                };
+                Ok(tonic::Response::new(list))
+            }
+            Some(Err(e)) => {
+                let error_msg = format!("Error {:?}: {}", shard_id, e);
+                error!("{}", error_msg);
+                Err(tonic::Status::internal(error_msg))
+            }
+            None => {
+                let message = format!("Shard not found {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn move_shard(
+        &self,
+        request: Request<MoveShardRequest>,
+    ) -> Result<Response<EmptyResponse>, Status> {
+        self.instrument(&request);
+
+        let request = request.into_inner();
+        let shard_id = request.shard_id.unwrap();
+
+        let service = self.inner.read().await;
+
+        let Some(shard) = service.get_shard(&shard_id) else {
+            return Err(tonic::Status::not_found(format!(
+                "Shard {} not found",
+                shard_id.id
+            )));
+        };
+
+        match Publisher::default()
+            .append(&shard.path)
+            // `unwrap` call is safe since the shard path already terminate by a valid file name.
+            .unwrap()
+            .retry_on_failure(RetryPolicy::MaxDuration(MAX_MOVE_SHARD_DURATION))
+            .send_to(&request.address)
+            .await
+        {
+            Ok(_) => {
+                debug!(
+                    "Shard {} moved to {} successfully",
+                    shard_id.id, request.address
+                );
+
+                Ok(tonic::Response::new(EmptyResponse {}))
+            }
+            Err(e) => {
+                let e = format!(
+                    "Error transfering shard {} to {}: {}",
+                    shard_id.id, request.address, e
+                );
+
+                error!("{}", e);
+
+                Err(tonic::Status::internal(e))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn accept_shard(
+        &self,
+        request: Request<AcceptShardRequest>,
+    ) -> Result<Response<EmptyResponse>, Status> {
+        self.instrument(&request);
+
+        let request = request.into_inner();
+        let shard_id = request.shard_id.unwrap();
+
+        if !request.override_shard && self.inner.read().await.get_shard(&shard_id).is_some() {
+            return Err(tonic::Status::already_exists(format!(
+                "Shard {} already exists",
+                shard_id.id
+            )));
+        }
+
+        match Listener::default()
+            .save_at(env::shards_path())
+            .listen_once(request.port as u16)
+            .await
+        {
+            Ok(_) => {
+                debug!("Shard {} received successfully", shard_id.id);
+
+                Ok(tonic::Response::new(EmptyResponse {}))
+            }
+            Err(e) => {
+                let e = format!("Error receiving shard {}: {}", shard_id.id, e);
+
+                error!("{}", e);
+
+                Err(tonic::Status::internal(e))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn get_metadata(
+        &self,
+        request: Request<EmptyQuery>,
+    ) -> Result<Response<NodeMetadata>, Status> {
+        self.instrument(&request);
+
+        match crate::node_metadata::NodeMetadata::load(&env::metadata_path()) {
+            Ok(node_metadata) => Ok(tonic::Response::new(node_metadata.into())),
+            Err(e) => {
+                let e = format!("Cannot get node metadata: {e}");
+
+                error!("{e}");
+
+                Err(tonic::Status::internal(e))
+            }
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    async fn gc(&self, request: Request<ShardId>) -> Result<Response<EmptyResponse>, Status> {
+        self.instrument(&request);
+
+        send_telemetry_event(TelemetryEvent::GarbageCollect).await;
+        let shard_id = request.into_inner();
+        debug!("Running garbage collection at {}", shard_id.id);
+        self.load_shard(&shard_id).await;
+
+        let inner = self.inner.read().await;
+        let result = inner.gc(&shard_id);
+        std::mem::drop(inner);
+        match result.transpose() {
+            Some(Ok(_)) => {
+                debug!("Garbage collection at {} was successful", shard_id.id);
+                let resp = EmptyResponse {};
+                Ok(tonic::Response::new(resp))
+            }
+            Some(Err(_)) => {
+                debug!("Garbage collection at {} raised an error", shard_id.id);
+                let resp = EmptyResponse {};
+                Ok(tonic::Response::new(resp))
+            }
+            None => {
+                debug!("{} was not found", shard_id.id);
+                let message = format!("Error loading shard {:?}", shard_id);
+                Err(tonic::Status::not_found(message))
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::net::SocketAddr;
+
+    use nucliadb_core::protos::node_writer_client::NodeWriterClient;
+    use nucliadb_core::protos::node_writer_server::NodeWriterServer;
+    use portpicker::pick_unused_port;
+    use tonic::transport::Server;
+    use tonic::Request;
+
+    use super::*;
+    use crate::env;
+    use crate::utils::socket_to_endpoint;
+
+    async fn start_test_server(address: SocketAddr) -> anyhow::Result<()> {
+        let node_writer = NodeWriterGRPCDriver::from(NodeWriterService::new());
+        std::fs::create_dir_all(env::shards_path())?;
+
+        tokio::spawn(async move {
+            let node_writer_server = NodeWriterServer::new(node_writer);
+            Server::builder()
+                .add_service(node_writer_server)
+                .serve(address)
+                .await
+        });
+        Ok(())
+    }
+
+    #[tokio::test]
+    async fn test_new_and_get_shard() -> anyhow::Result<()> {
+        let port: u16 = pick_unused_port().expect("No ports free");
+
+        let grpc_addr: SocketAddr = format!("127.0.0.1:{}", port).parse()?;
+        start_test_server(grpc_addr).await?;
+        let mut client = NodeWriterClient::new(socket_to_endpoint(grpc_addr)?.connect_lazy());
+
+        let response = client
+            .new_shard(Request::new(NewShardRequest::default()))
+            .await
+            .expect("Error in new_shard request");
+        let shard_id = &response.get_ref().id;
+
+        let response = client
+            .get_shard(Request::new(ShardId {
+                id: shard_id.clone(),
+            }))
+            .await
+            .expect("Error in get_shard request");
+        let response_id = &response.get_ref().id;
+
+        assert_eq!(shard_id, response_id);
+
+        Ok(())
+    }
+
+    #[tokio::test]
+    async fn test_list_shards() -> anyhow::Result<()> {
+        let port: u16 = pick_unused_port().expect("No ports free");
+        let grpc_addr: SocketAddr = format!("127.0.0.1:{}", port).parse()?;
+        start_test_server(grpc_addr).await?;
+        let mut request_ids: Vec<String> = Vec::new();
+
+        let mut client = NodeWriterClient::new(socket_to_endpoint(grpc_addr)?.connect_lazy());
+
+        for _ in 1..10 {
+            let response = client
+                .new_shard(Request::new(NewShardRequest::default()))
+                .await
+                .expect("Error in new_shard request");
+
+            request_ids.push(response.get_ref().id.clone());
+        }
+        let response = client
+            .list_shards(Request::new(EmptyQuery {}))
+            .await
+            .expect("Error in list_shards request");
+
+        let response_ids: Vec<String> = response
+            .get_ref()
+            .ids
+            .iter()
+            .map(|s| s.id.clone())
+            .collect();
+
+        assert!(request_ids.iter().all(|item| response_ids.contains(item)));
+
+        Ok(())
+    }
+
+    #[tokio::test]
+    async fn test_delete_shards() -> anyhow::Result<()> {
+        let port: u16 = pick_unused_port().expect("No ports free");
+        let grpc_addr: SocketAddr = format!("127.0.0.1:{}", port).parse()?;
+        start_test_server(grpc_addr).await?;
+        let mut request_ids: Vec<String> = Vec::new();
+
+        let mut client = NodeWriterClient::new(socket_to_endpoint(grpc_addr)?.connect_lazy());
+
+        let response = client
+            .list_shards(Request::new(EmptyQuery {}))
+            .await
+            .expect("Error in list_shards request");
+
+        assert_eq!(response.get_ref().ids.len(), 0);
+
+        for _ in 0..10 {
+            let response = client
+                .new_shard(Request::new(NewShardRequest::default()))
+                .await
+                .expect("Error in new_shard request");
+
+            request_ids.push(response.get_ref().id.clone());
+        }
+
+        for id in request_ids.iter().cloned() {
+            _ = client
+                .clean_and_upgrade_shard(Request::new(ShardId { id }))
+                .await
+                .expect("Error in new_shard request");
+        }
+
+        for (id, expected) in request_ids.iter().map(|v| (v.clone(), v.clone())) {
+            let response = client
+                .delete_shard(Request::new(ShardId { id }))
+                .await
+                .expect("Error in delete_shard request");
+            let deleted_id = response.get_ref().id.clone();
+            assert_eq!(deleted_id, expected);
+        }
+
+        let response = client
+            .list_shards(Request::new(EmptyQuery {}))
+            .await
+            .expect("Error in list_shards request");
+
+        assert_eq!(response.get_ref().ids.len(), 0);
+
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/src/writer/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/src/writer/mod.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,267 +1,267 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod grpc_driver;
-use std::collections::HashMap;
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::{
-    DeleteGraphNodes, JoinGraph, NewShardRequest, NewVectorSetRequest, OpStatus, Resource,
-    ResourceId, ShardCleaned, ShardCreated, ShardId, ShardIds, VectorSetId,
-};
-use nucliadb_core::thread::ThreadPoolBuilder;
-use nucliadb_core::tracing::{self, *};
-use nucliadb_vectors::data_point_provider::Merger as VectorsMerger;
-use uuid::Uuid;
-
-use crate::env;
-use crate::services::writer::ShardWriterService;
-
-#[derive(Debug)]
-pub struct NodeWriterService {
-    pub cache: HashMap<String, ShardWriterService>,
-}
-
-impl Default for NodeWriterService {
-    fn default() -> Self {
-        Self::new()
-    }
-}
-impl NodeWriterService {
-    pub fn new() -> Self {
-        // We shallow the error if the threadpools were already initialized
-        let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
-        let _ = VectorsMerger::install_global().map(std::thread::spawn);
-        Self {
-            cache: HashMap::new(),
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn shutdown(&mut self) {
-        for (shard_id, shard) in self.cache.iter_mut() {
-            info!("Stopping shard {}", shard_id);
-            ShardWriterService::stop(shard);
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn load_shards(&mut self) -> NodeResult<()> {
-        let shards_path = env::shards_path();
-        info!("Recovering shards from {shards_path:?}...");
-        for entry in std::fs::read_dir(&shards_path)? {
-            let entry = entry?;
-            let file_name = entry.file_name().to_str().unwrap().to_string();
-            let shard_path = entry.path();
-            let Ok(shard) = ShardWriterService::open(file_name.clone(), &shard_path) else {
-                error!("Shard {shard_path:?} could not be loaded from disk");
-                continue;
-            };
-            self.cache.insert(file_name, shard);
-            info!("Shard loaded: {shard_path:?}");
-        }
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn load_shard(&mut self, shard_id: &ShardId) {
-        let shard_name = shard_id.id.clone();
-        let shard_path = env::shards_path_id(&shard_id.id);
-        if self.cache.contains_key(&shard_id.id) {
-            info!("Shard {shard_path:?} is already on memory");
-            return;
-        }
-        if !shard_path.is_dir() {
-            error!("Shard {shard_path:?} is not on disk");
-            return;
-        }
-        let Ok(shard) = ShardWriterService::open(shard_name, &shard_path) else {
-            error!("Shard {shard_path:?} could not be loaded from disk");
-            return;
-        };
-        self.cache.insert(shard_id.id.clone(), shard);
-        info!("{shard_path:?}: Shard loaded");
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardWriterService> {
-        self.cache.get(&shard_id.id)
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_mut_shard(&mut self, shard_id: &ShardId) -> Option<&mut ShardWriterService> {
-        self.cache.get_mut(&shard_id.id)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn new_shard(&mut self, request: &NewShardRequest) -> NodeResult<ShardCreated> {
-        let shard_id = Uuid::new_v4().to_string();
-        let shard_path = env::shards_path_id(&shard_id);
-        let new_shard = ShardWriterService::new(shard_id.clone(), &shard_path, request)?;
-        let data = ShardCreated {
-            id: new_shard.id.clone(),
-            document_service: new_shard.document_version() as i32,
-            paragraph_service: new_shard.paragraph_version() as i32,
-            vector_service: new_shard.vector_version() as i32,
-            relation_service: new_shard.relation_version() as i32,
-        };
-        self.cache.insert(shard_id, new_shard);
-        Ok(data)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn delete_shard(&mut self, shard_id: &ShardId) -> NodeResult<()> {
-        if shard_id.id.is_empty() {
-            warn!("Shard id is empty");
-            return Ok(());
-        }
-
-        self.cache.remove(&shard_id.id);
-
-        let shard_path = env::shards_path_id(&shard_id.id);
-        if shard_path.exists() {
-            info!("Deleting {:?}", shard_path);
-            std::fs::remove_dir_all(shard_path)?;
-        }
-
-        Ok(())
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn clean_and_upgrade_shard(&mut self, shard_id: &ShardId) -> NodeResult<ShardCleaned> {
-        self.cache.remove(&shard_id.id);
-        let shard_id = shard_id.id.clone();
-        let shard_path = env::shards_path_id(&shard_id);
-        let new_shard = ShardWriterService::clean_and_create(shard_id.clone(), &shard_path)?;
-        let shard_data = ShardCleaned {
-            document_service: new_shard.document_version() as i32,
-            paragraph_service: new_shard.paragraph_version() as i32,
-            vector_service: new_shard.vector_version() as i32,
-            relation_service: new_shard.relation_version() as i32,
-        };
-        self.cache.insert(shard_id, new_shard);
-        Ok(shard_data)
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn set_resource(
-        &mut self,
-        shard_id: &ShardId,
-        resource: &Resource,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-
-        shard.set_resource(resource)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn add_vectorset(&mut self, request: &NewVectorSetRequest) -> NodeResult<Option<OpStatus>> {
-        let Some(setid) = &request.id else {
-            return Err(node_error!("missing vectorset id"));
-        };
-        let Some(shard_id) = &setid.shard else {
-            return Err(node_error!("missing shard id"));
-        };
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.add_vectorset(setid, request.similarity())?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn remove_vectorset(
-        &mut self,
-        shard_id: &ShardId,
-        setid: &VectorSetId,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.remove_vectorset(setid)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn join_relations_graph(
-        &mut self,
-        shard_id: &ShardId,
-        graph: &JoinGraph,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.join_relations_graph(graph)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn delete_relation_nodes(
-        &mut self,
-        shard_id: &ShardId,
-        request: &DeleteGraphNodes,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.delete_relation_nodes(request)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn remove_resource(
-        &mut self,
-        shard_id: &ShardId,
-        resource: &ResourceId,
-    ) -> NodeResult<Option<OpStatus>> {
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-        shard.remove_resource(resource)?;
-        Ok(Some(shard.get_opstatus()?))
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn gc(&mut self, shard_id: &ShardId) -> NodeResult<Option<()>> {
-        let Some(shard) = self.get_mut_shard(shard_id) else {
-            return Ok(None);
-        };
-        Ok(Some(shard.gc()?))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn list_vectorsets(&self, shard_id: &ShardId) -> NodeResult<Option<Vec<String>>> {
-        let Some(shard) = self.get_shard(shard_id) else {
-            return Ok(None);
-        };
-        let shard_response = shard.list_vectorsets()?;
-        Ok(Some(shard_response))
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn get_shard_ids(&self) -> ShardIds {
-        let ids = self
-            .cache
-            .keys()
-            .cloned()
-            .map(|id| ShardId { id })
-            .collect();
-        ShardIds { ids }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod grpc_driver;
+use std::collections::HashMap;
+
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::{
+    DeleteGraphNodes, JoinGraph, NewShardRequest, NewVectorSetRequest, OpStatus, Resource,
+    ResourceId, ShardCleaned, ShardCreated, ShardId, ShardIds, VectorSetId,
+};
+use nucliadb_core::thread::ThreadPoolBuilder;
+use nucliadb_core::tracing::{self, *};
+use nucliadb_vectors::data_point_provider::Merger as VectorsMerger;
+use uuid::Uuid;
+
+use crate::env;
+use crate::services::writer::ShardWriterService;
+
+#[derive(Debug)]
+pub struct NodeWriterService {
+    pub cache: HashMap<String, ShardWriterService>,
+}
+
+impl Default for NodeWriterService {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+impl NodeWriterService {
+    pub fn new() -> Self {
+        // We shallow the error if the threadpools were already initialized
+        let _ = ThreadPoolBuilder::new().num_threads(10).build_global();
+        let _ = VectorsMerger::install_global().map(std::thread::spawn);
+        Self {
+            cache: HashMap::new(),
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn shutdown(&self) {
+        for (shard_id, shard) in self.cache.iter() {
+            debug!("Stopping shard {}", shard_id);
+            shard.stop();
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn load_shards(&mut self) -> NodeResult<()> {
+        let shards_path = env::shards_path();
+        debug!("Recovering shards from {shards_path:?}...");
+        for entry in std::fs::read_dir(&shards_path)? {
+            let entry = entry?;
+            let file_name = entry.file_name().to_str().unwrap().to_string();
+            let shard_path = entry.path();
+            let Ok(shard) = ShardWriterService::open(file_name.clone(), &shard_path) else {
+                error!("Shard {shard_path:?} could not be loaded from disk");
+                continue;
+            };
+            self.cache.insert(file_name, shard);
+            debug!("Shard loaded: {shard_path:?}");
+        }
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn load_shard(&mut self, shard_id: &ShardId) {
+        let shard_name = shard_id.id.clone();
+        let shard_path = env::shards_path_id(&shard_id.id);
+        if self.cache.contains_key(&shard_id.id) {
+            debug!("Shard {shard_path:?} is already on memory");
+            return;
+        }
+        if !shard_path.is_dir() {
+            error!("Shard {shard_path:?} is not on disk");
+            return;
+        }
+        let Ok(shard) = ShardWriterService::open(shard_name, &shard_path) else {
+            error!("Shard {shard_path:?} could not be loaded from disk");
+            return;
+        };
+        self.cache.insert(shard_id.id.clone(), shard);
+        debug!("{shard_path:?}: Shard loaded");
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_shard(&self, shard_id: &ShardId) -> Option<&ShardWriterService> {
+        self.cache.get(&shard_id.id)
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_mut_shard(&mut self, shard_id: &ShardId) -> Option<&mut ShardWriterService> {
+        self.cache.get_mut(&shard_id.id)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn new_shard(&mut self, request: &NewShardRequest) -> NodeResult<ShardCreated> {
+        let shard_id = Uuid::new_v4().to_string();
+        let shard_path = env::shards_path_id(&shard_id);
+        let new_shard = ShardWriterService::new(shard_id.clone(), &shard_path, request)?;
+        let data = ShardCreated {
+            id: new_shard.id.clone(),
+            document_service: new_shard.document_version() as i32,
+            paragraph_service: new_shard.paragraph_version() as i32,
+            vector_service: new_shard.vector_version() as i32,
+            relation_service: new_shard.relation_version() as i32,
+        };
+        self.cache.insert(shard_id, new_shard);
+        Ok(data)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn delete_shard(&mut self, shard_id: &ShardId) -> NodeResult<()> {
+        if shard_id.id.is_empty() {
+            warn!("Shard id is empty");
+            return Ok(());
+        }
+
+        self.cache.remove(&shard_id.id);
+
+        let shard_path = env::shards_path_id(&shard_id.id);
+        if shard_path.exists() {
+            debug!("Deleting {:?}", shard_path);
+            std::fs::remove_dir_all(shard_path)?;
+        }
+
+        Ok(())
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn clean_and_upgrade_shard(&mut self, shard_id: &ShardId) -> NodeResult<ShardCleaned> {
+        self.cache.remove(&shard_id.id);
+        let shard_id = shard_id.id.clone();
+        let shard_path = env::shards_path_id(&shard_id);
+        let new_shard = ShardWriterService::clean_and_create(shard_id.clone(), &shard_path)?;
+        let shard_data = ShardCleaned {
+            document_service: new_shard.document_version() as i32,
+            paragraph_service: new_shard.paragraph_version() as i32,
+            vector_service: new_shard.vector_version() as i32,
+            relation_service: new_shard.relation_version() as i32,
+        };
+        self.cache.insert(shard_id, new_shard);
+        Ok(shard_data)
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn set_resource(
+        &self,
+        shard_id: &ShardId,
+        resource: &Resource,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+
+        shard.set_resource(resource)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn add_vectorset(&self, request: &NewVectorSetRequest) -> NodeResult<Option<OpStatus>> {
+        let Some(setid) = &request.id else {
+            return Err(node_error!("missing vectorset id"));
+        };
+        let Some(shard_id) = &setid.shard else {
+            return Err(node_error!("missing shard id"));
+        };
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.add_vectorset(setid, request.similarity())?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn remove_vectorset(
+        &self,
+        shard_id: &ShardId,
+        setid: &VectorSetId,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.remove_vectorset(setid)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn join_relations_graph(
+        &self,
+        shard_id: &ShardId,
+        graph: &JoinGraph,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.join_relations_graph(graph)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn delete_relation_nodes(
+        &self,
+        shard_id: &ShardId,
+        request: &DeleteGraphNodes,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.delete_relation_nodes(request)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn remove_resource(
+        &self,
+        shard_id: &ShardId,
+        resource: &ResourceId,
+    ) -> NodeResult<Option<OpStatus>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        shard.remove_resource(resource)?;
+        Ok(Some(shard.get_opstatus()?))
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn gc(&self, shard_id: &ShardId) -> NodeResult<Option<()>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        Ok(Some(shard.gc()?))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn list_vectorsets(&self, shard_id: &ShardId) -> NodeResult<Option<Vec<String>>> {
+        let Some(shard) = self.get_shard(shard_id) else {
+            return Ok(None);
+        };
+        let shard_response = shard.list_vectorsets()?;
+        Ok(Some(shard_response))
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn get_shard_ids(&self) -> ShardIds {
+        let ids = self
+            .cache
+            .keys()
+            .cloned()
+            .map(|id| ShardId { id })
+            .collect();
+        ShardIds { ids }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/common/constants.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/common/constants.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::net::{IpAddr, Ipv4Addr, SocketAddr};
-use std::time::Duration;
-
-use once_cell::sync::Lazy;
-
-const READER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
-const READER_PORT: u16 = 18031;
-pub static READER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(READER_IP, READER_PORT));
-
-const WRITER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
-const WRITER_PORT: u16 = 18030;
-pub static WRITER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(WRITER_IP, WRITER_PORT));
-
-pub const SERVER_STARTUP_TIMEOUT: Duration = Duration::from_secs(5);
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::net::{IpAddr, Ipv4Addr, SocketAddr};
+use std::time::Duration;
+
+use once_cell::sync::Lazy;
+
+const READER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
+const READER_PORT: u16 = 18031;
+pub static READER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(READER_IP, READER_PORT));
+
+const WRITER_IP: IpAddr = IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1));
+const WRITER_PORT: u16 = 18030;
+pub static WRITER_ADDR: Lazy<SocketAddr> = Lazy::new(|| SocketAddr::new(WRITER_IP, WRITER_PORT));
+
+pub const SERVER_STARTUP_TIMEOUT: Duration = Duration::from_secs(5);
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/common/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/common/mod.rs`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-/// Utilities to test NucliaDB node
-
-#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
-mod constants;
-#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
-mod node_services;
-
-pub use constants::*;
-pub use node_services::*;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+/// Utilities to test NucliaDB node
+
+#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
+mod constants;
+#[allow(dead_code)] // clippy don't detect it's used in our integration tests]
+mod node_services;
+
+pub use constants::*;
+pub use node_services::*;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/common/node_services.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/common/node_services.rs`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::net::SocketAddr;
-use std::sync::Arc;
-use std::time::Duration;
-
-use nucliadb_core::protos::node_reader_client::NodeReaderClient;
-use nucliadb_core::protos::node_reader_server::NodeReaderServer;
-use nucliadb_core::protos::node_writer_client::NodeWriterClient;
-use nucliadb_core::protos::node_writer_server::NodeWriterServer;
-use nucliadb_node::reader::grpc_driver::NodeReaderGRPCDriver;
-use nucliadb_node::reader::NodeReaderService;
-use nucliadb_node::writer::grpc_driver::NodeWriterGRPCDriver;
-use nucliadb_node::writer::NodeWriterService;
-use once_cell::sync::Lazy;
-use tokio::sync::Mutex;
-use tonic::transport::{Channel, Server};
-
-use crate::common::{READER_ADDR, SERVER_STARTUP_TIMEOUT, WRITER_ADDR};
-
-pub type TestNodeReader = NodeReaderClient<Channel>;
-pub type TestNodeWriter = NodeWriterClient<Channel>;
-
-static READER_SERVER_INITIALIZED: Lazy<Arc<Mutex<bool>>> =
-    Lazy::new(|| Arc::new(Mutex::new(false)));
-static WRITER_SERVER_INITIALIZED: Lazy<Arc<Mutex<bool>>> =
-    Lazy::new(|| Arc::new(Mutex::new(false)));
-
-async fn start_reader(addr: SocketAddr) {
-    let mut initialized_lock = READER_SERVER_INITIALIZED.lock().await;
-
-    if !*initialized_lock {
-        tokio::spawn(async move {
-            let reader_server =
-                NodeReaderServer::new(NodeReaderGRPCDriver::from(NodeReaderService::new()));
-            Server::builder()
-                .add_service(reader_server)
-                .serve(addr)
-                .await
-                .map_or_else(
-                    |err| {
-                        panic!("Error starting gRPC server: {err:?}");
-                    },
-                    |_| {
-                        *initialized_lock = true;
-                    },
-                );
-        });
-    }
-}
-
-async fn start_writer(addr: SocketAddr) {
-    let mut initialized_lock = WRITER_SERVER_INITIALIZED.lock().await;
-
-    if !*initialized_lock {
-        tokio::spawn(async move {
-            let writer_server =
-                NodeWriterServer::new(NodeWriterGRPCDriver::from(NodeWriterService::new()));
-            Server::builder()
-                .add_service(writer_server)
-                .serve(addr)
-                .await
-                .map_or_else(
-                    |err| {
-                        panic!("Error starting gRPC server: {err:?}");
-                    },
-                    |_| {
-                        *initialized_lock = true;
-                    },
-                );
-        });
-    }
-}
-
-async fn wait_for_service_ready(addr: SocketAddr, timeout: Duration) -> anyhow::Result<()> {
-    let server_uri = tonic::transport::Uri::builder()
-        .scheme("http")
-        .authority(addr.to_string())
-        .path_and_query("/")
-        .build()
-        .unwrap();
-
-    backoff::future::retry(
-        backoff::ExponentialBackoffBuilder::new()
-            .with_max_elapsed_time(Some(timeout))
-            .build(),
-        || async {
-            match Channel::builder(server_uri.clone()).connect().await {
-                Ok(_channel) => Ok(()),
-                Err(err) => Err(backoff::Error::Transient {
-                    err,
-                    retry_after: None,
-                }),
-            }
-        },
-    )
-    .await?;
-
-    Ok(())
-}
-
-async fn node_reader_server() -> anyhow::Result<()> {
-    start_reader(*READER_ADDR).await;
-    wait_for_service_ready(*READER_ADDR, SERVER_STARTUP_TIMEOUT).await?;
-    Ok(())
-}
-
-async fn node_writer_server() -> anyhow::Result<()> {
-    start_writer(*WRITER_ADDR).await;
-    wait_for_service_ready(*WRITER_ADDR, SERVER_STARTUP_TIMEOUT).await?;
-    Ok(())
-}
-
-async fn node_reader_client() -> TestNodeReader {
-    let endpoint = format!("http://{}", *READER_ADDR);
-    NodeReaderClient::connect(endpoint)
-        .await
-        .expect("Error creating gRPC reader client")
-}
-
-async fn node_writer_client() -> TestNodeWriter {
-    let endpoint = format!("http://{}", *WRITER_ADDR);
-    NodeWriterClient::connect(endpoint)
-        .await
-        .expect("Error creating gRPC reader client")
-}
-
-pub async fn node_reader() -> TestNodeReader {
-    node_reader_server()
-        .await
-        .expect("Error starting node reader");
-    node_reader_client().await
-}
-
-pub async fn node_writer() -> TestNodeWriter {
-    node_writer_server()
-        .await
-        .expect("Error starting node writer");
-    node_writer_client().await
-}
-
-pub async fn node_services() -> (TestNodeReader, TestNodeWriter) {
-    (node_reader().await, node_writer().await)
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::net::SocketAddr;
+use std::sync::Arc;
+use std::time::Duration;
+
+use nucliadb_core::protos::node_reader_client::NodeReaderClient;
+use nucliadb_core::protos::node_reader_server::NodeReaderServer;
+use nucliadb_core::protos::node_writer_client::NodeWriterClient;
+use nucliadb_core::protos::node_writer_server::NodeWriterServer;
+use nucliadb_node::reader::grpc_driver::NodeReaderGRPCDriver;
+use nucliadb_node::reader::NodeReaderService;
+use nucliadb_node::writer::grpc_driver::NodeWriterGRPCDriver;
+use nucliadb_node::writer::NodeWriterService;
+use once_cell::sync::Lazy;
+use tokio::sync::Mutex;
+use tonic::transport::{Channel, Server};
+
+use crate::common::{READER_ADDR, SERVER_STARTUP_TIMEOUT, WRITER_ADDR};
+
+pub type TestNodeReader = NodeReaderClient<Channel>;
+pub type TestNodeWriter = NodeWriterClient<Channel>;
+
+static READER_SERVER_INITIALIZED: Lazy<Arc<Mutex<bool>>> =
+    Lazy::new(|| Arc::new(Mutex::new(false)));
+static WRITER_SERVER_INITIALIZED: Lazy<Arc<Mutex<bool>>> =
+    Lazy::new(|| Arc::new(Mutex::new(false)));
+
+async fn start_reader(addr: SocketAddr) {
+    let mut initialized_lock = READER_SERVER_INITIALIZED.lock().await;
+
+    if !*initialized_lock {
+        tokio::spawn(async move {
+            let reader_server =
+                NodeReaderServer::new(NodeReaderGRPCDriver::from(NodeReaderService::new()));
+            Server::builder()
+                .add_service(reader_server)
+                .serve(addr)
+                .await
+                .map_or_else(
+                    |err| {
+                        panic!("Error starting gRPC server: {err:?}");
+                    },
+                    |_| {
+                        *initialized_lock = true;
+                    },
+                );
+        });
+    }
+}
+
+async fn start_writer(addr: SocketAddr) {
+    let mut initialized_lock = WRITER_SERVER_INITIALIZED.lock().await;
+
+    if !*initialized_lock {
+        tokio::spawn(async move {
+            let writer_server =
+                NodeWriterServer::new(NodeWriterGRPCDriver::from(NodeWriterService::new()));
+            Server::builder()
+                .add_service(writer_server)
+                .serve(addr)
+                .await
+                .map_or_else(
+                    |err| {
+                        panic!("Error starting gRPC server: {err:?}");
+                    },
+                    |_| {
+                        *initialized_lock = true;
+                    },
+                );
+        });
+    }
+}
+
+async fn wait_for_service_ready(addr: SocketAddr, timeout: Duration) -> anyhow::Result<()> {
+    let server_uri = tonic::transport::Uri::builder()
+        .scheme("http")
+        .authority(addr.to_string())
+        .path_and_query("/")
+        .build()
+        .unwrap();
+
+    backoff::future::retry(
+        backoff::ExponentialBackoffBuilder::new()
+            .with_max_elapsed_time(Some(timeout))
+            .build(),
+        || async {
+            match Channel::builder(server_uri.clone()).connect().await {
+                Ok(_channel) => Ok(()),
+                Err(err) => Err(backoff::Error::Transient {
+                    err,
+                    retry_after: None,
+                }),
+            }
+        },
+    )
+    .await?;
+
+    Ok(())
+}
+
+async fn node_reader_server() -> anyhow::Result<()> {
+    start_reader(*READER_ADDR).await;
+    wait_for_service_ready(*READER_ADDR, SERVER_STARTUP_TIMEOUT).await?;
+    Ok(())
+}
+
+async fn node_writer_server() -> anyhow::Result<()> {
+    start_writer(*WRITER_ADDR).await;
+    wait_for_service_ready(*WRITER_ADDR, SERVER_STARTUP_TIMEOUT).await?;
+    Ok(())
+}
+
+async fn node_reader_client() -> TestNodeReader {
+    let endpoint = format!("http://{}", *READER_ADDR);
+    NodeReaderClient::connect(endpoint)
+        .await
+        .expect("Error creating gRPC reader client")
+}
+
+async fn node_writer_client() -> TestNodeWriter {
+    let endpoint = format!("http://{}", *WRITER_ADDR);
+    NodeWriterClient::connect(endpoint)
+        .await
+        .expect("Error creating gRPC reader client")
+}
+
+pub async fn node_reader() -> TestNodeReader {
+    node_reader_server()
+        .await
+        .expect("Error starting node reader");
+    node_reader_client().await
+}
+
+pub async fn node_writer() -> TestNodeWriter {
+    node_writer_server()
+        .await
+        .expect("Error starting node writer");
+    node_writer_client().await
+}
+
+pub async fn node_services() -> (TestNodeReader, TestNodeWriter) {
+    (node_reader().await, node_writer().await)
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/test_search_relations.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/test_search_relations.rs`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,617 +1,617 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod common;
-
-use std::collections::{HashMap, HashSet};
-use std::time::SystemTime;
-
-use common::{node_services, TestNodeWriter};
-use nucliadb_core::protos::op_status::Status;
-use nucliadb_core::protos::prost_types::Timestamp;
-use nucliadb_core::protos::relation::RelationType;
-use nucliadb_core::protos::relation_node::NodeType;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::{
-    EntitiesSubgraphRequest, IndexMetadata, NewShardRequest, Relation, RelationEdgeFilter,
-    RelationNode, RelationNodeFilter, RelationPrefixSearchRequest, RelationSearchRequest,
-    RelationSearchResponse, Resource, ResourceId,
-};
-use tonic::Request;
-use uuid::Uuid;
-
-async fn create_knowledge_graph(
-    writer: &mut TestNodeWriter,
-    shard_id: String,
-) -> HashMap<String, RelationNode> {
-    let rid = Uuid::new_v4();
-
-    let mut relation_nodes = HashMap::new();
-    relation_nodes.insert(
-        rid.to_string(),
-        RelationNode {
-            value: rid.to_string(),
-            ntype: NodeType::Resource as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Animal".to_string(),
-        RelationNode {
-            value: "Animal".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Batman".to_string(),
-        RelationNode {
-            value: "Batman".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Becquer".to_string(),
-        RelationNode {
-            value: "Becquer".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Cat".to_string(),
-        RelationNode {
-            value: "Cat".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: "animal".to_string(),
-        },
-    );
-    relation_nodes.insert(
-        "Catwoman".to_string(),
-        RelationNode {
-            value: "Catwoman".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: "superhero".to_string(),
-        },
-    );
-    relation_nodes.insert(
-        "Eric".to_string(),
-        RelationNode {
-            value: "Eric".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Fly".to_string(),
-        RelationNode {
-            value: "Fly".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Gravity".to_string(),
-        RelationNode {
-            value: "Gravity".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Joan Antoni".to_string(),
-        RelationNode {
-            value: "Joan Antoni".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Joker".to_string(),
-        RelationNode {
-            value: "Joker".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Newton".to_string(),
-        RelationNode {
-            value: "Newton".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Physics".to_string(),
-        RelationNode {
-            value: "Physics".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Poetry".to_string(),
-        RelationNode {
-            value: "Poetry".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-    relation_nodes.insert(
-        "Swallow".to_string(),
-        RelationNode {
-            value: "Swallow".to_string(),
-            ntype: NodeType::Entity as i32,
-            subtype: String::new(),
-        },
-    );
-
-    let relation_edges = vec![
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Batman").unwrap().clone()),
-            to: Some(relation_nodes.get("Catwoman").unwrap().clone()),
-            relation_label: "love".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Batman").unwrap().clone()),
-            to: Some(relation_nodes.get("Joker").unwrap().clone()),
-            relation_label: "fight".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Joker").unwrap().clone()),
-            to: Some(relation_nodes.get("Physics").unwrap().clone()),
-            relation_label: "enjoy".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Catwoman").unwrap().clone()),
-            to: Some(relation_nodes.get("Cat").unwrap().clone()),
-            relation_label: "imitate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Cat").unwrap().clone()),
-            to: Some(relation_nodes.get("Animal").unwrap().clone()),
-            relation_label: "species".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Newton").unwrap().clone()),
-            to: Some(relation_nodes.get("Physics").unwrap().clone()),
-            relation_label: "study".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Newton").unwrap().clone()),
-            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
-            relation_label: "formulate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Eric").unwrap().clone()),
-            to: Some(relation_nodes.get("Cat").unwrap().clone()),
-            relation_label: "like".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Eric").unwrap().clone()),
-            to: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
-            relation_label: "collaborate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
-            to: Some(relation_nodes.get("Eric").unwrap().clone()),
-            relation_label: "collaborate".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
-            to: Some(relation_nodes.get("Becquer").unwrap().clone()),
-            relation_label: "read".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
-            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            relation_label: "write".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
-            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            relation_label: "like".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::About as i32,
-            source: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            to: Some(relation_nodes.get("Swallow").unwrap().clone()),
-            relation_label: "about".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Other as i32,
-            source: Some(relation_nodes.get(&rid.to_string()).unwrap().clone()),
-            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
-            relation_label: "subject".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
-            to: Some(relation_nodes.get("Animal").unwrap().clone()),
-            relation_label: "species".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
-            to: Some(relation_nodes.get("Fly").unwrap().clone()),
-            relation_label: "can".to_string(),
-            ..Default::default()
-        },
-        Relation {
-            relation: RelationType::Entity as i32,
-            source: Some(relation_nodes.get("Fly").unwrap().clone()),
-            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
-            relation_label: "defy".to_string(),
-            ..Default::default()
-        },
-    ];
-
-    let now = SystemTime::now()
-        .duration_since(SystemTime::UNIX_EPOCH)
-        .unwrap();
-    let timestamp = Timestamp {
-        seconds: now.as_secs() as i64,
-        nanos: 0,
-    };
-
-    let r = writer
-        .set_resource(Resource {
-            shard_id: shard_id.clone(),
-            resource: Some(ResourceId {
-                shard_id: shard_id.clone(),
-                uuid: rid.to_string(),
-            }),
-            status: ResourceStatus::Processed as i32,
-            relations: relation_edges.clone(),
-            metadata: Some(IndexMetadata {
-                created: Some(timestamp.clone()),
-                modified: Some(timestamp),
-            }),
-            texts: HashMap::new(),
-            ..Default::default()
-        })
-        .await
-        .unwrap();
-
-    assert_eq!(r.get_ref().status(), Status::Ok);
-
-    relation_nodes
-}
-
-#[tokio::test]
-async fn test_search_relations_prefixed() -> Result<(), Box<dyn std::error::Error>> {
-    let (mut reader, mut writer) = node_services().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    let nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with empty term. Results are limited
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: String::new(),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = &prefix_response.nodes;
-    // TODO: get constants from RelationsReaderService (.../relations/service/reader.rs)
-    assert_eq!(results.len(), nodes.len());
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with "cat" term (some results)
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "cat".to_string(),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Cat".to_string(), "Catwoman".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with "cat" and filters
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "cat".to_string(),
-                node_filters: vec![RelationNodeFilter {
-                    node_subtype: Some("animal".to_string()),
-                    node_type: NodeType::Entity as i32,
-                }],
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Cat".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "cat".to_string(),
-                node_filters: vec![RelationNodeFilter {
-                    node_subtype: Some("superhero".to_string()),
-                    node_type: NodeType::Entity as i32,
-                }],
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Catwoman".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with node filters and empty query
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: String::new(),
-                node_filters: vec![RelationNodeFilter {
-                    node_type: NodeType::Entity as i32,
-                    node_subtype: Some("animal".to_string()),
-                }],
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter(["Cat".to_string()]);
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = prefix_response
-        .nodes
-        .iter()
-        .map(|node| node.value.to_owned())
-        .collect::<HashSet<_>>();
-    assert_eq!(results, expected);
-
-    // --------------------------------------------------------------
-    // Test: prefixed search with "zzz" term (empty results)
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            prefix: Some(RelationPrefixSearchRequest {
-                prefix: "zzz".to_string(),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    assert!(response.get_ref().prefix.is_some());
-    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
-    let results = &prefix_response.nodes;
-    assert!(results.is_empty());
-
-    Ok(())
-}
-
-#[tokio::test]
-async fn test_search_relations_neighbours() -> Result<(), Box<dyn std::error::Error>> {
-    let (mut reader, mut writer) = node_services().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    let relation_nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
-
-    fn extract_relations(response: &RelationSearchResponse) -> HashSet<(String, String)> {
-        response
-            .subgraph
-            .iter()
-            .flat_map(|neighbours| neighbours.relations.iter())
-            .flat_map(|node| {
-                [(
-                    node.source.as_ref().unwrap().value.to_owned(),
-                    node.to.as_ref().unwrap().value.to_owned(),
-                )]
-            })
-            .collect::<HashSet<_>>()
-    }
-
-    // --------------------------------------------------------------
-    // Test: neighbours search on existent node
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![relation_nodes.get("Swallow").unwrap().clone()],
-                depth: Some(1),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter([
-        ("Poetry".to_string(), "Swallow".to_string()),
-        ("Swallow".to_string(), "Animal".to_string()),
-        ("Swallow".to_string(), "Fly".to_string()),
-    ]);
-    let neighbour_relations = extract_relations(response.get_ref());
-    assert_eq!(neighbour_relations, expected);
-
-    // --------------------------------------------------------------
-    // Test: neighbours search on multiple existent nodes
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![
-                    relation_nodes.get("Becquer").unwrap().clone(),
-                    relation_nodes.get("Newton").unwrap().clone(),
-                ],
-                depth: Some(1),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter([
-        ("Newton".to_string(), "Physics".to_string()),
-        ("Newton".to_string(), "Gravity".to_string()),
-        ("Becquer".to_string(), "Poetry".to_string()),
-        ("Joan Antoni".to_string(), "Becquer".to_string()),
-    ]);
-    let neighbour_relations = extract_relations(response.get_ref());
-    assert_eq!(neighbour_relations, expected);
-
-    // --------------------------------------------------------------
-    // Test: neighbours search on non existent node
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![RelationNode {
-                    value: "Fake".to_string(),
-                    ntype: NodeType::Entity as i32,
-                    subtype: String::new(),
-                }],
-                depth: Some(1),
-                ..Default::default()
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let neighbours = extract_relations(response.get_ref());
-    assert!(neighbours.is_empty());
-
-    // --------------------------------------------------------------
-    // Test: neighbours search with filters
-    // --------------------------------------------------------------
-
-    let response = reader
-        .relation_search(RelationSearchRequest {
-            shard_id: shard_id.clone(),
-            subgraph: Some(EntitiesSubgraphRequest {
-                entry_points: vec![relation_nodes.get("Poetry").unwrap().clone()],
-                node_filters: vec![RelationNodeFilter {
-                    node_type: NodeType::Entity as i32,
-                    ..Default::default()
-                }],
-                edge_filters: vec![RelationEdgeFilter {
-                    relation_type: RelationType::About as i32,
-                    ..Default::default()
-                }],
-                depth: Some(1),
-            }),
-            ..Default::default()
-        })
-        .await?;
-
-    let expected = HashSet::from_iter([("Poetry".to_string(), "Swallow".to_string())]);
-    let neighbour_relations = extract_relations(response.get_ref());
-    assert_eq!(neighbour_relations, expected);
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod common;
+
+use std::collections::{HashMap, HashSet};
+use std::time::SystemTime;
+
+use common::{node_services, TestNodeWriter};
+use nucliadb_core::protos::op_status::Status;
+use nucliadb_core::protos::prost_types::Timestamp;
+use nucliadb_core::protos::relation::RelationType;
+use nucliadb_core::protos::relation_node::NodeType;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{
+    EntitiesSubgraphRequest, IndexMetadata, NewShardRequest, Relation, RelationEdgeFilter,
+    RelationNode, RelationNodeFilter, RelationPrefixSearchRequest, RelationSearchRequest,
+    RelationSearchResponse, Resource, ResourceId,
+};
+use tonic::Request;
+use uuid::Uuid;
+
+async fn create_knowledge_graph(
+    writer: &mut TestNodeWriter,
+    shard_id: String,
+) -> HashMap<String, RelationNode> {
+    let rid = Uuid::new_v4();
+
+    let mut relation_nodes = HashMap::new();
+    relation_nodes.insert(
+        rid.to_string(),
+        RelationNode {
+            value: rid.to_string(),
+            ntype: NodeType::Resource as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Animal".to_string(),
+        RelationNode {
+            value: "Animal".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Batman".to_string(),
+        RelationNode {
+            value: "Batman".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Becquer".to_string(),
+        RelationNode {
+            value: "Becquer".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Cat".to_string(),
+        RelationNode {
+            value: "Cat".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: "animal".to_string(),
+        },
+    );
+    relation_nodes.insert(
+        "Catwoman".to_string(),
+        RelationNode {
+            value: "Catwoman".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: "superhero".to_string(),
+        },
+    );
+    relation_nodes.insert(
+        "Eric".to_string(),
+        RelationNode {
+            value: "Eric".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Fly".to_string(),
+        RelationNode {
+            value: "Fly".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Gravity".to_string(),
+        RelationNode {
+            value: "Gravity".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Joan Antoni".to_string(),
+        RelationNode {
+            value: "Joan Antoni".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Joker".to_string(),
+        RelationNode {
+            value: "Joker".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Newton".to_string(),
+        RelationNode {
+            value: "Newton".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Physics".to_string(),
+        RelationNode {
+            value: "Physics".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Poetry".to_string(),
+        RelationNode {
+            value: "Poetry".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+    relation_nodes.insert(
+        "Swallow".to_string(),
+        RelationNode {
+            value: "Swallow".to_string(),
+            ntype: NodeType::Entity as i32,
+            subtype: String::new(),
+        },
+    );
+
+    let relation_edges = vec![
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Batman").unwrap().clone()),
+            to: Some(relation_nodes.get("Catwoman").unwrap().clone()),
+            relation_label: "love".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Batman").unwrap().clone()),
+            to: Some(relation_nodes.get("Joker").unwrap().clone()),
+            relation_label: "fight".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Joker").unwrap().clone()),
+            to: Some(relation_nodes.get("Physics").unwrap().clone()),
+            relation_label: "enjoy".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Catwoman").unwrap().clone()),
+            to: Some(relation_nodes.get("Cat").unwrap().clone()),
+            relation_label: "imitate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Cat").unwrap().clone()),
+            to: Some(relation_nodes.get("Animal").unwrap().clone()),
+            relation_label: "species".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Newton").unwrap().clone()),
+            to: Some(relation_nodes.get("Physics").unwrap().clone()),
+            relation_label: "study".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Newton").unwrap().clone()),
+            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
+            relation_label: "formulate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Eric").unwrap().clone()),
+            to: Some(relation_nodes.get("Cat").unwrap().clone()),
+            relation_label: "like".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Eric").unwrap().clone()),
+            to: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
+            relation_label: "collaborate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
+            to: Some(relation_nodes.get("Eric").unwrap().clone()),
+            relation_label: "collaborate".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Joan Antoni").unwrap().clone()),
+            to: Some(relation_nodes.get("Becquer").unwrap().clone()),
+            relation_label: "read".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
+            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            relation_label: "write".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Becquer").unwrap().clone()),
+            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            relation_label: "like".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::About as i32,
+            source: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            to: Some(relation_nodes.get("Swallow").unwrap().clone()),
+            relation_label: "about".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Other as i32,
+            source: Some(relation_nodes.get(&rid.to_string()).unwrap().clone()),
+            to: Some(relation_nodes.get("Poetry").unwrap().clone()),
+            relation_label: "subject".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
+            to: Some(relation_nodes.get("Animal").unwrap().clone()),
+            relation_label: "species".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Swallow").unwrap().clone()),
+            to: Some(relation_nodes.get("Fly").unwrap().clone()),
+            relation_label: "can".to_string(),
+            ..Default::default()
+        },
+        Relation {
+            relation: RelationType::Entity as i32,
+            source: Some(relation_nodes.get("Fly").unwrap().clone()),
+            to: Some(relation_nodes.get("Gravity").unwrap().clone()),
+            relation_label: "defy".to_string(),
+            ..Default::default()
+        },
+    ];
+
+    let now = SystemTime::now()
+        .duration_since(SystemTime::UNIX_EPOCH)
+        .unwrap();
+    let timestamp = Timestamp {
+        seconds: now.as_secs() as i64,
+        nanos: 0,
+    };
+
+    let r = writer
+        .set_resource(Resource {
+            shard_id: shard_id.clone(),
+            resource: Some(ResourceId {
+                shard_id: shard_id.clone(),
+                uuid: rid.to_string(),
+            }),
+            status: ResourceStatus::Processed as i32,
+            relations: relation_edges.clone(),
+            metadata: Some(IndexMetadata {
+                created: Some(timestamp.clone()),
+                modified: Some(timestamp),
+            }),
+            texts: HashMap::new(),
+            ..Default::default()
+        })
+        .await
+        .unwrap();
+
+    assert_eq!(r.get_ref().status(), Status::Ok);
+
+    relation_nodes
+}
+
+#[tokio::test]
+async fn test_search_relations_prefixed() -> Result<(), Box<dyn std::error::Error>> {
+    let (mut reader, mut writer) = node_services().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    let nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with empty term. Results are limited
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: String::new(),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = &prefix_response.nodes;
+    // TODO: get constants from RelationsReaderService (.../relations/service/reader.rs)
+    assert_eq!(results.len(), nodes.len());
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with "cat" term (some results)
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "cat".to_string(),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Cat".to_string(), "Catwoman".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with "cat" and filters
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "cat".to_string(),
+                node_filters: vec![RelationNodeFilter {
+                    node_subtype: Some("animal".to_string()),
+                    node_type: NodeType::Entity as i32,
+                }],
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Cat".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "cat".to_string(),
+                node_filters: vec![RelationNodeFilter {
+                    node_subtype: Some("superhero".to_string()),
+                    node_type: NodeType::Entity as i32,
+                }],
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Catwoman".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with node filters and empty query
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: String::new(),
+                node_filters: vec![RelationNodeFilter {
+                    node_type: NodeType::Entity as i32,
+                    node_subtype: Some("animal".to_string()),
+                }],
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter(["Cat".to_string()]);
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = prefix_response
+        .nodes
+        .iter()
+        .map(|node| node.value.to_owned())
+        .collect::<HashSet<_>>();
+    assert_eq!(results, expected);
+
+    // --------------------------------------------------------------
+    // Test: prefixed search with "zzz" term (empty results)
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            prefix: Some(RelationPrefixSearchRequest {
+                prefix: "zzz".to_string(),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    assert!(response.get_ref().prefix.is_some());
+    let prefix_response = response.get_ref().prefix.as_ref().unwrap();
+    let results = &prefix_response.nodes;
+    assert!(results.is_empty());
+
+    Ok(())
+}
+
+#[tokio::test]
+async fn test_search_relations_neighbours() -> Result<(), Box<dyn std::error::Error>> {
+    let (mut reader, mut writer) = node_services().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    let relation_nodes = create_knowledge_graph(&mut writer, shard_id.clone()).await;
+
+    fn extract_relations(response: &RelationSearchResponse) -> HashSet<(String, String)> {
+        response
+            .subgraph
+            .iter()
+            .flat_map(|neighbours| neighbours.relations.iter())
+            .flat_map(|node| {
+                [(
+                    node.source.as_ref().unwrap().value.to_owned(),
+                    node.to.as_ref().unwrap().value.to_owned(),
+                )]
+            })
+            .collect::<HashSet<_>>()
+    }
+
+    // --------------------------------------------------------------
+    // Test: neighbours search on existent node
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![relation_nodes.get("Swallow").unwrap().clone()],
+                depth: Some(1),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter([
+        ("Poetry".to_string(), "Swallow".to_string()),
+        ("Swallow".to_string(), "Animal".to_string()),
+        ("Swallow".to_string(), "Fly".to_string()),
+    ]);
+    let neighbour_relations = extract_relations(response.get_ref());
+    assert_eq!(neighbour_relations, expected);
+
+    // --------------------------------------------------------------
+    // Test: neighbours search on multiple existent nodes
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![
+                    relation_nodes.get("Becquer").unwrap().clone(),
+                    relation_nodes.get("Newton").unwrap().clone(),
+                ],
+                depth: Some(1),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter([
+        ("Newton".to_string(), "Physics".to_string()),
+        ("Newton".to_string(), "Gravity".to_string()),
+        ("Becquer".to_string(), "Poetry".to_string()),
+        ("Joan Antoni".to_string(), "Becquer".to_string()),
+    ]);
+    let neighbour_relations = extract_relations(response.get_ref());
+    assert_eq!(neighbour_relations, expected);
+
+    // --------------------------------------------------------------
+    // Test: neighbours search on non existent node
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![RelationNode {
+                    value: "Fake".to_string(),
+                    ntype: NodeType::Entity as i32,
+                    subtype: String::new(),
+                }],
+                depth: Some(1),
+                ..Default::default()
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let neighbours = extract_relations(response.get_ref());
+    assert!(neighbours.is_empty());
+
+    // --------------------------------------------------------------
+    // Test: neighbours search with filters
+    // --------------------------------------------------------------
+
+    let response = reader
+        .relation_search(RelationSearchRequest {
+            shard_id: shard_id.clone(),
+            subgraph: Some(EntitiesSubgraphRequest {
+                entry_points: vec![relation_nodes.get("Poetry").unwrap().clone()],
+                node_filters: vec![RelationNodeFilter {
+                    node_type: NodeType::Entity as i32,
+                    ..Default::default()
+                }],
+                edge_filters: vec![RelationEdgeFilter {
+                    relation_type: RelationType::About as i32,
+                    ..Default::default()
+                }],
+                depth: Some(1),
+            }),
+            ..Default::default()
+        })
+        .await?;
+
+    let expected = HashSet::from_iter([("Poetry".to_string(), "Swallow".to_string())]);
+    let neighbour_relations = extract_relations(response.get_ref());
+    assert_eq!(neighbour_relations, expected);
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_node/tests/test_search_sorting.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/tests/test_search_sorting.rs`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,187 +1,187 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod common;
-
-use std::collections::HashMap;
-use std::time::SystemTime;
-
-use common::{node_services, TestNodeReader, TestNodeWriter};
-use nucliadb_core::protos::op_status::Status;
-use nucliadb_core::protos::prost_types::Timestamp;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::{IndexMetadata, NewShardRequest, Resource, ResourceId, SearchRequest};
-use tonic::Request;
-use uuid::Uuid;
-
-async fn create_dummy_resources(total: u8, writer: &mut TestNodeWriter, shard_id: String) {
-    let resource_creation_delay = std::time::Duration::from_secs(1);
-    for i in 0..total {
-        let rid = Uuid::new_v4();
-        let field = format!("dummy-{i:0>3}");
-
-        let now = SystemTime::now()
-            .duration_since(SystemTime::UNIX_EPOCH)
-            .unwrap();
-        let timestamp = Timestamp {
-            seconds: now.as_secs() as i64,
-            nanos: 0,
-        };
-
-        let labels = vec![format!("/dummy{i:0>3}")];
-        let mut texts = HashMap::new();
-        texts.insert(
-            field,
-            nucliadb_protos::TextInformation {
-                text: format!("Dummy text {i:0>3}"),
-                labels: vec![],
-            },
-        );
-        let result = writer
-            .set_resource(Resource {
-                shard_id: shard_id.clone(),
-                resource: Some(ResourceId {
-                    shard_id: shard_id.clone(),
-                    uuid: rid.to_string(),
-                }),
-                status: ResourceStatus::Processed as i32,
-                metadata: Some(IndexMetadata {
-                    created: Some(timestamp.clone()),
-                    modified: Some(timestamp),
-                }),
-                labels,
-                texts,
-                ..Default::default()
-            })
-            .await
-            .unwrap();
-
-        assert_eq!(result.get_ref().status(), Status::Ok);
-        std::thread::sleep(resource_creation_delay);
-    }
-}
-
-#[tokio::test]
-async fn test_search_sorting() -> Result<(), Box<dyn std::error::Error>> {
-    let (mut reader, mut writer) = node_services().await;
-
-    let new_shard_response = writer
-        .new_shard(Request::new(NewShardRequest::default()))
-        .await?;
-    let shard_id = &new_shard_response.get_ref().id;
-
-    create_dummy_resources(20, &mut writer, shard_id.clone()).await;
-
-    async fn paginated_search(
-        reader: &mut TestNodeReader,
-        shard_id: String,
-        order: Option<nucliadb_protos::OrderBy>,
-        page_size: i32,
-    ) -> Vec<String> {
-        let mut fields = Vec::new();
-        let mut page = 0;
-        let mut next_page = true;
-
-        while next_page {
-            let response = reader
-                .search(SearchRequest {
-                    shard: shard_id.clone(),
-                    order: order.clone(),
-                    document: true,
-                    page_number: page,
-                    result_per_page: page_size,
-                    ..Default::default()
-                })
-                .await
-                .unwrap();
-
-            let document_response = response.get_ref().document.as_ref().unwrap();
-            fields.extend(document_response.results.iter().cloned().map(|r| r.field));
-
-            next_page = document_response.next_page;
-            page += 1;
-        }
-
-        fields
-    }
-
-    // --------------------------------------------------------------
-    // Test: sort by creation date in ascending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    assert_eq!(fields, sorted_fields);
-
-    // --------------------------------------------------------------
-    // Test: sort by modified date in ascending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    assert_eq!(fields, sorted_fields);
-
-    // --------------------------------------------------------------
-    // Test: sort by creation date in descending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    sorted_fields.reverse();
-    assert_eq!(fields, sorted_fields);
-
-    // --------------------------------------------------------------
-    // Test: sort by modified date in descending order
-    // --------------------------------------------------------------
-
-    let order = Some(nucliadb_protos::OrderBy {
-        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
-        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
-        ..Default::default()
-    });
-    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
-
-    let mut sorted_fields = fields.clone();
-    sorted_fields.sort();
-    sorted_fields.reverse();
-    assert_eq!(fields, sorted_fields);
-
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod common;
+
+use std::collections::HashMap;
+use std::time::SystemTime;
+
+use common::{node_services, TestNodeReader, TestNodeWriter};
+use nucliadb_core::protos::op_status::Status;
+use nucliadb_core::protos::prost_types::Timestamp;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::{IndexMetadata, NewShardRequest, Resource, ResourceId, SearchRequest};
+use tonic::Request;
+use uuid::Uuid;
+
+async fn create_dummy_resources(total: u8, writer: &mut TestNodeWriter, shard_id: String) {
+    let resource_creation_delay = std::time::Duration::from_secs(1);
+    for i in 0..total {
+        let rid = Uuid::new_v4();
+        let field = format!("dummy-{i:0>3}");
+
+        let now = SystemTime::now()
+            .duration_since(SystemTime::UNIX_EPOCH)
+            .unwrap();
+        let timestamp = Timestamp {
+            seconds: now.as_secs() as i64,
+            nanos: 0,
+        };
+
+        let labels = vec![format!("/dummy{i:0>3}")];
+        let mut texts = HashMap::new();
+        texts.insert(
+            field,
+            nucliadb_protos::TextInformation {
+                text: format!("Dummy text {i:0>3}"),
+                labels: vec![],
+            },
+        );
+        let result = writer
+            .set_resource(Resource {
+                shard_id: shard_id.clone(),
+                resource: Some(ResourceId {
+                    shard_id: shard_id.clone(),
+                    uuid: rid.to_string(),
+                }),
+                status: ResourceStatus::Processed as i32,
+                metadata: Some(IndexMetadata {
+                    created: Some(timestamp.clone()),
+                    modified: Some(timestamp),
+                }),
+                labels,
+                texts,
+                ..Default::default()
+            })
+            .await
+            .unwrap();
+
+        assert_eq!(result.get_ref().status(), Status::Ok);
+        std::thread::sleep(resource_creation_delay);
+    }
+}
+
+#[tokio::test]
+async fn test_search_sorting() -> Result<(), Box<dyn std::error::Error>> {
+    let (mut reader, mut writer) = node_services().await;
+
+    let new_shard_response = writer
+        .new_shard(Request::new(NewShardRequest::default()))
+        .await?;
+    let shard_id = &new_shard_response.get_ref().id;
+
+    create_dummy_resources(20, &mut writer, shard_id.clone()).await;
+
+    async fn paginated_search(
+        reader: &mut TestNodeReader,
+        shard_id: String,
+        order: Option<nucliadb_protos::OrderBy>,
+        page_size: i32,
+    ) -> Vec<String> {
+        let mut fields = Vec::new();
+        let mut page = 0;
+        let mut next_page = true;
+
+        while next_page {
+            let response = reader
+                .search(SearchRequest {
+                    shard: shard_id.clone(),
+                    order: order.clone(),
+                    document: true,
+                    page_number: page,
+                    result_per_page: page_size,
+                    ..Default::default()
+                })
+                .await
+                .unwrap();
+
+            let document_response = response.get_ref().document.as_ref().unwrap();
+            fields.extend(document_response.results.iter().cloned().map(|r| r.field));
+
+            next_page = document_response.next_page;
+            page += 1;
+        }
+
+        fields
+    }
+
+    // --------------------------------------------------------------
+    // Test: sort by creation date in ascending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    assert_eq!(fields, sorted_fields);
+
+    // --------------------------------------------------------------
+    // Test: sort by modified date in ascending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Asc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    assert_eq!(fields, sorted_fields);
+
+    // --------------------------------------------------------------
+    // Test: sort by creation date in descending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Created.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    sorted_fields.reverse();
+    assert_eq!(fields, sorted_fields);
+
+    // --------------------------------------------------------------
+    // Test: sort by modified date in descending order
+    // --------------------------------------------------------------
+
+    let order = Some(nucliadb_protos::OrderBy {
+        sort_by: nucliadb_protos::order_by::OrderField::Modified.into(),
+        r#type: nucliadb_protos::order_by::OrderType::Desc.into(),
+        ..Default::default()
+    });
+    let fields = paginated_search(&mut reader, shard_id.clone(), order, 5).await;
+
+    let mut sorted_fields = fields.clone();
+    sorted_fields.sort();
+    sorted_fields.reverse();
+    assert_eq!(fields, sorted_fields);
+
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/Cargo.toml` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/Cargo.toml`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/bfs_engine.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/bfs_engine.rs`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,263 +1,263 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::{HashMap, HashSet, LinkedList};
-
-use super::errors::*;
-use crate::graph_db::*;
-
-// BfsGuide allows the user to modify how the search will be performed.
-// By default a BfsGuide does not interfere in the search.
-pub trait BfsGuide {
-    fn free_jump(&self, _cnx: GCnx) -> bool {
-        false
-    }
-    fn node_allowed(&self, _node: Entity) -> bool {
-        true
-    }
-    fn edge_allowed(&self, _edge: Entity) -> bool {
-        true
-    }
-}
-
-// Refers to a node that has been reached by the BFS
-// but has not been expanded yet.
-#[derive(Clone, Copy, Debug)]
-struct BfsNode {
-    // GId of the node.
-    point: Entity,
-    // Depth at which the node was found.
-    depth: usize,
-}
-
-#[derive(derive_builder::Builder)]
-#[builder(name = "BfsEngineBuilder", pattern = "owned")]
-pub struct BfsEngine<'a, Guide>
-where Guide: BfsGuide
-{
-    #[builder(setter(skip))]
-    #[builder(default = "LinkedList::new()")]
-    work_stack: LinkedList<BfsNode>,
-
-    #[builder(setter(skip))]
-    #[builder(default = "HashSet::new()")]
-    visited: HashSet<Entity>,
-
-    #[builder(setter(skip))]
-    #[builder(default = "HashSet::new()")]
-    subgraph: HashSet<GCnx>,
-
-    entry_points: Vec<Entity>,
-    max_depth: usize,
-    guide: Guide,
-    txn: &'a RoToken<'a>,
-    graph: &'a GraphDB,
-}
-
-impl<'a, Guide> BfsEngineBuilder<'a, Guide>
-where Guide: BfsGuide
-{
-    pub fn new() -> BfsEngineBuilder<'a, Guide> {
-        BfsEngineBuilder::create_empty()
-    }
-}
-impl<'a, Guide> BfsEngine<'a, Guide>
-where Guide: BfsGuide
-{
-    pub fn search(mut self) -> RResult<impl Iterator<Item = GCnx>> {
-        self.entry_points
-            .iter()
-            .copied()
-            .map(|point| (BfsNode { point, depth: 0 }, self.visited.insert(point)))
-            .filter(|(_, v)| *v)
-            .for_each(|(e, _)| self.work_stack.push_back(e));
-        while let Some(node) = self.work_stack.pop_front() {
-            self.expand(node)?;
-        }
-        Ok(self.subgraph.into_iter())
-    }
-    fn expand(&mut self, node: BfsNode) -> RResult<()> {
-        // same_level nodes are reached by a free_edge
-        // which means that they belong to the level being explored now.
-        let mut same_level = HashMap::new();
-        // next_level nodes are reached by a edge that increases the level.
-        let mut next_level = HashMap::new();
-        self.graph
-            .get_outedges(self.txn, node.point)?
-            .chain(self.graph.get_inedges(self.txn, node.point)?)
-            .flat_map(|a| a.ok().into_iter())
-            .filter(|edge| node.depth < self.max_depth || self.guide.free_jump(*edge))
-            .filter(|edge| self.guide.edge_allowed(edge.edge()))
-            .filter(|edge| self.guide.node_allowed(edge.to()))
-            .for_each(|edge| {
-                let is_free_jump = self.guide.free_jump(edge);
-                let can_use_free_jump = same_level.contains_key(&node.point);
-                if !is_free_jump && !can_use_free_jump {
-                    let node = BfsNode {
-                        point: edge.to(),
-                        // Exploring a further node without free jump increases
-                        // by one the depth of the BFS, i.e., the distance to
-                        // the entry point
-                        depth: node.depth + 1,
-                    };
-                    next_level.insert(node.point, node);
-                } else if is_free_jump {
-                    let node = BfsNode {
-                        point: edge.to(),
-                        depth: node.depth,
-                    };
-                    next_level.remove(&node.point);
-                    same_level.insert(node.point, node);
-                }
-                self.subgraph.insert(edge);
-            });
-        same_level.into_values().for_each(|node| {
-            if !self.visited.contains(&node.point) {
-                self.visited.insert(node.point);
-                // In order to maintain all the advantages of BFS
-                // even when free edges are present we need to maintain the following invariant:
-                // For every i,j if i < j then the nodes from level i are visited before the nodes
-                // from level j.
-                // The invariant only holds if the nodes reached by a
-                // free edge are pushed to the front of the stack. We are avoiding
-                // the aditional complexity of Dijkstra's algorithm.
-                self.work_stack.push_front(node);
-            }
-        });
-        next_level.into_values().for_each(|node| {
-            if !self.visited.contains(&node.point) {
-                self.visited.insert(node.point);
-                self.work_stack.push_back(node);
-            }
-        });
-        Ok(())
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use std::path::Path;
-
-    use super::*;
-    use crate::graph_test_utils::*;
-    fn graph(dir: &Path) -> (Vec<Entity>, GraphDB) {
-        let graphdb = GraphDB::new(dir, SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let ids = UNodes
-            .take(4)
-            .map(|node| graphdb.add_node(&mut txn, &node).unwrap())
-            .collect::<Vec<_>>();
-        UEdges
-            .take(ids.len() - 1)
-            .enumerate()
-            .for_each(|(i, edge)| {
-                graphdb
-                    .connect(&mut txn, ids[i], &edge, ids[i + 1], None)
-                    .unwrap();
-            });
-        let backedge = UEdges.next().unwrap();
-        graphdb
-            .connect(&mut txn, ids[3], &backedge, ids[0], None)
-            .unwrap();
-        txn.commit().unwrap();
-        (ids, graphdb)
-    }
-
-    #[test]
-    fn full_search() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[0]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(AllGuide)
-            .max_depth(usize::MAX)
-            .build()
-            .unwrap();
-        let expected = &nodes;
-        let result = bfs.search().unwrap();
-        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-
-    #[test]
-    fn full_reverse_search() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[3]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(AllGuide)
-            .max_depth(usize::MAX)
-            .build()
-            .unwrap();
-        let expected = &nodes;
-        let result = bfs.search().unwrap();
-        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-
-    #[test]
-    fn limit_depth_search() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[0]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(AllGuide)
-            .max_depth(1)
-            .build()
-            .unwrap();
-        let expected = vec![nodes[0], nodes[1], nodes[3]];
-        let result = bfs.search().unwrap();
-        let mut result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        result.push(nodes[0]);
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-
-    #[test]
-    fn always_jump() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (nodes, graphdb) = graph(dir.path());
-        let txn = graphdb.ro_txn().unwrap();
-        let bfs = BfsEngineBuilder::new()
-            .entry_points(vec![nodes[0]])
-            .graph(&graphdb)
-            .txn(&txn)
-            .guide(FreeJumps)
-            .max_depth(0)
-            .build()
-            .unwrap();
-        let expected = &nodes;
-        let result = bfs.search().unwrap();
-        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
-        assert_eq!(result.len(), expected.len());
-        assert!(result.iter().copied().all(|n| expected.contains(&n)));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::{HashMap, HashSet, LinkedList};
+
+use super::errors::*;
+use crate::graph_db::*;
+
+// BfsGuide allows the user to modify how the search will be performed.
+// By default a BfsGuide does not interfere in the search.
+pub trait BfsGuide {
+    fn free_jump(&self, _cnx: GCnx) -> bool {
+        false
+    }
+    fn node_allowed(&self, _node: Entity) -> bool {
+        true
+    }
+    fn edge_allowed(&self, _edge: Entity) -> bool {
+        true
+    }
+}
+
+// Refers to a node that has been reached by the BFS
+// but has not been expanded yet.
+#[derive(Clone, Copy, Debug)]
+struct BfsNode {
+    // GId of the node.
+    point: Entity,
+    // Depth at which the node was found.
+    depth: usize,
+}
+
+#[derive(derive_builder::Builder)]
+#[builder(name = "BfsEngineBuilder", pattern = "owned")]
+pub struct BfsEngine<'a, Guide>
+where Guide: BfsGuide
+{
+    #[builder(setter(skip))]
+    #[builder(default = "LinkedList::new()")]
+    work_stack: LinkedList<BfsNode>,
+
+    #[builder(setter(skip))]
+    #[builder(default = "HashSet::new()")]
+    visited: HashSet<Entity>,
+
+    #[builder(setter(skip))]
+    #[builder(default = "HashSet::new()")]
+    subgraph: HashSet<GCnx>,
+
+    entry_points: Vec<Entity>,
+    max_depth: usize,
+    guide: Guide,
+    txn: &'a RoToken<'a>,
+    graph: &'a GraphDB,
+}
+
+impl<'a, Guide> BfsEngineBuilder<'a, Guide>
+where Guide: BfsGuide
+{
+    pub fn new() -> BfsEngineBuilder<'a, Guide> {
+        BfsEngineBuilder::create_empty()
+    }
+}
+impl<'a, Guide> BfsEngine<'a, Guide>
+where Guide: BfsGuide
+{
+    pub fn search(mut self) -> RResult<impl Iterator<Item = GCnx>> {
+        self.entry_points
+            .iter()
+            .copied()
+            .map(|point| (BfsNode { point, depth: 0 }, self.visited.insert(point)))
+            .filter(|(_, v)| *v)
+            .for_each(|(e, _)| self.work_stack.push_back(e));
+        while let Some(node) = self.work_stack.pop_front() {
+            self.expand(node)?;
+        }
+        Ok(self.subgraph.into_iter())
+    }
+    fn expand(&mut self, node: BfsNode) -> RResult<()> {
+        // same_level nodes are reached by a free_edge
+        // which means that they belong to the level being explored now.
+        let mut same_level = HashMap::new();
+        // next_level nodes are reached by a edge that increases the level.
+        let mut next_level = HashMap::new();
+        self.graph
+            .get_outedges(self.txn, node.point)?
+            .chain(self.graph.get_inedges(self.txn, node.point)?)
+            .flat_map(|a| a.ok().into_iter())
+            .filter(|edge| node.depth < self.max_depth || self.guide.free_jump(*edge))
+            .filter(|edge| self.guide.edge_allowed(edge.edge()))
+            .filter(|edge| self.guide.node_allowed(edge.to()))
+            .for_each(|edge| {
+                let is_free_jump = self.guide.free_jump(edge);
+                let can_use_free_jump = same_level.contains_key(&node.point);
+                if !is_free_jump && !can_use_free_jump {
+                    let node = BfsNode {
+                        point: edge.to(),
+                        // Exploring a further node without free jump increases
+                        // by one the depth of the BFS, i.e., the distance to
+                        // the entry point
+                        depth: node.depth + 1,
+                    };
+                    next_level.insert(node.point, node);
+                } else if is_free_jump {
+                    let node = BfsNode {
+                        point: edge.to(),
+                        depth: node.depth,
+                    };
+                    next_level.remove(&node.point);
+                    same_level.insert(node.point, node);
+                }
+                self.subgraph.insert(edge);
+            });
+        same_level.into_values().for_each(|node| {
+            if !self.visited.contains(&node.point) {
+                self.visited.insert(node.point);
+                // In order to maintain all the advantages of BFS
+                // even when free edges are present we need to maintain the following invariant:
+                // For every i,j if i < j then the nodes from level i are visited before the nodes
+                // from level j.
+                // The invariant only holds if the nodes reached by a
+                // free edge are pushed to the front of the stack. We are avoiding
+                // the aditional complexity of Dijkstra's algorithm.
+                self.work_stack.push_front(node);
+            }
+        });
+        next_level.into_values().for_each(|node| {
+            if !self.visited.contains(&node.point) {
+                self.visited.insert(node.point);
+                self.work_stack.push_back(node);
+            }
+        });
+        Ok(())
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use std::path::Path;
+
+    use super::*;
+    use crate::graph_test_utils::*;
+    fn graph(dir: &Path) -> (Vec<Entity>, GraphDB) {
+        let graphdb = GraphDB::new(dir, SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let ids = UNodes
+            .take(4)
+            .map(|node| graphdb.add_node(&mut txn, &node).unwrap())
+            .collect::<Vec<_>>();
+        UEdges
+            .take(ids.len() - 1)
+            .enumerate()
+            .for_each(|(i, edge)| {
+                graphdb
+                    .connect(&mut txn, ids[i], &edge, ids[i + 1], None)
+                    .unwrap();
+            });
+        let backedge = UEdges.next().unwrap();
+        graphdb
+            .connect(&mut txn, ids[3], &backedge, ids[0], None)
+            .unwrap();
+        txn.commit().unwrap();
+        (ids, graphdb)
+    }
+
+    #[test]
+    fn full_search() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[0]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(AllGuide)
+            .max_depth(usize::MAX)
+            .build()
+            .unwrap();
+        let expected = &nodes;
+        let result = bfs.search().unwrap();
+        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+
+    #[test]
+    fn full_reverse_search() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[3]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(AllGuide)
+            .max_depth(usize::MAX)
+            .build()
+            .unwrap();
+        let expected = &nodes;
+        let result = bfs.search().unwrap();
+        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+
+    #[test]
+    fn limit_depth_search() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[0]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(AllGuide)
+            .max_depth(1)
+            .build()
+            .unwrap();
+        let expected = vec![nodes[0], nodes[1], nodes[3]];
+        let result = bfs.search().unwrap();
+        let mut result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        result.push(nodes[0]);
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+
+    #[test]
+    fn always_jump() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (nodes, graphdb) = graph(dir.path());
+        let txn = graphdb.ro_txn().unwrap();
+        let bfs = BfsEngineBuilder::new()
+            .entry_points(vec![nodes[0]])
+            .graph(&graphdb)
+            .txn(&txn)
+            .guide(FreeJumps)
+            .max_depth(0)
+            .build()
+            .unwrap();
+        let expected = &nodes;
+        let result = bfs.search().unwrap();
+        let result = result.map(|cnx| cnx.to()).collect::<Vec<_>>();
+        assert_eq!(result.len(), expected.len());
+        assert!(result.iter().copied().all(|n| expected.contains(&n)));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/errors.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/errors.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,51 +1,51 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use nucliadb_core::fs_state::FsError;
-use tantivy::TantivyError;
-
-#[derive(Debug, thiserror::Error)]
-pub enum RelationsErr {
-    #[error("Graph error: {0}")]
-    GraphDBError(String),
-    #[error("Bincode error: {0}")]
-    BincodeError(#[from] bincode::Error),
-    #[error("IO error: {0}")]
-    IOError(#[from] std::io::Error),
-    #[error("Disk error: {0}")]
-    DiskError(#[from] FsError),
-    #[error("Tantivy error: {0}")]
-    TantivyError(#[from] TantivyError),
-    #[error("Database is full")]
-    NeedsResize,
-    #[error("UBehaviour")]
-    UBehaviour,
-}
-
-impl From<heed::Error> for RelationsErr {
-    fn from(err: heed::Error) -> Self {
-        use heed::{Error, MdbError};
-        match err {
-            Error::Mdb(MdbError::MapFull) => RelationsErr::NeedsResize,
-            err => RelationsErr::GraphDBError(format!("{err:?}")),
-        }
-    }
-}
-pub type RResult<O> = Result<O, RelationsErr>;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use nucliadb_core::fs_state::FsError;
+use tantivy::TantivyError;
+
+#[derive(Debug, thiserror::Error)]
+pub enum RelationsErr {
+    #[error("Graph error: {0}")]
+    GraphDBError(String),
+    #[error("Bincode error: {0}")]
+    BincodeError(#[from] bincode::Error),
+    #[error("IO error: {0}")]
+    IOError(#[from] std::io::Error),
+    #[error("Disk error: {0}")]
+    DiskError(#[from] FsError),
+    #[error("Tantivy error: {0}")]
+    TantivyError(#[from] TantivyError),
+    #[error("Database is full")]
+    NeedsResize,
+    #[error("UBehaviour")]
+    UBehaviour,
+}
+
+impl From<heed::Error> for RelationsErr {
+    fn from(err: heed::Error) -> Self {
+        use heed::{Error, MdbError};
+        match err {
+            Error::Mdb(MdbError::MapFull) => RelationsErr::NeedsResize,
+            err => RelationsErr::GraphDBError(format!("{err:?}")),
+        }
+    }
+}
+pub type RResult<O> = Result<O, RelationsErr>;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/graph_db.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/graph_db.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,568 +1,567 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::collections::HashSet;
-use std::path::Path;
-
-use heed::flags::Flags;
-use heed::types::{SerdeBincode, Str, Unit};
-use heed::{Database, Env, EnvOpenOptions, RoTxn, RwTxn};
-use serde::de::DeserializeOwned;
-use serde::{Deserialize, Serialize};
-use uuid::Uuid;
-
-use super::errors::{RResult, RelationsErr};
-use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
-
-pub type RwToken<'a> = RwTxn<'a, 'a>;
-pub type RoToken<'a> = RoTxn<'a>;
-pub trait GIdProperty: Serialize + DeserializeOwned + 'static {}
-impl<T: Serialize + DeserializeOwned + 'static> GIdProperty for T {}
-
-#[derive(Serialize, Deserialize, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Clone, Copy)]
-pub struct Entity(Uuid);
-
-mod gid_impl {
-    use uuid::Uuid;
-
-    use super::Entity;
-    impl Default for Entity {
-        fn default() -> Self {
-            Entity(Uuid::new_v4())
-        }
-    }
-    impl Entity {
-        pub fn new() -> Self {
-            Self::default()
-        }
-    }
-    impl std::fmt::Display for Entity {
-        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-            self.0.fmt(f)
-        }
-    }
-}
-
-fn encode_connexion(
-    from: Option<Entity>,
-    to: Option<Entity>,
-    with: Option<Entity>,
-) -> RResult<String> {
-    match (from, to, with) {
-        (Some(from), Some(to), Some(with)) => Ok(format!("({from},{to},{with})")),
-        (Some(from), Some(to), None) => Ok(format!("({from},{to},")),
-        (Some(from), None, None) => Ok(format!("({from},")),
-        _ => Err(RelationsErr::UBehaviour),
-    }
-}
-fn decode_connexion(elements: &str) -> (Entity, Entity, Entity) {
-    let uuids: Vec<_> = elements
-        .strip_prefix('(')
-        .unwrap()
-        .strip_suffix(')')
-        .unwrap()
-        .split(',')
-        .into_iter()
-        .map(|v| Uuid::parse_str(v).unwrap())
-        .collect();
-    (Entity(uuids[0]), Entity(uuids[1]), Entity(uuids[2]))
-}
-
-#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
-pub struct GCnx(Entity, Entity, Entity);
-impl GCnx {
-    fn decode(elems: &str) -> GCnx {
-        let (from, to, with) = decode_connexion(elems);
-        GCnx(from, to, with)
-    }
-    fn decode_inversed(elems: &str) -> GCnx {
-        let (to, from, with) = decode_connexion(elems);
-        GCnx(from, to, with)
-    }
-    fn encode(&self) -> RResult<String> {
-        encode_connexion(Some(self.0), Some(self.1), Some(self.2))
-    }
-    pub fn new(from: Entity, to: Entity, with: Entity) -> GCnx {
-        GCnx(from, to, with)
-    }
-    pub fn from(&self) -> Entity {
-        self.0
-    }
-    pub fn to(&self) -> Entity {
-        self.1
-    }
-    pub fn edge(&self) -> Entity {
-        self.2
-    }
-}
-
-pub struct GraphDB {
-    env: Env,
-    // Given a node enconding returns its Entity
-    nodes: Database<Str, SerdeBincode<Entity>>,
-    // Encodings for the out edges (source, edge, target)
-    outedges: Database<Str, Unit>,
-    // Encodings for the in edges (target, edge, source)
-    inedges: Database<Str, Unit>,
-    // Given an entity, it returns its edge component
-    edge_component: Database<SerdeBincode<Entity>, SerdeBincode<IoEdge>>,
-    // Given an entity, it returns its edge metadata. It is not stored
-    // inside the IoEdge type to avoid the deserialization cost during BFS.
-    edge_metadata: Database<SerdeBincode<Entity>, SerdeBincode<IoEdgeMetadata>>,
-    // Given an entity, it returns its node component
-    node_component: Database<SerdeBincode<Entity>, SerdeBincode<IoNode>>,
-}
-
-macro_rules! database_name {
-    (const $l:ident) => {
-        const $l: &str = stringify!($l);
-    };
-}
-impl GraphDB {
-    const MAX_DBS: u32 = 6;
-    database_name!(const NODES);
-    database_name!(const OUTEDGES);
-    database_name!(const INEDGES);
-    database_name!(const NODE_COMPONENT);
-    database_name!(const EDGE_COMPONENT);
-    database_name!(const EDGE_METADATA);
-    pub fn new(path: &Path, db_size: usize) -> RResult<Self> {
-        let mut env_builder = EnvOpenOptions::new();
-        env_builder.max_dbs(Self::MAX_DBS);
-        env_builder.map_size(db_size);
-        unsafe {
-            env_builder.flag(Flags::MdbNoLock);
-        }
-        let env = env_builder.open(path)?;
-        let nodes = env.create_database(Some(Self::NODES))?;
-        let outedges = env.create_database(Some(Self::OUTEDGES))?;
-        let inedges = env.create_database(Some(Self::INEDGES))?;
-        let node_component = env.create_database(Some(Self::NODE_COMPONENT))?;
-        let edge_component = env.create_database(Some(Self::EDGE_COMPONENT))?;
-        let edge_metadata = env.create_database(Some(Self::EDGE_METADATA))?;
-        Ok(GraphDB {
-            env,
-            nodes,
-            outedges,
-            inedges,
-            node_component,
-            edge_component,
-            edge_metadata,
-        })
-    }
-    pub fn rw_txn(&self) -> RResult<RwToken<'_>> {
-        Ok(self.env.write_txn()?)
-    }
-    pub fn ro_txn(&self) -> RResult<RoTxn<'_>> {
-        Ok(self.env.read_txn()?)
-    }
-    pub fn add_node(&self, txn: &mut RwTxn, node: &IoNode) -> RResult<Entity> {
-        match self.nodes.get(txn, node.hash())? {
-            Some(v) => Ok(v),
-            None => {
-                let id = Entity::new();
-                self.nodes.put(txn, node.hash(), &id)?;
-                self.node_component.put(txn, &id, node)?;
-                Ok(id)
-            }
-        }
-    }
-    pub fn delete_node(&self, txn: &mut RwTxn, node_id: Entity) -> RResult<HashSet<Entity>> {
-        let node = self.get_node(txn, node_id)?;
-        self.nodes.delete(txn, node.hash())?;
-        self.node_component.delete(txn, &node_id)?;
-        let prefix = encode_connexion(Some(node_id), None, None)?;
-        // Deleting out connexions
-        let mut affected = HashSet::new();
-        let mut in_edges = vec![];
-        let mut out_iter = self.outedges.prefix_iter_mut(txn, &prefix)?;
-        while let Some((out_edge, _)) = out_iter.next().transpose()? {
-            let in_edge = GCnx::decode_inversed(out_edge);
-            affected.insert(in_edge.from());
-            in_edges.push(in_edge);
-            out_iter.del_current()?;
-        }
-        std::mem::drop(out_iter);
-        for edge in in_edges.into_iter().map(|e| e.encode()) {
-            let edge = edge?;
-            self.inedges.delete(txn, &edge)?;
-        }
-        // Deleting in connexions
-        let mut out_edges = vec![];
-        let mut in_iter = self.inedges.prefix_iter_mut(txn, &prefix)?;
-        while let Some((out_edge, _)) = in_iter.next().transpose()? {
-            let out_edge = GCnx::decode_inversed(out_edge);
-            affected.insert(out_edge.from());
-            out_edges.push(out_edge);
-            in_iter.del_current()?;
-        }
-        std::mem::drop(in_iter);
-        for edge in out_edges.into_iter().map(|e| e.encode()) {
-            let edge = edge?;
-            self.outedges.delete(txn, &edge)?;
-        }
-        Ok(affected)
-    }
-    pub fn connect(
-        &self,
-        txn: &mut RwTxn,
-        from: Entity,
-        edge: &IoEdge,
-        to: Entity,
-        edge_metadata: Option<&IoEdgeMetadata>,
-    ) -> RResult<bool> {
-        let mut exits = false;
-        let mut iter = self.connected_by(txn, from, to)?;
-        loop {
-            match iter.next() {
-                _ if exits => break,
-                Some(cnx) => {
-                    let cnx = cnx?;
-                    let cnx = self.get_edge(txn, cnx.edge())?;
-                    exits = *edge == cnx;
-                }
-                None => {
-                    std::mem::drop(iter);
-                    let with = Entity::new();
-                    let out_edge = GCnx::new(from, to, with).encode()?;
-                    let in_edge = GCnx::new(to, from, with).encode()?;
-                    self.edge_component.put(txn, &with, edge)?;
-                    self.outedges.put(txn, &out_edge, &())?;
-                    self.inedges.put(txn, &in_edge, &())?;
-                    if let Some(metadata) = edge_metadata {
-                        self.edge_metadata.put(txn, &with, metadata)?;
-                    }
-                    break;
-                }
-            }
-        }
-        Ok(!exits)
-    }
-    pub fn iter_node_ids<'a>(
-        &self,
-        txn: &'a RoTxn,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        Ok(self
-            .node_component
-            .iter(txn)
-            .map_err(RelationsErr::from)?
-            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
-    }
-    pub fn iter_edge_ids<'a>(
-        &self,
-        txn: &'a RoTxn,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        Ok(self
-            .edge_component
-            .iter(txn)
-            .map_err(RelationsErr::from)?
-            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
-    }
-    pub fn get_node(&self, txn: &RoTxn, x: Entity) -> RResult<IoNode> {
-        let node = self.node_component.get(txn, &x)?;
-        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
-    }
-    pub fn get_nodeid(&self, txn: &RoTxn, x: &str) -> RResult<Option<Entity>> {
-        Ok(self.nodes.get(txn, x)?)
-    }
-    pub fn get_edge(&self, txn: &RoTxn, x: Entity) -> RResult<IoEdge> {
-        let node = self.edge_component.get(txn, &x)?;
-        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
-    }
-    pub fn get_edge_metadata(&self, txn: &RoTxn, x: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        Ok(self.edge_metadata.get(txn, &x)?)
-    }
-    pub fn connected_by<'a>(
-        &self,
-        txn: &'a RoTxn,
-        x: Entity,
-        y: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        let right_arrow = encode_connexion(Some(x), Some(y), None)?;
-        let out = self.outedges.prefix_iter(txn, &right_arrow)?;
-        let iter = out.map(|r| r.map_err(RelationsErr::from).map(|(v, _)| GCnx::decode(v)));
-        Ok(iter)
-    }
-    pub fn get_outedges<'a>(
-        &self,
-        txn: &'a RoTxn,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        let prefix = encode_connexion(Some(from), None, None)?;
-        let out = self.outedges.prefix_iter(txn, &prefix)?;
-        let iter = out
-            .map(|r| r.map_err(RelationsErr::from))
-            .map(|r| r.map(|(v, _)| GCnx::decode(v)));
-        Ok(iter)
-    }
-    pub fn get_inedges<'a>(
-        &self,
-        txn: &'a RoTxn,
-        to: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        let prefix = encode_connexion(Some(to), None, None)?;
-        let xin = self.inedges.prefix_iter(txn, &prefix)?;
-        let inv_iter = xin
-            .map(|r| r.map_err(RelationsErr::from))
-            .map(|r| r.map(|(v, _)| GCnx::decode_inversed(v)));
-        Ok(inv_iter)
-    }
-
-    pub fn no_nodes(&self, txn: &RoTxn) -> RResult<u64> {
-        Ok(self.nodes.len(txn)?)
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::HashSet;
-
-    use super::*;
-    use crate::graph_test_utils::*;
-
-    #[test]
-    fn creation_test() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let node1 = fresh_node();
-        let node2 = fresh_node();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let id1 = graphdb.add_node(&mut txn, &node1).unwrap();
-        let id2 = graphdb.add_node(&mut txn, &node2).unwrap();
-        let idr = graphdb.add_node(&mut txn, &node1).unwrap();
-        txn.commit().unwrap();
-        assert_eq!(id1, idr);
-        assert_ne!(id1, id2);
-
-        let txn = graphdb.ro_txn().unwrap();
-        let gnode1 = graphdb.get_node(&txn, id1).unwrap();
-        let gnode2 = graphdb.get_node(&txn, id2).unwrap();
-        assert_eq!(gnode1, node1);
-        assert_eq!(gnode2, node2);
-    }
-
-    #[test]
-    fn deletion_test() {
-        // N1 -e1-> N2
-        // N1 -e2-> N3
-        // N2 -e3-> N4
-        // N3 -e4-> N4
-        // N4 -e5-> N1
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let e1 = fresh_edge();
-        let e2 = fresh_edge();
-        let e3 = fresh_edge();
-        let e4 = fresh_edge();
-        let e5 = fresh_edge();
-        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
-        // Deleting N4
-        // N1 should have 0 in-edges
-        // N3 should have 0 out-edges
-        // N2 should have 0 out-edges
-        let io_n4 = graphdb.get_node(&txn, n4).unwrap();
-        let expected_affected = HashSet::from([n1, n2, n3]);
-        let got_affected = graphdb.delete_node(&mut txn, n4).unwrap();
-        assert_eq!(expected_affected, got_affected);
-        let n1_out = graphdb.get_outedges(&txn, n1).unwrap().count();
-        let n1_in = graphdb.get_inedges(&txn, n1).unwrap().count();
-        let n2_in = graphdb.get_inedges(&txn, n2).unwrap().count();
-        let n2_out = graphdb.get_outedges(&txn, n2).unwrap().count();
-        let n3_in = graphdb.get_inedges(&txn, n3).unwrap().count();
-        let n3_out = graphdb.get_outedges(&txn, n3).unwrap().count();
-        let n4_out = graphdb.get_outedges(&txn, n4).unwrap().count();
-        let n4_in = graphdb.get_inedges(&txn, n4).unwrap().count();
-        assert_eq!(n4_out, 0);
-        assert_eq!(n4_in, 0);
-        assert_eq!(n1_out, 2);
-        assert_eq!(n1_in, 0);
-        assert_eq!(n2_in, 1);
-        assert_eq!(n2_out, 0);
-        assert_eq!(n3_in, 1);
-        assert_eq!(n3_out, 0);
-        let n4_id = graphdb.get_nodeid(&txn, io_n4.hash()).unwrap();
-        assert_eq!(n4_id, None);
-    }
-    #[test]
-    fn connexions_test() {
-        // N1 -e1-> N2
-        // N1 -e2-> N3
-        // N2 -e3-> N4
-        // N3 -e4-> N4
-        // N4 -e5-> N1
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let e1 = fresh_edge();
-        let e2 = fresh_edge();
-        let e3 = fresh_edge();
-        let e4 = fresh_edge();
-        let e5 = fresh_edge();
-        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
-
-        // out edges test
-        let expected_outn1 = HashSet::from([(n2, e1.clone()), (n3, e2.clone())]);
-        let expected_outn2 = HashSet::from([(n4, e3.clone())]);
-        let expected_outn3 = HashSet::from([(n4, e4.clone())]);
-        let expected_outn4 = HashSet::from([(n1, e5.clone())]);
-        let got_outn1 = graphdb
-            .get_outedges(&txn, n1)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_outn2 = graphdb
-            .get_outedges(&txn, n2)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_outn3 = graphdb
-            .get_outedges(&txn, n3)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_outn4 = graphdb
-            .get_outedges(&txn, n4)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        assert_eq!(got_outn1.len(), expected_outn1.len());
-        assert!(got_outn1.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n1 && expected_outn1.contains(&(cnx.to(), edge))
-        }));
-        assert_eq!(got_outn2.len(), expected_outn2.len());
-        assert!(got_outn2.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n2 && expected_outn2.contains(&(cnx.to(), edge))
-        }));
-        assert_eq!(got_outn3.len(), expected_outn3.len());
-        assert!(got_outn3.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n3 && expected_outn3.contains(&(cnx.to(), edge))
-        }));
-        assert_eq!(got_outn4.len(), expected_outn4.len());
-        assert!(got_outn4.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.from() == n4 && expected_outn4.contains(&(cnx.to(), edge))
-        }));
-        // in edges test
-        // N1 -e1-> N2
-        // N1 -e2-> N3
-        // N2 -e3-> N4
-        // N3 -e4-> N4
-        // N4 -e5-> N1
-        let expected_inn1 = HashSet::from([(n4, e5.clone())]);
-        let expected_inn2 = HashSet::from([(n1, e1.clone())]);
-        let expected_inn3 = HashSet::from([(n1, e2.clone())]);
-        let expected_inn4 = HashSet::from([(n2, e3.clone()), (n3, e4.clone())]);
-        let got_inn1 = graphdb
-            .get_inedges(&txn, n1)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_inn2 = graphdb
-            .get_inedges(&txn, n2)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_inn3 = graphdb
-            .get_inedges(&txn, n3)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        let got_inn4 = graphdb
-            .get_inedges(&txn, n4)
-            .unwrap()
-            .collect::<RResult<Vec<_>>>()
-            .unwrap();
-        assert_eq!(got_inn1.len(), expected_inn1.len());
-        assert!(got_inn1.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n1 && expected_inn1.contains(&(cnx.from(), edge))
-        }));
-        assert_eq!(got_inn2.len(), expected_inn2.len());
-        assert!(got_inn2.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n2 && expected_inn2.contains(&(cnx.from(), edge))
-        }));
-        assert_eq!(got_inn3.len(), expected_inn3.len());
-        assert!(got_inn3.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n3 && expected_inn3.contains(&(cnx.from(), edge))
-        }));
-        assert_eq!(got_inn4.len(), expected_inn4.len());
-        assert!(got_inn4.into_iter().all(|cnx| {
-            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
-            cnx.to() == n4 && expected_inn4.contains(&(cnx.from(), edge))
-        }));
-    }
-
-    #[test]
-    fn connexion_similarity_test() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
-        let mut txn = graphdb.rw_txn().unwrap();
-        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
-        let e1 = fresh_edge();
-        let e2 = fresh_edge();
-        let e3 = fresh_edge();
-        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
-        assert!(graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
-        assert!(!graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
-        assert!(!graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
-        assert!(!graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
-        let expect = HashSet::from([e1, e3]);
-        let n1_n2 = graphdb
-            .connected_by(&txn, n1, n2)
-            .unwrap()
-            .map(|c| c.unwrap())
-            .map(|c| c.edge())
-            .map(|c| graphdb.get_edge(&txn, c).unwrap())
-            .collect::<Vec<_>>();
-        assert_eq!(n1_n2.len(), expect.len());
-        assert!(n1_n2.iter().all(|v| expect.contains(v)));
-        let mut n2_n1 = graphdb
-            .connected_by(&txn, n2, n1)
-            .unwrap()
-            .map(|c| c.unwrap())
-            .map(|c| c.edge())
-            .map(|c| graphdb.get_edge(&txn, c).unwrap());
-        assert_eq!(Some(e2), n2_n1.next());
-        assert_eq!(None, n2_n1.next());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::collections::HashSet;
+use std::path::Path;
+
+use heed::flags::Flags;
+use heed::types::{SerdeBincode, Str, Unit};
+use heed::{Database, Env, EnvOpenOptions, RoTxn, RwTxn};
+use serde::de::DeserializeOwned;
+use serde::{Deserialize, Serialize};
+use uuid::Uuid;
+
+use super::errors::{RResult, RelationsErr};
+use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
+
+pub type RwToken<'a> = RwTxn<'a, 'a>;
+pub type RoToken<'a> = RoTxn<'a>;
+pub trait GIdProperty: Serialize + DeserializeOwned + 'static {}
+impl<T: Serialize + DeserializeOwned + 'static> GIdProperty for T {}
+
+#[derive(Serialize, Deserialize, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Clone, Copy)]
+pub struct Entity(Uuid);
+
+mod gid_impl {
+    use uuid::Uuid;
+
+    use super::Entity;
+    impl Default for Entity {
+        fn default() -> Self {
+            Entity(Uuid::new_v4())
+        }
+    }
+    impl Entity {
+        pub fn new() -> Self {
+            Self::default()
+        }
+    }
+    impl std::fmt::Display for Entity {
+        fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+            self.0.fmt(f)
+        }
+    }
+}
+
+fn encode_connexion(
+    from: Option<Entity>,
+    to: Option<Entity>,
+    with: Option<Entity>,
+) -> RResult<String> {
+    match (from, to, with) {
+        (Some(from), Some(to), Some(with)) => Ok(format!("({from},{to},{with})")),
+        (Some(from), Some(to), None) => Ok(format!("({from},{to},")),
+        (Some(from), None, None) => Ok(format!("({from},")),
+        _ => Err(RelationsErr::UBehaviour),
+    }
+}
+fn decode_connexion(elements: &str) -> (Entity, Entity, Entity) {
+    let uuids: Vec<_> = elements
+        .strip_prefix('(')
+        .unwrap()
+        .strip_suffix(')')
+        .unwrap()
+        .split(',')
+        .map(|v| Uuid::parse_str(v).unwrap())
+        .collect();
+    (Entity(uuids[0]), Entity(uuids[1]), Entity(uuids[2]))
+}
+
+#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
+pub struct GCnx(Entity, Entity, Entity);
+impl GCnx {
+    fn decode(elems: &str) -> GCnx {
+        let (from, to, with) = decode_connexion(elems);
+        GCnx(from, to, with)
+    }
+    fn decode_inversed(elems: &str) -> GCnx {
+        let (to, from, with) = decode_connexion(elems);
+        GCnx(from, to, with)
+    }
+    fn encode(&self) -> RResult<String> {
+        encode_connexion(Some(self.0), Some(self.1), Some(self.2))
+    }
+    pub fn new(from: Entity, to: Entity, with: Entity) -> GCnx {
+        GCnx(from, to, with)
+    }
+    pub fn from(&self) -> Entity {
+        self.0
+    }
+    pub fn to(&self) -> Entity {
+        self.1
+    }
+    pub fn edge(&self) -> Entity {
+        self.2
+    }
+}
+
+pub struct GraphDB {
+    env: Env,
+    // Given a node enconding returns its Entity
+    nodes: Database<Str, SerdeBincode<Entity>>,
+    // Encodings for the out edges (source, edge, target)
+    outedges: Database<Str, Unit>,
+    // Encodings for the in edges (target, edge, source)
+    inedges: Database<Str, Unit>,
+    // Given an entity, it returns its edge component
+    edge_component: Database<SerdeBincode<Entity>, SerdeBincode<IoEdge>>,
+    // Given an entity, it returns its edge metadata. It is not stored
+    // inside the IoEdge type to avoid the deserialization cost during BFS.
+    edge_metadata: Database<SerdeBincode<Entity>, SerdeBincode<IoEdgeMetadata>>,
+    // Given an entity, it returns its node component
+    node_component: Database<SerdeBincode<Entity>, SerdeBincode<IoNode>>,
+}
+
+macro_rules! database_name {
+    (const $l:ident) => {
+        const $l: &str = stringify!($l);
+    };
+}
+impl GraphDB {
+    const MAX_DBS: u32 = 6;
+    database_name!(const NODES);
+    database_name!(const OUTEDGES);
+    database_name!(const INEDGES);
+    database_name!(const NODE_COMPONENT);
+    database_name!(const EDGE_COMPONENT);
+    database_name!(const EDGE_METADATA);
+    pub fn new(path: &Path, db_size: usize) -> RResult<Self> {
+        let mut env_builder = EnvOpenOptions::new();
+        env_builder.max_dbs(Self::MAX_DBS);
+        env_builder.map_size(db_size);
+        unsafe {
+            env_builder.flag(Flags::MdbNoLock);
+        }
+        let env = env_builder.open(path)?;
+        let nodes = env.create_database(Some(Self::NODES))?;
+        let outedges = env.create_database(Some(Self::OUTEDGES))?;
+        let inedges = env.create_database(Some(Self::INEDGES))?;
+        let node_component = env.create_database(Some(Self::NODE_COMPONENT))?;
+        let edge_component = env.create_database(Some(Self::EDGE_COMPONENT))?;
+        let edge_metadata = env.create_database(Some(Self::EDGE_METADATA))?;
+        Ok(GraphDB {
+            env,
+            nodes,
+            outedges,
+            inedges,
+            node_component,
+            edge_component,
+            edge_metadata,
+        })
+    }
+    pub fn rw_txn(&self) -> RResult<RwToken<'_>> {
+        Ok(self.env.write_txn()?)
+    }
+    pub fn ro_txn(&self) -> RResult<RoTxn<'_>> {
+        Ok(self.env.read_txn()?)
+    }
+    pub fn add_node(&self, txn: &mut RwTxn, node: &IoNode) -> RResult<Entity> {
+        match self.nodes.get(txn, node.hash())? {
+            Some(v) => Ok(v),
+            None => {
+                let id = Entity::new();
+                self.nodes.put(txn, node.hash(), &id)?;
+                self.node_component.put(txn, &id, node)?;
+                Ok(id)
+            }
+        }
+    }
+    pub fn delete_node(&self, txn: &mut RwTxn, node_id: Entity) -> RResult<HashSet<Entity>> {
+        let node = self.get_node(txn, node_id)?;
+        self.nodes.delete(txn, node.hash())?;
+        self.node_component.delete(txn, &node_id)?;
+        let prefix = encode_connexion(Some(node_id), None, None)?;
+        // Deleting out connexions
+        let mut affected = HashSet::new();
+        let mut in_edges = vec![];
+        let mut out_iter = self.outedges.prefix_iter_mut(txn, &prefix)?;
+        while let Some((out_edge, _)) = out_iter.next().transpose()? {
+            let in_edge = GCnx::decode_inversed(out_edge);
+            affected.insert(in_edge.from());
+            in_edges.push(in_edge);
+            out_iter.del_current()?;
+        }
+        std::mem::drop(out_iter);
+        for edge in in_edges.into_iter().map(|e| e.encode()) {
+            let edge = edge?;
+            self.inedges.delete(txn, &edge)?;
+        }
+        // Deleting in connexions
+        let mut out_edges = vec![];
+        let mut in_iter = self.inedges.prefix_iter_mut(txn, &prefix)?;
+        while let Some((out_edge, _)) = in_iter.next().transpose()? {
+            let out_edge = GCnx::decode_inversed(out_edge);
+            affected.insert(out_edge.from());
+            out_edges.push(out_edge);
+            in_iter.del_current()?;
+        }
+        std::mem::drop(in_iter);
+        for edge in out_edges.into_iter().map(|e| e.encode()) {
+            let edge = edge?;
+            self.outedges.delete(txn, &edge)?;
+        }
+        Ok(affected)
+    }
+    pub fn connect(
+        &self,
+        txn: &mut RwTxn,
+        from: Entity,
+        edge: &IoEdge,
+        to: Entity,
+        edge_metadata: Option<&IoEdgeMetadata>,
+    ) -> RResult<bool> {
+        let mut exits = false;
+        let mut iter = self.connected_by(txn, from, to)?;
+        loop {
+            match iter.next() {
+                _ if exits => break,
+                Some(cnx) => {
+                    let cnx = cnx?;
+                    let cnx = self.get_edge(txn, cnx.edge())?;
+                    exits = *edge == cnx;
+                }
+                None => {
+                    std::mem::drop(iter);
+                    let with = Entity::new();
+                    let out_edge = GCnx::new(from, to, with).encode()?;
+                    let in_edge = GCnx::new(to, from, with).encode()?;
+                    self.edge_component.put(txn, &with, edge)?;
+                    self.outedges.put(txn, &out_edge, &())?;
+                    self.inedges.put(txn, &in_edge, &())?;
+                    if let Some(metadata) = edge_metadata {
+                        self.edge_metadata.put(txn, &with, metadata)?;
+                    }
+                    break;
+                }
+            }
+        }
+        Ok(!exits)
+    }
+    pub fn iter_node_ids<'a>(
+        &self,
+        txn: &'a RoTxn,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        Ok(self
+            .node_component
+            .iter(txn)
+            .map_err(RelationsErr::from)?
+            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
+    }
+    pub fn iter_edge_ids<'a>(
+        &self,
+        txn: &'a RoTxn,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        Ok(self
+            .edge_component
+            .iter(txn)
+            .map_err(RelationsErr::from)?
+            .map(|r| r.map(|n| n.0).map_err(RelationsErr::from)))
+    }
+    pub fn get_node(&self, txn: &RoTxn, x: Entity) -> RResult<IoNode> {
+        let node = self.node_component.get(txn, &x)?;
+        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
+    }
+    pub fn get_nodeid(&self, txn: &RoTxn, x: &str) -> RResult<Option<Entity>> {
+        Ok(self.nodes.get(txn, x)?)
+    }
+    pub fn get_edge(&self, txn: &RoTxn, x: Entity) -> RResult<IoEdge> {
+        let node = self.edge_component.get(txn, &x)?;
+        node.map_or_else(|| Err(RelationsErr::UBehaviour), Ok)
+    }
+    pub fn get_edge_metadata(&self, txn: &RoTxn, x: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        Ok(self.edge_metadata.get(txn, &x)?)
+    }
+    pub fn connected_by<'a>(
+        &self,
+        txn: &'a RoTxn,
+        x: Entity,
+        y: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        let right_arrow = encode_connexion(Some(x), Some(y), None)?;
+        let out = self.outedges.prefix_iter(txn, &right_arrow)?;
+        let iter = out.map(|r| r.map_err(RelationsErr::from).map(|(v, _)| GCnx::decode(v)));
+        Ok(iter)
+    }
+    pub fn get_outedges<'a>(
+        &self,
+        txn: &'a RoTxn,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        let prefix = encode_connexion(Some(from), None, None)?;
+        let out = self.outedges.prefix_iter(txn, &prefix)?;
+        let iter = out
+            .map(|r| r.map_err(RelationsErr::from))
+            .map(|r| r.map(|(v, _)| GCnx::decode(v)));
+        Ok(iter)
+    }
+    pub fn get_inedges<'a>(
+        &self,
+        txn: &'a RoTxn,
+        to: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        let prefix = encode_connexion(Some(to), None, None)?;
+        let xin = self.inedges.prefix_iter(txn, &prefix)?;
+        let inv_iter = xin
+            .map(|r| r.map_err(RelationsErr::from))
+            .map(|r| r.map(|(v, _)| GCnx::decode_inversed(v)));
+        Ok(inv_iter)
+    }
+
+    pub fn no_nodes(&self, txn: &RoTxn) -> RResult<u64> {
+        Ok(self.nodes.len(txn)?)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashSet;
+
+    use super::*;
+    use crate::graph_test_utils::*;
+
+    #[test]
+    fn creation_test() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let node1 = fresh_node();
+        let node2 = fresh_node();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let id1 = graphdb.add_node(&mut txn, &node1).unwrap();
+        let id2 = graphdb.add_node(&mut txn, &node2).unwrap();
+        let idr = graphdb.add_node(&mut txn, &node1).unwrap();
+        txn.commit().unwrap();
+        assert_eq!(id1, idr);
+        assert_ne!(id1, id2);
+
+        let txn = graphdb.ro_txn().unwrap();
+        let gnode1 = graphdb.get_node(&txn, id1).unwrap();
+        let gnode2 = graphdb.get_node(&txn, id2).unwrap();
+        assert_eq!(gnode1, node1);
+        assert_eq!(gnode2, node2);
+    }
+
+    #[test]
+    fn deletion_test() {
+        // N1 -e1-> N2
+        // N1 -e2-> N3
+        // N2 -e3-> N4
+        // N3 -e4-> N4
+        // N4 -e5-> N1
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let e1 = fresh_edge();
+        let e2 = fresh_edge();
+        let e3 = fresh_edge();
+        let e4 = fresh_edge();
+        let e5 = fresh_edge();
+        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
+        // Deleting N4
+        // N1 should have 0 in-edges
+        // N3 should have 0 out-edges
+        // N2 should have 0 out-edges
+        let io_n4 = graphdb.get_node(&txn, n4).unwrap();
+        let expected_affected = HashSet::from([n1, n2, n3]);
+        let got_affected = graphdb.delete_node(&mut txn, n4).unwrap();
+        assert_eq!(expected_affected, got_affected);
+        let n1_out = graphdb.get_outedges(&txn, n1).unwrap().count();
+        let n1_in = graphdb.get_inedges(&txn, n1).unwrap().count();
+        let n2_in = graphdb.get_inedges(&txn, n2).unwrap().count();
+        let n2_out = graphdb.get_outedges(&txn, n2).unwrap().count();
+        let n3_in = graphdb.get_inedges(&txn, n3).unwrap().count();
+        let n3_out = graphdb.get_outedges(&txn, n3).unwrap().count();
+        let n4_out = graphdb.get_outedges(&txn, n4).unwrap().count();
+        let n4_in = graphdb.get_inedges(&txn, n4).unwrap().count();
+        assert_eq!(n4_out, 0);
+        assert_eq!(n4_in, 0);
+        assert_eq!(n1_out, 2);
+        assert_eq!(n1_in, 0);
+        assert_eq!(n2_in, 1);
+        assert_eq!(n2_out, 0);
+        assert_eq!(n3_in, 1);
+        assert_eq!(n3_out, 0);
+        let n4_id = graphdb.get_nodeid(&txn, io_n4.hash()).unwrap();
+        assert_eq!(n4_id, None);
+    }
+    #[test]
+    fn connexions_test() {
+        // N1 -e1-> N2
+        // N1 -e2-> N3
+        // N2 -e3-> N4
+        // N3 -e4-> N4
+        // N4 -e5-> N1
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n3 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n4 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let e1 = fresh_edge();
+        let e2 = fresh_edge();
+        let e3 = fresh_edge();
+        let e4 = fresh_edge();
+        let e5 = fresh_edge();
+        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n1, &e2, n3, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n2, &e3, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n3, &e4, n4, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n4, &e5, n1, None).unwrap());
+
+        // out edges test
+        let expected_outn1 = HashSet::from([(n2, e1.clone()), (n3, e2.clone())]);
+        let expected_outn2 = HashSet::from([(n4, e3.clone())]);
+        let expected_outn3 = HashSet::from([(n4, e4.clone())]);
+        let expected_outn4 = HashSet::from([(n1, e5.clone())]);
+        let got_outn1 = graphdb
+            .get_outedges(&txn, n1)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_outn2 = graphdb
+            .get_outedges(&txn, n2)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_outn3 = graphdb
+            .get_outedges(&txn, n3)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_outn4 = graphdb
+            .get_outedges(&txn, n4)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        assert_eq!(got_outn1.len(), expected_outn1.len());
+        assert!(got_outn1.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n1 && expected_outn1.contains(&(cnx.to(), edge))
+        }));
+        assert_eq!(got_outn2.len(), expected_outn2.len());
+        assert!(got_outn2.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n2 && expected_outn2.contains(&(cnx.to(), edge))
+        }));
+        assert_eq!(got_outn3.len(), expected_outn3.len());
+        assert!(got_outn3.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n3 && expected_outn3.contains(&(cnx.to(), edge))
+        }));
+        assert_eq!(got_outn4.len(), expected_outn4.len());
+        assert!(got_outn4.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.from() == n4 && expected_outn4.contains(&(cnx.to(), edge))
+        }));
+        // in edges test
+        // N1 -e1-> N2
+        // N1 -e2-> N3
+        // N2 -e3-> N4
+        // N3 -e4-> N4
+        // N4 -e5-> N1
+        let expected_inn1 = HashSet::from([(n4, e5.clone())]);
+        let expected_inn2 = HashSet::from([(n1, e1.clone())]);
+        let expected_inn3 = HashSet::from([(n1, e2.clone())]);
+        let expected_inn4 = HashSet::from([(n2, e3.clone()), (n3, e4.clone())]);
+        let got_inn1 = graphdb
+            .get_inedges(&txn, n1)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_inn2 = graphdb
+            .get_inedges(&txn, n2)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_inn3 = graphdb
+            .get_inedges(&txn, n3)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        let got_inn4 = graphdb
+            .get_inedges(&txn, n4)
+            .unwrap()
+            .collect::<RResult<Vec<_>>>()
+            .unwrap();
+        assert_eq!(got_inn1.len(), expected_inn1.len());
+        assert!(got_inn1.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n1 && expected_inn1.contains(&(cnx.from(), edge))
+        }));
+        assert_eq!(got_inn2.len(), expected_inn2.len());
+        assert!(got_inn2.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n2 && expected_inn2.contains(&(cnx.from(), edge))
+        }));
+        assert_eq!(got_inn3.len(), expected_inn3.len());
+        assert!(got_inn3.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n3 && expected_inn3.contains(&(cnx.from(), edge))
+        }));
+        assert_eq!(got_inn4.len(), expected_inn4.len());
+        assert!(got_inn4.into_iter().all(|cnx| {
+            let edge = graphdb.get_edge(&txn, cnx.edge()).unwrap();
+            cnx.to() == n4 && expected_inn4.contains(&(cnx.from(), edge))
+        }));
+    }
+
+    #[test]
+    fn connexion_similarity_test() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let graphdb = GraphDB::new(dir.path(), SIZE).unwrap();
+        let mut txn = graphdb.rw_txn().unwrap();
+        let n1 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let n2 = graphdb.add_node(&mut txn, &fresh_node()).unwrap();
+        let e1 = fresh_edge();
+        let e2 = fresh_edge();
+        let e3 = fresh_edge();
+        assert!(graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
+        assert!(graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
+        assert!(!graphdb.connect(&mut txn, n1, &e1, n2, None).unwrap());
+        assert!(!graphdb.connect(&mut txn, n2, &e2, n1, None).unwrap());
+        assert!(!graphdb.connect(&mut txn, n1, &e3, n2, None).unwrap());
+        let expect = HashSet::from([e1, e3]);
+        let n1_n2 = graphdb
+            .connected_by(&txn, n1, n2)
+            .unwrap()
+            .map(|c| c.unwrap())
+            .map(|c| c.edge())
+            .map(|c| graphdb.get_edge(&txn, c).unwrap())
+            .collect::<Vec<_>>();
+        assert_eq!(n1_n2.len(), expect.len());
+        assert!(n1_n2.iter().all(|v| expect.contains(v)));
+        let mut n2_n1 = graphdb
+            .connected_by(&txn, n2, n1)
+            .unwrap()
+            .map(|c| c.unwrap())
+            .map(|c| c.edge())
+            .map(|c| graphdb.get_edge(&txn, c).unwrap());
+        assert_eq!(Some(e2), n2_n1.next());
+        assert_eq!(None, n2_n1.next());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/index.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/index.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,307 +1,307 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashSet;
-use std::path::Path;
-
-use super::bfs_engine::{BfsEngineBuilder, BfsGuide};
-use super::errors::*;
-pub use super::graph_db::{Entity, GCnx};
-use super::graph_db::{GraphDB, RoToken, RwToken};
-use super::node_dictionary::{DReader, DWriter, NodeDictionary};
-pub use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
-
-pub struct RMode(DReader);
-pub struct WMode(DWriter);
-
-pub struct Index {
-    graphdb: GraphDB,
-    dictionary: NodeDictionary,
-}
-impl Index {
-    const DICTIONARY_PATH: &str = "NodeDictionary";
-    const GRAPH_PATH: &str = "GraphDB";
-    const GRAPH_SIZE: usize = 1048576 * 100000;
-    fn connect(
-        &self,
-        graph_txn: &mut RwToken,
-        dict_writer: &DWriter,
-        from: &IoNode,
-        to: &IoNode,
-        edge: &IoEdge,
-        edge_metadata: Option<&IoEdgeMetadata>,
-    ) -> RResult<bool> {
-        self.dictionary.add_node(dict_writer, from)?;
-        self.dictionary.add_node(dict_writer, to)?;
-        let from = self.graphdb.add_node(graph_txn, from)?;
-        let to = self.graphdb.add_node(graph_txn, to)?;
-        self.graphdb
-            .connect(graph_txn, from, edge, to, edge_metadata)
-    }
-    fn graph_search<G: BfsGuide>(
-        &self,
-        txn: &RoToken,
-        guide: G,
-        max_depth: usize,
-        entry_points: Vec<Entity>,
-    ) -> RResult<impl Iterator<Item = GCnx>> {
-        BfsEngineBuilder::new()
-            .graph(&self.graphdb)
-            .txn(txn)
-            .max_depth(max_depth)
-            .guide(guide)
-            .entry_points(entry_points)
-            .build()
-            .unwrap()
-            .search()
-    }
-    fn delete_node(
-        &self,
-        graph_txn: &mut RwToken,
-        dict_writer: &DWriter,
-        node_id: Entity,
-    ) -> RResult<HashSet<Entity>> {
-        let value = self.graphdb.get_node(graph_txn, node_id)?;
-        self.dictionary.delete_node(dict_writer, &value);
-        self.graphdb.delete_node(graph_txn, node_id)
-    }
-    fn no_nodes(&self, txn: &RoToken) -> RResult<u64> {
-        self.graphdb.no_nodes(txn)
-    }
-    fn iter_node_ids<'a>(
-        &self,
-        txn: &'a RoToken,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.graphdb.iter_node_ids(txn)
-    }
-    fn iter_edge_ids<'a>(
-        &self,
-        txn: &'a RoToken,
-    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.graphdb.iter_edge_ids(txn)
-    }
-    pub fn get_outedges<'a>(
-        &self,
-        txn: &'a RoToken,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.graphdb.get_outedges(txn, from)
-    }
-    fn get_edge(&self, txn: &RoToken, id: Entity) -> RResult<IoEdge> {
-        self.graphdb.get_edge(txn, id)
-    }
-    fn get_edge_metadata(&self, txn: &RoToken, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        self.graphdb.get_edge_metadata(txn, id)
-    }
-    fn get_node(&self, txn: &RoToken, id: Entity) -> RResult<IoNode> {
-        self.graphdb.get_node(txn, id)
-    }
-    fn get_nodeid(&self, txn: &RoToken, x: &str) -> RResult<Option<Entity>> {
-        self.graphdb.get_nodeid(txn, x)
-    }
-    fn prefix_search(&self, dict_reader: &DReader, prefix: &str) -> RResult<Vec<String>> {
-        self.dictionary.search(dict_reader, prefix)
-    }
-    fn get_inedges<'a>(
-        &self,
-        txn: &'a RoToken,
-        to: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.graphdb.get_inedges(txn, to)
-    }
-    pub fn new_writer(location: &Path) -> RResult<(Index, WMode)> {
-        let dictionary_address = location.join(Self::DICTIONARY_PATH);
-        let graph_address = location.join(Self::GRAPH_PATH);
-        if !graph_address.exists() {
-            std::fs::create_dir(&graph_address)?;
-        }
-        if !dictionary_address.exists() {
-            std::fs::create_dir(&dictionary_address)?;
-        }
-        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
-        let (dictionary, mode) = NodeDictionary::new_writer(&dictionary_address)?;
-        let index = Index {
-            graphdb,
-            dictionary,
-        };
-        Ok((index, WMode(mode)))
-    }
-    pub fn new_reader(location: &Path) -> RResult<(Index, RMode)> {
-        let dictionary_address = location.join(Self::DICTIONARY_PATH);
-        let graph_address = location.join(Self::GRAPH_PATH);
-        if !graph_address.exists() {
-            std::fs::create_dir(&graph_address)?;
-        }
-        if !dictionary_address.exists() {
-            std::fs::create_dir(&dictionary_address)?;
-        }
-        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
-        let (dictionary, mode) = NodeDictionary::new_reader(&dictionary_address)?;
-        let index = Index {
-            graphdb,
-            dictionary,
-        };
-        Ok((index, RMode(mode)))
-    }
-    pub fn start_reading(&self) -> RResult<GraphReader> {
-        Ok(GraphReader {
-            graph_txn: self.graphdb.ro_txn()?,
-            index: self,
-        })
-    }
-    pub fn start_writing(&self) -> RResult<GraphWriter> {
-        Ok(GraphWriter {
-            graph_txn: self.graphdb.rw_txn()?,
-            index: self,
-        })
-    }
-}
-
-pub struct GraphReader<'a> {
-    graph_txn: RoToken<'a>,
-    index: &'a Index,
-}
-impl<'a> GraphReader<'a> {
-    pub fn reload(&self, RMode(reader): &RMode) -> RResult<()> {
-        Ok(reader.reload()?)
-    }
-    pub fn get_outedges(
-        &'a self,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_outedges(&self.graph_txn, from)
-    }
-    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_inedges(&self.graph_txn, to)
-    }
-    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_node_ids(&self.graph_txn)
-    }
-    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_edge_ids(&self.graph_txn)
-    }
-    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
-        self.index.get_edge(&self.graph_txn, id)
-    }
-    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        self.index.get_edge_metadata(&self.graph_txn, id)
-    }
-    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
-        self.index.get_node(&self.graph_txn, id)
-    }
-    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
-        self.index.get_nodeid(&self.graph_txn, x)
-    }
-    pub fn prefix_search(&self, RMode(reader): &RMode, prefix: &str) -> RResult<Vec<String>> {
-        self.index.prefix_search(reader, prefix)
-    }
-    pub fn search<G: BfsGuide>(
-        &self,
-        guide: G,
-        max_depth: usize,
-        entry_points: Vec<Entity>,
-    ) -> RResult<impl Iterator<Item = GCnx>> {
-        self.index
-            .graph_search(&self.graph_txn, guide, max_depth, entry_points)
-    }
-    pub fn no_nodes(&self) -> RResult<u64> {
-        self.index.no_nodes(&self.graph_txn)
-    }
-}
-
-pub struct GraphWriter<'a> {
-    graph_txn: RwToken<'a>,
-    index: &'a Index,
-}
-impl<'a> GraphWriter<'a> {
-    pub fn commit(self, WMode(writer): &mut WMode) -> RResult<()> {
-        writer.commit()?;
-        self.graph_txn.commit()?;
-        Ok(())
-    }
-    pub fn get_outedges(
-        &'a self,
-        from: Entity,
-    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_outedges(&self.graph_txn, from)
-    }
-    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
-        self.index.get_inedges(&self.graph_txn, to)
-    }
-    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_node_ids(&self.graph_txn)
-    }
-    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
-        self.index.iter_edge_ids(&self.graph_txn)
-    }
-    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
-        self.index.get_edge(&self.graph_txn, id)
-    }
-    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
-        self.index.get_edge_metadata(&self.graph_txn, id)
-    }
-    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
-        self.index.get_node(&self.graph_txn, id)
-    }
-    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
-        self.index.get_nodeid(&self.graph_txn, x)
-    }
-    pub fn no_nodes(&self) -> RResult<u64> {
-        self.index.no_nodes(&self.graph_txn)
-    }
-    pub fn connect(
-        &mut self,
-        WMode(writer): &WMode,
-        from: &IoNode,
-        to: &IoNode,
-        edge: &IoEdge,
-        edge_metadata: Option<&IoEdgeMetadata>,
-    ) -> RResult<bool> {
-        self.index
-            .connect(&mut self.graph_txn, writer, from, to, edge, edge_metadata)
-    }
-    pub fn delete_node(
-        &mut self,
-        WMode(writer): &WMode,
-        node_id: Entity,
-    ) -> RResult<HashSet<Entity>> {
-        self.index.delete_node(&mut self.graph_txn, writer, node_id)
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    #[test]
-    pub fn create_and_insert() {
-        let dir = tempfile::TempDir::new().unwrap();
-        let (index, mut wmode) = Index::new_writer(dir.path()).unwrap();
-        let n1 = IoNode::new("N1".to_string(), "T1".to_string(), None);
-        let n2 = IoNode::new("N2".to_string(), "T2".to_string(), None);
-        let edge = IoEdge::new("T2".to_string(), None);
-        let mut writer = index.start_writing().unwrap();
-        writer.connect(&wmode, &n1, &n2, &edge, None).unwrap();
-        writer.connect(&wmode, &n2, &n1, &edge, None).unwrap();
-        writer.commit(&mut wmode).unwrap();
-        let (index, _rmode) = Index::new_reader(dir.path()).unwrap();
-        let reader = index.start_reading().unwrap();
-        std::mem::drop(reader);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashSet;
+use std::path::Path;
+
+use super::bfs_engine::{BfsEngineBuilder, BfsGuide};
+use super::errors::*;
+pub use super::graph_db::{Entity, GCnx};
+use super::graph_db::{GraphDB, RoToken, RwToken};
+use super::node_dictionary::{DReader, DWriter, NodeDictionary};
+pub use super::relations_io::{IoEdge, IoEdgeMetadata, IoNode};
+
+pub struct RMode(DReader);
+pub struct WMode(DWriter);
+
+pub struct Index {
+    graphdb: GraphDB,
+    dictionary: NodeDictionary,
+}
+impl Index {
+    const DICTIONARY_PATH: &str = "NodeDictionary";
+    const GRAPH_PATH: &str = "GraphDB";
+    const GRAPH_SIZE: usize = 1048576 * 100000;
+    fn connect(
+        &self,
+        graph_txn: &mut RwToken,
+        dict_writer: &DWriter,
+        from: &IoNode,
+        to: &IoNode,
+        edge: &IoEdge,
+        edge_metadata: Option<&IoEdgeMetadata>,
+    ) -> RResult<bool> {
+        self.dictionary.add_node(dict_writer, from)?;
+        self.dictionary.add_node(dict_writer, to)?;
+        let from = self.graphdb.add_node(graph_txn, from)?;
+        let to = self.graphdb.add_node(graph_txn, to)?;
+        self.graphdb
+            .connect(graph_txn, from, edge, to, edge_metadata)
+    }
+    fn graph_search<G: BfsGuide>(
+        &self,
+        txn: &RoToken,
+        guide: G,
+        max_depth: usize,
+        entry_points: Vec<Entity>,
+    ) -> RResult<impl Iterator<Item = GCnx>> {
+        BfsEngineBuilder::new()
+            .graph(&self.graphdb)
+            .txn(txn)
+            .max_depth(max_depth)
+            .guide(guide)
+            .entry_points(entry_points)
+            .build()
+            .unwrap()
+            .search()
+    }
+    fn delete_node(
+        &self,
+        graph_txn: &mut RwToken,
+        dict_writer: &DWriter,
+        node_id: Entity,
+    ) -> RResult<HashSet<Entity>> {
+        let value = self.graphdb.get_node(graph_txn, node_id)?;
+        self.dictionary.delete_node(dict_writer, &value);
+        self.graphdb.delete_node(graph_txn, node_id)
+    }
+    fn no_nodes(&self, txn: &RoToken) -> RResult<u64> {
+        self.graphdb.no_nodes(txn)
+    }
+    fn iter_node_ids<'a>(
+        &self,
+        txn: &'a RoToken,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.graphdb.iter_node_ids(txn)
+    }
+    fn iter_edge_ids<'a>(
+        &self,
+        txn: &'a RoToken,
+    ) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.graphdb.iter_edge_ids(txn)
+    }
+    pub fn get_outedges<'a>(
+        &self,
+        txn: &'a RoToken,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.graphdb.get_outedges(txn, from)
+    }
+    fn get_edge(&self, txn: &RoToken, id: Entity) -> RResult<IoEdge> {
+        self.graphdb.get_edge(txn, id)
+    }
+    fn get_edge_metadata(&self, txn: &RoToken, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        self.graphdb.get_edge_metadata(txn, id)
+    }
+    fn get_node(&self, txn: &RoToken, id: Entity) -> RResult<IoNode> {
+        self.graphdb.get_node(txn, id)
+    }
+    fn get_nodeid(&self, txn: &RoToken, x: &str) -> RResult<Option<Entity>> {
+        self.graphdb.get_nodeid(txn, x)
+    }
+    fn prefix_search(&self, dict_reader: &DReader, prefix: &str) -> RResult<Vec<String>> {
+        self.dictionary.search(dict_reader, prefix)
+    }
+    fn get_inedges<'a>(
+        &self,
+        txn: &'a RoToken,
+        to: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.graphdb.get_inedges(txn, to)
+    }
+    pub fn new_writer(location: &Path) -> RResult<(Index, WMode)> {
+        let dictionary_address = location.join(Self::DICTIONARY_PATH);
+        let graph_address = location.join(Self::GRAPH_PATH);
+        if !graph_address.exists() {
+            std::fs::create_dir(&graph_address)?;
+        }
+        if !dictionary_address.exists() {
+            std::fs::create_dir(&dictionary_address)?;
+        }
+        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
+        let (dictionary, mode) = NodeDictionary::new_writer(&dictionary_address)?;
+        let index = Index {
+            graphdb,
+            dictionary,
+        };
+        Ok((index, WMode(mode)))
+    }
+    pub fn new_reader(location: &Path) -> RResult<(Index, RMode)> {
+        let dictionary_address = location.join(Self::DICTIONARY_PATH);
+        let graph_address = location.join(Self::GRAPH_PATH);
+        if !graph_address.exists() {
+            std::fs::create_dir(&graph_address)?;
+        }
+        if !dictionary_address.exists() {
+            std::fs::create_dir(&dictionary_address)?;
+        }
+        let graphdb = GraphDB::new(&graph_address, Self::GRAPH_SIZE)?;
+        let (dictionary, mode) = NodeDictionary::new_reader(&dictionary_address)?;
+        let index = Index {
+            graphdb,
+            dictionary,
+        };
+        Ok((index, RMode(mode)))
+    }
+    pub fn start_reading(&self) -> RResult<GraphReader> {
+        Ok(GraphReader {
+            graph_txn: self.graphdb.ro_txn()?,
+            index: self,
+        })
+    }
+    pub fn start_writing(&self) -> RResult<GraphWriter> {
+        Ok(GraphWriter {
+            graph_txn: self.graphdb.rw_txn()?,
+            index: self,
+        })
+    }
+}
+
+pub struct GraphReader<'a> {
+    graph_txn: RoToken<'a>,
+    index: &'a Index,
+}
+impl<'a> GraphReader<'a> {
+    pub fn reload(&self, RMode(reader): &RMode) -> RResult<()> {
+        Ok(reader.reload()?)
+    }
+    pub fn get_outedges(
+        &'a self,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_outedges(&self.graph_txn, from)
+    }
+    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_inedges(&self.graph_txn, to)
+    }
+    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_node_ids(&self.graph_txn)
+    }
+    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_edge_ids(&self.graph_txn)
+    }
+    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
+        self.index.get_edge(&self.graph_txn, id)
+    }
+    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        self.index.get_edge_metadata(&self.graph_txn, id)
+    }
+    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
+        self.index.get_node(&self.graph_txn, id)
+    }
+    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
+        self.index.get_nodeid(&self.graph_txn, x)
+    }
+    pub fn prefix_search(&self, RMode(reader): &RMode, prefix: &str) -> RResult<Vec<String>> {
+        self.index.prefix_search(reader, prefix)
+    }
+    pub fn search<G: BfsGuide>(
+        &self,
+        guide: G,
+        max_depth: usize,
+        entry_points: Vec<Entity>,
+    ) -> RResult<impl Iterator<Item = GCnx>> {
+        self.index
+            .graph_search(&self.graph_txn, guide, max_depth, entry_points)
+    }
+    pub fn no_nodes(&self) -> RResult<u64> {
+        self.index.no_nodes(&self.graph_txn)
+    }
+}
+
+pub struct GraphWriter<'a> {
+    graph_txn: RwToken<'a>,
+    index: &'a Index,
+}
+impl<'a> GraphWriter<'a> {
+    pub fn commit(self, WMode(writer): &mut WMode) -> RResult<()> {
+        writer.commit()?;
+        self.graph_txn.commit()?;
+        Ok(())
+    }
+    pub fn get_outedges(
+        &'a self,
+        from: Entity,
+    ) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_outedges(&self.graph_txn, from)
+    }
+    pub fn get_inedges(&'a self, to: Entity) -> RResult<impl Iterator<Item = RResult<GCnx>> + 'a> {
+        self.index.get_inedges(&self.graph_txn, to)
+    }
+    pub fn iter_node_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_node_ids(&self.graph_txn)
+    }
+    pub fn iter_edge_ids(&'a self) -> RResult<impl Iterator<Item = RResult<Entity>> + 'a> {
+        self.index.iter_edge_ids(&self.graph_txn)
+    }
+    pub fn get_edge(&self, id: Entity) -> RResult<IoEdge> {
+        self.index.get_edge(&self.graph_txn, id)
+    }
+    pub fn get_edge_metadata(&self, id: Entity) -> RResult<Option<IoEdgeMetadata>> {
+        self.index.get_edge_metadata(&self.graph_txn, id)
+    }
+    pub fn get_node(&self, id: Entity) -> RResult<IoNode> {
+        self.index.get_node(&self.graph_txn, id)
+    }
+    pub fn get_node_id(&self, x: &str) -> RResult<Option<Entity>> {
+        self.index.get_nodeid(&self.graph_txn, x)
+    }
+    pub fn no_nodes(&self) -> RResult<u64> {
+        self.index.no_nodes(&self.graph_txn)
+    }
+    pub fn connect(
+        &mut self,
+        WMode(writer): &WMode,
+        from: &IoNode,
+        to: &IoNode,
+        edge: &IoEdge,
+        edge_metadata: Option<&IoEdgeMetadata>,
+    ) -> RResult<bool> {
+        self.index
+            .connect(&mut self.graph_txn, writer, from, to, edge, edge_metadata)
+    }
+    pub fn delete_node(
+        &mut self,
+        WMode(writer): &WMode,
+        node_id: Entity,
+    ) -> RResult<HashSet<Entity>> {
+        self.index.delete_node(&mut self.graph_txn, writer, node_id)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    #[test]
+    pub fn create_and_insert() {
+        let dir = tempfile::TempDir::new().unwrap();
+        let (index, mut wmode) = Index::new_writer(dir.path()).unwrap();
+        let n1 = IoNode::new("N1".to_string(), "T1".to_string(), None);
+        let n2 = IoNode::new("N2".to_string(), "T2".to_string(), None);
+        let edge = IoEdge::new("T2".to_string(), None);
+        let mut writer = index.start_writing().unwrap();
+        writer.connect(&wmode, &n1, &n2, &edge, None).unwrap();
+        writer.connect(&wmode, &n2, &n1, &edge, None).unwrap();
+        writer.commit(&mut wmode).unwrap();
+        let (index, _rmode) = Index::new_reader(dir.path()).unwrap();
+        let reader = index.start_reading().unwrap();
+        std::mem::drop(reader);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/lib.rs`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod bfs_engine;
-mod errors;
-mod graph_db;
-#[cfg(test)]
-mod graph_test_utils;
-pub mod index;
-mod node_dictionary;
-mod relations_io;
-pub mod service;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod bfs_engine;
+mod errors;
+mod graph_db;
+#[cfg(test)]
+mod graph_test_utils;
+pub mod index;
+mod node_dictionary;
+mod relations_io;
+pub mod service;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/node_dictionary.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/node_dictionary.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,156 +1,156 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::path::Path;
-
-use itertools::Itertools;
-use tantivy::collector::DocSetCollector;
-use tantivy::query::RegexQuery;
-use tantivy::schema::{Field, Schema, Term, TextFieldIndexing, TextOptions, STORED, STRING};
-use tantivy::{doc, Index, IndexReader, IndexWriter, ReloadPolicy};
-
-use super::errors::*;
-use super::relations_io::IoNode;
-
-pub type DReader = IndexReader;
-pub type DWriter = IndexWriter;
-
-pub struct NodeDictionary {
-    node_value: Field,
-    node_hash: Field,
-    #[allow(unused)]
-    index: Index,
-}
-impl NodeDictionary {
-    const NODE_VALUE: &str = "value";
-    const NODE_HASH: &str = "hash";
-    const NUM_THREADS: usize = 1;
-    const MEM_LIMIT: usize = 6_000_000;
-
-    fn adapt_text(&self, text: &str) -> String {
-        deunicode::deunicode(text).to_lowercase()
-    }
-    fn build_query(&self, text: &str) -> String {
-        let query = text
-            .split(' ')
-            .filter(|s| !s.is_empty())
-            .map(|s| regex::escape(s.trim()))
-            .join(r"\s+");
-        format!("(?im){query}.*")
-    }
-    fn new(path: &Path) -> RResult<NodeDictionary> {
-        let text_options = TextOptions::default()
-            .set_indexing_options(TextFieldIndexing::default().set_tokenizer("raw"))
-            .set_stored();
-        let mut schema_builder = Schema::builder();
-        let node_hash = schema_builder.add_text_field(Self::NODE_HASH, STRING | STORED);
-        let node_value = schema_builder.add_text_field(Self::NODE_VALUE, text_options);
-        let schema = schema_builder.build();
-        let index = Index::create_in_dir(path, schema).or_else(|_| Index::open_in_dir(path))?;
-        Ok(NodeDictionary {
-            index,
-            node_hash,
-            node_value,
-        })
-    }
-    pub fn new_writer(path: &Path) -> RResult<(NodeDictionary, IndexWriter)> {
-        let dictionary = Self::new(path)?;
-        let writer = dictionary
-            .index
-            .writer_with_num_threads(Self::NUM_THREADS, Self::MEM_LIMIT)?;
-        Ok((dictionary, writer))
-    }
-    pub fn new_reader(path: &Path) -> RResult<(NodeDictionary, IndexReader)> {
-        let dictionary = Self::new(path)?;
-        let reader = dictionary
-            .index
-            .reader_builder()
-            .reload_policy(ReloadPolicy::OnCommit)
-            .try_into()?;
-        Ok((dictionary, reader))
-    }
-    pub fn search(&self, reader: &IndexReader, query: &str) -> RResult<Vec<String>> {
-        let query = self.adapt_text(query);
-        let query = self.build_query(&query);
-        let termq = Box::new(RegexQuery::from_pattern(&query, self.node_value)?);
-        let collector = DocSetCollector;
-        let searcher = reader.searcher();
-        let results = searcher
-            .search(termq.as_ref(), &collector)?
-            .into_iter()
-            .flat_map(|d| searcher.doc(d).ok())
-            .flat_map(|d| {
-                d.get_first(self.node_hash)
-                    .and_then(|v| v.as_text())
-                    .map(|v| v.to_string())
-            })
-            .collect();
-        Ok(results)
-    }
-    pub fn add_node(&self, writer: &IndexWriter, node: &IoNode) -> RResult<()> {
-        let document = doc!(
-            self.node_hash => node.hash(),
-            self.node_value => self.adapt_text(node.name())
-        );
-        self.delete_node(writer, node);
-        writer.add_document(document)?;
-        Ok(())
-    }
-    pub fn delete_node(&self, writer: &IndexWriter, node: &IoNode) {
-        writer.delete_term(Term::from_field_text(self.node_hash, node.hash()));
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use super::*;
-    #[test]
-    fn search_test() {
-        let dir = tempfile::tempdir().unwrap();
-        let (index, mut writer) = NodeDictionary::new_writer(dir.path()).unwrap();
-        let node = IoNode::new("New york".to_string(), "Untyped".to_string(), None);
-        index.add_node(&writer, &node).unwrap();
-        let node = IoNode::new("Barcelona".to_string(), "Untyped".to_string(), None);
-        index.add_node(&writer, &node).unwrap();
-        writer.commit().unwrap();
-        let (index, reader) = NodeDictionary::new_reader(dir.path()).unwrap();
-        let r1 = index.search(&reader, "new york").unwrap();
-        let r2 = index.search(&reader, "new York").unwrap();
-        let r3 = index.search(&reader, "new").unwrap();
-        let r4 = index.search(&reader, "york").unwrap();
-        let r5 = index.search(&reader, "bár").unwrap();
-
-        assert_eq!(r1.len(), 1);
-        assert_eq!(r2.len(), 1);
-        assert_eq!(r3.len(), 1);
-        assert_eq!(r4.len(), 0);
-        assert_eq!(r5.len(), 1);
-    }
-    #[test]
-    fn open_reader() {
-        let dir = tempfile::tempdir().unwrap();
-        NodeDictionary::new_reader(dir.path()).unwrap();
-    }
-    #[test]
-    fn open_writer() {
-        let dir = tempfile::tempdir().unwrap();
-        NodeDictionary::new_writer(dir.path()).unwrap();
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::path::Path;
+
+use itertools::Itertools;
+use tantivy::collector::DocSetCollector;
+use tantivy::query::RegexQuery;
+use tantivy::schema::{Field, Schema, Term, TextFieldIndexing, TextOptions, STORED, STRING};
+use tantivy::{doc, Index, IndexReader, IndexWriter, ReloadPolicy};
+
+use super::errors::*;
+use super::relations_io::IoNode;
+
+pub type DReader = IndexReader;
+pub type DWriter = IndexWriter;
+
+pub struct NodeDictionary {
+    node_value: Field,
+    node_hash: Field,
+    #[allow(unused)]
+    index: Index,
+}
+impl NodeDictionary {
+    const NODE_VALUE: &str = "value";
+    const NODE_HASH: &str = "hash";
+    const NUM_THREADS: usize = 1;
+    const MEM_LIMIT: usize = 6_000_000;
+
+    fn adapt_text(&self, text: &str) -> String {
+        deunicode::deunicode(text).to_lowercase()
+    }
+    fn build_query(&self, text: &str) -> String {
+        let query = text
+            .split(' ')
+            .filter(|s| !s.is_empty())
+            .map(|s| regex::escape(s.trim()))
+            .join(r"\s+");
+        format!("(?im){query}.*")
+    }
+    fn new(path: &Path) -> RResult<NodeDictionary> {
+        let text_options = TextOptions::default()
+            .set_indexing_options(TextFieldIndexing::default().set_tokenizer("raw"))
+            .set_stored();
+        let mut schema_builder = Schema::builder();
+        let node_hash = schema_builder.add_text_field(Self::NODE_HASH, STRING | STORED);
+        let node_value = schema_builder.add_text_field(Self::NODE_VALUE, text_options);
+        let schema = schema_builder.build();
+        let index = Index::create_in_dir(path, schema).or_else(|_| Index::open_in_dir(path))?;
+        Ok(NodeDictionary {
+            index,
+            node_hash,
+            node_value,
+        })
+    }
+    pub fn new_writer(path: &Path) -> RResult<(NodeDictionary, IndexWriter)> {
+        let dictionary = Self::new(path)?;
+        let writer = dictionary
+            .index
+            .writer_with_num_threads(Self::NUM_THREADS, Self::MEM_LIMIT)?;
+        Ok((dictionary, writer))
+    }
+    pub fn new_reader(path: &Path) -> RResult<(NodeDictionary, IndexReader)> {
+        let dictionary = Self::new(path)?;
+        let reader = dictionary
+            .index
+            .reader_builder()
+            .reload_policy(ReloadPolicy::OnCommit)
+            .try_into()?;
+        Ok((dictionary, reader))
+    }
+    pub fn search(&self, reader: &IndexReader, query: &str) -> RResult<Vec<String>> {
+        let query = self.adapt_text(query);
+        let query = self.build_query(&query);
+        let termq = Box::new(RegexQuery::from_pattern(&query, self.node_value)?);
+        let collector = DocSetCollector;
+        let searcher = reader.searcher();
+        let results = searcher
+            .search(termq.as_ref(), &collector)?
+            .into_iter()
+            .flat_map(|d| searcher.doc(d).ok())
+            .flat_map(|d| {
+                d.get_first(self.node_hash)
+                    .and_then(|v| v.as_text())
+                    .map(|v| v.to_string())
+            })
+            .collect();
+        Ok(results)
+    }
+    pub fn add_node(&self, writer: &IndexWriter, node: &IoNode) -> RResult<()> {
+        let document = doc!(
+            self.node_hash => node.hash(),
+            self.node_value => self.adapt_text(node.name())
+        );
+        self.delete_node(writer, node);
+        writer.add_document(document)?;
+        Ok(())
+    }
+    pub fn delete_node(&self, writer: &IndexWriter, node: &IoNode) {
+        writer.delete_term(Term::from_field_text(self.node_hash, node.hash()));
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use super::*;
+    #[test]
+    fn search_test() {
+        let dir = tempfile::tempdir().unwrap();
+        let (index, mut writer) = NodeDictionary::new_writer(dir.path()).unwrap();
+        let node = IoNode::new("New york".to_string(), "Untyped".to_string(), None);
+        index.add_node(&writer, &node).unwrap();
+        let node = IoNode::new("Barcelona".to_string(), "Untyped".to_string(), None);
+        index.add_node(&writer, &node).unwrap();
+        writer.commit().unwrap();
+        let (index, reader) = NodeDictionary::new_reader(dir.path()).unwrap();
+        let r1 = index.search(&reader, "new york").unwrap();
+        let r2 = index.search(&reader, "new York").unwrap();
+        let r3 = index.search(&reader, "new").unwrap();
+        let r4 = index.search(&reader, "york").unwrap();
+        let r5 = index.search(&reader, "bár").unwrap();
+
+        assert_eq!(r1.len(), 1);
+        assert_eq!(r2.len(), 1);
+        assert_eq!(r3.len(), 1);
+        assert_eq!(r4.len(), 0);
+        assert_eq!(r5.len(), 1);
+    }
+    #[test]
+    fn open_reader() {
+        let dir = tempfile::tempdir().unwrap();
+        NodeDictionary::new_reader(dir.path()).unwrap();
+    }
+    #[test]
+    fn open_writer() {
+        let dir = tempfile::tempdir().unwrap();
+        NodeDictionary::new_writer(dir.path()).unwrap();
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/relations_io.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/relations_io.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::hash::Hash;
-
-use data_encoding::HEXUPPER;
-use nucliadb_core::protos::RelationMetadata;
-use ring::digest::{Context, SHA256};
-use serde::{Deserialize, Serialize};
-
-pub fn compute_hash<D: AsRef<[u8]>>(data: &[D]) -> String {
-    let mut context = Context::new(&SHA256);
-    data.iter().for_each(|d| context.update(d.as_ref()));
-    let digest = context.finish();
-    HEXUPPER.encode(digest.as_ref())
-}
-
-#[derive(Default, Debug, Serialize, Deserialize, Clone, Copy, Eq, Ord, PartialEq, PartialOrd)]
-pub enum Source {
-    #[default]
-    Null,
-    User,
-    System,
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd)]
-pub struct IoNode {
-    source: Source,
-    name: String,
-    xtype: String,
-    subtype: Option<String>,
-    hash: String,
-}
-impl IoNode {
-    fn inner_new(source: Source, name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        let hash = compute_hash(&[
-            name.as_bytes(),
-            xtype.as_bytes(),
-            subtype.as_ref().map(|s| s.as_bytes()).unwrap_or(&[]),
-        ]);
-        IoNode {
-            name,
-            xtype,
-            subtype,
-            hash,
-            source,
-        }
-    }
-    pub fn user_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        IoNode::inner_new(Source::User, name, xtype, subtype)
-    }
-    pub fn system_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        IoNode::inner_new(Source::System, name, xtype, subtype)
-    }
-    pub fn new(name: String, xtype: String, subtype: Option<String>) -> IoNode {
-        IoNode::inner_new(Source::default(), name, xtype, subtype)
-    }
-    pub fn name(&self) -> &str {
-        &self.name
-    }
-    pub fn xtype(&self) -> &str {
-        &self.xtype
-    }
-    pub fn subtype(&self) -> Option<&str> {
-        self.subtype.as_deref()
-    }
-    pub fn defined_by_user(&self) -> bool {
-        self.source == Source::User
-    }
-    pub fn hash(&self) -> &str {
-        &self.hash
-    }
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
-pub struct IoEdgeMetadata {
-    pub paragraph_id: Option<String>,
-    pub source_start: Option<i32>,
-    pub source_end: Option<i32>,
-    pub to_start: Option<i32>,
-    pub to_end: Option<i32>,
-}
-impl From<RelationMetadata> for IoEdgeMetadata {
-    fn from(value: RelationMetadata) -> Self {
-        IoEdgeMetadata {
-            paragraph_id: value.paragraph_id,
-            source_start: value.source_start,
-            source_end: value.source_end,
-            to_start: value.to_start,
-            to_end: value.to_end,
-        }
-    }
-}
-
-impl From<IoEdgeMetadata> for RelationMetadata {
-    fn from(value: IoEdgeMetadata) -> Self {
-        RelationMetadata {
-            paragraph_id: value.paragraph_id,
-            source_start: value.source_start,
-            source_end: value.source_end,
-            to_start: value.to_start,
-            to_end: value.to_end,
-        }
-    }
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
-pub struct IoEdge {
-    xtype: String,
-    subtype: Option<String>,
-}
-impl IoEdge {
-    pub fn new(xtype: String, subtype: Option<String>) -> IoEdge {
-        IoEdge { xtype, subtype }
-    }
-    pub fn xtype(&self) -> &str {
-        &self.xtype
-    }
-    pub fn subtype(&self) -> Option<&str> {
-        self.subtype.as_deref()
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use crate::graph_db::*;
-    #[test]
-    fn graph_insertion() {
-        let dir = tempfile::tempdir().unwrap();
-        let graph = GraphDB::new(dir.path(), 1048576 * 100000).unwrap();
-        let node1 = IoNode::new("N1".to_string(), "T1".to_string(), Some("ST1".to_string()));
-        let node1p = IoNode::new("N1".to_string(), "T1".to_string(), None);
-        let node2 = IoNode::new("N2".to_string(), "T2".to_string(), Some("ST2".to_string()));
-        let mut txn = graph.rw_txn().unwrap();
-        let node1_uid = graph.add_node(&mut txn, &node1).unwrap();
-        let node1_uidf = graph.add_node(&mut txn, &node1).unwrap();
-        assert_eq!(node1_uid, node1_uidf);
-        let node1p_uid = graph.add_node(&mut txn, &node1p).unwrap();
-        assert_ne!(node1_uid, node1p_uid);
-        let node2_uid = graph.add_node(&mut txn, &node2).unwrap();
-        assert_ne!(node1_uid, node2_uid);
-        assert_ne!(node1p_uid, node2_uid);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::hash::Hash;
+
+use data_encoding::HEXUPPER;
+use nucliadb_core::protos::RelationMetadata;
+use ring::digest::{Context, SHA256};
+use serde::{Deserialize, Serialize};
+
+pub fn compute_hash<D: AsRef<[u8]>>(data: &[D]) -> String {
+    let mut context = Context::new(&SHA256);
+    data.iter().for_each(|d| context.update(d.as_ref()));
+    let digest = context.finish();
+    HEXUPPER.encode(digest.as_ref())
+}
+
+#[derive(Default, Debug, Serialize, Deserialize, Clone, Copy, Eq, Ord, PartialEq, PartialOrd)]
+pub enum Source {
+    #[default]
+    Null,
+    User,
+    System,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd)]
+pub struct IoNode {
+    source: Source,
+    name: String,
+    xtype: String,
+    subtype: Option<String>,
+    hash: String,
+}
+impl IoNode {
+    fn inner_new(source: Source, name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        let hash = compute_hash(&[
+            name.as_bytes(),
+            xtype.as_bytes(),
+            subtype.as_ref().map(|s| s.as_bytes()).unwrap_or(&[]),
+        ]);
+        IoNode {
+            name,
+            xtype,
+            subtype,
+            hash,
+            source,
+        }
+    }
+    pub fn user_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        IoNode::inner_new(Source::User, name, xtype, subtype)
+    }
+    pub fn system_node(name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        IoNode::inner_new(Source::System, name, xtype, subtype)
+    }
+    pub fn new(name: String, xtype: String, subtype: Option<String>) -> IoNode {
+        IoNode::inner_new(Source::default(), name, xtype, subtype)
+    }
+    pub fn name(&self) -> &str {
+        &self.name
+    }
+    pub fn xtype(&self) -> &str {
+        &self.xtype
+    }
+    pub fn subtype(&self) -> Option<&str> {
+        self.subtype.as_deref()
+    }
+    pub fn defined_by_user(&self) -> bool {
+        self.source == Source::User
+    }
+    pub fn hash(&self) -> &str {
+        &self.hash
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
+pub struct IoEdgeMetadata {
+    pub paragraph_id: Option<String>,
+    pub source_start: Option<i32>,
+    pub source_end: Option<i32>,
+    pub to_start: Option<i32>,
+    pub to_end: Option<i32>,
+}
+impl From<RelationMetadata> for IoEdgeMetadata {
+    fn from(value: RelationMetadata) -> Self {
+        IoEdgeMetadata {
+            paragraph_id: value.paragraph_id,
+            source_start: value.source_start,
+            source_end: value.source_end,
+            to_start: value.to_start,
+            to_end: value.to_end,
+        }
+    }
+}
+
+impl From<IoEdgeMetadata> for RelationMetadata {
+    fn from(value: IoEdgeMetadata) -> Self {
+        RelationMetadata {
+            paragraph_id: value.paragraph_id,
+            source_start: value.source_start,
+            source_end: value.source_end,
+            to_start: value.to_start,
+            to_end: value.to_end,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone, Eq, Ord, PartialEq, PartialOrd, Hash)]
+pub struct IoEdge {
+    xtype: String,
+    subtype: Option<String>,
+}
+impl IoEdge {
+    pub fn new(xtype: String, subtype: Option<String>) -> IoEdge {
+        IoEdge { xtype, subtype }
+    }
+    pub fn xtype(&self) -> &str {
+        &self.xtype
+    }
+    pub fn subtype(&self) -> Option<&str> {
+        self.subtype.as_deref()
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::graph_db::*;
+    #[test]
+    fn graph_insertion() {
+        let dir = tempfile::tempdir().unwrap();
+        let graph = GraphDB::new(dir.path(), 1048576 * 100000).unwrap();
+        let node1 = IoNode::new("N1".to_string(), "T1".to_string(), Some("ST1".to_string()));
+        let node1p = IoNode::new("N1".to_string(), "T1".to_string(), None);
+        let node2 = IoNode::new("N2".to_string(), "T2".to_string(), Some("ST2".to_string()));
+        let mut txn = graph.rw_txn().unwrap();
+        let node1_uid = graph.add_node(&mut txn, &node1).unwrap();
+        let node1_uidf = graph.add_node(&mut txn, &node1).unwrap();
+        assert_eq!(node1_uid, node1_uidf);
+        let node1p_uid = graph.add_node(&mut txn, &node1p).unwrap();
+        assert_ne!(node1_uid, node1p_uid);
+        let node2_uid = graph.add_node(&mut txn, &node2).unwrap();
+        assert_ne!(node1_uid, node2_uid);
+        assert_ne!(node1p_uid, node2_uid);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/bfs.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/bfs.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,78 +1,78 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashSet;
-use std::fmt::Debug;
-
-use nucliadb_core::tracing::*;
-
-use crate::bfs_engine::*;
-use crate::errors::*;
-use crate::index::*;
-
-pub struct GrpcGuide<'a> {
-    pub reader: &'a GraphReader<'a>,
-    pub node_filters: HashSet<(&'a str, Option<&'a str>)>,
-    pub edge_filters: HashSet<(&'a str, Option<&'a str>)>,
-    pub jump_always: &'a str,
-}
-impl<'a> GrpcGuide<'a> {
-    fn treat_bfs_error<A, B, F>(&self, default: B, input: A, f: F) -> B
-    where
-        F: Fn(A) -> RResult<B>,
-        A: Debug + Copy,
-        B: Default,
-    {
-        match f(input) {
-            Err(e) => {
-                info!("{e:?} during BFS looking at {input:?}");
-                default
-            }
-            Ok(result) => result,
-        }
-    }
-}
-impl<'a> BfsGuide for GrpcGuide<'a> {
-    fn edge_allowed(&self, edge: Entity) -> bool {
-        self.treat_bfs_error(false, edge, |edge: Entity| {
-            self.reader.get_edge(edge).map(|edge| {
-                self.edge_filters.is_empty()
-                    || self.edge_filters.contains(&(edge.xtype(), edge.subtype()))
-                    || self.edge_filters.contains(&(edge.xtype(), None))
-            })
-        })
-    }
-    fn node_allowed(&self, node: Entity) -> bool {
-        self.treat_bfs_error(false, node, |node: Entity| {
-            self.reader.get_node(node).map(|node| {
-                self.node_filters.is_empty()
-                    || self.node_filters.contains(&(node.xtype(), node.subtype()))
-                    || self.node_filters.contains(&(node.xtype(), None))
-            })
-        })
-    }
-    fn free_jump(&self, cnx: GCnx) -> bool {
-        self.treat_bfs_error(false, cnx.edge(), |edge: Entity| {
-            self.reader
-                .get_edge(edge)
-                .map(|edge| edge.xtype() == self.jump_always)
-        })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashSet;
+use std::fmt::Debug;
+
+use nucliadb_core::tracing::*;
+
+use crate::bfs_engine::*;
+use crate::errors::*;
+use crate::index::*;
+
+pub struct GrpcGuide<'a> {
+    pub reader: &'a GraphReader<'a>,
+    pub node_filters: HashSet<(&'a str, Option<&'a str>)>,
+    pub edge_filters: HashSet<(&'a str, Option<&'a str>)>,
+    pub jump_always: &'a str,
+}
+impl<'a> GrpcGuide<'a> {
+    fn treat_bfs_error<A, B, F>(&self, default: B, input: A, f: F) -> B
+    where
+        F: Fn(A) -> RResult<B>,
+        A: Debug + Copy,
+        B: Default,
+    {
+        match f(input) {
+            Err(e) => {
+                info!("{e:?} during BFS looking at {input:?}");
+                default
+            }
+            Ok(result) => result,
+        }
+    }
+}
+impl<'a> BfsGuide for GrpcGuide<'a> {
+    fn edge_allowed(&self, edge: Entity) -> bool {
+        self.treat_bfs_error(false, edge, |edge: Entity| {
+            self.reader.get_edge(edge).map(|edge| {
+                self.edge_filters.is_empty()
+                    || self.edge_filters.contains(&(edge.xtype(), edge.subtype()))
+                    || self.edge_filters.contains(&(edge.xtype(), None))
+            })
+        })
+    }
+    fn node_allowed(&self, node: Entity) -> bool {
+        self.treat_bfs_error(false, node, |node: Entity| {
+            self.reader.get_node(node).map(|node| {
+                self.node_filters.is_empty()
+                    || self.node_filters.contains(&(node.xtype(), node.subtype()))
+                    || self.node_filters.contains(&(node.xtype(), None))
+            })
+        })
+    }
+    fn free_jump(&self, cnx: GCnx) -> bool {
+        self.treat_bfs_error(false, cnx.edge(), |edge: Entity| {
+            self.reader
+                .get_edge(edge)
+                .map(|edge| edge.xtype() == self.jump_always)
+        })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/mod.rs`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod bfs;
-pub mod reader;
-#[cfg(test)]
-mod tests;
-mod utils;
-pub mod writer;
-
-pub use reader::*;
-pub use writer::*;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod bfs;
+pub mod reader;
+#[cfg(test)]
+mod tests;
+mod utils;
+pub mod writer;
+
+pub use reader::*;
+pub use writer::*;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/reader.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/reader.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,361 +1,403 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashSet;
-use std::fmt::Debug;
-use std::time::SystemTime;
-
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::*;
-use nucliadb_core::tracing::{self, *};
-
-use super::bfs::GrpcGuide;
-use super::utils::*;
-use crate::index::*;
-use crate::relations_io;
-
-pub struct RelationsReaderService {
-    rmode: RMode,
-    index: Index,
-}
-impl Debug for RelationsReaderService {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("RelationsReaderService").finish()
-    }
-}
-
-impl RelationsReaderService {
-    #[tracing::instrument(skip_all)]
-    fn graph_search(
-        &self,
-        request: &RelationSearchRequest,
-    ) -> NodeResult<Option<EntitiesSubgraphResponse>> {
-        let Some(bfs_request) = request.subgraph.as_ref() else {
-            return Ok(None);
-        };
-
-        let id = Some(&request.shard_id);
-        let time = SystemTime::now();
-        let reader = self.index.start_reading()?;
-        let depth = bfs_request.depth.map(|v| v as usize).unwrap_or(usize::MAX);
-        let mut entry_points = Vec::with_capacity(bfs_request.entry_points.len());
-        let mut node_filters = HashSet::with_capacity(bfs_request.node_filters.len());
-        let mut edge_filters = HashSet::with_capacity(bfs_request.edge_filters.len());
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} -  Creating entry points: starts {v} ms");
-        }
-        for node in bfs_request.entry_points.iter() {
-            let name = node.value.clone();
-            let type_info = node_type_parsing(node.ntype(), &node.subtype);
-            let xtype = type_info.0.to_string();
-            let subtype = type_info.1.map(|s| s.to_string());
-            let node = IoNode::new(name, xtype, subtype);
-            match reader.get_node_id(node.hash()) {
-                Ok(None) => (),
-                Ok(Some(id)) => entry_points.push(id),
-                Err(e) => error!("{e:?} during {node:?}"),
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} -  Creating entry points: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - adding query type filters: starts {v} ms");
-        }
-        bfs_request.node_filters.iter().for_each(|filter| {
-            let node_type = filter.node_type();
-            let node_subtype = filter
-                .node_subtype
-                .as_ref()
-                .map_or_else(|| "", |subtype| subtype);
-            let type_info = node_type_parsing(node_type, node_subtype);
-            node_filters.insert(type_info);
-        });
-        bfs_request.edge_filters.iter().for_each(|filter| {
-            let relation_type = filter.relation_type();
-            let relation_subtype = filter
-                .relation_subtype
-                .as_ref()
-                .map_or_else(|| "", |subtype| subtype);
-            let type_info = relation_type_parsing(relation_type, relation_subtype);
-            edge_filters.insert(type_info);
-        });
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - adding query type filters: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - running the search: starts {v} ms");
-        }
-        let guide = GrpcGuide {
-            node_filters,
-            edge_filters,
-            reader: &reader,
-            jump_always: dictionary::SYNONYM,
-        };
-        let mut subgraph = vec![];
-        for i in reader.search(guide, depth, entry_points)? {
-            let from = reader.get_node(i.from()).map(|node| RelationNode {
-                value: node.name().to_string(),
-                subtype: node.subtype().map(|s| s.to_string()).unwrap_or_default(),
-                ntype: string_to_node_type(node.xtype()) as i32,
-            })?;
-
-            let to = reader.get_node(i.to()).map(|node| RelationNode {
-                value: node.name().to_string(),
-                subtype: node.subtype().map(|s| s.to_string()).unwrap_or_default(),
-                ntype: string_to_node_type(node.xtype()) as i32,
-            })?;
-            let relation_metadata = reader.get_edge_metadata(i.edge())?;
-            let relation = reader.get_edge(i.edge()).map(|edge| Relation {
-                to: Some(to),
-                source: Some(from),
-                relation: string_to_rtype(edge.xtype()) as i32,
-                metadata: relation_metadata.map(RelationMetadata::from),
-                relation_label: edge.subtype().map(|s| s.to_string()).unwrap_or_default(),
-            })?;
-            subgraph.push(relation);
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - running the search: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - Ending at {v} ms");
-        }
-        Ok(Some(EntitiesSubgraphResponse {
-            relations: subgraph,
-        }))
-    }
-    #[tracing::instrument(skip_all)]
-    fn prefix_search(
-        &self,
-        request: &RelationSearchRequest,
-    ) -> NodeResult<Option<RelationPrefixSearchResponse>> {
-        use crate::bfs_engine::BfsGuide;
-        let Some(prefix_request) = request.prefix.as_ref() else {
-            return Ok(None);
-        };
-
-        let id = Some(&request.shard_id);
-        let time = SystemTime::now();
-        let prefix = &prefix_request.prefix;
-        let reader = self.index.start_reading()?;
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - running prefix search: starts {v} ms");
-        }
-        let prefixes = reader
-            .prefix_search(&self.rmode, prefix)?
-            .into_iter()
-            .flat_map(|key| reader.get_node_id(&key).ok().flatten());
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - running prefix search: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - generating results: starts {v} ms");
-        }
-
-        let mut node_filters = HashSet::new();
-        prefix_request.node_filters.iter().for_each(|filter| {
-            let node_type = filter.node_type();
-            let node_subtype = filter
-                .node_subtype
-                .as_ref()
-                .map_or_else(|| "", |subtype| subtype);
-            let type_info = node_type_parsing(node_type, node_subtype);
-            node_filters.insert(type_info);
-        });
-
-        let guide = GrpcGuide {
-            node_filters,
-            edge_filters: HashSet::new(),
-            reader: &reader,
-            jump_always: dictionary::SYNONYM,
-        };
-
-        let nodes = prefixes
-            .into_iter()
-            .filter(|n| guide.node_allowed(*n))
-            .map(|id| {
-                reader.get_node(id).map(|node| RelationNode {
-                    value: node.name().to_string(),
-                    subtype: node.subtype().map(|s| s.to_string()).unwrap_or_default(),
-                    ntype: string_to_node_type(node.xtype()) as i32,
-                })
-            });
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - generating results: ends {v} ms");
-        }
-
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - Ending at {v} ms");
-        }
-        Ok(Some(RelationPrefixSearchResponse {
-            nodes: nodes.collect::<Result<Vec<_>, _>>()?,
-        }))
-    }
-}
-impl RelationReader for RelationsReaderService {
-    #[tracing::instrument(skip_all)]
-    fn count(&self) -> NodeResult<usize> {
-        Ok(self
-            .index
-            .start_reading()
-            .and_then(|reader| reader.no_nodes())
-            .map(|v| v as usize)?)
-    }
-    #[tracing::instrument(skip_all)]
-    fn get_edges(&self) -> NodeResult<EdgeList> {
-        let id: Option<String> = None;
-        let time = SystemTime::now();
-        let reader = self.index.start_reading()?;
-        let iter = reader.iter_edge_ids()?;
-        let mut edges = Vec::new();
-        let mut found = HashSet::new();
-        for id in iter {
-            let id = id?;
-            let edge = reader.get_edge(id)?;
-            let xtype = edge.xtype();
-            let subtype = edge
-                .subtype()
-                .map_or_else(String::default, |s| s.to_string());
-            let hash = relations_io::compute_hash(&[xtype.as_bytes(), subtype.as_bytes()]);
-            if found.insert(hash) {
-                edges.push(RelationEdge {
-                    edge_type: string_to_rtype(xtype) as i32,
-                    property: subtype,
-                });
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - Ending at {v} ms");
-        }
-        Ok(EdgeList { list: edges })
-    }
-    #[tracing::instrument(skip_all)]
-    fn get_node_types(&self) -> NodeResult<TypeList> {
-        let id: Option<String> = None;
-        let time = SystemTime::now();
-        let mut found = HashSet::new();
-        let mut types = Vec::new();
-        let reader = self.index.start_reading()?;
-        let iter = reader.iter_node_ids()?;
-        for id in iter {
-            let id = id?;
-            let node = reader.get_node(id)?;
-            let xtype = node.xtype();
-            let subtype = node
-                .subtype()
-                .map_or_else(String::default, |s| s.to_string());
-            let hash = relations_io::compute_hash(&[xtype.as_bytes(), subtype.as_bytes()]);
-            if found.insert(hash) {
-                types.push(RelationTypeListMember {
-                    with_type: string_to_node_type(xtype) as i32,
-                    with_subtype: subtype,
-                });
-            }
-        }
-        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
-            info!("{id:?} - Ending at {v} ms");
-        }
-        Ok(TypeList { list: types })
-    }
-}
-
-impl ReaderChild for RelationsReaderService {
-    type Request = RelationSearchRequest;
-    type Response = RelationSearchResponse;
-    #[tracing::instrument(skip_all)]
-    fn stop(&self) -> NodeResult<()> {
-        info!("Stopping relation reader Service");
-        Ok(())
-    }
-    #[tracing::instrument(skip_all)]
-    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response> {
-        Ok(RelationSearchResponse {
-            subgraph: self.graph_search(request)?,
-            prefix: self.prefix_search(request)?,
-        })
-    }
-    #[tracing::instrument(skip_all)]
-    fn stored_ids(&self) -> NodeResult<Vec<String>> {
-        let reader = self.index.start_reading()?;
-        let ids = reader
-            .iter_node_ids()?
-            .filter_map(|node| node.ok())
-            .filter_map(|id| reader.get_node(id).ok())
-            .map(|s| format!("{s:?}"))
-            .collect();
-        Ok(ids)
-    }
-    #[tracing::instrument(skip_all)]
-    fn reload(&self) {
-        let _v = self
-            .index
-            .start_reading()
-            .and_then(|reader| reader.reload(&self.rmode))
-            .map_err(|err| error!("Reload error {err:?}"));
-    }
-}
-
-impl RelationsReaderService {
-    #[tracing::instrument(skip_all)]
-    pub fn start(config: &RelationConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if !path.exists() {
-            match RelationsReaderService::new(config) {
-                Err(e) if path.exists() => {
-                    std::fs::remove_dir(path)?;
-                    Err(e)
-                }
-                Err(e) => Err(e),
-                Ok(v) => Ok(v),
-            }
-        } else {
-            Ok(RelationsReaderService::open(config)?)
-        }
-    }
-    #[tracing::instrument(skip_all)]
-    pub fn new(config: &RelationConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if path.exists() {
-            Err(node_error!("Shard does exist".to_string()))
-        } else {
-            std::fs::create_dir_all(path)?;
-            let (index, rmode) = Index::new_reader(path)?;
-            Ok(RelationsReaderService { index, rmode })
-        }
-    }
-
-    #[tracing::instrument(skip_all)]
-    pub fn open(config: &RelationConfig) -> NodeResult<Self> {
-        let path = std::path::Path::new(&config.path);
-        if !path.exists() {
-            Err(node_error!("Shard does not exist".to_string()))
-        } else {
-            let (index, rmode) = Index::new_reader(path)?;
-            Ok(RelationsReaderService { index, rmode })
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashSet;
+use std::fmt::Debug;
+use std::time::SystemTime;
+
+use nucliadb_core::context;
+use nucliadb_core::metrics::request_time;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::*;
+use nucliadb_core::tracing::{self, *};
+
+use super::bfs::GrpcGuide;
+use super::utils::*;
+use crate::index::*;
+use crate::relations_io;
+
+pub struct RelationsReaderService {
+    rmode: RMode,
+    index: Index,
+}
+impl Debug for RelationsReaderService {
+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
+        f.debug_struct("RelationsReaderService").finish()
+    }
+}
+
+impl RelationsReaderService {
+    #[tracing::instrument(skip_all)]
+    fn graph_search(
+        &self,
+        request: &RelationSearchRequest,
+    ) -> NodeResult<Option<EntitiesSubgraphResponse>> {
+        let Some(bfs_request) = request.subgraph.as_ref() else {
+            return Ok(None);
+        };
+
+        let id = Some(&request.shard_id);
+        let time = SystemTime::now();
+        let reader = self.index.start_reading()?;
+        let depth = bfs_request.depth.map(|v| v as usize).unwrap_or(usize::MAX);
+        let mut entry_points = Vec::with_capacity(bfs_request.entry_points.len());
+        let mut node_filters = HashSet::with_capacity(bfs_request.node_filters.len());
+        let mut edge_filters = HashSet::with_capacity(bfs_request.edge_filters.len());
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} -  Creating entry points: starts {v} ms");
+        }
+        for node in bfs_request.entry_points.iter() {
+            let name = node.value.clone();
+            let type_info = node_type_parsing(node.ntype(), &node.subtype);
+            let xtype = type_info.0.to_string();
+            let subtype = type_info.1.map(|s| s.to_string());
+            let node = IoNode::new(name, xtype, subtype);
+            match reader.get_node_id(node.hash()) {
+                Ok(None) => (),
+                Ok(Some(id)) => entry_points.push(id),
+                Err(e) => error!("{e:?} during {node:?}"),
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} -  Creating entry points: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - adding query type filters: starts {v} ms");
+        }
+        bfs_request.node_filters.iter().for_each(|filter| {
+            let node_type = filter.node_type();
+            let node_subtype = filter
+                .node_subtype
+                .as_ref()
+                .map_or_else(|| "", |subtype| subtype);
+            let type_info = node_type_parsing(node_type, node_subtype);
+            node_filters.insert(type_info);
+        });
+        bfs_request.edge_filters.iter().for_each(|filter| {
+            let relation_type = filter.relation_type();
+            let relation_subtype = filter
+                .relation_subtype
+                .as_ref()
+                .map_or_else(|| "", |subtype| subtype);
+            let type_info = relation_type_parsing(relation_type, relation_subtype);
+            edge_filters.insert(type_info);
+        });
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - adding query type filters: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - running the search: starts {v} ms");
+        }
+        let guide = GrpcGuide {
+            node_filters,
+            edge_filters,
+            reader: &reader,
+            jump_always: dictionary::SYNONYM,
+        };
+        let mut subgraph = vec![];
+        for i in reader.search(guide, depth, entry_points)? {
+            let from = reader.get_node(i.from()).map(|node| RelationNode {
+                value: node.name().to_string(),
+                subtype: node.subtype().map(|s| s.to_string()).unwrap_or_default(),
+                ntype: string_to_node_type(node.xtype()) as i32,
+            })?;
+
+            let to = reader.get_node(i.to()).map(|node| RelationNode {
+                value: node.name().to_string(),
+                subtype: node.subtype().map(|s| s.to_string()).unwrap_or_default(),
+                ntype: string_to_node_type(node.xtype()) as i32,
+            })?;
+            let relation_metadata = reader.get_edge_metadata(i.edge())?;
+            let relation = reader.get_edge(i.edge()).map(|edge| Relation {
+                to: Some(to),
+                source: Some(from),
+                relation: string_to_rtype(edge.xtype()) as i32,
+                metadata: relation_metadata.map(RelationMetadata::from),
+                relation_label: edge.subtype().map(|s| s.to_string()).unwrap_or_default(),
+            })?;
+            subgraph.push(relation);
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - running the search: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms");
+        }
+        Ok(Some(EntitiesSubgraphResponse {
+            relations: subgraph,
+        }))
+    }
+    #[tracing::instrument(skip_all)]
+    fn prefix_search(
+        &self,
+        request: &RelationSearchRequest,
+    ) -> NodeResult<Option<RelationPrefixSearchResponse>> {
+        use crate::bfs_engine::BfsGuide;
+        let Some(prefix_request) = request.prefix.as_ref() else {
+            return Ok(None);
+        };
+
+        let id = Some(&request.shard_id);
+        let time = SystemTime::now();
+        let prefix = &prefix_request.prefix;
+        let reader = self.index.start_reading()?;
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - running prefix search: starts {v} ms");
+        }
+        let prefixes = reader
+            .prefix_search(&self.rmode, prefix)?
+            .into_iter()
+            .flat_map(|key| reader.get_node_id(&key).ok().flatten());
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - running prefix search: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - generating results: starts {v} ms");
+        }
+
+        let mut node_filters = HashSet::new();
+        prefix_request.node_filters.iter().for_each(|filter| {
+            let node_type = filter.node_type();
+            let node_subtype = filter
+                .node_subtype
+                .as_ref()
+                .map_or_else(|| "", |subtype| subtype);
+            let type_info = node_type_parsing(node_type, node_subtype);
+            node_filters.insert(type_info);
+        });
+
+        let guide = GrpcGuide {
+            node_filters,
+            edge_filters: HashSet::new(),
+            reader: &reader,
+            jump_always: dictionary::SYNONYM,
+        };
+
+        let nodes = prefixes
+            .into_iter()
+            .filter(|n| guide.node_allowed(*n))
+            .map(|id| {
+                reader.get_node(id).map(|node| RelationNode {
+                    value: node.name().to_string(),
+                    subtype: node.subtype().map(|s| s.to_string()).unwrap_or_default(),
+                    ntype: string_to_node_type(node.xtype()) as i32,
+                })
+            });
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - generating results: ends {v} ms");
+        }
+
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms");
+        }
+        Ok(Some(RelationPrefixSearchResponse {
+            nodes: nodes.collect::<Result<Vec<_>, _>>()?,
+        }))
+    }
+}
+impl RelationReader for RelationsReaderService {
+    #[tracing::instrument(skip_all)]
+    fn count(&self) -> NodeResult<usize> {
+        let time = SystemTime::now();
+
+        let result = Ok(self
+            .index
+            .start_reading()
+            .and_then(|reader| reader.no_nodes())
+            .map(|v| v as usize)?);
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("count".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+    #[tracing::instrument(skip_all)]
+    fn get_edges(&self) -> NodeResult<EdgeList> {
+        let time = SystemTime::now();
+
+        let id: Option<String> = None;
+        let reader = self.index.start_reading()?;
+        let iter = reader.iter_edge_ids()?;
+        let mut edges = Vec::new();
+        let mut found = HashSet::new();
+        for id in iter {
+            let id = id?;
+            let edge = reader.get_edge(id)?;
+            let xtype = edge.xtype();
+            let subtype = edge
+                .subtype()
+                .map_or_else(String::default, |s| s.to_string());
+            let hash = relations_io::compute_hash(&[xtype.as_bytes(), subtype.as_bytes()]);
+            if found.insert(hash) {
+                edges.push(RelationEdge {
+                    edge_type: string_to_rtype(xtype) as i32,
+                    property: subtype,
+                });
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms");
+        }
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("get_edges".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(EdgeList { list: edges })
+    }
+    #[tracing::instrument(skip_all)]
+    fn get_node_types(&self) -> NodeResult<TypeList> {
+        let time = SystemTime::now();
+
+        let id: Option<String> = None;
+        let mut found = HashSet::new();
+        let mut types = Vec::new();
+        let reader = self.index.start_reading()?;
+        let iter = reader.iter_node_ids()?;
+        for id in iter {
+            let id = id?;
+            let node = reader.get_node(id)?;
+            let xtype = node.xtype();
+            let subtype = node
+                .subtype()
+                .map_or_else(String::default, |s| s.to_string());
+            let hash = relations_io::compute_hash(&[xtype.as_bytes(), subtype.as_bytes()]);
+            if found.insert(hash) {
+                types.push(RelationTypeListMember {
+                    with_type: string_to_node_type(xtype) as i32,
+                    with_subtype: subtype,
+                });
+            }
+        }
+        if let Ok(v) = time.elapsed().map(|s| s.as_millis()) {
+            debug!("{id:?} - Ending at {v} ms");
+        }
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("get_node_types".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(TypeList { list: types })
+    }
+}
+
+impl ReaderChild for RelationsReaderService {
+    type Request = RelationSearchRequest;
+    type Response = RelationSearchResponse;
+    #[tracing::instrument(skip_all)]
+    fn stop(&self) -> NodeResult<()> {
+        debug!("Stopping relation reader Service");
+        Ok(())
+    }
+    #[tracing::instrument(skip_all)]
+    fn search(&self, request: &Self::Request) -> NodeResult<Self::Response> {
+        let time = SystemTime::now();
+
+        let result = Ok(RelationSearchResponse {
+            subgraph: self.graph_search(request)?,
+            prefix: self.prefix_search(request)?,
+        });
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("search".to_string());
+        metrics.record_request_time(metric, took);
+
+        result
+    }
+    #[tracing::instrument(skip_all)]
+    fn stored_ids(&self) -> NodeResult<Vec<String>> {
+        let time = SystemTime::now();
+
+        let reader = self.index.start_reading()?;
+        let ids = reader
+            .iter_node_ids()?
+            .filter_map(|node| node.ok())
+            .filter_map(|id| reader.get_node(id).ok())
+            .map(|s| format!("{s:?}"))
+            .collect();
+
+        let metrics = context::get_metrics();
+        let took = time.elapsed().map(|i| i.as_secs_f64()).unwrap_or(f64::NAN);
+        let metric = request_time::RequestTimeKey::relations("stored_ids".to_string());
+        metrics.record_request_time(metric, took);
+
+        Ok(ids)
+    }
+    #[tracing::instrument(skip_all)]
+    fn reload(&self) {
+        let _v = self
+            .index
+            .start_reading()
+            .and_then(|reader| reader.reload(&self.rmode))
+            .map_err(|err| error!("Reload error {err:?}"));
+    }
+}
+
+impl RelationsReaderService {
+    #[tracing::instrument(skip_all)]
+    pub fn start(config: &RelationConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            match RelationsReaderService::new(config) {
+                Err(e) if path.exists() => {
+                    std::fs::remove_dir(path)?;
+                    Err(e)
+                }
+                Err(e) => Err(e),
+                Ok(v) => Ok(v),
+            }
+        } else {
+            Ok(RelationsReaderService::open(config)?)
+        }
+    }
+    #[tracing::instrument(skip_all)]
+    pub fn new(config: &RelationConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if path.exists() {
+            Err(node_error!("Shard does exist".to_string()))
+        } else {
+            std::fs::create_dir_all(path)?;
+            let (index, rmode) = Index::new_reader(path)?;
+            Ok(RelationsReaderService { index, rmode })
+        }
+    }
+
+    #[tracing::instrument(skip_all)]
+    pub fn open(config: &RelationConfig) -> NodeResult<Self> {
+        let path = std::path::Path::new(&config.path);
+        if !path.exists() {
+            Err(node_error!("Shard does not exist".to_string()))
+        } else {
+            let (index, rmode) = Index::new_reader(path)?;
+            Ok(RelationsReaderService { index, rmode })
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/tests.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/tests.rs`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,435 +1,435 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::collections::HashMap;
-use std::path::Path;
-
-use lazy_static::lazy_static;
-use nucliadb_core::prelude::*;
-use nucliadb_core::protos::resource::ResourceStatus;
-use nucliadb_core::protos::*;
-use prost_types::Timestamp;
-use relation::*;
-use relation_node::NodeType;
-
-use super::*;
-
-lazy_static! {
-    static ref SHARD_ID: String = "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string();
-    static ref E0: RelationNode = RelationNode {
-        value: "E0".to_string(),
-        ntype: NodeType::Entity as i32,
-        subtype: "".to_string(),
-    };
-    static ref E1: RelationNode = RelationNode {
-        value: "E1".to_string(),
-        ntype: NodeType::Entity as i32,
-        subtype: "Official".to_string(),
-    };
-    static ref E2: RelationNode = RelationNode {
-        value: "E2".to_string(),
-        ntype: NodeType::Entity as i32,
-        subtype: "Propaganda".to_string(),
-    };
-    static ref NODE_TYPES: TypeList = TypeList {
-        list: vec![
-            RelationTypeListMember {
-                with_type: NodeType::Entity as i32,
-                with_subtype: "Official".to_string(),
-            },
-            RelationTypeListMember {
-                with_type: NodeType::Entity as i32,
-                with_subtype: "".to_string(),
-            },
-            RelationTypeListMember {
-                with_type: NodeType::Entity as i32,
-                with_subtype: "Propaganda".to_string(),
-            },
-        ]
-    };
-    static ref REQUEST_BONES: RelationSearchRequest = RelationSearchRequest {
-        shard_id: SHARD_ID.clone(),
-        reload: false,
-        prefix: None,
-        subgraph: None,
-    };
-    static ref REQUEST0: EntitiesSubgraphRequest = EntitiesSubgraphRequest {
-        entry_points: vec![E0.clone()],
-        node_filters: vec![
-            RelationNodeFilter {
-                node_type: NodeType::Entity as i32,
-                node_subtype: None
-            },
-            RelationNodeFilter {
-                node_type: NodeType::Entity as i32,
-                node_subtype: Some("Nonexisting".to_string())
-            }
-        ],
-        depth: Some(1),
-        edge_filters: vec![],
-    };
-    static ref RESPONSE0: Vec<RelationNode> = vec![E0.clone(), E1.clone(), E2.clone()];
-    static ref REQUEST1: EntitiesSubgraphRequest = EntitiesSubgraphRequest {
-        entry_points: vec![E0.clone()],
-        node_filters: vec![RelationNodeFilter {
-            node_type: NodeType::Entity as i32,
-            node_subtype: Some("Official".to_string())
-        },],
-        depth: Some(1),
-        edge_filters: vec![],
-    };
-    static ref RESPONSE1: Vec<RelationNode> = vec![E0.clone(), E1.clone()];
-    static ref EDGE_LIST: EdgeList = EdgeList {
-        list: vec![
-            RelationEdge {
-                edge_type: RelationType::Entity as i32,
-                property: "".to_string()
-            },
-            RelationEdge {
-                edge_type: RelationType::Child as i32,
-                property: "".to_string()
-            },
-        ]
-    };
-}
-
-fn create_empty_resource(shard_id: String) -> Resource {
-    let resource_id = ResourceId {
-        shard_id: SHARD_ID.clone(),
-        uuid: SHARD_ID.clone(),
-    };
-    let timestamp = Timestamp {
-        seconds: 0,
-        nanos: 0,
-    };
-
-    let metadata = IndexMetadata {
-        created: Some(timestamp.clone()),
-        modified: Some(timestamp),
-    };
-
-    Resource {
-        resource: Some(resource_id),
-        metadata: Some(metadata),
-        texts: HashMap::with_capacity(0),
-        status: ResourceStatus::Processed as i32,
-        labels: vec![],
-        paragraphs: HashMap::with_capacity(0),
-        paragraphs_to_delete: vec![],
-        sentences_to_delete: vec![],
-        relations_to_delete: vec![],
-        relations: vec![],
-        vectors: HashMap::default(),
-        vectors_to_delete: HashMap::default(),
-        shard_id,
-    }
-}
-
-fn empty_graph() -> Vec<Relation> {
-    vec![]
-}
-
-fn entities(mut edges: Vec<Relation>) -> Vec<Relation> {
-    let metadata = RelationMetadata {
-        paragraph_id: Some("r0".to_string()),
-        ..Default::default()
-    };
-    let r0 = Relation {
-        relation: RelationType::Child as i32,
-        source: Some(E1.clone()),
-        to: Some(E2.clone()),
-        relation_label: "".to_string(),
-        metadata: Some(metadata),
-    };
-    let metadata = RelationMetadata {
-        paragraph_id: Some("r1".to_string()),
-        ..Default::default()
-    };
-    let r1 = Relation {
-        relation: RelationType::Entity as i32,
-        source: Some(E0.clone()),
-        to: Some(E2.clone()),
-        relation_label: "".to_string(),
-        metadata: Some(metadata),
-    };
-    let metadata = RelationMetadata {
-        paragraph_id: Some("r2".to_string()),
-        ..Default::default()
-    };
-    let r2 = Relation {
-        relation: RelationType::Entity as i32,
-        source: Some(E0.clone()),
-        to: Some(E1.clone()),
-        relation_label: "".to_string(),
-        metadata: Some(metadata),
-    };
-    edges.append(&mut vec![r0, r1, r2]);
-    edges
-}
-
-fn similatity_edges(mut edges: Vec<Relation>) -> Vec<Relation> {
-    let r0 = Relation {
-        relation: RelationType::Synonym as i32,
-        source: Some(E0.clone()),
-        to: Some(E1.clone()),
-        relation_label: "".to_string(),
-        metadata: None,
-    };
-    let r1 = Relation {
-        relation: RelationType::Synonym as i32,
-        source: Some(E1.clone()),
-        to: Some(E2.clone()),
-        relation_label: "".to_string(),
-        metadata: None,
-    };
-    edges.append(&mut vec![r0, r1]);
-    edges
-}
-
-fn simple_graph(at: &Path) -> (RelationsWriterService, RelationsReaderService) {
-    let rsc = RelationConfig {
-        path: at.join("relations"),
-    };
-    println!("Writer starts");
-    let writer = RelationsWriterService::start(&rsc).unwrap();
-    let reader = RelationsReaderService::open(&rsc).unwrap();
-    (writer, reader)
-}
-
-#[test]
-fn simple_request() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-
-    reader.reload();
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST0.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE0.contains(&v.0))
-        .filter(|v| RESPONSE0.contains(&v.1))
-        .count();
-    assert_eq!(len + 1, RESPONSE0.len());
-    Ok(())
-}
-
-#[test]
-fn join_graph_test() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = empty_graph();
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    let got = reader.count().unwrap();
-    assert_eq!(got, 0);
-
-    let graph = JoinGraph {
-        nodes: HashMap::from([(0i32, E0.clone()), (1i32, E1.clone()), (2i32, E2.clone())]),
-        edges: vec![
-            JoinGraphEdge {
-                source: 2,
-                target: 1,
-                rtype: RelationType::Child as i32,
-                rsubtype: "".to_string(),
-                metadata: None,
-            },
-            JoinGraphEdge {
-                source: 0,
-                target: 2,
-                rtype: RelationType::Entity as i32,
-                rsubtype: "".to_string(),
-                metadata: None,
-            },
-            JoinGraphEdge {
-                source: 0,
-                target: 1,
-                rtype: RelationType::Entity as i32,
-                rsubtype: "".to_string(),
-                metadata: None,
-            },
-        ],
-    };
-    writer.join_graph(&graph).unwrap();
-    reader.reload();
-
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST0.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE0.contains(&v.0))
-        .filter(|v| RESPONSE0.contains(&v.1))
-        .count();
-    assert_eq!(len + 1, RESPONSE0.len());
-    Ok(())
-}
-
-#[test]
-fn simple_request_with_similarity() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = similatity_edges(entities(empty_graph()));
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST0.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE0.contains(&v.0))
-        .filter(|v| RESPONSE0.contains(&v.1))
-        .count();
-    assert_eq!(len, RESPONSE0.len() + 2);
-
-    Ok(())
-}
-
-#[test]
-fn typed_request() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-
-    let mut request = REQUEST_BONES.clone();
-    request.subgraph = Some(REQUEST1.clone());
-    let got = reader.search(&request).unwrap();
-    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
-
-    let len = bfs_response
-        .relations
-        .into_iter()
-        .flat_map(|v| v.to.zip(v.source))
-        .filter(|v| RESPONSE1.contains(&v.0))
-        .filter(|v| RESPONSE1.contains(&v.1))
-        .count();
-    assert_eq!(len + 1, RESPONSE1.len());
-
-    Ok(())
-}
-
-#[test]
-fn just_prefix_querying() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-
-    reader.reload();
-    let mut request = REQUEST_BONES.clone();
-    request.prefix = Some(RelationPrefixSearchRequest {
-        prefix: "E".to_string(),
-        ..Default::default()
-    });
-    let got = reader.search(&request).unwrap();
-    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
-    let is_permutation = prefix_response
-        .nodes
-        .iter()
-        .all(|member| RESPONSE0.contains(member));
-    assert!((prefix_response.nodes.len() == RESPONSE0.len()) && is_permutation);
-
-    request.prefix = Some(RelationPrefixSearchRequest {
-        prefix: "e".to_string(),
-        ..Default::default()
-    });
-    let got = reader.search(&request).unwrap();
-    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
-    let is_permutation = prefix_response
-        .nodes
-        .iter()
-        .all(|member| RESPONSE0.contains(member));
-    assert!((prefix_response.nodes.len() == RESPONSE0.len()) && is_permutation);
-
-    request.prefix = Some(RelationPrefixSearchRequest {
-        prefix: "not".to_string(),
-        ..Default::default()
-    });
-    let got = reader.search(&request).unwrap();
-    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
-    assert!(prefix_response.nodes.is_empty());
-
-    Ok(())
-}
-
-#[test]
-fn getting_node_types() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-    let node_types = reader.get_node_types().unwrap();
-    assert_eq!(node_types.list.len(), NODE_TYPES.list.len());
-    assert!(node_types
-        .list
-        .iter()
-        .all(|member| NODE_TYPES.list.contains(member)));
-
-    let edges = reader.get_edges().unwrap();
-    assert_eq!(edges.list.len(), EDGE_LIST.list.len(),);
-    assert!(edges
-        .list
-        .iter()
-        .all(|member| EDGE_LIST.list.contains(member)));
-    Ok(())
-}
-
-#[test]
-fn getting_edges() -> NodeResult<()> {
-    let dir = tempfile::tempdir().unwrap();
-    let (mut writer, reader) = simple_graph(dir.path());
-    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
-    let graph = entities(empty_graph());
-    resource.relations = graph;
-    writer.set_resource(&resource).unwrap();
-    reader.reload();
-    let edges = reader.get_edges().unwrap();
-    assert_eq!(edges.list.len(), EDGE_LIST.list.len(),);
-    assert!(edges
-        .list
-        .iter()
-        .all(|member| EDGE_LIST.list.contains(member)));
-    Ok(())
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::collections::HashMap;
+use std::path::Path;
+
+use lazy_static::lazy_static;
+use nucliadb_core::prelude::*;
+use nucliadb_core::protos::resource::ResourceStatus;
+use nucliadb_core::protos::*;
+use prost_types::Timestamp;
+use relation::*;
+use relation_node::NodeType;
+
+use super::*;
+
+lazy_static! {
+    static ref SHARD_ID: String = "f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string();
+    static ref E0: RelationNode = RelationNode {
+        value: "E0".to_string(),
+        ntype: NodeType::Entity as i32,
+        subtype: "".to_string(),
+    };
+    static ref E1: RelationNode = RelationNode {
+        value: "E1".to_string(),
+        ntype: NodeType::Entity as i32,
+        subtype: "Official".to_string(),
+    };
+    static ref E2: RelationNode = RelationNode {
+        value: "E2".to_string(),
+        ntype: NodeType::Entity as i32,
+        subtype: "Propaganda".to_string(),
+    };
+    static ref NODE_TYPES: TypeList = TypeList {
+        list: vec![
+            RelationTypeListMember {
+                with_type: NodeType::Entity as i32,
+                with_subtype: "Official".to_string(),
+            },
+            RelationTypeListMember {
+                with_type: NodeType::Entity as i32,
+                with_subtype: "".to_string(),
+            },
+            RelationTypeListMember {
+                with_type: NodeType::Entity as i32,
+                with_subtype: "Propaganda".to_string(),
+            },
+        ]
+    };
+    static ref REQUEST_BONES: RelationSearchRequest = RelationSearchRequest {
+        shard_id: SHARD_ID.clone(),
+        reload: false,
+        prefix: None,
+        subgraph: None,
+    };
+    static ref REQUEST0: EntitiesSubgraphRequest = EntitiesSubgraphRequest {
+        entry_points: vec![E0.clone()],
+        node_filters: vec![
+            RelationNodeFilter {
+                node_type: NodeType::Entity as i32,
+                node_subtype: None
+            },
+            RelationNodeFilter {
+                node_type: NodeType::Entity as i32,
+                node_subtype: Some("Nonexisting".to_string())
+            }
+        ],
+        depth: Some(1),
+        edge_filters: vec![],
+    };
+    static ref RESPONSE0: Vec<RelationNode> = vec![E0.clone(), E1.clone(), E2.clone()];
+    static ref REQUEST1: EntitiesSubgraphRequest = EntitiesSubgraphRequest {
+        entry_points: vec![E0.clone()],
+        node_filters: vec![RelationNodeFilter {
+            node_type: NodeType::Entity as i32,
+            node_subtype: Some("Official".to_string())
+        },],
+        depth: Some(1),
+        edge_filters: vec![],
+    };
+    static ref RESPONSE1: Vec<RelationNode> = vec![E0.clone(), E1.clone()];
+    static ref EDGE_LIST: EdgeList = EdgeList {
+        list: vec![
+            RelationEdge {
+                edge_type: RelationType::Entity as i32,
+                property: "".to_string()
+            },
+            RelationEdge {
+                edge_type: RelationType::Child as i32,
+                property: "".to_string()
+            },
+        ]
+    };
+}
+
+fn create_empty_resource(shard_id: String) -> Resource {
+    let resource_id = ResourceId {
+        shard_id: SHARD_ID.clone(),
+        uuid: SHARD_ID.clone(),
+    };
+    let timestamp = Timestamp {
+        seconds: 0,
+        nanos: 0,
+    };
+
+    let metadata = IndexMetadata {
+        created: Some(timestamp.clone()),
+        modified: Some(timestamp),
+    };
+
+    Resource {
+        resource: Some(resource_id),
+        metadata: Some(metadata),
+        texts: HashMap::with_capacity(0),
+        status: ResourceStatus::Processed as i32,
+        labels: vec![],
+        paragraphs: HashMap::with_capacity(0),
+        paragraphs_to_delete: vec![],
+        sentences_to_delete: vec![],
+        relations_to_delete: vec![],
+        relations: vec![],
+        vectors: HashMap::default(),
+        vectors_to_delete: HashMap::default(),
+        shard_id,
+    }
+}
+
+fn empty_graph() -> Vec<Relation> {
+    vec![]
+}
+
+fn entities(mut edges: Vec<Relation>) -> Vec<Relation> {
+    let metadata = RelationMetadata {
+        paragraph_id: Some("r0".to_string()),
+        ..Default::default()
+    };
+    let r0 = Relation {
+        relation: RelationType::Child as i32,
+        source: Some(E1.clone()),
+        to: Some(E2.clone()),
+        relation_label: "".to_string(),
+        metadata: Some(metadata),
+    };
+    let metadata = RelationMetadata {
+        paragraph_id: Some("r1".to_string()),
+        ..Default::default()
+    };
+    let r1 = Relation {
+        relation: RelationType::Entity as i32,
+        source: Some(E0.clone()),
+        to: Some(E2.clone()),
+        relation_label: "".to_string(),
+        metadata: Some(metadata),
+    };
+    let metadata = RelationMetadata {
+        paragraph_id: Some("r2".to_string()),
+        ..Default::default()
+    };
+    let r2 = Relation {
+        relation: RelationType::Entity as i32,
+        source: Some(E0.clone()),
+        to: Some(E1.clone()),
+        relation_label: "".to_string(),
+        metadata: Some(metadata),
+    };
+    edges.append(&mut vec![r0, r1, r2]);
+    edges
+}
+
+fn similatity_edges(mut edges: Vec<Relation>) -> Vec<Relation> {
+    let r0 = Relation {
+        relation: RelationType::Synonym as i32,
+        source: Some(E0.clone()),
+        to: Some(E1.clone()),
+        relation_label: "".to_string(),
+        metadata: None,
+    };
+    let r1 = Relation {
+        relation: RelationType::Synonym as i32,
+        source: Some(E1.clone()),
+        to: Some(E2.clone()),
+        relation_label: "".to_string(),
+        metadata: None,
+    };
+    edges.append(&mut vec![r0, r1]);
+    edges
+}
+
+fn simple_graph(at: &Path) -> (RelationsWriterService, RelationsReaderService) {
+    let rsc = RelationConfig {
+        path: at.join("relations"),
+    };
+    println!("Writer starts");
+    let writer = RelationsWriterService::start(&rsc).unwrap();
+    let reader = RelationsReaderService::open(&rsc).unwrap();
+    (writer, reader)
+}
+
+#[test]
+fn simple_request() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = entities(empty_graph());
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+
+    reader.reload();
+    let mut request = REQUEST_BONES.clone();
+    request.subgraph = Some(REQUEST0.clone());
+    let got = reader.search(&request).unwrap();
+    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
+    let len = bfs_response
+        .relations
+        .into_iter()
+        .flat_map(|v| v.to.zip(v.source))
+        .filter(|v| RESPONSE0.contains(&v.0))
+        .filter(|v| RESPONSE0.contains(&v.1))
+        .count();
+    assert_eq!(len + 1, RESPONSE0.len());
+    Ok(())
+}
+
+#[test]
+fn join_graph_test() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = empty_graph();
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+    let got = reader.count().unwrap();
+    assert_eq!(got, 0);
+
+    let graph = JoinGraph {
+        nodes: HashMap::from([(0i32, E0.clone()), (1i32, E1.clone()), (2i32, E2.clone())]),
+        edges: vec![
+            JoinGraphEdge {
+                source: 2,
+                target: 1,
+                rtype: RelationType::Child as i32,
+                rsubtype: "".to_string(),
+                metadata: None,
+            },
+            JoinGraphEdge {
+                source: 0,
+                target: 2,
+                rtype: RelationType::Entity as i32,
+                rsubtype: "".to_string(),
+                metadata: None,
+            },
+            JoinGraphEdge {
+                source: 0,
+                target: 1,
+                rtype: RelationType::Entity as i32,
+                rsubtype: "".to_string(),
+                metadata: None,
+            },
+        ],
+    };
+    writer.join_graph(&graph).unwrap();
+    reader.reload();
+
+    let mut request = REQUEST_BONES.clone();
+    request.subgraph = Some(REQUEST0.clone());
+    let got = reader.search(&request).unwrap();
+    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
+    let len = bfs_response
+        .relations
+        .into_iter()
+        .flat_map(|v| v.to.zip(v.source))
+        .filter(|v| RESPONSE0.contains(&v.0))
+        .filter(|v| RESPONSE0.contains(&v.1))
+        .count();
+    assert_eq!(len + 1, RESPONSE0.len());
+    Ok(())
+}
+
+#[test]
+fn simple_request_with_similarity() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = similatity_edges(entities(empty_graph()));
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+    reader.reload();
+
+    let mut request = REQUEST_BONES.clone();
+    request.subgraph = Some(REQUEST0.clone());
+    let got = reader.search(&request).unwrap();
+    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
+    let len = bfs_response
+        .relations
+        .into_iter()
+        .flat_map(|v| v.to.zip(v.source))
+        .filter(|v| RESPONSE0.contains(&v.0))
+        .filter(|v| RESPONSE0.contains(&v.1))
+        .count();
+    assert_eq!(len, RESPONSE0.len() + 2);
+
+    Ok(())
+}
+
+#[test]
+fn typed_request() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = entities(empty_graph());
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+    reader.reload();
+
+    let mut request = REQUEST_BONES.clone();
+    request.subgraph = Some(REQUEST1.clone());
+    let got = reader.search(&request).unwrap();
+    let Some(bfs_response) = got.subgraph else { unreachable!("Wrong variant") };
+
+    let len = bfs_response
+        .relations
+        .into_iter()
+        .flat_map(|v| v.to.zip(v.source))
+        .filter(|v| RESPONSE1.contains(&v.0))
+        .filter(|v| RESPONSE1.contains(&v.1))
+        .count();
+    assert_eq!(len + 1, RESPONSE1.len());
+
+    Ok(())
+}
+
+#[test]
+fn just_prefix_querying() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = entities(empty_graph());
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+
+    reader.reload();
+    let mut request = REQUEST_BONES.clone();
+    request.prefix = Some(RelationPrefixSearchRequest {
+        prefix: "E".to_string(),
+        ..Default::default()
+    });
+    let got = reader.search(&request).unwrap();
+    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
+    let is_permutation = prefix_response
+        .nodes
+        .iter()
+        .all(|member| RESPONSE0.contains(member));
+    assert!((prefix_response.nodes.len() == RESPONSE0.len()) && is_permutation);
+
+    request.prefix = Some(RelationPrefixSearchRequest {
+        prefix: "e".to_string(),
+        ..Default::default()
+    });
+    let got = reader.search(&request).unwrap();
+    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
+    let is_permutation = prefix_response
+        .nodes
+        .iter()
+        .all(|member| RESPONSE0.contains(member));
+    assert!((prefix_response.nodes.len() == RESPONSE0.len()) && is_permutation);
+
+    request.prefix = Some(RelationPrefixSearchRequest {
+        prefix: "not".to_string(),
+        ..Default::default()
+    });
+    let got = reader.search(&request).unwrap();
+    let Some(prefix_response) = got.prefix else { unreachable!("Wrong variant") };
+    assert!(prefix_response.nodes.is_empty());
+
+    Ok(())
+}
+
+#[test]
+fn getting_node_types() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = entities(empty_graph());
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+    reader.reload();
+    let node_types = reader.get_node_types().unwrap();
+    assert_eq!(node_types.list.len(), NODE_TYPES.list.len());
+    assert!(node_types
+        .list
+        .iter()
+        .all(|member| NODE_TYPES.list.contains(member)));
+
+    let edges = reader.get_edges().unwrap();
+    assert_eq!(edges.list.len(), EDGE_LIST.list.len(),);
+    assert!(edges
+        .list
+        .iter()
+        .all(|member| EDGE_LIST.list.contains(member)));
+    Ok(())
+}
+
+#[test]
+fn getting_edges() -> NodeResult<()> {
+    let dir = tempfile::tempdir().unwrap();
+    let (mut writer, reader) = simple_graph(dir.path());
+    let mut resource = create_empty_resource("f56c58ac-b4f9-4d61-a077-ffccaadd0001".to_string());
+    let graph = entities(empty_graph());
+    resource.relations = graph;
+    writer.set_resource(&resource).unwrap();
+    reader.reload();
+    let edges = reader.get_edges().unwrap();
+    assert_eq!(edges.list.len(), EDGE_LIST.list.len(),);
+    assert!(edges
+        .list
+        .iter()
+        .all(|member| EDGE_LIST.list.contains(member)));
+    Ok(())
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_relations/src/service/utils.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_relations/src/service/utils.rs`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,87 +1,87 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use nucliadb_core::protos::relation::RelationType;
-use nucliadb_core::protos::relation_node::NodeType;
-
-pub mod dictionary {
-    pub const ENTITY: &str = "Entity";
-    pub const ABOUT: &str = "About";
-    pub const CHILD: &str = "Child";
-    pub const COLAB: &str = "Colab";
-    pub const SYNONYM: &str = "Synonym";
-    pub const OTHER: &str = "Other";
-    pub const RESOURCE: &str = "Resource";
-    pub const USER: &str = "User";
-    pub const LABEL: &str = "Label";
-}
-
-pub fn relation_type_parsing(rtype: RelationType, subtype: &str) -> (&str, Option<&str>) {
-    let subtype = if subtype.is_empty() {
-        None
-    } else {
-        Some(subtype)
-    };
-    let xtype = match rtype {
-        RelationType::Entity => dictionary::ENTITY,
-        RelationType::About => dictionary::ABOUT,
-        RelationType::Child => dictionary::CHILD,
-        RelationType::Colab => dictionary::COLAB,
-        RelationType::Synonym => dictionary::SYNONYM,
-        RelationType::Other => dictionary::OTHER,
-    };
-    (xtype, subtype)
-}
-
-pub fn string_to_rtype(rtype: &str) -> RelationType {
-    match rtype {
-        dictionary::ENTITY => RelationType::Entity,
-        dictionary::ABOUT => RelationType::About,
-        dictionary::CHILD => RelationType::Child,
-        dictionary::COLAB => RelationType::Colab,
-        dictionary::SYNONYM => RelationType::Synonym,
-        dictionary::OTHER => RelationType::Other,
-        v => unreachable!("unknown type {v}"),
-    }
-}
-
-pub fn node_type_parsing(rtype: NodeType, subtype: &str) -> (&'static str, Option<&str>) {
-    let subtype = if subtype.is_empty() {
-        None
-    } else {
-        Some(subtype)
-    };
-    let xtype = match rtype {
-        NodeType::Entity => dictionary::ENTITY,
-        NodeType::Label => dictionary::LABEL,
-        NodeType::Resource => dictionary::RESOURCE,
-        NodeType::User => dictionary::USER,
-    };
-    (xtype, subtype)
-}
-
-pub fn string_to_node_type(rtype: &str) -> NodeType {
-    match rtype {
-        dictionary::ENTITY => NodeType::Entity,
-        dictionary::LABEL => NodeType::Label,
-        dictionary::RESOURCE => NodeType::Resource,
-        dictionary::USER => NodeType::User,
-        v => panic!("Invalid node type {}", v),
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use nucliadb_core::protos::relation::RelationType;
+use nucliadb_core::protos::relation_node::NodeType;
+
+pub mod dictionary {
+    pub const ENTITY: &str = "Entity";
+    pub const ABOUT: &str = "About";
+    pub const CHILD: &str = "Child";
+    pub const COLAB: &str = "Colab";
+    pub const SYNONYM: &str = "Synonym";
+    pub const OTHER: &str = "Other";
+    pub const RESOURCE: &str = "Resource";
+    pub const USER: &str = "User";
+    pub const LABEL: &str = "Label";
+}
+
+pub fn relation_type_parsing(rtype: RelationType, subtype: &str) -> (&str, Option<&str>) {
+    let subtype = if subtype.is_empty() {
+        None
+    } else {
+        Some(subtype)
+    };
+    let xtype = match rtype {
+        RelationType::Entity => dictionary::ENTITY,
+        RelationType::About => dictionary::ABOUT,
+        RelationType::Child => dictionary::CHILD,
+        RelationType::Colab => dictionary::COLAB,
+        RelationType::Synonym => dictionary::SYNONYM,
+        RelationType::Other => dictionary::OTHER,
+    };
+    (xtype, subtype)
+}
+
+pub fn string_to_rtype(rtype: &str) -> RelationType {
+    match rtype {
+        dictionary::ENTITY => RelationType::Entity,
+        dictionary::ABOUT => RelationType::About,
+        dictionary::CHILD => RelationType::Child,
+        dictionary::COLAB => RelationType::Colab,
+        dictionary::SYNONYM => RelationType::Synonym,
+        dictionary::OTHER => RelationType::Other,
+        v => unreachable!("unknown type {v}"),
+    }
+}
+
+pub fn node_type_parsing(rtype: NodeType, subtype: &str) -> (&'static str, Option<&str>) {
+    let subtype = if subtype.is_empty() {
+        None
+    } else {
+        Some(subtype)
+    };
+    let xtype = match rtype {
+        NodeType::Entity => dictionary::ENTITY,
+        NodeType::Label => dictionary::LABEL,
+        NodeType::Resource => dictionary::RESOURCE,
+        NodeType::User => dictionary::USER,
+    };
+    (xtype, subtype)
+}
+
+pub fn string_to_node_type(rtype: &str) -> NodeType {
+    match rtype {
+        dictionary::ENTITY => NodeType::Entity,
+        dictionary::LABEL => NodeType::Label,
+        dictionary::RESOURCE => NodeType::Resource,
+        dictionary::USER => NodeType::User,
+        v => panic!("Invalid node type {}", v),
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/Cargo.toml` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/Cargo.toml`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-[package]
-name = "nucliadb_telemetry"
-version = "0.1.0"
-authors = ["nucliadb <nucliadb@nuclia.com>"]
-edition = "2021"
-license = "AGPL-3.0-or-later"
-description = "nucliadb telemetry"
-repository = "https://github.com/nuclia/nucliadb"
-homepage = "https://nucliadb.io/"
-documentation = "https://nucliadb.io/docs/"
-
-
-[dependencies]
-once_cell = "1.8.0"
-reqwest = { version = "0.11", default-features=false, features = ["json", "rustls-tls", "blocking"] }
-tokio = {version = "1", features = ["full"]}
-serde = {version="1", features = ["derive"]}
-uuid = { version= "1.1", features = ["v4", "serde"]}
-tracing = "0.1"
-async-trait = "0.1"
-hostname = "0.3"
-username = "0.2"
-md5 = "0.7"
+[package]
+name = "nucliadb_telemetry"
+version = "0.1.0"
+authors = ["nucliadb <nucliadb@nuclia.com>"]
+edition = "2021"
+license = "AGPL-3.0-or-later"
+description = "nucliadb telemetry"
+repository = "https://github.com/nuclia/nucliadb"
+homepage = "https://nucliadb.io/"
+documentation = "https://nucliadb.io/docs/"
+
+
+[dependencies]
+once_cell = "1.8.0"
+reqwest = { version = "0.11", default-features=false, features = ["json", "rustls-tls", "blocking"] }
+tokio = {version = "1", features = ["full"]}
+serde = {version="1", features = ["derive"]}
+uuid = { version= "1.1", features = ["v4", "serde"]}
+tracing = "0.1"
+async-trait = "0.1"
+hostname = "0.3"
+username = "0.2"
+md5 = "0.7"
 lazy_static = "1.4.0"
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/README.md` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/README.md`

 * *Files 12% similar despite different names*

```diff
@@ -1,208 +1,208 @@
-# NucliaDB Telemetry
-
-Open telemetry compatible plugin to propagate traceid on FastAPI, Nats and GRPC with Asyncio.
-
-ENV vars:
-
-```
-    JAEGER_ENABLED = True
-    JAEGER_HOST = "127.0.0.1"
-    JAEGER_PORT = server.port
-```
-
-On FastAPI you should add:
-
-```python
-    tracer_provider = get_telemetry("HTTP_SERVICE")
-    app = FastAPI(title="Test API")  # type: ignore
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    FastAPIInstrumentor.instrument_app(app, tracer_provider=tracer_provider)
-
-    ..
-    await init_telemetry(tracer_provider)  # To start asyncio task
-    ..
-
-```
-
-On GRPC Server you should add:
-
-```python
-    tracer_provider = get_telemetry("GRPC_SERVER_SERVICE")
-    telemetry_grpc = OpenTelemetryGRPC("GRPC_CLIENT_SERVICE", tracer_provider)
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    server = telemetry_grpc.init_server()
-    helloworld_pb2_grpc.add_GreeterServicer_to_server(SERVICER, server)
-
-    ..
-    await init_telemetry(tracer_provider)  # To start asyncio task
-    ..
-```
-
-On GRPC Client you should add:
-
-```python
-    tracer_provider = get_telemetry("GRPC_CLIENT_SERVICE")
-    telemetry_grpc = OpenTelemetryGRPC("GRPC_CLIENT_SERVICE", tracer_provider)
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
-    stub = helloworld_pb2_grpc.GreeterStub(channel)
-
-    ..
-    await init_telemetry(tracer_provider)  # To start asyncio task
-    ..
-
-```
-
-On Nats jetstream push subscriber you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-    set_global_textmap(B3MultiFormat())
-    jsotel = JetStreamContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-    subscription = await jsotel.subscribe(
-        subject="testing.telemetry",
-        stream="testing",
-        cb=handler,
-    )
-
-```
-
-On Nats publisher you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    jsotel = JetStreamContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-     await jsotel.publish("testing.telemetry", request.name.encode())
-
-```
-
-
-On Nats jetstream pull subscription you can use different patterns if you want to
-just get one message and exit or pull several ones. For just one message
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-    set_global_textmap(B3MultiFormat())
-    jsotel = JetStreamContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-    # You can use either pull_subscribe or pull_subscribe_bind
-    subscription = await jsotel.pull_subscribe(
-        subject="testing.telemetry",
-        durable="consumer_name"
-        stream="testing",
-    )
-
-    async def callback(message):
-        # Do something with your message
-        # and optionally return something
-        return True
-
-    try:
-        result = await jsotel.pull_one(subscription, callback)
-    except errors.TimeoutError
-        pass
-
-```
-For multiple messages just wrap it in a loop:
-
-```python
-    while True:
-        try:
-            result = await jsotel.pull_one(subscription, callback)
-        except errors.TimeoutError
-            pass
-
-```
-
-
-On Nats client (NO Jestream! ) publisher you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    ncotel = NatsClientTelemetry(
-        nc, "NATS_SERVICE", tracer_provider
-    )
-
-     await ncotel.publish("testing.telemetry", request.name.encode())
-
-```
-
-On Nats client (NO Jestream! ) subscriber you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-    set_global_textmap(B3MultiFormat())
-    ncotel = NatsClientContextTelemetry(
-        js, "NATS_SERVICE", tracer_provider
-    )
-
-    subscription = await ncotel.subscribe(
-        subject="testing.telemetry",
-        queue="queue_nname",
-        cb=handler,
-    )
-
-```
-
-
-On Nats client (NO Jestream! ) request you should add:
-
-```python
-    nc = await nats.connect(servers=[self.natsd])
-    js = self.nc.jetstream()
-    tracer_provider = get_telemetry("NATS_SERVICE")
-    if not tracer_provider.initialized:
-        await init_telemetry(tracer_provider)
-
-    set_global_textmap(B3MultiFormat())
-    ncotel = NatsClientTelemetry(
-        nc, "NATS_SERVICE", tracer_provider
-    )
-
-    response = await ncotel.request("testing.telemetry", request.name.encode())
-
-```
-
-And to handle responses on the other side, you can use the same pattern as in plain Nats client
-subscriber, just adding the `msg.respond()` on the handler when done
+# NucliaDB Telemetry
+
+Open telemetry compatible plugin to propagate traceid on FastAPI, Nats and GRPC with Asyncio.
+
+ENV vars:
+
+```
+    JAEGER_ENABLED = True
+    JAEGER_HOST = "127.0.0.1"
+    JAEGER_PORT = server.port
+```
+
+On FastAPI you should add:
+
+```python
+    tracer_provider = get_telemetry("HTTP_SERVICE")
+    app = FastAPI(title="Test API")  # type: ignore
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    FastAPIInstrumentor.instrument_app(app, tracer_provider=tracer_provider)
+
+    ..
+    await init_telemetry(tracer_provider)  # To start asyncio task
+    ..
+
+```
+
+On GRPC Server you should add:
+
+```python
+    tracer_provider = get_telemetry("GRPC_SERVER_SERVICE")
+    telemetry_grpc = GRPCTelemetry("GRPC_CLIENT_SERVICE", tracer_provider)
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    server = telemetry_grpc.init_server()
+    helloworld_pb2_grpc.add_GreeterServicer_to_server(SERVICER, server)
+
+    ..
+    await init_telemetry(tracer_provider)  # To start asyncio task
+    ..
+```
+
+On GRPC Client you should add:
+
+```python
+    tracer_provider = get_telemetry("GRPC_CLIENT_SERVICE")
+    telemetry_grpc = GRPCTelemetry("GRPC_CLIENT_SERVICE", tracer_provider)
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
+    stub = helloworld_pb2_grpc.GreeterStub(channel)
+
+    ..
+    await init_telemetry(tracer_provider)  # To start asyncio task
+    ..
+
+```
+
+On Nats jetstream push subscriber you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+    set_global_textmap(B3MultiFormat())
+    jsotel = JetStreamContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+    subscription = await jsotel.subscribe(
+        subject="testing.telemetry",
+        stream="testing",
+        cb=handler,
+    )
+
+```
+
+On Nats publisher you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    jsotel = JetStreamContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+     await jsotel.publish("testing.telemetry", request.name.encode())
+
+```
+
+
+On Nats jetstream pull subscription you can use different patterns if you want to
+just get one message and exit or pull several ones. For just one message
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+    set_global_textmap(B3MultiFormat())
+    jsotel = JetStreamContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+    # You can use either pull_subscribe or pull_subscribe_bind
+    subscription = await jsotel.pull_subscribe(
+        subject="testing.telemetry",
+        durable="consumer_name"
+        stream="testing",
+    )
+
+    async def callback(message):
+        # Do something with your message
+        # and optionally return something
+        return True
+
+    try:
+        result = await jsotel.pull_one(subscription, callback)
+    except errors.TimeoutError
+        pass
+
+```
+For multiple messages just wrap it in a loop:
+
+```python
+    while True:
+        try:
+            result = await jsotel.pull_one(subscription, callback)
+        except errors.TimeoutError
+            pass
+
+```
+
+
+On Nats client (NO Jestream! ) publisher you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    ncotel = NatsClientTelemetry(
+        nc, "NATS_SERVICE", tracer_provider
+    )
+
+     await ncotel.publish("testing.telemetry", request.name.encode())
+
+```
+
+On Nats client (NO Jestream! ) subscriber you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+    set_global_textmap(B3MultiFormat())
+    ncotel = NatsClientContextTelemetry(
+        js, "NATS_SERVICE", tracer_provider
+    )
+
+    subscription = await ncotel.subscribe(
+        subject="testing.telemetry",
+        queue="queue_nname",
+        cb=handler,
+    )
+
+```
+
+
+On Nats client (NO Jestream! ) request you should add:
+
+```python
+    nc = await nats.connect(servers=[self.natsd])
+    js = self.nc.jetstream()
+    tracer_provider = get_telemetry("NATS_SERVICE")
+    if not tracer_provider.initialized:
+        await init_telemetry(tracer_provider)
+
+    set_global_textmap(B3MultiFormat())
+    ncotel = NatsClientTelemetry(
+        nc, "NATS_SERVICE", tracer_provider
+    )
+
+    response = await ncotel.request("testing.telemetry", request.name.encode())
+
+```
+
+And to handle responses on the other side, you can use the same pattern as in plain Nats client
+subscriber, just adding the `msg.respond()` on the handler when done
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/__init__.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,22 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import logging
-
-logger = logging.getLogger("nucliadb_telemetry")
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/batch_span.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,312 +1,312 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import List, Optional
-
-from opentelemetry.context import (  # type: ignore
-    _SUPPRESS_INSTRUMENTATION_KEY,
-    Context,
-    attach,
-    detach,
-    set_value,
-)
-from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
-from opentelemetry.sdk.trace.export import SpanExporter  # type: ignore
-from opentelemetry.util._time import _time_ns  # type: ignore
-
-from nucliadb_telemetry import logger
-
-
-class _FlushRequest:
-    """Represents a request for the BatchSpanProcessor to flush spans."""
-
-    __slots__ = ["event", "num_spans"]
-
-    def __init__(self):
-        self.event = asyncio.Event()
-        self.num_spans = 0
-
-
-class BatchSpanProcessor(SpanProcessor):
-    """Batch span processor implementation.
-    `BatchSpanProcessor` is an implementation of `SpanProcessor` that
-    batches ended spans and pushes them to the configured `SpanExporter`.
-    `BatchSpanProcessor` is configurable with the following environment
-    variables which correspond to constructor parameters:
-    - :envvar:`OTEL_BSP_SCHEDULE_DELAY`
-    - :envvar:`OTEL_BSP_MAX_QUEUE_SIZE`
-    - :envvar:`OTEL_BSP_MAX_EXPORT_BATCH_SIZE`
-    - :envvar:`OTEL_BSP_EXPORT_TIMEOUT`
-    """
-
-    def __init__(
-        self,
-        span_exporter: SpanExporter,
-        max_queue_size: int = 2048,
-        schedule_delay_millis: int = 5000,
-        max_export_batch_size: int = 512,
-        export_timeout_millis: int = 30000,
-    ):
-        if max_queue_size <= 0:
-            raise ValueError("max_queue_size must be a positive integer.")
-
-        if schedule_delay_millis <= 0:
-            raise ValueError("schedule_delay_millis must be positive.")
-
-        if max_export_batch_size <= 0:
-            raise ValueError("max_export_batch_size must be a positive integer.")
-
-        if max_export_batch_size > max_queue_size:
-            raise ValueError(
-                "max_export_batch_size must be less than or equal to max_queue_size."
-            )
-
-        self.span_exporter = span_exporter
-        self.queue = asyncio.Queue(maxsize=max_queue_size)  # type: asyncio.Queue[Span]
-        self.worker_task: asyncio.Task = asyncio.create_task(
-            self.worker(), name="OtelBatchSpanProcessor"
-        )
-        self.condition = asyncio.Condition()
-        self._flush_request = None  # type: Optional[_FlushRequest]
-        self.schedule_delay_millis = schedule_delay_millis
-        self.max_export_batch_size = max_export_batch_size
-        self.max_queue_size = max_queue_size
-        self.export_timeout_millis = export_timeout_millis
-        self.done = False
-        # flag that indicates that spans are being dropped
-        self._spans_dropped = False
-        # precallocated list to send spans to exporter
-        self.spans_list = [
-            None
-        ] * self.max_export_batch_size  # type: List[Optional[Span]]
-
-    def on_start(self, span: Span, parent_context: Optional[Context] = None) -> None:
-        pass
-
-    def on_end(self, span: ReadableSpan) -> None:
-        if self.done:
-            logger.warning("Already shutdown, dropping span.")
-            return
-        if not span.context.trace_flags.sampled:
-            return
-        if self.queue.full():
-            if not self._spans_dropped:
-                logger.warning("Queue is full, likely spans will be dropped.")
-                self._spans_dropped = True
-
-        try:
-            self.queue.put_nowait(span)  # type: ignore
-        except asyncio.QueueFull:
-            logger.warning(f"Queue is full. Queue size : {self.queue.qsize()}")
-        except Exception as e:
-            logger.exception(e)
-
-        if self.queue.qsize() >= self.max_export_batch_size:
-            asyncio.create_task(self.notify())
-
-    async def notify(self):
-        async with self.condition:
-            self.condition.notify()
-
-    async def notify_all(self):
-        async with self.condition:
-            self.condition.notify_all()
-
-    async def worker(self):
-        try:
-            logger.info("Batch telemetry event loop started")
-            await self._worker()
-        except Exception as e:
-            logger.exception(e)
-            raise e
-
-    async def _worker(self) -> None:
-        timeout = self.schedule_delay_millis / 1e3
-        flush_request = None  # type: Optional[_FlushRequest]
-        while not self.done:
-            logger.debug("Waiting condition")
-            async with self.condition:
-                if self.done:
-                    # done flag may have changed, avoid waiting
-                    break
-                logger.debug(f"{self.queue.qsize()} spans on queue")
-                flush_request = self._get_and_unset_flush_request()
-                if (
-                    self.queue.qsize() < self.max_export_batch_size
-                    and flush_request is None
-                ):
-                    try:
-                        await asyncio.wait_for(self.condition.wait(), timeout)
-                    except asyncio.TimeoutError:
-                        pass
-                    flush_request = self._get_and_unset_flush_request()
-                    if not self.queue:
-                        # spurious notification, let's wait again, reset timeout
-                        timeout = self.schedule_delay_millis / 1e3
-                        self._notify_flush_request_finished(flush_request)
-                        flush_request = None
-                        continue
-                    if self.done:
-                        # missing spans will be sent when calling flush
-                        break
-
-            # subtract the duration of this export call to the next timeout
-            start = _time_ns()
-            try:
-                await asyncio.wait_for(self._export(flush_request), timeout)
-            except asyncio.TimeoutError:
-                logger.exception("Took to much time to export, network problem ahead")
-            end = _time_ns()
-            duration = (end - start) / 1e9
-            timeout = self.schedule_delay_millis / 1e3 - duration
-
-            self._notify_flush_request_finished(flush_request)
-            flush_request = None
-
-        # there might have been a new flush request while export was running
-        # and before the done flag switched to true
-        async with self.condition:
-            shutdown_flush_request = self._get_and_unset_flush_request()
-
-        # be sure that all spans are sent
-        await self._drain_queue()
-        self._notify_flush_request_finished(flush_request)
-        self._notify_flush_request_finished(shutdown_flush_request)
-
-    def _get_and_unset_flush_request(
-        self,
-    ) -> Optional[_FlushRequest]:
-        """Returns the current flush request and makes it invisible to the
-        worker thread for subsequent calls.
-        """
-        flush_request = self._flush_request
-        self._flush_request = None
-        if flush_request is not None:
-            flush_request.num_spans = self.queue.qsize()
-        return flush_request
-
-    @staticmethod
-    def _notify_flush_request_finished(
-        flush_request: Optional[_FlushRequest],
-    ):
-        """Notifies the flush initiator(s) waiting on the given request/event
-        that the flush operation was finished.
-        """
-        if flush_request is not None:
-            flush_request.event.set()
-
-    def _get_or_create_flush_request(self) -> _FlushRequest:
-        """Either returns the current active flush event or creates a new one.
-        The flush event will be visible and read by the worker thread before an
-        export operation starts. Callers of a flush operation may wait on the
-        returned event to be notified when the flush/export operation was
-        finished.
-        This method is not thread-safe, i.e. callers need to take care about
-        synchronization/locking.
-        """
-        if self._flush_request is None:
-            self._flush_request = _FlushRequest()
-        return self._flush_request
-
-    async def _export(self, flush_request: Optional[_FlushRequest]):
-        """Exports spans considering the given flush_request.
-        In case of a given flush_requests spans are exported in batches until
-        the number of exported spans reached or exceeded the number of spans in
-        the flush request.
-        In no flush_request was given at most max_export_batch_size spans are
-        exported.
-        """
-        if not flush_request:
-            await self._export_batch()
-            return
-
-        num_spans = flush_request.num_spans
-        while self.queue.qsize():
-            num_exported = await self._export_batch()
-            num_spans -= num_exported
-
-            if num_spans <= 0:
-                break
-
-    async def _export_batch(self) -> int:
-        """Exports at most max_export_batch_size spans and returns the number of
-        exported spans.
-        """
-        idx = 0
-        # currently only a single thread acts as consumer, so queue.pop() will
-        # not raise an exception
-        while idx < self.max_export_batch_size and self.queue.qsize():
-            self.spans_list[idx] = self.queue.get_nowait()
-            idx += 1
-        token = attach(set_value(_SUPPRESS_INSTRUMENTATION_KEY, True))
-        try:
-            # Ignore type b/c the Optional[None]+slicing is too "clever"
-            # for mypy
-            await self.span_exporter.async_export(self.spans_list[:idx])  # type: ignore
-        except asyncio.CancelledError:
-            logger.exception("Task was canceled while exporting Span batch")
-        except Exception:  # pylint: disable=broad-except
-            logger.exception("Exception while exporting Span batch)")
-        detach(token)
-
-        # clean up list
-        for index in range(idx):
-            self.spans_list[index] = None
-        return idx
-
-    async def _drain_queue(self):
-        """Export all elements until queue is empty.
-        Can only be called from the worker thread context because it invokes
-        `export` that is not thread safe.
-        """
-        while self.queue.qsize():
-            await self._export_batch()
-
-    async def async_force_flush(self, timeout_millis: Optional[int] = None) -> bool:
-        if timeout_millis is None:
-            timeout_millis = self.export_timeout_millis
-
-        if self.done:
-            logger.warning("Already shutdown, ignoring call to async_force_flush().")
-            return True
-
-        async with self.condition:
-            flush_request = self._get_or_create_flush_request()
-            # signal the worker task to flush and wait for it to finish
-            self.condition.notify_all()
-
-        if flush_request.num_spans == 0:
-            return True
-
-        # wait for token to be processed
-        ret = await asyncio.wait_for(flush_request.event.wait(), timeout_millis)
-        if not ret:
-            logger.warning("Timeout was exceeded in async_force_flush().")
-            return False
-        return ret
-
-    def shutdown(self) -> None:
-        # signal the worker thread to finish and then wait for it
-        self.done = True
-        self.span_exporter.shutdown()
-        try:
-            self.worker_task.cancel()
-        except RuntimeError:
-            pass
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import List, Optional
+
+from opentelemetry.context import (  # type: ignore
+    _SUPPRESS_INSTRUMENTATION_KEY,
+    Context,
+    attach,
+    detach,
+    set_value,
+)
+from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
+from opentelemetry.sdk.trace.export import SpanExporter  # type: ignore
+from opentelemetry.util._time import _time_ns  # type: ignore
+
+from nucliadb_telemetry import logger
+
+
+class _FlushRequest:
+    """Represents a request for the BatchSpanProcessor to flush spans."""
+
+    __slots__ = ["event", "num_spans"]
+
+    def __init__(self):
+        self.event = asyncio.Event()
+        self.num_spans = 0
+
+
+class BatchSpanProcessor(SpanProcessor):
+    """Batch span processor implementation.
+    `BatchSpanProcessor` is an implementation of `SpanProcessor` that
+    batches ended spans and pushes them to the configured `SpanExporter`.
+    `BatchSpanProcessor` is configurable with the following environment
+    variables which correspond to constructor parameters:
+    - :envvar:`OTEL_BSP_SCHEDULE_DELAY`
+    - :envvar:`OTEL_BSP_MAX_QUEUE_SIZE`
+    - :envvar:`OTEL_BSP_MAX_EXPORT_BATCH_SIZE`
+    - :envvar:`OTEL_BSP_EXPORT_TIMEOUT`
+    """
+
+    def __init__(
+        self,
+        span_exporter: SpanExporter,
+        max_queue_size: int = 2048,
+        schedule_delay_millis: int = 5000,
+        max_export_batch_size: int = 512,
+        export_timeout_millis: int = 30000,
+    ):
+        if max_queue_size <= 0:
+            raise ValueError("max_queue_size must be a positive integer.")
+
+        if schedule_delay_millis <= 0:
+            raise ValueError("schedule_delay_millis must be positive.")
+
+        if max_export_batch_size <= 0:
+            raise ValueError("max_export_batch_size must be a positive integer.")
+
+        if max_export_batch_size > max_queue_size:
+            raise ValueError(
+                "max_export_batch_size must be less than or equal to max_queue_size."
+            )
+
+        self.span_exporter = span_exporter
+        self.queue = asyncio.Queue(maxsize=max_queue_size)  # type: asyncio.Queue[Span]
+        self.worker_task: asyncio.Task = asyncio.create_task(
+            self.worker(), name="OtelBatchSpanProcessor"
+        )
+        self.condition = asyncio.Condition()
+        self._flush_request = None  # type: Optional[_FlushRequest]
+        self.schedule_delay_millis = schedule_delay_millis
+        self.max_export_batch_size = max_export_batch_size
+        self.max_queue_size = max_queue_size
+        self.export_timeout_millis = export_timeout_millis
+        self.done = False
+        # flag that indicates that spans are being dropped
+        self._spans_dropped = False
+        # precallocated list to send spans to exporter
+        self.spans_list = [
+            None
+        ] * self.max_export_batch_size  # type: List[Optional[Span]]
+
+    def on_start(self, span: Span, parent_context: Optional[Context] = None) -> None:
+        pass
+
+    def on_end(self, span: ReadableSpan) -> None:
+        if self.done:
+            logger.warning("Already shutdown, dropping span.")
+            return
+        if not span.context.trace_flags.sampled:
+            return
+        if self.queue.full():
+            if not self._spans_dropped:
+                logger.warning("Queue is full, likely spans will be dropped.")
+                self._spans_dropped = True
+
+        try:
+            self.queue.put_nowait(span)  # type: ignore
+        except asyncio.QueueFull:
+            logger.warning(f"Queue is full. Queue size : {self.queue.qsize()}")
+        except Exception as e:
+            logger.exception(e)
+
+        if self.queue.qsize() >= self.max_export_batch_size:
+            asyncio.create_task(self.notify())
+
+    async def notify(self):
+        async with self.condition:
+            self.condition.notify()
+
+    async def notify_all(self):
+        async with self.condition:
+            self.condition.notify_all()
+
+    async def worker(self):
+        try:
+            logger.info("Batch telemetry event loop started")
+            await self._worker()
+        except Exception as e:
+            logger.exception(e)
+            raise e
+
+    async def _worker(self) -> None:
+        timeout = self.schedule_delay_millis / 1e3
+        flush_request = None  # type: Optional[_FlushRequest]
+        while not self.done:
+            logger.debug("Waiting condition")
+            async with self.condition:
+                if self.done:
+                    # done flag may have changed, avoid waiting
+                    break
+                logger.debug(f"{self.queue.qsize()} spans on queue")
+                flush_request = self._get_and_unset_flush_request()
+                if (
+                    self.queue.qsize() < self.max_export_batch_size
+                    and flush_request is None
+                ):
+                    try:
+                        await asyncio.wait_for(self.condition.wait(), timeout)
+                    except asyncio.TimeoutError:
+                        pass
+                    flush_request = self._get_and_unset_flush_request()
+                    if not self.queue:
+                        # spurious notification, let's wait again, reset timeout
+                        timeout = self.schedule_delay_millis / 1e3
+                        self._notify_flush_request_finished(flush_request)
+                        flush_request = None
+                        continue
+                    if self.done:
+                        # missing spans will be sent when calling flush
+                        break
+
+            # subtract the duration of this export call to the next timeout
+            start = _time_ns()
+            try:
+                await asyncio.wait_for(self._export(flush_request), timeout)
+            except asyncio.TimeoutError:
+                logger.exception("Took to much time to export, network problem ahead")
+            end = _time_ns()
+            duration = (end - start) / 1e9
+            timeout = self.schedule_delay_millis / 1e3 - duration
+
+            self._notify_flush_request_finished(flush_request)
+            flush_request = None
+
+        # there might have been a new flush request while export was running
+        # and before the done flag switched to true
+        async with self.condition:
+            shutdown_flush_request = self._get_and_unset_flush_request()
+
+        # be sure that all spans are sent
+        await self._drain_queue()
+        self._notify_flush_request_finished(flush_request)
+        self._notify_flush_request_finished(shutdown_flush_request)
+
+    def _get_and_unset_flush_request(
+        self,
+    ) -> Optional[_FlushRequest]:
+        """Returns the current flush request and makes it invisible to the
+        worker thread for subsequent calls.
+        """
+        flush_request = self._flush_request
+        self._flush_request = None
+        if flush_request is not None:
+            flush_request.num_spans = self.queue.qsize()
+        return flush_request
+
+    @staticmethod
+    def _notify_flush_request_finished(
+        flush_request: Optional[_FlushRequest],
+    ):
+        """Notifies the flush initiator(s) waiting on the given request/event
+        that the flush operation was finished.
+        """
+        if flush_request is not None:
+            flush_request.event.set()
+
+    def _get_or_create_flush_request(self) -> _FlushRequest:
+        """Either returns the current active flush event or creates a new one.
+        The flush event will be visible and read by the worker thread before an
+        export operation starts. Callers of a flush operation may wait on the
+        returned event to be notified when the flush/export operation was
+        finished.
+        This method is not thread-safe, i.e. callers need to take care about
+        synchronization/locking.
+        """
+        if self._flush_request is None:
+            self._flush_request = _FlushRequest()
+        return self._flush_request
+
+    async def _export(self, flush_request: Optional[_FlushRequest]):
+        """Exports spans considering the given flush_request.
+        In case of a given flush_requests spans are exported in batches until
+        the number of exported spans reached or exceeded the number of spans in
+        the flush request.
+        In no flush_request was given at most max_export_batch_size spans are
+        exported.
+        """
+        if not flush_request:
+            await self._export_batch()
+            return
+
+        num_spans = flush_request.num_spans
+        while self.queue.qsize():
+            num_exported = await self._export_batch()
+            num_spans -= num_exported
+
+            if num_spans <= 0:
+                break
+
+    async def _export_batch(self) -> int:
+        """Exports at most max_export_batch_size spans and returns the number of
+        exported spans.
+        """
+        idx = 0
+        # currently only a single thread acts as consumer, so queue.pop() will
+        # not raise an exception
+        while idx < self.max_export_batch_size and self.queue.qsize():
+            self.spans_list[idx] = self.queue.get_nowait()
+            idx += 1
+        token = attach(set_value(_SUPPRESS_INSTRUMENTATION_KEY, True))
+        try:
+            # Ignore type b/c the Optional[None]+slicing is too "clever"
+            # for mypy
+            await self.span_exporter.async_export(self.spans_list[:idx])  # type: ignore
+        except asyncio.CancelledError:
+            logger.exception("Task was canceled while exporting Span batch")
+        except Exception:  # pylint: disable=broad-except
+            logger.exception("Exception while exporting Span batch)")
+        detach(token)
+
+        # clean up list
+        for index in range(idx):
+            self.spans_list[index] = None
+        return idx
+
+    async def _drain_queue(self):
+        """Export all elements until queue is empty.
+        Can only be called from the worker thread context because it invokes
+        `export` that is not thread safe.
+        """
+        while self.queue.qsize():
+            await self._export_batch()
+
+    async def async_force_flush(self, timeout_millis: Optional[int] = None) -> bool:
+        if timeout_millis is None:
+            timeout_millis = self.export_timeout_millis
+
+        if self.done:
+            logger.warning("Already shutdown, ignoring call to async_force_flush().")
+            return True
+
+        async with self.condition:
+            flush_request = self._get_or_create_flush_request()
+            # signal the worker task to flush and wait for it to finish
+            self.condition.notify_all()
+
+        if flush_request.num_spans == 0:
+            return True
+
+        # wait for token to be processed
+        ret = await asyncio.wait_for(flush_request.event.wait(), timeout_millis)
+        if not ret:
+            logger.warning("Timeout was exceeded in async_force_flush().")
+            return False
+        return ret
+
+    def shutdown(self) -> None:
+        # signal the worker thread to finish and then wait for it
+        self.done = True
+        self.span_exporter.shutdown()
+        try:
+            self.worker_task.cancel()
+        except RuntimeError:
+            pass
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/common.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import traceback
-
-from opentelemetry.sdk.trace import Span  # type: ignore
-from opentelemetry.trace.status import Status, StatusCode  # type: ignore
-
-
-def set_span_exception(span: Span, exception: Exception):
-    description = traceback.format_exc()
-
-    span.set_status(
-        Status(
-            status_code=StatusCode.ERROR,
-            description=description,
-        )
-    )
-    span.record_exception(exception)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import traceback
+
+from opentelemetry.sdk.trace import Span  # type: ignore
+from opentelemetry.trace.status import Status, StatusCode  # type: ignore
+
+
+def set_span_exception(span: Span, exception: Exception):
+    description = traceback.format_exc()
+
+    span.set_status(
+        Status(
+            status_code=StatusCode.ERROR,
+            description=description,
+        )
+    )
+    span.record_exception(exception)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/errors.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,99 +1,99 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-# abstract advanced error handling into its own module to prevent
-# code from handling sentry integration everywhere
-import logging
-from typing import Any, ContextManager, Optional
-
-import pydantic
-
-try:
-    import sentry_sdk
-    from sentry_sdk import Scope
-    from sentry_sdk.integrations.logging import LoggingIntegration
-
-    SENTRY = True
-except ImportError:  # pragma: no cover
-    Scope = LoggingIntegration = sentry_sdk = None  # type: ignore
-    SENTRY = False
-
-
-def capture_exception(error: BaseException) -> Optional[str]:
-    if SENTRY:
-        return sentry_sdk.capture_exception(error)
-    return None
-
-
-def capture_message(
-    error_msg: str,
-    level: Optional[str] = None,
-    scope: Optional[Any] = None,
-    **scope_args: Any
-) -> Optional[str]:
-    if SENTRY:
-        return sentry_sdk.capture_message(error_msg, level, scope, **scope_args)
-    return None
-
-
-class NoopScope:
-    def __enter__(self):
-        return self
-
-    def __exit__(self, *args):
-        ...
-
-    def set_extra(self, key: str, value: Any) -> None:
-        ...
-
-
-def push_scope(**kwargs: Any) -> ContextManager[Scope]:
-    if SENTRY:
-        return sentry_sdk.push_scope(**kwargs)
-    else:
-        return NoopScope()  # type: ignore
-
-
-class ErrorHandlingSettings(pydantic.BaseSettings):
-    sentry_url: Optional[str] = None
-    environment: str = pydantic.Field(
-        "local", env=["environment", "running_environment"]
-    )
-    logging_integration: bool = False
-
-
-def setup_error_handling(version: str) -> None:
-    settings = ErrorHandlingSettings()
-
-    if settings.sentry_url:
-        enabled_integrations: list[Any] = []
-
-        if settings.logging_integration:
-            sentry_logging = LoggingIntegration(
-                level=logging.CRITICAL, event_level=logging.CRITICAL
-            )
-            enabled_integrations.append(sentry_logging)
-
-        sentry_sdk.init(
-            release=version,
-            environment=settings.environment,
-            dsn=settings.sentry_url,
-            integrations=enabled_integrations,
-            default_integrations=False,
-        )
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+# abstract advanced error handling into its own module to prevent
+# code from handling sentry integration everywhere
+import logging
+from typing import Any, ContextManager, Optional
+
+import pydantic
+
+try:
+    import sentry_sdk
+    from sentry_sdk import Scope
+    from sentry_sdk.integrations.logging import LoggingIntegration
+
+    SENTRY = True
+except ImportError:  # pragma: no cover
+    Scope = LoggingIntegration = sentry_sdk = None  # type: ignore
+    SENTRY = False
+
+
+def capture_exception(error: BaseException) -> Optional[str]:
+    if SENTRY:
+        return sentry_sdk.capture_exception(error)
+    return None
+
+
+def capture_message(
+    error_msg: str,
+    level: Optional[str] = None,
+    scope: Optional[Any] = None,
+    **scope_args: Any
+) -> Optional[str]:
+    if SENTRY:
+        return sentry_sdk.capture_message(error_msg, level, scope, **scope_args)
+    return None
+
+
+class NoopScope:
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *args):
+        ...
+
+    def set_extra(self, key: str, value: Any) -> None:
+        ...
+
+
+def push_scope(**kwargs: Any) -> ContextManager[Scope]:
+    if SENTRY:
+        return sentry_sdk.push_scope(**kwargs)
+    else:
+        return NoopScope()  # type: ignore
+
+
+class ErrorHandlingSettings(pydantic.BaseSettings):
+    sentry_url: Optional[str] = None
+    environment: str = pydantic.Field(
+        "local", env=["environment", "running_environment"]
+    )
+    logging_integration: bool = False
+
+
+def setup_error_handling(version: str) -> None:
+    settings = ErrorHandlingSettings()
+
+    if settings.sentry_url:
+        enabled_integrations: list[Any] = []
+
+        if settings.logging_integration:
+            sentry_logging = LoggingIntegration(
+                level=logging.CRITICAL, event_level=logging.CRITICAL
+            )
+            enabled_integrations.append(sentry_logging)
+
+        sentry_sdk.init(
+            release=version,
+            environment=settings.environment,
+            dsn=settings.sentry_url,
+            integrations=enabled_integrations,
+            default_integrations=False,
+        )
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/fastapi/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,71 +1,84 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from typing import Callable, Iterable, List, Optional
-from urllib.parse import urlparse
-
-from fastapi import FastAPI
-from opentelemetry.instrumentation.asgi import OpenTelemetryMiddleware  # type: ignore
-from opentelemetry.instrumentation.fastapi import _get_route_details  # type: ignore
-from opentelemetry.trace import Span  # type: ignore
-
-try:
-    from sentry_sdk.integrations.asgi import SentryAsgiMiddleware
-except ImportError:  # pragma: no cover
-    SentryAsgiMiddleware = None  # type: ignore
-
-
-class ExcludeList:
-
-    """Class to exclude certain paths (given as a list of regexes) from tracing requests"""
-
-    def __init__(self, excluded_urls: Iterable[str]):
-        self._excluded_urls = excluded_urls
-
-    def url_disabled(self, url: str) -> bool:
-        return bool(self._excluded_urls and urlparse(url).path in self._excluded_urls)
-
-
-_ServerRequestHookT = Optional[Callable[[Span, dict], None]]
-_ClientRequestHookT = Optional[Callable[[Span, dict], None]]
-_ClientResponseHookT = Optional[Callable[[Span, dict], None]]
-
-
-def instrument_app(
-    app: FastAPI,
-    excluded_urls: List[str],
-    server_request_hook: _ServerRequestHookT = None,
-    client_request_hook: _ClientRequestHookT = None,
-    client_response_hook: _ClientResponseHookT = None,
-    tracer_provider=None,
-):
-    if SentryAsgiMiddleware is not None:
-        app.add_middleware(SentryAsgiMiddleware)
-
-    excluded_urls_obj = ExcludeList(excluded_urls)
-
-    app.add_middleware(
-        OpenTelemetryMiddleware,
-        excluded_urls=excluded_urls_obj,
-        default_span_details=_get_route_details,
-        server_request_hook=server_request_hook,
-        client_request_hook=client_request_hook,
-        client_response_hook=client_response_hook,
-        tracer_provider=tracer_provider,
-    )
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from typing import Iterable, List
+from urllib.parse import urlparse
+
+import prometheus_client  # type: ignore
+from fastapi import FastAPI
+from opentelemetry.instrumentation.fastapi import _get_route_details  # type: ignore
+from prometheus_client import CONTENT_TYPE_LATEST
+from starlette.responses import PlainTextResponse
+
+from nucliadb_telemetry.fastapi.metrics import PrometheusMiddleware
+from nucliadb_telemetry.fastapi.tracing import (
+    OpenTelemetryMiddleware,
+    ServerRequestHookT,
+)
+
+try:
+    from sentry_sdk.integrations.asgi import SentryAsgiMiddleware
+except ImportError:  # pragma: no cover
+    SentryAsgiMiddleware = None  # type: ignore
+
+
+async def metrics_endpoint(request):
+    output = prometheus_client.exposition.generate_latest()
+    return PlainTextResponse(
+        output.decode("utf8"), headers={"Content-Type": CONTENT_TYPE_LATEST}
+    )
+
+
+application_metrics = FastAPI(title="Metrics")  # type: ignore
+application_metrics.add_route("/metrics", metrics_endpoint)
+
+
+class ExcludeList:
+
+    """Class to exclude certain paths (given as a list of regexes) from tracing requests"""
+
+    def __init__(self, excluded_urls: Iterable[str]):
+        self._excluded_urls = excluded_urls
+
+    def url_disabled(self, url: str) -> bool:
+        return bool(self._excluded_urls and urlparse(url).path in self._excluded_urls)
+
+
+def instrument_app(
+    app: FastAPI,
+    excluded_urls: List[str],
+    server_request_hook: ServerRequestHookT = None,
+    tracer_provider=None,
+    metrics=False,
+):
+    if metrics:
+        # b/w compat
+        app.add_middleware(PrometheusMiddleware, filter_unhandled_paths=True)
+    if SentryAsgiMiddleware is not None:
+        app.add_middleware(SentryAsgiMiddleware)
+
+    excluded_urls_obj = ExcludeList(excluded_urls)
+
+    app.add_middleware(
+        OpenTelemetryMiddleware,
+        excluded_urls=excluded_urls_obj,
+        default_span_details=_get_route_details,
+        server_request_hook=server_request_hook,
+        tracer_provider=tracer_provider,
+    )
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/grpc.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,399 +1,409 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import functools
-from collections import OrderedDict
-from concurrent import futures
-from contextlib import contextmanager
-from typing import Any, Awaitable, Callable, List, Optional
-
-import grpc
-from grpc import ChannelCredentials, ClientCallDetails, aio  # type: ignore
-from grpc.experimental import wrap_server_method_handler  # type: ignore
-from opentelemetry.context import attach, detach
-from opentelemetry.propagate import extract, inject
-from opentelemetry.propagators.textmap import CarrierT, Setter  # type: ignore
-from opentelemetry.sdk.trace import Span  # type: ignore
-from opentelemetry.sdk.trace import TracerProvider  # type: ignore
-from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
-from opentelemetry.trace import SpanKind  # type: ignore
-from opentelemetry.trace import Tracer  # type: ignore
-from opentelemetry.trace.status import Status, StatusCode  # type: ignore
-
-from nucliadb_telemetry import logger
-from nucliadb_telemetry.common import set_span_exception
-
-
-class _CarrierSetter(Setter):
-    """We use a custom setter in order to be able to lower case
-    keys as is required by grpc.
-    """
-
-    def set(self, carrier: CarrierT, key: str, value: str):  # type: ignore
-        carrier[key.lower()] = value  # type: ignore
-
-
-_carrier_setter = _CarrierSetter()
-
-
-def finish_span_grpc(span: Span, result):
-    code = result._cython_call._status.code()
-    if code != grpc.StatusCode.OK:
-        span.set_status(
-            Status(
-                status_code=StatusCode.OK,
-            )
-        )
-    else:
-        span.set_status(
-            Status(
-                status_code=StatusCode.ERROR,
-                description=result._cython_call._status.details(),
-            )
-        )
-    span.end()
-
-
-def start_span_client(
-    tracer: Tracer,
-    client_call_details: grpc.ClientCallDetails,
-    set_status_on_exception=False,
-):
-    if isinstance(client_call_details.method, bytes):
-        service, meth = client_call_details.method.decode().lstrip("/").split("/", 1)
-        method_name = client_call_details.method.decode()
-    else:
-        service, meth = client_call_details.method.lstrip("/").split("/", 1)
-        method_name = client_call_details.method
-
-    attributes = {
-        SpanAttributes.RPC_SYSTEM: "grpc",
-        SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
-        SpanAttributes.RPC_METHOD: meth,
-        SpanAttributes.RPC_SERVICE: service,
-    }
-
-    # add some attributes from the metadata
-    if client_call_details.metadata is not None:
-        mutable_metadata = OrderedDict(tuple(client_call_details.metadata))
-        inject(mutable_metadata, setter=_carrier_setter)  # type: ignore
-        for key, value in mutable_metadata.items():
-            client_call_details.metadata.add(key=key, value=value)  # type: ignore
-
-    span = tracer.start_span(  # type: ignore
-        name=method_name,
-        kind=SpanKind.CLIENT,
-        attributes=attributes,  # type: ignore
-        set_status_on_exception=set_status_on_exception,
-    )
-    return span
-
-
-class OpenTelemetryServerInterceptor(aio.ServerInterceptor):
-    """
-    A gRPC server interceptor, to add OpenTelemetry.
-    Usage::
-        tracer = some OpenTelemetry tracer
-        interceptors = [
-            OpenTelemetryServerInterceptor(tracer),
-        ]
-        server = grpc.server(
-            futures.ThreadPoolExecutor(max_workers=concurrency),
-            interceptors = interceptors)
-    """
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    def start_span_server(
-        self,
-        handler_call_details: grpc.HandlerCallDetails,
-        context: grpc.ServicerContext,
-        set_status_on_exception=False,
-    ):
-        service, meth = handler_call_details.method.lstrip("/").split("/", 1)  # type: ignore
-
-        attributes = {
-            SpanAttributes.RPC_SYSTEM: "grpc",
-            SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
-            SpanAttributes.RPC_METHOD: meth,
-            SpanAttributes.RPC_SERVICE: service,
-        }
-
-        # add some attributes from the metadata
-        metadata = dict(context.invocation_metadata())
-        if "user-agent" in metadata:
-            attributes["rpc.user_agent"] = metadata["user-agent"]
-
-        # Split up the peer to keep with how other telemetry sources
-        # do it.  This looks like:
-        # * ipv6:[::1]:57284
-        # * ipv4:127.0.0.1:57284
-        # * ipv4:10.2.1.1:57284,127.0.0.1:57284
-        #
-        try:
-            ip, port = context.peer().split(",")[0].split(":", 1)[1].rsplit(":", 1)
-            attributes.update(
-                {SpanAttributes.NET_PEER_IP: ip, SpanAttributes.NET_PEER_PORT: port}
-            )
-
-            # other telemetry sources add this, so we will too
-            if ip in ("[::1]", "127.0.0.1"):
-                attributes[SpanAttributes.NET_PEER_NAME] = "localhost"
-
-        except IndexError:
-            logger.warning("Failed to parse peer address '%s'", context.peer())
-
-        return self.tracer.start_as_current_span(  # type: ignore
-            name=handler_call_details.method,  # type: ignore
-            kind=SpanKind.SERVER,
-            attributes=attributes,
-            set_status_on_exception=set_status_on_exception,
-        )
-
-    # Handle streaming responses separately - we have to do this
-    # to return a *new* generator or various upstream things
-    # get confused, or we'll lose the consistent trace
-    async def _intercept_server_stream(
-        self, behavior, handler_call_details, request_or_iterator, context
-    ):
-        with self._set_remote_context(context):
-            with self.start_span_server(
-                handler_call_details, context, set_status_on_exception=False
-            ) as span:
-                try:
-                    async for response in behavior(request_or_iterator, context):
-                        yield response
-
-                except Exception as error:
-                    # pylint:disable=unidiomatic-typecheck
-                    if type(error) != Exception:
-                        span.record_exception(error)
-                    raise error
-
-    @contextmanager
-    def _set_remote_context(self, servicer_context):
-        metadata = servicer_context.invocation_metadata()
-        if metadata:
-            md_dict = {md.key: md.value for md in metadata}
-            ctx = extract(md_dict)
-            token = attach(ctx)
-            try:
-                yield
-            finally:
-                detach(token)
-        else:
-            yield
-
-    async def intercept_service(
-        self,
-        continuation: Callable[
-            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]
-        ],
-        handler_call_details: grpc.HandlerCallDetails,
-    ) -> grpc.RpcMethodHandler:
-        handler = await continuation(handler_call_details)
-        if handler and (
-            handler.request_streaming or handler.response_streaming
-        ):  # pytype: disable=attribute-error
-            return handler
-
-        def wrapper(behavior: Callable[[Any, aio.ServicerContext], Any]):
-            @functools.wraps(behavior)
-            async def wrapper(request: Any, context: aio.ServicerContext) -> Any:
-                with self._set_remote_context(context):
-                    with self.start_span_server(
-                        handler_call_details,
-                        context,
-                        set_status_on_exception=False,
-                    ) as span:
-                        # And now we run the actual RPC.
-                        try:
-                            value = await behavior(request, context)
-
-                        except Exception as error:
-                            # Bare exceptions are likely to be gRPC aborts, which
-                            # we handle in our context wrapper.
-                            # Here, we're interested in uncaught exceptions.
-                            # pylint:disable=unidiomatic-typecheck
-                            if type(error) != Exception:
-                                span.record_exception(error)
-                            raise error
-                return value
-
-            return wrapper
-
-        if "grpc.health.v1.Health" in handler_call_details.method:  # type: ignore
-            return handler
-
-        return wrap_server_method_handler(wrapper, handler)
-
-
-class UnaryUnaryClientInterceptor(aio.UnaryUnaryClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_unary_unary(
-        self, continuation, client_call_details: ClientCallDetails, request
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-        try:
-            call = await continuation(client_call_details, request)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-        return call
-
-
-class UnaryStreamClientInterceptor(aio.UnaryStreamClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_unary_stream(
-        self, continuation, client_call_details: ClientCallDetails, request
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-
-        try:
-            call = await continuation(client_call_details, request)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-
-        return call
-
-
-class StreamStreamClientInterceptor(aio.StreamStreamClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_stream_stream(
-        self, continuation, client_call_details: ClientCallDetails, request_iterator
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-        try:
-            call = await continuation(client_call_details, request_iterator)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-
-        return call
-
-
-class StreamUnaryClientInterceptor(aio.StreamUnaryClientInterceptor):
-    """Interceptor used for testing if the interceptor is being called"""
-
-    def __init__(self, tracer):
-        self.tracer = tracer
-
-    async def intercept_stream_unary(
-        self, continuation, client_call_details: ClientCallDetails, request_iterator
-    ):
-        span = start_span_client(self.tracer, client_call_details)
-        try:
-            call = await continuation(client_call_details, request_iterator)
-        except Exception as error:
-            if type(error) != Exception:
-                set_span_exception(span, error)
-            raise error
-        else:
-            call.add_done_callback(functools.partial(finish_span_grpc, span))
-
-        return call
-
-
-class OpenTelemetryGRPC:
-    initialized: bool = False
-
-    def __init__(self, service_name: str, tracer_provider: TracerProvider):
-        self.service_name = service_name
-        self.tracer_provider = tracer_provider
-
-    def init_client(
-        self,
-        server_addr: str,
-        max_send_message: int = 100,
-        credentials: Optional[ChannelCredentials] = None,
-    ):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_grpc_client")
-        options = [
-            ("grpc.max_receive_message_length", max_send_message * 1024 * 1024),
-            ("grpc.max_send_message_length", max_send_message * 1024 * 1024),
-        ]
-        if credentials is not None:
-            channel = aio.secure_channel(
-                server_addr,
-                options=options,
-                credentials=credentials,
-                interceptors=[
-                    UnaryUnaryClientInterceptor(tracer=tracer),
-                    UnaryStreamClientInterceptor(tracer=tracer),
-                    StreamStreamClientInterceptor(tracer=tracer),
-                    StreamUnaryClientInterceptor(tracer=tracer),
-                ],
-            )
-
-        else:
-            channel = aio.insecure_channel(
-                server_addr,
-                options=options,
-                interceptors=[
-                    UnaryUnaryClientInterceptor(tracer=tracer),
-                    UnaryStreamClientInterceptor(tracer=tracer),
-                    StreamStreamClientInterceptor(tracer=tracer),
-                    StreamUnaryClientInterceptor(tracer=tracer),
-                ],
-            )
-        return channel
-
-    def init_server(
-        self,
-        concurrency: int = 4,
-        max_receive_message: int = 100,
-        interceptors: Optional[List[aio.ServerInterceptor]] = None,
-    ):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_grpc_server")
-        _interceptors = [OpenTelemetryServerInterceptor(tracer=tracer)]
-        if interceptors is not None:
-            _interceptors.extend(interceptors)
-        options = [
-            ("grpc.max_send_message_length", max_receive_message * 1024 * 1024),
-            ("grpc.max_receive_message_length", max_receive_message * 1024 * 1024),
-        ]
-        server = aio.server(
-            futures.ThreadPoolExecutor(max_workers=concurrency),
-            interceptors=_interceptors,
-            options=options,
-        )
-        return server
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import functools
+from collections import OrderedDict
+from concurrent import futures
+from contextlib import contextmanager
+from typing import Any, Awaitable, Callable, List, Optional
+
+import grpc
+from grpc import ChannelCredentials, ClientCallDetails, aio  # type: ignore
+from grpc.experimental import wrap_server_method_handler  # type: ignore
+from opentelemetry.context import attach, detach
+from opentelemetry.propagate import extract, inject
+from opentelemetry.propagators.textmap import CarrierT, Setter  # type: ignore
+from opentelemetry.sdk.trace import Span  # type: ignore
+from opentelemetry.sdk.trace import TracerProvider  # type: ignore
+from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
+from opentelemetry.trace import SpanKind  # type: ignore
+from opentelemetry.trace import Tracer  # type: ignore
+from opentelemetry.trace.status import Status, StatusCode  # type: ignore
+
+from nucliadb_telemetry import grpc_metrics, logger
+from nucliadb_telemetry.common import set_span_exception
+
+
+class _CarrierSetter(Setter):
+    """We use a custom setter in order to be able to lower case
+    keys as is required by grpc.
+    """
+
+    def set(self, carrier: CarrierT, key: str, value: str):  # type: ignore
+        carrier[key.lower()] = value  # type: ignore
+
+
+_carrier_setter = _CarrierSetter()
+
+
+def finish_span_grpc(span: Span, result):
+    code = result._cython_call._status.code()
+    if code != grpc.StatusCode.OK:
+        span.set_status(
+            Status(
+                status_code=StatusCode.OK,
+            )
+        )
+    else:
+        span.set_status(
+            Status(
+                status_code=StatusCode.ERROR,
+                description=result._cython_call._status.details(),
+            )
+        )
+    span.end()
+
+
+def start_span_client(
+    tracer: Tracer,
+    client_call_details: grpc.ClientCallDetails,
+    set_status_on_exception=False,
+):
+    if isinstance(client_call_details.method, bytes):
+        service, meth = client_call_details.method.decode().lstrip("/").split("/", 1)
+        method_name = client_call_details.method.decode()
+    else:
+        service, meth = client_call_details.method.lstrip("/").split("/", 1)
+        method_name = client_call_details.method
+
+    attributes = {
+        SpanAttributes.RPC_SYSTEM: "grpc",
+        SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
+        SpanAttributes.RPC_METHOD: meth,
+        SpanAttributes.RPC_SERVICE: service,
+    }
+
+    # add some attributes from the metadata
+    if client_call_details.metadata is not None:
+        mutable_metadata = OrderedDict(tuple(client_call_details.metadata))
+        inject(mutable_metadata, setter=_carrier_setter)  # type: ignore
+        for key, value in mutable_metadata.items():
+            client_call_details.metadata.add(key=key, value=value)  # type: ignore
+
+    span = tracer.start_span(  # type: ignore
+        name=method_name,
+        kind=SpanKind.CLIENT,
+        attributes=attributes,  # type: ignore
+        set_status_on_exception=set_status_on_exception,
+    )
+    return span
+
+
+class OpenTelemetryServerInterceptor(aio.ServerInterceptor):
+    """
+    A gRPC server interceptor, to add OpenTelemetry.
+    Usage::
+        tracer = some OpenTelemetry tracer
+        interceptors = [
+            OpenTelemetryServerInterceptor(tracer),
+        ]
+        server = grpc.server(
+            futures.ThreadPoolExecutor(max_workers=concurrency),
+            interceptors = interceptors)
+    """
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    def start_span_server(
+        self,
+        handler_call_details: grpc.HandlerCallDetails,
+        context: grpc.ServicerContext,
+        set_status_on_exception=False,
+    ):
+        service, meth = handler_call_details.method.lstrip("/").split("/", 1)  # type: ignore
+
+        attributes = {
+            SpanAttributes.RPC_SYSTEM: "grpc",
+            SpanAttributes.RPC_GRPC_STATUS_CODE: grpc.StatusCode.OK.value[0],  # type: ignore
+            SpanAttributes.RPC_METHOD: meth,
+            SpanAttributes.RPC_SERVICE: service,
+        }
+
+        # add some attributes from the metadata
+        metadata = dict(context.invocation_metadata())
+        if "user-agent" in metadata:
+            attributes["rpc.user_agent"] = metadata["user-agent"]
+
+        # Split up the peer to keep with how other telemetry sources
+        # do it.  This looks like:
+        # * ipv6:[::1]:57284
+        # * ipv4:127.0.0.1:57284
+        # * ipv4:10.2.1.1:57284,127.0.0.1:57284
+        #
+        try:
+            ip, port = context.peer().split(",")[0].split(":", 1)[1].rsplit(":", 1)
+            attributes.update(
+                {SpanAttributes.NET_PEER_IP: ip, SpanAttributes.NET_PEER_PORT: port}
+            )
+
+            # other telemetry sources add this, so we will too
+            if ip in ("[::1]", "127.0.0.1"):
+                attributes[SpanAttributes.NET_PEER_NAME] = "localhost"
+
+        except IndexError:
+            logger.warning("Failed to parse peer address '%s'", context.peer())
+
+        return self.tracer.start_as_current_span(  # type: ignore
+            name=handler_call_details.method,  # type: ignore
+            kind=SpanKind.SERVER,
+            attributes=attributes,
+            set_status_on_exception=set_status_on_exception,
+        )
+
+    # Handle streaming responses separately - we have to do this
+    # to return a *new* generator or various upstream things
+    # get confused, or we'll lose the consistent trace
+    async def _intercept_server_stream(
+        self, behavior, handler_call_details, request_or_iterator, context
+    ):
+        with self._set_remote_context(context):
+            with self.start_span_server(
+                handler_call_details, context, set_status_on_exception=False
+            ) as span:
+                try:
+                    async for response in behavior(request_or_iterator, context):
+                        yield response
+
+                except Exception as error:
+                    # pylint:disable=unidiomatic-typecheck
+                    if type(error) != Exception:
+                        span.record_exception(error)
+                    raise error
+
+    @contextmanager
+    def _set_remote_context(self, servicer_context):
+        metadata = servicer_context.invocation_metadata()
+        if metadata:
+            md_dict = {md.key: md.value for md in metadata}
+            ctx = extract(md_dict)
+            token = attach(ctx)
+            try:
+                yield
+            finally:
+                detach(token)
+        else:
+            yield
+
+    async def intercept_service(
+        self,
+        continuation: Callable[
+            [grpc.HandlerCallDetails], Awaitable[grpc.RpcMethodHandler]
+        ],
+        handler_call_details: grpc.HandlerCallDetails,
+    ) -> grpc.RpcMethodHandler:
+        handler = await continuation(handler_call_details)
+        if handler and (
+            handler.request_streaming or handler.response_streaming
+        ):  # pytype: disable=attribute-error
+            return handler
+
+        def wrapper(behavior: Callable[[Any, aio.ServicerContext], Any]):
+            @functools.wraps(behavior)
+            async def wrapper(request: Any, context: aio.ServicerContext) -> Any:
+                with self._set_remote_context(context):
+                    with self.start_span_server(
+                        handler_call_details,
+                        context,
+                        set_status_on_exception=False,
+                    ) as span:
+                        # And now we run the actual RPC.
+                        try:
+                            value = await behavior(request, context)
+
+                        except Exception as error:
+                            # Bare exceptions are likely to be gRPC aborts, which
+                            # we handle in our context wrapper.
+                            # Here, we're interested in uncaught exceptions.
+                            # pylint:disable=unidiomatic-typecheck
+                            if type(error) != Exception:
+                                span.record_exception(error)
+                            raise error
+                return value
+
+            return wrapper
+
+        if "grpc.health.v1.Health" in handler_call_details.method:  # type: ignore
+            return handler
+
+        return wrap_server_method_handler(wrapper, handler)
+
+
+class UnaryUnaryClientInterceptor(aio.UnaryUnaryClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_unary_unary(
+        self, continuation, client_call_details: ClientCallDetails, request
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+        try:
+            call = await continuation(client_call_details, request)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+        return call
+
+
+class UnaryStreamClientInterceptor(aio.UnaryStreamClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_unary_stream(
+        self, continuation, client_call_details: ClientCallDetails, request
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+
+        try:
+            call = await continuation(client_call_details, request)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+
+        return call
+
+
+class StreamStreamClientInterceptor(aio.StreamStreamClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_stream_stream(
+        self, continuation, client_call_details: ClientCallDetails, request_iterator
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+        try:
+            call = await continuation(client_call_details, request_iterator)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+
+        return call
+
+
+class StreamUnaryClientInterceptor(aio.StreamUnaryClientInterceptor):
+    """Interceptor used for testing if the interceptor is being called"""
+
+    def __init__(self, tracer):
+        self.tracer = tracer
+
+    async def intercept_stream_unary(
+        self, continuation, client_call_details: ClientCallDetails, request_iterator
+    ):
+        span = start_span_client(self.tracer, client_call_details)
+        try:
+            call = await continuation(client_call_details, request_iterator)
+        except Exception as error:
+            if type(error) != Exception:
+                set_span_exception(span, error)
+            raise error
+        else:
+            call.add_done_callback(functools.partial(finish_span_grpc, span))
+
+        return call
+
+
+def get_client_interceptors(service_name: str, tracer_provider: TracerProvider):
+    tracer = tracer_provider.get_tracer(f"{service_name}_grpc_client")
+    return [
+        UnaryUnaryClientInterceptor(tracer),
+        UnaryStreamClientInterceptor(tracer),
+        StreamUnaryClientInterceptor(tracer),
+        StreamStreamClientInterceptor(tracer),
+    ]
+
+
+def get_server_interceptors(service_name: str, tracer_provider: TracerProvider):
+    tracer = tracer_provider.get_tracer(f"{service_name}_grpc_server")
+    return [OpenTelemetryServerInterceptor(tracer)]
+
+
+class GRPCTelemetry:
+    initialized: bool = False
+
+    def __init__(self, service_name: str, tracer_provider: TracerProvider):
+        self.service_name = service_name
+        self.tracer_provider = tracer_provider
+
+    def init_client(
+        self,
+        server_addr: str,
+        max_send_message: int = 100,
+        credentials: Optional[ChannelCredentials] = None,
+    ):
+        options = [
+            ("grpc.max_receive_message_length", max_send_message * 1024 * 1024),
+            ("grpc.max_send_message_length", max_send_message * 1024 * 1024),
+        ]
+        interceptors = (
+            get_client_interceptors(self.service_name, self.tracer_provider)
+            + grpc_metrics.CLIENT_INTERCEPTORS
+        )
+        if credentials is not None:
+            channel = aio.secure_channel(
+                server_addr,
+                options=options,
+                credentials=credentials,
+                interceptors=interceptors,
+            )
+        else:
+            channel = aio.insecure_channel(
+                server_addr, options=options, interceptors=interceptors
+            )
+        return channel
+
+    def init_server(
+        self,
+        concurrency: int = 4,
+        max_receive_message: int = 100,
+        interceptors: Optional[List[aio.ServerInterceptor]] = None,
+    ):
+        _interceptors = (
+            get_server_interceptors(self.service_name, self.tracer_provider)
+            + grpc_metrics.SERVER_INTERCEPTORS
+        )
+        if interceptors is not None:
+            _interceptors.extend(interceptors)
+        options = [
+            ("grpc.max_send_message_length", max_receive_message * 1024 * 1024),
+            ("grpc.max_receive_message_length", max_receive_message * 1024 * 1024),
+        ]
+        server = aio.server(
+            futures.ThreadPoolExecutor(max_workers=concurrency),
+            interceptors=_interceptors,
+            options=options,
+        )
+        return server
+
+
+OpenTelemetryGRPC = GRPCTelemetry  # b/w compat import
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jaeger.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,189 +1,189 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import math
-import socket
-from asyncio import Future
-from functools import partial
-from typing import List
-
-from opentelemetry.exporter.jaeger.thrift import JaegerExporter  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.gen.agent import Agent  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.gen.jaeger import Collector  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.translate import Translate  # type: ignore
-from opentelemetry.exporter.jaeger.thrift.translate import (  # type: ignore
-    ThriftTranslator,
-)
-from opentelemetry.sdk.resources import SERVICE_NAME  # type: ignore
-from opentelemetry.sdk.trace import Span  # type: ignore
-from opentelemetry.sdk.trace.export import SpanExportResult  # type: ignore
-from thrift.protocol import TCompactProtocol  # type: ignore
-from thrift.transport import TTransport  # type: ignore
-
-from nucliadb_telemetry import logger
-
-UDP_PACKET_MAX_LENGTH = 65000
-
-
-class JaegerExporterAsync(JaegerExporter):
-    def __init__(self, **kwags):
-        super(JaegerExporterAsync, self).__init__(**kwags)
-        self._agent_client = AgentClientUDPAsync(
-            host_name=self.agent_host_name,
-            port=self.agent_port,
-            split_oversized_batches=self.udp_split_oversized_batches,
-        )
-
-    async def async_export(self, spans: List[Span]) -> SpanExportResult:
-        # Populate service_name from first span
-        # We restrict any SpanProcessor to be only associated with a single
-        # TracerProvider, so it is safe to assume that all Spans in a single
-        # batch all originate from one TracerProvider (and in turn have all
-        # the same service.name)
-        if len(spans) == 0:
-            return SpanExportResult.SUCCESS
-        if spans:
-            service_name = spans[0].resource.attributes.get(SERVICE_NAME)
-            if service_name:
-                self.service_name = service_name
-        translator = Translate(spans)
-        thrift_translator = ThriftTranslator(self._max_tag_value_length)
-        jaeger_spans = translator._translate(thrift_translator)
-        batch = Collector.Batch(
-            spans=jaeger_spans,
-            process=Collector.Process(serviceName=self.service_name),
-        )
-        if self._collector_http_client is not None:
-            raise Exception("Not supported on asyncio")
-            # self._collector_http_client.submit(batch)
-        else:
-            await self._agent_client.emit(batch)
-
-        return SpanExportResult.SUCCESS
-
-
-class JaegerClientProtocol:
-    def __init__(self, message: bytes, on_con_lost: Future):
-        self.message = message
-        self.on_con_lost = on_con_lost
-        self.transport = None
-
-    def error_received(self, exc):
-        logger.exception("Error received from Jaeger", exc_info=exc)
-        if not self.on_con_lost.done():
-            self.on_con_lost.set_result(False)
-
-    def connection_lost(self, exc):
-        logger.exception("Connection lost with Jaeger", exc_info=exc)
-        if not self.on_con_lost.done():
-            self.on_con_lost.set_result(True)
-
-    def connection_made(self, transport):
-        self.transport = transport
-        self.transport.sendto(self.message)
-
-
-class AgentClientUDPAsync:
-    """Implement a UDP client to agent.
-
-    Args:
-        host_name: The host name of the Jaeger server.
-        port: The port of the Jaeger server.
-        max_packet_size: Maximum size of UDP packet.
-        client: Class for creating new client objects for agencies.
-        split_oversized_batches: Re-emit oversized batches in smaller chunks.
-    """
-
-    def __init__(
-        self,
-        host_name,
-        port,
-        max_packet_size=UDP_PACKET_MAX_LENGTH,
-        client=Agent.Client,
-        split_oversized_batches=False,
-    ):
-        self.host_name = host_name
-        self.port = port
-        self.max_packet_size = max_packet_size
-        self.buffer = TTransport.TMemoryBuffer()
-        self.client = client(iprot=TCompactProtocol.TCompactProtocol(trans=self.buffer))
-        self.split_oversized_batches = split_oversized_batches
-        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        self._sock.setblocking(False)
-        self._addr = (host_name, int(port))
-
-    async def emit(self, batch: Collector.Batch):
-        """
-        Args:
-            batch: Object to emit Jaeger spans.
-        """
-
-        # pylint: disable=protected-access
-        self.client._seqid = 0
-        #  truncate and reset the position of BytesIO object
-        self.buffer._buffer.truncate(0)
-        self.buffer._buffer.seek(0)
-        self.client.emitBatch(batch)
-        buff = self.buffer.getvalue()
-        if len(buff) > self.max_packet_size:
-            if self.split_oversized_batches and len(batch.spans) > 1:
-                packets = math.ceil(len(buff) / self.max_packet_size)
-                div = math.ceil(len(batch.spans) / packets)
-                for packet in range(packets):
-                    start = packet * div
-                    end = (packet + 1) * div
-                    if start < len(batch.spans):
-                        await self.emit(
-                            Collector.Batch(
-                                process=batch.process,
-                                spans=batch.spans[start:end],
-                            )
-                        )
-            else:
-                logger.warning(
-                    "Data exceeds the max UDP packet size; size %r, max %r",
-                    len(buff),
-                    self.max_packet_size,
-                )
-            return
-
-        loop = asyncio.get_running_loop()
-        on_con_lost = loop.create_future()
-
-        send_to = partial(self._sendto, buff, on_con_lost)
-        loop.add_writer(self._sock.fileno(), send_to)
-        try:
-            await on_con_lost
-        except Exception:
-            logger.exception("Exception on sending to jaeger", stack_info=True)
-        finally:
-            loop.remove_writer(self._sock.fileno())
-
-    def _sendto(self, buff, on_con_lost: asyncio.Future):
-        try:
-            self._sock.sendto(buff, self._addr)
-        except (BlockingIOError, InterruptedError):
-            return
-        except OSError as exc:
-            on_con_lost.set_exception(exc)
-        except Exception as exc:
-            on_con_lost.set_exception(exc)
-        else:
-            on_con_lost.set_result(True)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import math
+import socket
+from asyncio import Future
+from functools import partial
+from typing import List
+
+from opentelemetry.exporter.jaeger.thrift import JaegerExporter  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.gen.agent import Agent  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.gen.jaeger import Collector  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.translate import Translate  # type: ignore
+from opentelemetry.exporter.jaeger.thrift.translate import (  # type: ignore
+    ThriftTranslator,
+)
+from opentelemetry.sdk.resources import SERVICE_NAME  # type: ignore
+from opentelemetry.sdk.trace import Span  # type: ignore
+from opentelemetry.sdk.trace.export import SpanExportResult  # type: ignore
+from thrift.protocol import TCompactProtocol  # type: ignore
+from thrift.transport import TTransport  # type: ignore
+
+from nucliadb_telemetry import logger
+
+UDP_PACKET_MAX_LENGTH = 65000
+
+
+class JaegerExporterAsync(JaegerExporter):
+    def __init__(self, **kwags):
+        super(JaegerExporterAsync, self).__init__(**kwags)
+        self._agent_client = AgentClientUDPAsync(
+            host_name=self.agent_host_name,
+            port=self.agent_port,
+            split_oversized_batches=self.udp_split_oversized_batches,
+        )
+
+    async def async_export(self, spans: List[Span]) -> SpanExportResult:
+        # Populate service_name from first span
+        # We restrict any SpanProcessor to be only associated with a single
+        # TracerProvider, so it is safe to assume that all Spans in a single
+        # batch all originate from one TracerProvider (and in turn have all
+        # the same service.name)
+        if len(spans) == 0:
+            return SpanExportResult.SUCCESS
+        if spans:
+            service_name = spans[0].resource.attributes.get(SERVICE_NAME)
+            if service_name:
+                self.service_name = service_name
+        translator = Translate(spans)
+        thrift_translator = ThriftTranslator(self._max_tag_value_length)
+        jaeger_spans = translator._translate(thrift_translator)
+        batch = Collector.Batch(
+            spans=jaeger_spans,
+            process=Collector.Process(serviceName=self.service_name),
+        )
+        if self._collector_http_client is not None:
+            raise Exception("Not supported on asyncio")
+            # self._collector_http_client.submit(batch)
+        else:
+            await self._agent_client.emit(batch)
+
+        return SpanExportResult.SUCCESS
+
+
+class JaegerClientProtocol:
+    def __init__(self, message: bytes, on_con_lost: Future):
+        self.message = message
+        self.on_con_lost = on_con_lost
+        self.transport = None
+
+    def error_received(self, exc):
+        logger.exception("Error received from Jaeger", exc_info=exc)
+        if not self.on_con_lost.done():
+            self.on_con_lost.set_result(False)
+
+    def connection_lost(self, exc):
+        logger.exception("Connection lost with Jaeger", exc_info=exc)
+        if not self.on_con_lost.done():
+            self.on_con_lost.set_result(True)
+
+    def connection_made(self, transport):
+        self.transport = transport
+        self.transport.sendto(self.message)
+
+
+class AgentClientUDPAsync:
+    """Implement a UDP client to agent.
+
+    Args:
+        host_name: The host name of the Jaeger server.
+        port: The port of the Jaeger server.
+        max_packet_size: Maximum size of UDP packet.
+        client: Class for creating new client objects for agencies.
+        split_oversized_batches: Re-emit oversized batches in smaller chunks.
+    """
+
+    def __init__(
+        self,
+        host_name,
+        port,
+        max_packet_size=UDP_PACKET_MAX_LENGTH,
+        client=Agent.Client,
+        split_oversized_batches=False,
+    ):
+        self.host_name = host_name
+        self.port = port
+        self.max_packet_size = max_packet_size
+        self.buffer = TTransport.TMemoryBuffer()
+        self.client = client(iprot=TCompactProtocol.TCompactProtocol(trans=self.buffer))
+        self.split_oversized_batches = split_oversized_batches
+        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        self._sock.setblocking(False)
+        self._addr = (host_name, int(port))
+
+    async def emit(self, batch: Collector.Batch):
+        """
+        Args:
+            batch: Object to emit Jaeger spans.
+        """
+
+        # pylint: disable=protected-access
+        self.client._seqid = 0
+        #  truncate and reset the position of BytesIO object
+        self.buffer._buffer.truncate(0)
+        self.buffer._buffer.seek(0)
+        self.client.emitBatch(batch)
+        buff = self.buffer.getvalue()
+        if len(buff) > self.max_packet_size:
+            if self.split_oversized_batches and len(batch.spans) > 1:
+                packets = math.ceil(len(buff) / self.max_packet_size)
+                div = math.ceil(len(batch.spans) / packets)
+                for packet in range(packets):
+                    start = packet * div
+                    end = (packet + 1) * div
+                    if start < len(batch.spans):
+                        await self.emit(
+                            Collector.Batch(
+                                process=batch.process,
+                                spans=batch.spans[start:end],
+                            )
+                        )
+            else:
+                logger.warning(
+                    "Data exceeds the max UDP packet size; size %r, max %r",
+                    len(buff),
+                    self.max_packet_size,
+                )
+            return
+
+        loop = asyncio.get_running_loop()
+        on_con_lost = loop.create_future()
+
+        send_to = partial(self._sendto, buff, on_con_lost)
+        loop.add_writer(self._sock.fileno(), send_to)
+        try:
+            await on_con_lost
+        except Exception:
+            logger.exception("Exception on sending to jaeger", stack_info=True)
+        finally:
+            loop.remove_writer(self._sock.fileno())
+
+    def _sendto(self, buff, on_con_lost: asyncio.Future):
+        try:
+            self._sock.sendto(buff, self._addr)
+        except (BlockingIOError, InterruptedError):
+            return
+        except OSError as exc:
+            on_con_lost.set_exception(exc)
+        except Exception as exc:
+            on_con_lost.set_exception(exc)
+        else:
+            on_con_lost.set_result(True)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/jetstream.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,231 +1,231 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from functools import partial
-from typing import Any, Callable, Dict, List, Optional
-
-from nats.aio.client import Client
-from nats.aio.msg import Msg
-from nats.js.client import JetStreamContext
-from opentelemetry.context import attach
-from opentelemetry.propagate import extract, inject
-from opentelemetry.sdk.trace import TracerProvider  # type: ignore
-from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
-from opentelemetry.trace import SpanKind  # type: ignore
-from opentelemetry.trace import Tracer  # type: ignore
-
-from nucliadb_telemetry import logger
-from nucliadb_telemetry.common import set_span_exception
-
-
-def start_span_message_receiver(tracer: Tracer, msg: Msg):
-    attributes = {
-        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
-        SpanAttributes.MESSAGING_MESSAGE_PAYLOAD_SIZE_BYTES: len(msg.data),
-        SpanAttributes.MESSAGING_MESSAGE_ID: msg.reply,
-    }
-
-    # add some attributes from the metadata
-    ctx = extract(msg.headers)
-    token = attach(ctx)
-
-    span = tracer.start_as_current_span(  # type: ignore
-        name=f"Received from {msg.subject}",
-        kind=SpanKind.SERVER,
-        attributes=attributes,
-    )
-    span._token = token
-    return span
-
-
-def start_span_message_publisher(tracer: Tracer, subject: str):
-    attributes = {
-        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
-        SpanAttributes.MESSAGING_DESTINATION: subject,
-    }
-
-    span = tracer.start_as_current_span(  # type: ignore
-        name=f"Published on {subject}",
-        kind=SpanKind.CLIENT,
-        attributes=attributes,
-    )
-    return span
-
-
-class JetStreamContextTelemetry:
-    def __init__(
-        self, js: JetStreamContext, service_name: str, tracer_provider: TracerProvider
-    ):
-        self.js = js
-        self.service_name = service_name
-        self.tracer_provider = tracer_provider
-
-    async def stream_info(self, name: str):
-        return await self.js.stream_info(name)
-
-    async def add_stream(self, name: str, subjects: List[str]):
-        return await self.js.add_stream(name=name, subjects=subjects)
-
-    async def subscribe(self, cb, **kwargs):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_subscriber")
-
-        async def wrapper(origin_cb, tracer, msg: Msg):
-            # Execute the callback without tracing
-            if msg.headers is None:
-                logger.warning("Message received without headers, skipping span")
-                await origin_cb(msg)
-                return
-
-            with start_span_message_receiver(tracer, msg) as span:
-                try:
-                    await origin_cb(msg)
-                except Exception as error:
-                    set_span_exception(span, error)
-                    raise error
-
-        wrapped_cb = partial(wrapper, cb, tracer)
-        return await self.js.subscribe(cb=wrapped_cb, **kwargs)
-
-    async def publish(
-        self,
-        subject: str,
-        body: bytes,
-        headers: Optional[Dict[str, str]] = None,
-        **kwargs,
-    ):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_publisher")
-        headers = {} if headers is None else headers
-        inject(headers)
-        with start_span_message_publisher(tracer, subject) as span:
-            try:
-                result = await self.js.publish(subject, body, headers=headers, **kwargs)
-            except Exception as error:
-                if type(error) != Exception:
-                    set_span_exception(span, error)
-                raise error
-
-        return result
-
-    # Just for convenience, to wrap all we use in the context of
-    # telemetry-instrumented stuff using the JetStreamContextTelemetry class
-
-    async def pull_subscribe(
-        self, *args, **kwargs
-    ) -> JetStreamContext.PullSubscription:
-        return await self.js.pull_subscribe(*args, **kwargs)
-
-    async def pull_subscribe_bind(
-        self, *args, **kwargs
-    ) -> JetStreamContext.PullSubscription:
-        return await self.js.pull_subscribe_bind(*args, **kwargs)
-
-    async def pull_one(
-        self,
-        subscription: JetStreamContext.PullSubscription,
-        cb: Callable[[Msg], Any],
-        timeout: int = 5,
-    ) -> Msg:
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_pull_one")
-        messages = await subscription.fetch(1, timeout=timeout)
-
-        # If there is no message, fetch will raise a timeout
-        message = messages[0]
-
-        # Execute the callback without tracing
-        if message.headers is None:
-            logger.warning("Message received without headers, skipping span")
-            return await cb(message)
-
-        with start_span_message_receiver(tracer, message) as span:
-            try:
-                return await cb(message)
-            except Exception as error:
-                set_span_exception(span, error)
-                raise error
-
-
-class NatsClientTelemetry:
-    def __init__(self, nc: Client, service_name: str, tracer_provider: TracerProvider):
-        self.nc = nc
-        self.service_name = service_name
-        self.tracer_provider = tracer_provider
-
-    async def subscribe(self, cb, **kwargs):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_subscriber")
-
-        async def wrapper(origin_cb, tracer, msg: Msg):
-            # Execute the callback without tracing
-            if msg.headers is None:
-                logger.warning("Message received without headers, skipping span")
-                await origin_cb(msg)
-                return
-
-            with start_span_message_receiver(tracer, msg) as span:
-                try:
-                    await origin_cb(msg)
-                except Exception as error:
-                    set_span_exception(span, error)
-                    raise error
-
-        wrapped_cb = partial(wrapper, cb, tracer)
-        return await self.nc.subscribe(cb=wrapped_cb, **kwargs)
-
-    async def publish(
-        self,
-        subject: str,
-        body: bytes,
-        headers: Optional[Dict[str, str]] = None,
-        **kwargs,
-    ):
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_publisher")
-        headers = {} if headers is None else headers
-        inject(headers)
-
-        with start_span_message_publisher(tracer, subject) as span:
-            try:
-                result = await self.nc.publish(subject, body, headers=headers, **kwargs)
-            except Exception as error:
-                if type(error) != Exception:
-                    set_span_exception(span, error)
-                raise error
-
-        return result
-
-    async def request(
-        self,
-        subject: str,
-        payload: bytes = b"",
-        timeout: float = 0.5,
-        old_style: bool = False,
-        headers: Optional[Dict[str, Any]] = None,
-    ) -> Msg:
-        headers = {} if headers is None else headers
-        inject(headers)
-        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_request")
-        with start_span_message_publisher(tracer, subject) as span:
-            try:
-                result = await self.nc.request(
-                    subject, payload, timeout, old_style, headers  # type: ignore
-                )
-            except Exception as error:
-                if type(error) != Exception:
-                    set_span_exception(span, error)
-                raise error
-
-        return result
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from functools import partial
+from typing import Any, Callable, Dict, List, Optional
+
+from nats.aio.client import Client
+from nats.aio.msg import Msg
+from nats.js.client import JetStreamContext
+from opentelemetry.context import attach
+from opentelemetry.propagate import extract, inject
+from opentelemetry.sdk.trace import TracerProvider  # type: ignore
+from opentelemetry.semconv.trace import SpanAttributes  # type: ignore
+from opentelemetry.trace import SpanKind  # type: ignore
+from opentelemetry.trace import Tracer  # type: ignore
+
+from nucliadb_telemetry import logger
+from nucliadb_telemetry.common import set_span_exception
+
+
+def start_span_message_receiver(tracer: Tracer, msg: Msg):
+    attributes = {
+        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
+        SpanAttributes.MESSAGING_MESSAGE_PAYLOAD_SIZE_BYTES: len(msg.data),
+        SpanAttributes.MESSAGING_MESSAGE_ID: msg.reply,
+    }
+
+    # add some attributes from the metadata
+    ctx = extract(msg.headers)
+    token = attach(ctx)
+
+    span = tracer.start_as_current_span(  # type: ignore
+        name=f"Received from {msg.subject}",
+        kind=SpanKind.SERVER,
+        attributes=attributes,
+    )
+    span._token = token
+    return span
+
+
+def start_span_message_publisher(tracer: Tracer, subject: str):
+    attributes = {
+        SpanAttributes.MESSAGING_DESTINATION_KIND: "nats",
+        SpanAttributes.MESSAGING_DESTINATION: subject,
+    }
+
+    span = tracer.start_as_current_span(  # type: ignore
+        name=f"Published on {subject}",
+        kind=SpanKind.CLIENT,
+        attributes=attributes,
+    )
+    return span
+
+
+class JetStreamContextTelemetry:
+    def __init__(
+        self, js: JetStreamContext, service_name: str, tracer_provider: TracerProvider
+    ):
+        self.js = js
+        self.service_name = service_name
+        self.tracer_provider = tracer_provider
+
+    async def stream_info(self, name: str):
+        return await self.js.stream_info(name)
+
+    async def add_stream(self, name: str, subjects: List[str]):
+        return await self.js.add_stream(name=name, subjects=subjects)
+
+    async def subscribe(self, cb, **kwargs):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_subscriber")
+
+        async def wrapper(origin_cb, tracer, msg: Msg):
+            # Execute the callback without tracing
+            if msg.headers is None:
+                logger.warning("Message received without headers, skipping span")
+                await origin_cb(msg)
+                return
+
+            with start_span_message_receiver(tracer, msg) as span:
+                try:
+                    await origin_cb(msg)
+                except Exception as error:
+                    set_span_exception(span, error)
+                    raise error
+
+        wrapped_cb = partial(wrapper, cb, tracer)
+        return await self.js.subscribe(cb=wrapped_cb, **kwargs)
+
+    async def publish(
+        self,
+        subject: str,
+        body: bytes,
+        headers: Optional[Dict[str, str]] = None,
+        **kwargs,
+    ):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_publisher")
+        headers = {} if headers is None else headers
+        inject(headers)
+        with start_span_message_publisher(tracer, subject) as span:
+            try:
+                result = await self.js.publish(subject, body, headers=headers, **kwargs)
+            except Exception as error:
+                if type(error) != Exception:
+                    set_span_exception(span, error)
+                raise error
+
+        return result
+
+    # Just for convenience, to wrap all we use in the context of
+    # telemetry-instrumented stuff using the JetStreamContextTelemetry class
+
+    async def pull_subscribe(
+        self, *args, **kwargs
+    ) -> JetStreamContext.PullSubscription:
+        return await self.js.pull_subscribe(*args, **kwargs)
+
+    async def pull_subscribe_bind(
+        self, *args, **kwargs
+    ) -> JetStreamContext.PullSubscription:
+        return await self.js.pull_subscribe_bind(*args, **kwargs)
+
+    async def pull_one(
+        self,
+        subscription: JetStreamContext.PullSubscription,
+        cb: Callable[[Msg], Any],
+        timeout: int = 5,
+    ) -> Msg:
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_js_pull_one")
+        messages = await subscription.fetch(1, timeout=timeout)
+
+        # If there is no message, fetch will raise a timeout
+        message = messages[0]
+
+        # Execute the callback without tracing
+        if message.headers is None:
+            logger.warning("Message received without headers, skipping span")
+            return await cb(message)
+
+        with start_span_message_receiver(tracer, message) as span:
+            try:
+                return await cb(message)
+            except Exception as error:
+                set_span_exception(span, error)
+                raise error
+
+
+class NatsClientTelemetry:
+    def __init__(self, nc: Client, service_name: str, tracer_provider: TracerProvider):
+        self.nc = nc
+        self.service_name = service_name
+        self.tracer_provider = tracer_provider
+
+    async def subscribe(self, cb, **kwargs):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_subscriber")
+
+        async def wrapper(origin_cb, tracer, msg: Msg):
+            # Execute the callback without tracing
+            if msg.headers is None:
+                logger.warning("Message received without headers, skipping span")
+                await origin_cb(msg)
+                return
+
+            with start_span_message_receiver(tracer, msg) as span:
+                try:
+                    await origin_cb(msg)
+                except Exception as error:
+                    set_span_exception(span, error)
+                    raise error
+
+        wrapped_cb = partial(wrapper, cb, tracer)
+        return await self.nc.subscribe(cb=wrapped_cb, **kwargs)
+
+    async def publish(
+        self,
+        subject: str,
+        body: bytes,
+        headers: Optional[Dict[str, str]] = None,
+        **kwargs,
+    ):
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_publisher")
+        headers = {} if headers is None else headers
+        inject(headers)
+
+        with start_span_message_publisher(tracer, subject) as span:
+            try:
+                result = await self.nc.publish(subject, body, headers=headers, **kwargs)
+            except Exception as error:
+                if type(error) != Exception:
+                    set_span_exception(span, error)
+                raise error
+
+        return result
+
+    async def request(
+        self,
+        subject: str,
+        payload: bytes = b"",
+        timeout: float = 0.5,
+        old_style: bool = False,
+        headers: Optional[Dict[str, Any]] = None,
+    ) -> Msg:
+        headers = {} if headers is None else headers
+        inject(headers)
+        tracer = self.tracer_provider.get_tracer(f"{self.service_name}_nc_request")
+        with start_span_message_publisher(tracer, subject) as span:
+            try:
+                result = await self.nc.request(
+                    subject, payload, timeout, old_style, headers  # type: ignore
+                )
+            except Exception as error:
+                if type(error) != Exception:
+                    set_span_exception(span, error)
+                raise error
+
+        return result
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/settings.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,31 +1,49 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from pydantic import BaseSettings
-
-
-class TelemetrySettings(BaseSettings):
-    jaeger_agent_host: str = "localhost"
-    jaeger_agent_port: int = 6831
-    jaeger_enabled: bool = False
-    jaeger_query_port: int = 16686
-    jaeger_query_host: str = "jaeger.observability.svc.cluster.local"
-
-
-telemetry_settings = TelemetrySettings()
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import enum
+from typing import Optional
+
+from pydantic import BaseSettings
+
+
+class TelemetrySettings(BaseSettings):
+    jaeger_agent_host: str = "localhost"
+    jaeger_agent_port: int = 6831
+    jaeger_enabled: bool = False
+    jaeger_query_port: int = 16686
+    jaeger_query_host: str = "jaeger.observability.svc.cluster.local"
+
+
+telemetry_settings = TelemetrySettings()
+
+
+class LogLevel(enum.Enum):
+    DEBUG = "DEBUG"
+    INFO = "INFO"
+    WARNING = "WARNING"
+    ERROR = "ERROR"
+    FATAL = "FATAL"
+    CRITICAL = "CRITICAL"
+
+
+class LogSettings(BaseSettings):
+    debug: bool = False
+    log_level: LogLevel = LogLevel.WARNING
+    logger_levels: Optional[dict[str, LogLevel]] = None
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/__init__.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,19 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/conftest.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_node/nucliadb_node/tests/conftest.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,27 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-pytest_plugins = [
-    "pytest_docker_fixtures",
-    "nucliadb_utils.tests.nats",
-    "nucliadb_telemetry.tests.telemetry",
-]
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+#
+pytest_plugins = [
+    "pytest_docker_fixtures",
+    "nucliadb_utils.tests.nats",
+    "nucliadb_utils.tests.gcs",
+    "nucliadb_utils.tests.s3",
+    "nucliadb_utils.tests.indexing",
+    "nucliadb_node.tests.fixtures",
+]
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld.proto`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-// Copyright 2015 gRPC authors.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-syntax = "proto3";
-
-option java_package = "ex.grpc";
-option objc_class_prefix = "HSW";
-
-package hellostreamingworld;
-
-// The greeting service definition.
-service MultiGreeter {
-  // Sends multiple greetings
-  rpc sayHello (HelloRequest) returns (stream HelloReply) {}
-}
-
-// The request message containing the user's name and how many greetings
-// they want.
-message HelloRequest {
-  string name = 1;
-  string num_greetings = 2;
-}
-
-// A response message containing a greeting
-message HelloReply {
-  string message = 1;
-}
+// Copyright 2015 gRPC authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+syntax = "proto3";
+
+option java_package = "ex.grpc";
+option objc_class_prefix = "HSW";
+
+package hellostreamingworld;
+
+// The greeting service definition.
+service MultiGreeter {
+  // Sends multiple greetings
+  rpc sayHello (HelloRequest) returns (stream HelloReply) {}
+}
+
+// The request message containing the user's name and how many greetings
+// they want.
+message HelloRequest {
+  string name = 1;
+  string num_greetings = 2;
+}
+
+// A response message containing a greeting
+message HelloReply {
+  string message = 1;
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,55 +1,55 @@
-# -*- coding: utf-8 -*-
-# Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
-"""Generated protocol buffer code."""
-from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
-from google.protobuf import message as _message
-from google.protobuf import reflection as _reflection
-from google.protobuf import symbol_database as _symbol_database
-
-# @@protoc_insertion_point(imports)
-
-_sym_db = _symbol_database.Default()
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
-    b'\n7nucliadb_telemetry/tests/grpc/hellostreamingworld.proto\x12\x13hellostreamingworld"3\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x15\n\rnum_greetings\x18\x02 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2b\n\x0cMultiGreeter\x12R\n\x08sayHello\x12!.hellostreamingworld.HelloRequest\x1a\x1f.hellostreamingworld.HelloReply"\x00\x30\x01\x42\x0f\n\x07\x65x.grpc\xa2\x02\x03HSWb\x06proto3'
-)
-
-
-_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
-_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
-HelloRequest = _reflection.GeneratedProtocolMessageType(
-    "HelloRequest",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREQUEST,
-        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
-        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloRequest)
-    },
-)
-_sym_db.RegisterMessage(HelloRequest)
-
-HelloReply = _reflection.GeneratedProtocolMessageType(
-    "HelloReply",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREPLY,
-        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
-        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloReply)
-    },
-)
-_sym_db.RegisterMessage(HelloReply)
-
-_MULTIGREETER = DESCRIPTOR.services_by_name["MultiGreeter"]
-if _descriptor._USE_C_DESCRIPTORS == False:
-    DESCRIPTOR._options = None
-    DESCRIPTOR._serialized_options = b"\n\007ex.grpc\242\002\003HSW"
-    _HELLOREQUEST._serialized_start = 80
-    _HELLOREQUEST._serialized_end = 131
-    _HELLOREPLY._serialized_start = 133
-    _HELLOREPLY._serialized_end = 162
-    _MULTIGREETER._serialized_start = 164
-    _MULTIGREETER._serialized_end = 262
-# @@protoc_insertion_point(module_scope)
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: nucliadb_telemetry/tests/grpc/hellostreamingworld.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n7nucliadb_telemetry/tests/grpc/hellostreamingworld.proto\x12\x13hellostreamingworld"3\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x15\n\rnum_greetings\x18\x02 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2b\n\x0cMultiGreeter\x12R\n\x08sayHello\x12!.hellostreamingworld.HelloRequest\x1a\x1f.hellostreamingworld.HelloReply"\x00\x30\x01\x42\x0f\n\x07\x65x.grpc\xa2\x02\x03HSWb\x06proto3'
+)
+
+
+_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
+_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
+HelloRequest = _reflection.GeneratedProtocolMessageType(
+    "HelloRequest",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREQUEST,
+        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
+        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloRequest)
+    },
+)
+_sym_db.RegisterMessage(HelloRequest)
+
+HelloReply = _reflection.GeneratedProtocolMessageType(
+    "HelloReply",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREPLY,
+        "__module__": "nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2"
+        # @@protoc_insertion_point(class_scope:hellostreamingworld.HelloReply)
+    },
+)
+_sym_db.RegisterMessage(HelloReply)
+
+_MULTIGREETER = DESCRIPTOR.services_by_name["MultiGreeter"]
+if _descriptor._USE_C_DESCRIPTORS == False:
+    DESCRIPTOR._options = None
+    DESCRIPTOR._serialized_options = b"\n\007ex.grpc\242\002\003HSW"
+    _HELLOREQUEST._serialized_start = 80
+    _HELLOREQUEST._serialized_end = 131
+    _HELLOREPLY._serialized_start = 133
+    _HELLOREPLY._serialized_end = 162
+    _MULTIGREETER._serialized_start = 164
+    _MULTIGREETER._serialized_end = 262
+# @@protoc_insertion_point(module_scope)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
-"""Client and server classes corresponding to protobuf-defined services."""
-import grpc
-
-from nucliadb_telemetry.tests.grpc import (
-    hellostreamingworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2,
-)
-
-
-class MultiGreeterStub(object):
-    """The greeting service definition."""
-
-    def __init__(self, channel):
-        """Constructor.
-
-        Args:
-            channel: A grpc.Channel.
-        """
-        self.sayHello = channel.unary_stream(
-            "/hellostreamingworld.MultiGreeter/sayHello",
-            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
-            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
-        )
-
-
-class MultiGreeterServicer(object):
-    """The greeting service definition."""
-
-    def sayHello(self, request, context):
-        """Sends multiple greetings"""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
-
-
-def add_MultiGreeterServicer_to_server(servicer, server):
-    rpc_method_handlers = {
-        "sayHello": grpc.unary_stream_rpc_method_handler(
-            servicer.sayHello,
-            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.FromString,
-            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.SerializeToString,
-        ),
-    }
-    generic_handler = grpc.method_handlers_generic_handler(
-        "hellostreamingworld.MultiGreeter", rpc_method_handlers
-    )
-    server.add_generic_rpc_handlers((generic_handler,))
-
-
-# This class is part of an EXPERIMENTAL API.
-class MultiGreeter(object):
-    """The greeting service definition."""
-
-    @staticmethod
-    def sayHello(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_stream(
-            request,
-            target,
-            "/hellostreamingworld.MultiGreeter/sayHello",
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+from nucliadb_telemetry.tests.grpc import (
+    hellostreamingworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2,
+)
+
+
+class MultiGreeterStub(object):
+    """The greeting service definition."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.sayHello = channel.unary_stream(
+            "/hellostreamingworld.MultiGreeter/sayHello",
+            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
+            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
+        )
+
+
+class MultiGreeterServicer(object):
+    """The greeting service definition."""
+
+    def sayHello(self, request, context):
+        """Sends multiple greetings"""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details("Method not implemented!")
+        raise NotImplementedError("Method not implemented!")
+
+
+def add_MultiGreeterServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+        "sayHello": grpc.unary_stream_rpc_method_handler(
+            servicer.sayHello,
+            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.FromString,
+            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.SerializeToString,
+        ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+        "hellostreamingworld.MultiGreeter", rpc_method_handlers
+    )
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+# This class is part of an EXPERIMENTAL API.
+class MultiGreeter(object):
+    """The greeting service definition."""
+
+    @staticmethod
+    def sayHello(
+        request,
+        target,
+        options=(),
+        channel_credentials=None,
+        call_credentials=None,
+        insecure=False,
+        compression=None,
+        wait_for_ready=None,
+        timeout=None,
+        metadata=None,
+    ):
+        return grpc.experimental.unary_stream(
+            request,
+            target,
+            "/hellostreamingworld.MultiGreeter/sayHello",
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloRequest.SerializeToString,
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_hellostreamingworld__pb2.HelloReply.FromString,
+            options,
+            channel_credentials,
+            insecure,
+            call_credentials,
+            compression,
+            wait_for_ready,
+            timeout,
+            metadata,
+        )
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2_grpc.pyi`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-"""
-@generated by mypy-protobuf.  Do not edit manually!
-isort:skip_file
-"""
-import abc
-import grpc
-import nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2
-import typing
-
-class MultiGreeterStub:
-    """The greeting service definition."""
-
-    def __init__(self, channel: grpc.Channel) -> None: ...
-    sayHello: grpc.UnaryStreamMultiCallable[
-        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
-        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply,
-    ]
-    """Sends multiple greetings"""
-
-class MultiGreeterServicer(metaclass=abc.ABCMeta):
-    """The greeting service definition."""
-
-    @abc.abstractmethod
-    def sayHello(
-        self,
-        request: nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
-        context: grpc.ServicerContext,
-    ) -> typing.Iterator[
-        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply
-    ]:
-        """Sends multiple greetings"""
-        pass
-
-def add_MultiGreeterServicer_to_server(
-    servicer: MultiGreeterServicer, server: grpc.Server
-) -> None: ...
+"""
+@generated by mypy-protobuf.  Do not edit manually!
+isort:skip_file
+"""
+import abc
+import grpc
+import nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2
+import typing
+
+class MultiGreeterStub:
+    """The greeting service definition."""
+
+    def __init__(self, channel: grpc.Channel) -> None: ...
+    sayHello: grpc.UnaryStreamMultiCallable[
+        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
+        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply,
+    ]
+    """Sends multiple greetings"""
+
+class MultiGreeterServicer(metaclass=abc.ABCMeta):
+    """The greeting service definition."""
+
+    @abc.abstractmethod
+    def sayHello(
+        self,
+        request: nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloRequest,
+        context: grpc.ServicerContext,
+    ) -> typing.Iterator[
+        nucliadb_telemetry.tests.grpc.hellostreamingworld_pb2.HelloReply
+    ]:
+        """Sends multiple greetings"""
+        pass
+
+def add_MultiGreeterServicer_to_server(
+    servicer: MultiGreeterServicer, server: grpc.Server
+) -> None: ...
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld.proto`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-// Copyright 2015 gRPC authors.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-syntax = "proto3";
-
-option java_multiple_files = true;
-option java_package = "io.grpc.examples.helloworld";
-option java_outer_classname = "HelloWorldProto";
-option objc_class_prefix = "HLW";
-
-package helloworld;
-
-// The greeting service definition.
-service Greeter {
-  // Sends a greeting
-  rpc SayHello (HelloRequest) returns (HelloReply) {}
-}
-
-// The request message containing the user's name.
-message HelloRequest {
-  string name = 1;
-}
-
-// The response message containing the greetings
-message HelloReply {
-  string message = 1;
+// Copyright 2015 gRPC authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+syntax = "proto3";
+
+option java_multiple_files = true;
+option java_package = "io.grpc.examples.helloworld";
+option java_outer_classname = "HelloWorldProto";
+option objc_class_prefix = "HLW";
+
+package helloworld;
+
+// The greeting service definition.
+service Greeter {
+  // Sends a greeting
+  rpc SayHello (HelloRequest) returns (HelloReply) {}
+}
+
+// The request message containing the user's name.
+message HelloRequest {
+  string name = 1;
+}
+
+// The response message containing the greetings
+message HelloReply {
+  string message = 1;
 }
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,57 +1,57 @@
-# -*- coding: utf-8 -*-
-# Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: nucliadb_telemetry/tests/grpc/helloworld.proto
-"""Generated protocol buffer code."""
-from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
-from google.protobuf import message as _message
-from google.protobuf import reflection as _reflection
-from google.protobuf import symbol_database as _symbol_database
-
-# @@protoc_insertion_point(imports)
-
-_sym_db = _symbol_database.Default()
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
-    b'\n.nucliadb_telemetry/tests/grpc/helloworld.proto\x12\nhelloworld"\x1c\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2I\n\x07Greeter\x12>\n\x08SayHello\x12\x18.helloworld.HelloRequest\x1a\x16.helloworld.HelloReply"\x00\x42\x36\n\x1bio.grpc.examples.helloworldB\x0fHelloWorldProtoP\x01\xa2\x02\x03HLWb\x06proto3'
-)
-
-
-_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
-_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
-HelloRequest = _reflection.GeneratedProtocolMessageType(
-    "HelloRequest",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREQUEST,
-        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
-        # @@protoc_insertion_point(class_scope:helloworld.HelloRequest)
-    },
-)
-_sym_db.RegisterMessage(HelloRequest)
-
-HelloReply = _reflection.GeneratedProtocolMessageType(
-    "HelloReply",
-    (_message.Message,),
-    {
-        "DESCRIPTOR": _HELLOREPLY,
-        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
-        # @@protoc_insertion_point(class_scope:helloworld.HelloReply)
-    },
-)
-_sym_db.RegisterMessage(HelloReply)
-
-_GREETER = DESCRIPTOR.services_by_name["Greeter"]
-if _descriptor._USE_C_DESCRIPTORS == False:
-    DESCRIPTOR._options = None
-    DESCRIPTOR._serialized_options = (
-        b"\n\033io.grpc.examples.helloworldB\017HelloWorldProtoP\001\242\002\003HLW"
-    )
-    _HELLOREQUEST._serialized_start = 62
-    _HELLOREQUEST._serialized_end = 90
-    _HELLOREPLY._serialized_start = 92
-    _HELLOREPLY._serialized_end = 121
-    _GREETER._serialized_start = 123
-    _GREETER._serialized_end = 196
-# @@protoc_insertion_point(module_scope)
+# -*- coding: utf-8 -*-
+# Generated by the protocol buffer compiler.  DO NOT EDIT!
+# source: nucliadb_telemetry/tests/grpc/helloworld.proto
+"""Generated protocol buffer code."""
+from google.protobuf import descriptor as _descriptor
+from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
+from google.protobuf import symbol_database as _symbol_database
+
+# @@protoc_insertion_point(imports)
+
+_sym_db = _symbol_database.Default()
+
+
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n.nucliadb_telemetry/tests/grpc/helloworld.proto\x12\nhelloworld"\x1c\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2I\n\x07Greeter\x12>\n\x08SayHello\x12\x18.helloworld.HelloRequest\x1a\x16.helloworld.HelloReply"\x00\x42\x36\n\x1bio.grpc.examples.helloworldB\x0fHelloWorldProtoP\x01\xa2\x02\x03HLWb\x06proto3'
+)
+
+
+_HELLOREQUEST = DESCRIPTOR.message_types_by_name["HelloRequest"]
+_HELLOREPLY = DESCRIPTOR.message_types_by_name["HelloReply"]
+HelloRequest = _reflection.GeneratedProtocolMessageType(
+    "HelloRequest",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREQUEST,
+        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
+        # @@protoc_insertion_point(class_scope:helloworld.HelloRequest)
+    },
+)
+_sym_db.RegisterMessage(HelloRequest)
+
+HelloReply = _reflection.GeneratedProtocolMessageType(
+    "HelloReply",
+    (_message.Message,),
+    {
+        "DESCRIPTOR": _HELLOREPLY,
+        "__module__": "nucliadb_telemetry.tests.grpc.helloworld_pb2"
+        # @@protoc_insertion_point(class_scope:helloworld.HelloReply)
+    },
+)
+_sym_db.RegisterMessage(HelloReply)
+
+_GREETER = DESCRIPTOR.services_by_name["Greeter"]
+if _descriptor._USE_C_DESCRIPTORS == False:
+    DESCRIPTOR._options = None
+    DESCRIPTOR._serialized_options = (
+        b"\n\033io.grpc.examples.helloworldB\017HelloWorldProtoP\001\242\002\003HLW"
+    )
+    _HELLOREQUEST._serialized_start = 62
+    _HELLOREQUEST._serialized_end = 90
+    _HELLOREPLY._serialized_start = 92
+    _HELLOREPLY._serialized_end = 121
+    _GREETER._serialized_start = 123
+    _GREETER._serialized_end = 196
+# @@protoc_insertion_point(module_scope)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2.pyi` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/hellostreamingworld_pb2.pyi`

 * *Files 19% similar despite different names*

```diff
@@ -1,45 +1,53 @@
-"""
-@generated by mypy-protobuf.  Do not edit manually!
-isort:skip_file
-"""
-import builtins
-import google.protobuf.descriptor
-import google.protobuf.message
-import typing
-import typing_extensions
-
-DESCRIPTOR: google.protobuf.descriptor.FileDescriptor
-
-class HelloRequest(google.protobuf.message.Message):
-    """The request message containing the user's name."""
-
-    DESCRIPTOR: google.protobuf.descriptor.Descriptor
-    NAME_FIELD_NUMBER: builtins.int
-    name: typing.Text
-    def __init__(
-        self,
-        *,
-        name: typing.Text = ...,
-    ) -> None: ...
-    def ClearField(
-        self, field_name: typing_extensions.Literal["name", b"name"]
-    ) -> None: ...
-
-global___HelloRequest = HelloRequest
-
-class HelloReply(google.protobuf.message.Message):
-    """The response message containing the greetings"""
-
-    DESCRIPTOR: google.protobuf.descriptor.Descriptor
-    MESSAGE_FIELD_NUMBER: builtins.int
-    message: typing.Text
-    def __init__(
-        self,
-        *,
-        message: typing.Text = ...,
-    ) -> None: ...
-    def ClearField(
-        self, field_name: typing_extensions.Literal["message", b"message"]
-    ) -> None: ...
-
-global___HelloReply = HelloReply
+"""
+@generated by mypy-protobuf.  Do not edit manually!
+isort:skip_file
+"""
+import builtins
+import google.protobuf.descriptor
+import google.protobuf.message
+import typing
+import typing_extensions
+
+DESCRIPTOR: google.protobuf.descriptor.FileDescriptor
+
+class HelloRequest(google.protobuf.message.Message):
+    """The request message containing the user's name and how many greetings
+    they want.
+    """
+
+    DESCRIPTOR: google.protobuf.descriptor.Descriptor
+    NAME_FIELD_NUMBER: builtins.int
+    NUM_GREETINGS_FIELD_NUMBER: builtins.int
+    name: typing.Text
+    num_greetings: typing.Text
+    def __init__(
+        self,
+        *,
+        name: typing.Text = ...,
+        num_greetings: typing.Text = ...,
+    ) -> None: ...
+    def ClearField(
+        self,
+        field_name: typing_extensions.Literal[
+            "name", b"name", "num_greetings", b"num_greetings"
+        ],
+    ) -> None: ...
+
+global___HelloRequest = HelloRequest
+
+class HelloReply(google.protobuf.message.Message):
+    """A response message containing a greeting"""
+
+    DESCRIPTOR: google.protobuf.descriptor.Descriptor
+    MESSAGE_FIELD_NUMBER: builtins.int
+    message: typing.Text
+    def __init__(
+        self,
+        *,
+        message: typing.Text = ...,
+    ) -> None: ...
+    def ClearField(
+        self, field_name: typing_extensions.Literal["message", b"message"]
+    ) -> None: ...
+
+global___HelloReply = HelloReply
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
-"""Client and server classes corresponding to protobuf-defined services."""
-import grpc
-
-from nucliadb_telemetry.tests.grpc import (
-    helloworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2,
-)
-
-
-class GreeterStub(object):
-    """The greeting service definition."""
-
-    def __init__(self, channel):
-        """Constructor.
-
-        Args:
-            channel: A grpc.Channel.
-        """
-        self.SayHello = channel.unary_unary(
-            "/helloworld.Greeter/SayHello",
-            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
-            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
-        )
-
-
-class GreeterServicer(object):
-    """The greeting service definition."""
-
-    def SayHello(self, request, context):
-        """Sends a greeting"""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
-
-
-def add_GreeterServicer_to_server(servicer, server):
-    rpc_method_handlers = {
-        "SayHello": grpc.unary_unary_rpc_method_handler(
-            servicer.SayHello,
-            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.FromString,
-            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.SerializeToString,
-        ),
-    }
-    generic_handler = grpc.method_handlers_generic_handler(
-        "helloworld.Greeter", rpc_method_handlers
-    )
-    server.add_generic_rpc_handlers((generic_handler,))
-
-
-# This class is part of an EXPERIMENTAL API.
-class Greeter(object):
-    """The greeting service definition."""
-
-    @staticmethod
-    def SayHello(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
-            target,
-            "/helloworld.Greeter/SayHello",
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
-            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
+"""Client and server classes corresponding to protobuf-defined services."""
+import grpc
+
+from nucliadb_telemetry.tests.grpc import (
+    helloworld_pb2 as nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2,
+)
+
+
+class GreeterStub(object):
+    """The greeting service definition."""
+
+    def __init__(self, channel):
+        """Constructor.
+
+        Args:
+            channel: A grpc.Channel.
+        """
+        self.SayHello = channel.unary_unary(
+            "/helloworld.Greeter/SayHello",
+            request_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
+            response_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
+        )
+
+
+class GreeterServicer(object):
+    """The greeting service definition."""
+
+    def SayHello(self, request, context):
+        """Sends a greeting"""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details("Method not implemented!")
+        raise NotImplementedError("Method not implemented!")
+
+
+def add_GreeterServicer_to_server(servicer, server):
+    rpc_method_handlers = {
+        "SayHello": grpc.unary_unary_rpc_method_handler(
+            servicer.SayHello,
+            request_deserializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.FromString,
+            response_serializer=nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.SerializeToString,
+        ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+        "helloworld.Greeter", rpc_method_handlers
+    )
+    server.add_generic_rpc_handlers((generic_handler,))
+
+
+# This class is part of an EXPERIMENTAL API.
+class Greeter(object):
+    """The greeting service definition."""
+
+    @staticmethod
+    def SayHello(
+        request,
+        target,
+        options=(),
+        channel_credentials=None,
+        call_credentials=None,
+        insecure=False,
+        compression=None,
+        wait_for_ready=None,
+        timeout=None,
+        metadata=None,
+    ):
+        return grpc.experimental.unary_unary(
+            request,
+            target,
+            "/helloworld.Greeter/SayHello",
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloRequest.SerializeToString,
+            nucliadb__telemetry_dot_tests_dot_grpc_dot_helloworld__pb2.HelloReply.FromString,
+            options,
+            channel_credentials,
+            insecure,
+            call_credentials,
+            compression,
+            wait_for_ready,
+            timeout,
+            metadata,
+        )
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/helloworld_pb2_grpc.pyi`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-"""
-@generated by mypy-protobuf.  Do not edit manually!
-isort:skip_file
-"""
-import abc
-import grpc
-import nucliadb_telemetry.tests.grpc.helloworld_pb2
-
-class GreeterStub:
-    """The greeting service definition."""
-
-    def __init__(self, channel: grpc.Channel) -> None: ...
-    SayHello: grpc.UnaryUnaryMultiCallable[
-        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
-        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply,
-    ]
-    """Sends a greeting"""
-
-class GreeterServicer(metaclass=abc.ABCMeta):
-    """The greeting service definition."""
-
-    @abc.abstractmethod
-    def SayHello(
-        self,
-        request: nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
-        context: grpc.ServicerContext,
-    ) -> nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply:
-        """Sends a greeting"""
-        pass
-
-def add_GreeterServicer_to_server(
-    servicer: GreeterServicer, server: grpc.Server
-) -> None: ...
+"""
+@generated by mypy-protobuf.  Do not edit manually!
+isort:skip_file
+"""
+import abc
+import grpc
+import nucliadb_telemetry.tests.grpc.helloworld_pb2
+
+class GreeterStub:
+    """The greeting service definition."""
+
+    def __init__(self, channel: grpc.Channel) -> None: ...
+    SayHello: grpc.UnaryUnaryMultiCallable[
+        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
+        nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply,
+    ]
+    """Sends a greeting"""
+
+class GreeterServicer(metaclass=abc.ABCMeta):
+    """The greeting service definition."""
+
+    @abc.abstractmethod
+    def SayHello(
+        self,
+        request: nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloRequest,
+        context: grpc.ServicerContext,
+    ) -> nucliadb_telemetry.tests.grpc.helloworld_pb2.HelloReply:
+        """Sends a greeting"""
+        pass
+
+def add_GreeterServicer_to_server(
+    servicer: GreeterServicer, server: grpc.Server
+) -> None: ...
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/grpc/test_telemetry.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/integration/test_telemetry.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,95 +1,98 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import json
-
-import pytest
-from httpx import AsyncClient
-
-from nucliadb_telemetry.settings import telemetry_settings
-from nucliadb_telemetry.tests.telemetry import Greeter
-
-
-def fmt_span(span):
-    tags_by_key = {tag["key"]: tag["value"] for tag in span["tags"]}
-    return {
-        "time": span["startTime"],
-        "id": span["spanID"],
-        "parent": span["references"][0]["spanID"],
-        "process": span["processID"],
-        "scope": tags_by_key["otel.scope.name"],
-        "operation": span["operationName"],
-    }
-
-
-def debug_spans(spans):
-    print(
-        json.dumps(
-            sorted([fmt_span(span) for span in spans], key=lambda x: x["time"]),
-            indent=4,
-        )
-    )
-
-
-@pytest.mark.asyncio
-async def test_telemetry_dict(http_service: AsyncClient, greeter: Greeter):
-    resp = await http_service.get(
-        "http://test/",
-        headers={
-            "x-b3-traceid": "f13dc5318bf3bef64a0a5ea607db93a1",
-            "x-b3-spanid": "bfc2225c60b39d97",
-            "x-b3-sampled": "1",
-        },
-    )
-    assert resp.status_code == 200
-    for i in range(10):
-        if len(greeter.messages) == 0:
-            await asyncio.sleep(1)
-    assert (
-        greeter.messages[0].headers["x-b3-traceid"]
-        == "f13dc5318bf3bef64a0a5ea607db93a1"
-    )
-
-    assert len(greeter.messages) == 4
-
-    expected_spans = 19
-
-    await asyncio.sleep(2)
-    client = AsyncClient()
-    for _ in range(10):
-        resp = await client.get(
-            f"http://localhost:{telemetry_settings.jaeger_query_port}/api/traces/f13dc5318bf3bef64a0a5ea607db93a1",
-            headers={"Accept": "application/json"},
-        )
-        if (
-            resp.status_code != 200
-            or len(resp.json()["data"][0]["spans"]) < expected_spans
-        ):
-            await asyncio.sleep(2)
-        else:
-            break
-
-    assert resp.json()["data"][0]["traceID"] == "f13dc5318bf3bef64a0a5ea607db93a1"
-
-    # Enable this block for debugging purposes, to see sunmmarized and sorted details of all spans
-    # debug_spans(resp.json()["data"][0]["spans"])
-
-    assert len(resp.json()["data"][0]["spans"]) == expected_spans
-    assert len(resp.json()["data"][0]["processes"]) == 3
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import json
+
+import pytest
+from httpx import AsyncClient
+
+from nucliadb_telemetry import grpc_metrics
+from nucliadb_telemetry.settings import telemetry_settings
+from nucliadb_telemetry.tests.telemetry import Greeter
+
+
+def fmt_span(span):
+    tags_by_key = {tag["key"]: tag["value"] for tag in span["tags"]}
+    return {
+        "time": span["startTime"],
+        "id": span["spanID"],
+        "parent": span["references"][0]["spanID"],
+        "process": span["processID"],
+        "scope": tags_by_key["otel.scope.name"],
+        "operation": span["operationName"],
+    }
+
+
+def debug_spans(spans):
+    print(
+        json.dumps(
+            sorted([fmt_span(span) for span in spans], key=lambda x: x["time"]),
+            indent=4,
+        )
+    )
+
+
+@pytest.mark.asyncio
+async def test_telemetry_dict(http_service: AsyncClient, greeter: Greeter):
+    resp = await http_service.get(
+        "http://test/",
+        headers={
+            "x-b3-traceid": "f13dc5318bf3bef64a0a5ea607db93a1",
+            "x-b3-spanid": "bfc2225c60b39d97",
+            "x-b3-sampled": "1",
+        },
+    )
+    assert resp.status_code == 200
+    for i in range(10):
+        if len(greeter.messages) == 0:
+            await asyncio.sleep(1)
+    assert (
+        greeter.messages[0].headers["x-b3-traceid"]
+        == "f13dc5318bf3bef64a0a5ea607db93a1"
+    )
+    assert len(greeter.messages) == 4
+
+    expected_spans = 17
+
+    await asyncio.sleep(2)
+    client = AsyncClient()
+    for _ in range(10):
+        resp = await client.get(
+            f"http://localhost:{telemetry_settings.jaeger_query_port}/api/traces/f13dc5318bf3bef64a0a5ea607db93a1",
+            headers={"Accept": "application/json"},
+        )
+        if (
+            resp.status_code != 200
+            or len(resp.json()["data"][0]["spans"]) < expected_spans
+        ):
+            await asyncio.sleep(2)
+        else:
+            break
+
+    assert resp.json()["data"][0]["traceID"] == "f13dc5318bf3bef64a0a5ea607db93a1"
+
+    # Enable this block for debugging purposes, to see sunmmarized and sorted details of all spans
+    # debug_spans(resp.json()["data"][0]["spans"])
+
+    assert len(resp.json()["data"][0]["spans"]) == expected_spans
+    assert len(resp.json()["data"][0]["processes"]) == 3
+
+    assert grpc_metrics.grpc_client_observer.histogram.collect()[0].samples  # type: ignore
+    assert grpc_metrics.grpc_server_observer.histogram.collect()[0].samples  # type: ignore
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/telemetry.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,348 +1,348 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-import os
-
-import nats
-import pytest
-import requests
-from fastapi import FastAPI
-from grpc import aio  # type: ignore
-from httpx import AsyncClient
-from nats.aio.msg import Msg
-from nats.js import api
-from opentelemetry.propagate import set_global_textmap
-from opentelemetry.propagators.b3 import B3MultiFormat
-from pytest_docker_fixtures import images  # type: ignore
-from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
-
-from nucliadb_telemetry.fastapi import instrument_app
-from nucliadb_telemetry.grpc import OpenTelemetryGRPC
-from nucliadb_telemetry.jetstream import JetStreamContextTelemetry, NatsClientTelemetry
-from nucliadb_telemetry.settings import telemetry_settings
-from nucliadb_telemetry.tests.grpc import (
-    hellostreamingworld_pb2,
-    hellostreamingworld_pb2_grpc,
-    helloworld_pb2,
-    helloworld_pb2_grpc,
-)
-from nucliadb_telemetry.utils import (
-    clean_telemetry,
-    get_telemetry,
-    init_telemetry,
-    set_info_on_span,
-)
-
-images.settings["jaeger"] = {
-    "image": "jaegertracing/all-in-one",
-    "version": "1.33",
-    "options": {"ports": {"6831/udp": None, "16686": None}},
-}
-
-
-class Jaeger(BaseImage):
-    name = "jaeger"
-    port = 6831
-
-    def get_port(self, port=None):
-        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
-            return port if port else self.port
-        network = self.container_obj.attrs["NetworkSettings"]
-        service_port = "{0}/udp".format(port if port else self.port)
-        for netport in network["Ports"].keys():
-            if netport == service_port:
-                return network["Ports"][service_port][0]["HostPort"]
-
-    def get_http_port(self, port=None):
-        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
-            return 16686
-        network = self.container_obj.attrs["NetworkSettings"]
-        service_port = "16686/tcp"
-        for netport in network["Ports"].keys():
-            if netport == service_port:
-                return network["Ports"][service_port][0]["HostPort"]
-
-    def check(self):
-        resp = requests.get(f"http://{self.host}:{self.get_http_port()}")
-        return resp.status_code == 200
-
-
-@pytest.fixture(scope="function")
-async def jaeger_server():
-    server = Jaeger()
-    server.run()
-    yield server
-    server.stop()
-
-
-@pytest.fixture(scope="function")
-async def set_telemetry_settings(jaeger_server: Jaeger):
-    telemetry_settings.jaeger_enabled = True
-    telemetry_settings.jaeger_agent_host = "127.0.0.1"
-    telemetry_settings.jaeger_agent_port = jaeger_server.get_port()
-    telemetry_settings.jaeger_query_port = jaeger_server.get_http_port()
-
-
-@pytest.fixture(scope="function")
-async def telemetry_grpc(set_telemetry_settings):
-    tracer_provider = get_telemetry("GRPC_SERVICE")
-    await init_telemetry(tracer_provider)
-    util = OpenTelemetryGRPC("test_telemetry", tracer_provider)
-    yield util
-
-
-class GreeterStreaming(hellostreamingworld_pb2_grpc.MultiGreeterServicer):
-    def __init__(self, natsd):
-        self.natsd = natsd
-        self.nc = None
-        self.push_subscription = None
-        self.tracer_provider = None
-        self.messages = []
-
-    async def push_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("message_worker")
-        with tracer.start_as_current_span("message_worker_span") as _:
-            self.messages.append(msg)
-
-    async def initialize(self):
-        self.nc = await nats.connect(servers=[self.natsd])
-        self.js = self.nc.jetstream()
-
-        try:
-            await self.js.stream_info("testing")
-        except nats.js.errors.NotFoundError:
-            await self.js.add_stream(name="testing", subjects=["testing.*"])
-
-        self.tracer_provider = get_telemetry("NATS_SERVICE")
-        await init_telemetry(self.tracer_provider)
-        self.jsotel = JetStreamContextTelemetry(
-            self.js, "nats_service", self.tracer_provider
-        )
-
-        self.push_subscription = await self.jsotel.subscribe(
-            subject="testing.stelemetry",
-            stream="testing",
-            cb=self.push_subscription_worker,
-        )
-
-    async def finalize(self):
-        await self.push_subscription.unsubscribe()
-        await self.nc.drain()
-        await self.nc.close()
-
-    async def sayHello(self, request, context):
-        await self.jsotel.publish("testing.stelemetry", request.name.encode())
-        for _ in range(10):
-            yield hellostreamingworld_pb2.HelloReply(
-                message="Hello, %s!" % request.name
-            )
-
-
-class Greeter(helloworld_pb2_grpc.GreeterServicer):
-    def __init__(self, natsd):
-        self.natsd = natsd
-        self.nc = None
-        self.push_subscription = None
-        self.pull_subscription_one = None
-        self.puller_task = None
-        self.pubsub_subscription = None
-        self.tracer_provider = None
-        self.messages = []
-        self.puller_task_one = None
-
-    async def push_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("message_worker")
-        with tracer.start_as_current_span("message_worker_span") as _:
-            self.messages.append(msg)
-
-    async def pull_subscription_worker_one(self):
-        async def callback(message):
-            self.messages.append(message)
-
-        await self.jsotel.pull_one(self.pull_subscription_one, callback)
-
-    async def pubsub_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("pubsub_worker")
-        with tracer.start_as_current_span("pubsub_worker_span") as _:
-            self.messages.append(msg)
-
-    async def reqresp_subscription_worker(self, msg: Msg):
-        tracer = self.tracer_provider.get_tracer("reqresp_worker")
-        with tracer.start_as_current_span("reqresp_worker_span") as _:
-            self.messages.append(msg)
-            await msg.respond(b"Bye Bye!")
-
-    async def initialize(self):
-        self.nc = await nats.connect(servers=[self.natsd])
-        self.js = self.nc.jetstream()
-
-        try:
-            await self.js.stream_info("testing")
-        except nats.js.errors.NotFoundError:
-            await self.js.add_stream(name="testing", subjects=["testing.*"])
-
-        self.tracer_provider = get_telemetry("NATS_SERVICE")
-        await init_telemetry(self.tracer_provider)
-        self.jsotel = JetStreamContextTelemetry(
-            self.js, "nats_service", self.tracer_provider
-        )
-
-        self.push_subscription = await self.jsotel.subscribe(
-            subject="testing.telemetry",
-            stream="testing",
-            cb=self.push_subscription_worker,
-        )
-
-        # Nats Jetstream Pull subscription including consumer creation
-        # and task to pull one message
-        config = api.ConsumerConfig()
-        config.filter_subject = "testing.telemetry_pull_one"
-        config.durable_name = "testing_consumer_one"
-        await self.js._jsm.add_consumer(stream="testing", config=config)
-
-        self.pull_subscription_one = await self.jsotel.pull_subscribe(
-            subject=config.filter_subject, durable=config.durable_name, stream="testing"
-        )
-
-        self.puller_task_one = asyncio.create_task(self.pull_subscription_worker_one())
-
-        # Plain nats instrumentation and subscription
-        # (no streams neither consumers used here)
-        self.ncotel = NatsClientTelemetry(self.nc, "nats_service", self.tracer_provider)
-
-        self.pubsub_subscription = await self.ncotel.subscribe(
-            subject="testing.telemetry_nats_pubsub",
-            queue="telemetry_nats_pubsub",
-            cb=self.pubsub_subscription_worker,
-        )
-
-        # Plain nats request-response
-
-        self.reqresp_subscription = await self.ncotel.subscribe(
-            subject="testing.telemetry_nats_reqresp",
-            queue="telemetry_nats_reqresp",
-            cb=self.reqresp_subscription_worker,
-        )
-
-    async def finalize(self):
-        await self.push_subscription.unsubscribe()
-
-        await self.pull_subscription_one.unsubscribe()
-        self.puller_task_one.cancel()
-        await self.js._jsm.delete_consumer(
-            stream="testing", consumer="testing_consumer_one"
-        )
-
-        await self.pubsub_subscription.unsubscribe()
-        await self.nc.drain()
-        await self.nc.close()
-
-    async def SayHello(self, request, context):
-        # Send message to test Jetstream publish and subscribe message
-        await self.jsotel.publish("testing.telemetry", request.name.encode())
-
-        # Send message to test Jetstream pull subscriber one
-        await self.jsotel.publish("testing.telemetry_pull_one", request.name.encode())
-
-        # Test regular nats pubsub
-        await self.ncotel.publish(
-            "testing.telemetry_nats_pubsub", request.name.encode()
-        )
-
-        # Test regular nats request-response
-        await self.ncotel.request(
-            "testing.telemetry_nats_reqresp", request.name.encode()
-        )
-
-        return helloworld_pb2.HelloReply(
-            message=("Hello, %s!" % request.name) * 2_000_000
-        )
-
-
-@pytest.fixture(scope="function")
-async def greeter(set_telemetry_settings, natsd: str):
-    obj = Greeter(natsd)
-    await obj.initialize()
-    yield obj
-    await obj.finalize()
-
-
-@pytest.fixture(scope="function")
-async def greeter_streaming(set_telemetry_settings, natsd: str):
-    obj = GreeterStreaming(natsd)
-    await obj.initialize()
-    yield obj
-    await obj.finalize()
-
-
-@pytest.fixture(scope="function")
-async def grpc_service(
-    telemetry_grpc: OpenTelemetryGRPC,
-    greeter: Greeter,
-    greeter_streaming: GreeterStreaming,
-):
-    server = telemetry_grpc.init_server()
-    helloworld_pb2_grpc.add_GreeterServicer_to_server(greeter, server)
-    hellostreamingworld_pb2_grpc.add_MultiGreeterServicer_to_server(
-        greeter_streaming, server
-    )
-    port = server.add_insecure_port("[::]:0")
-    await server.start()
-    yield port
-    await server.stop(grace=True)
-
-
-@pytest.fixture(scope="function")
-async def http_service(
-    set_telemetry_settings, telemetry_grpc: OpenTelemetryGRPC, grpc_service: int
-):
-    tracer_provider = get_telemetry("HTTP_SERVICE")
-    await init_telemetry(tracer_provider)
-    app = FastAPI(title="Test API")  # type: ignore
-    set_global_textmap(B3MultiFormat())
-    instrument_app(app, tracer_provider=tracer_provider, excluded_urls=[])
-
-    @app.get("/")
-    async def simple_api():
-        set_info_on_span({"my.data": "is this"})
-        tracer = tracer_provider.get_tracer(__name__)
-        with tracer.start_as_current_span("simple_api_work") as _:
-            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
-            stub = helloworld_pb2_grpc.GreeterStub(channel)
-            response = await stub.SayHello(
-                helloworld_pb2.HelloRequest(name="you"),
-                # This metadata is here to make sure our instrumentor handles correctly
-                # requests with metadata, as it does some manipulation of in on start_client_span
-                # The servicer endpoint does not use this metadata
-                metadata=aio.Metadata.from_tuple((("header1", "value1"),)),
-            )
-        with tracer.start_as_current_span("simple_stream_api_work") as _:
-            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
-            stub = hellostreamingworld_pb2_grpc.MultiGreeterStub(channel)
-            async for sresponse in stub.sayHello(
-                helloworld_pb2.HelloRequest(name="you")
-            ):
-                assert sresponse
-        return response.message
-
-    client_base_url = "http://test"
-    client = AsyncClient(app=app, base_url=client_base_url)  # type: ignore
-    yield client
-    await clean_telemetry("HTTP_SERVICE")
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+import os
+
+import nats
+import pytest
+import requests
+from fastapi import FastAPI
+from grpc import aio  # type: ignore
+from httpx import AsyncClient
+from nats.aio.msg import Msg
+from nats.js import api
+from opentelemetry.propagate import set_global_textmap
+from opentelemetry.propagators.b3 import B3MultiFormat
+from pytest_docker_fixtures import images  # type: ignore
+from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
+
+from nucliadb_telemetry.fastapi import instrument_app
+from nucliadb_telemetry.grpc import GRPCTelemetry
+from nucliadb_telemetry.jetstream import JetStreamContextTelemetry, NatsClientTelemetry
+from nucliadb_telemetry.settings import telemetry_settings
+from nucliadb_telemetry.tests.grpc import (
+    hellostreamingworld_pb2,
+    hellostreamingworld_pb2_grpc,
+    helloworld_pb2,
+    helloworld_pb2_grpc,
+)
+from nucliadb_telemetry.utils import (
+    clean_telemetry,
+    get_telemetry,
+    init_telemetry,
+    set_info_on_span,
+)
+
+images.settings["jaeger"] = {
+    "image": "jaegertracing/all-in-one",
+    "version": "1.33",
+    "options": {"ports": {"6831/udp": None, "16686": None}},
+}
+
+
+class Jaeger(BaseImage):
+    name = "jaeger"
+    port = 6831
+
+    def get_port(self, port=None):
+        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
+            return port if port else self.port
+        network = self.container_obj.attrs["NetworkSettings"]
+        service_port = "{0}/udp".format(port if port else self.port)
+        for netport in network["Ports"].keys():
+            if netport == service_port:
+                return network["Ports"][service_port][0]["HostPort"]
+
+    def get_http_port(self, port=None):
+        if os.environ.get("TESTING", "") == "jenkins" or "TRAVIS" in os.environ:
+            return 16686
+        network = self.container_obj.attrs["NetworkSettings"]
+        service_port = "16686/tcp"
+        for netport in network["Ports"].keys():
+            if netport == service_port:
+                return network["Ports"][service_port][0]["HostPort"]
+
+    def check(self):
+        resp = requests.get(f"http://{self.host}:{self.get_http_port()}")
+        return resp.status_code == 200
+
+
+@pytest.fixture(scope="function")
+async def jaeger_server():
+    server = Jaeger()
+    server.run()
+    yield server
+    server.stop()
+
+
+@pytest.fixture(scope="function")
+async def set_telemetry_settings(jaeger_server: Jaeger):
+    telemetry_settings.jaeger_enabled = True
+    telemetry_settings.jaeger_agent_host = "127.0.0.1"
+    telemetry_settings.jaeger_agent_port = jaeger_server.get_port()
+    telemetry_settings.jaeger_query_port = jaeger_server.get_http_port()
+
+
+@pytest.fixture(scope="function")
+async def telemetry_grpc(set_telemetry_settings):
+    tracer_provider = get_telemetry("GRPC_SERVICE")
+    await init_telemetry(tracer_provider)
+    util = GRPCTelemetry("test_telemetry", tracer_provider)
+    yield util
+
+
+class GreeterStreaming(hellostreamingworld_pb2_grpc.MultiGreeterServicer):
+    def __init__(self, natsd):
+        self.natsd = natsd
+        self.nc = None
+        self.push_subscription = None
+        self.tracer_provider = None
+        self.messages = []
+
+    async def push_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("message_worker")
+        with tracer.start_as_current_span("message_worker_span") as _:
+            self.messages.append(msg)
+
+    async def initialize(self):
+        self.nc = await nats.connect(servers=[self.natsd])
+        self.js = self.nc.jetstream()
+
+        try:
+            await self.js.stream_info("testing")
+        except nats.js.errors.NotFoundError:
+            await self.js.add_stream(name="testing", subjects=["testing.*"])
+
+        self.tracer_provider = get_telemetry("NATS_SERVICE")
+        await init_telemetry(self.tracer_provider)
+        self.jsotel = JetStreamContextTelemetry(
+            self.js, "nats_service", self.tracer_provider
+        )
+
+        self.push_subscription = await self.jsotel.subscribe(
+            subject="testing.stelemetry",
+            stream="testing",
+            cb=self.push_subscription_worker,
+        )
+
+    async def finalize(self):
+        await self.push_subscription.unsubscribe()
+        await self.nc.drain()
+        await self.nc.close()
+
+    async def sayHello(self, request, context):
+        await self.jsotel.publish("testing.stelemetry", request.name.encode())
+        for _ in range(10):
+            yield hellostreamingworld_pb2.HelloReply(
+                message="Hello, %s!" % request.name
+            )
+
+
+class Greeter(helloworld_pb2_grpc.GreeterServicer):
+    def __init__(self, natsd):
+        self.natsd = natsd
+        self.nc = None
+        self.push_subscription = None
+        self.pull_subscription_one = None
+        self.puller_task = None
+        self.pubsub_subscription = None
+        self.tracer_provider = None
+        self.messages = []
+        self.puller_task_one = None
+
+    async def push_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("message_worker")
+        with tracer.start_as_current_span("message_worker_span") as _:
+            self.messages.append(msg)
+
+    async def pull_subscription_worker_one(self):
+        async def callback(message):
+            self.messages.append(message)
+
+        await self.jsotel.pull_one(self.pull_subscription_one, callback)
+
+    async def pubsub_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("pubsub_worker")
+        with tracer.start_as_current_span("pubsub_worker_span") as _:
+            self.messages.append(msg)
+
+    async def reqresp_subscription_worker(self, msg: Msg):
+        tracer = self.tracer_provider.get_tracer("reqresp_worker")
+        with tracer.start_as_current_span("reqresp_worker_span") as _:
+            self.messages.append(msg)
+            await msg.respond(b"Bye Bye!")
+
+    async def initialize(self):
+        self.nc = await nats.connect(servers=[self.natsd])
+        self.js = self.nc.jetstream()
+
+        try:
+            await self.js.stream_info("testing")
+        except nats.js.errors.NotFoundError:
+            await self.js.add_stream(name="testing", subjects=["testing.*"])
+
+        self.tracer_provider = get_telemetry("NATS_SERVICE")
+        await init_telemetry(self.tracer_provider)
+        self.jsotel = JetStreamContextTelemetry(
+            self.js, "nats_service", self.tracer_provider
+        )
+
+        self.push_subscription = await self.jsotel.subscribe(
+            subject="testing.telemetry",
+            stream="testing",
+            cb=self.push_subscription_worker,
+        )
+
+        # Nats Jetstream Pull subscription including consumer creation
+        # and task to pull one message
+        config = api.ConsumerConfig()
+        config.filter_subject = "testing.telemetry_pull_one"
+        config.durable_name = "testing_consumer_one"
+        await self.js._jsm.add_consumer(stream="testing", config=config)
+
+        self.pull_subscription_one = await self.jsotel.pull_subscribe(
+            subject=config.filter_subject, durable=config.durable_name, stream="testing"
+        )
+
+        self.puller_task_one = asyncio.create_task(self.pull_subscription_worker_one())
+
+        # Plain nats instrumentation and subscription
+        # (no streams neither consumers used here)
+        self.ncotel = NatsClientTelemetry(self.nc, "nats_service", self.tracer_provider)
+
+        self.pubsub_subscription = await self.ncotel.subscribe(
+            subject="testing.telemetry_nats_pubsub",
+            queue="telemetry_nats_pubsub",
+            cb=self.pubsub_subscription_worker,
+        )
+
+        # Plain nats request-response
+
+        self.reqresp_subscription = await self.ncotel.subscribe(
+            subject="testing.telemetry_nats_reqresp",
+            queue="telemetry_nats_reqresp",
+            cb=self.reqresp_subscription_worker,
+        )
+
+    async def finalize(self):
+        await self.push_subscription.unsubscribe()
+
+        await self.pull_subscription_one.unsubscribe()
+        self.puller_task_one.cancel()
+        await self.js._jsm.delete_consumer(
+            stream="testing", consumer="testing_consumer_one"
+        )
+
+        await self.pubsub_subscription.unsubscribe()
+        await self.nc.drain()
+        await self.nc.close()
+
+    async def SayHello(self, request, context):
+        # Send message to test Jetstream publish and subscribe message
+        await self.jsotel.publish("testing.telemetry", request.name.encode())
+
+        # Send message to test Jetstream pull subscriber one
+        await self.jsotel.publish("testing.telemetry_pull_one", request.name.encode())
+
+        # Test regular nats pubsub
+        await self.ncotel.publish(
+            "testing.telemetry_nats_pubsub", request.name.encode()
+        )
+
+        # Test regular nats request-response
+        await self.ncotel.request(
+            "testing.telemetry_nats_reqresp", request.name.encode()
+        )
+
+        return helloworld_pb2.HelloReply(
+            message=("Hello, %s!" % request.name) * 2_000_000
+        )
+
+
+@pytest.fixture(scope="function")
+async def greeter(set_telemetry_settings, natsd: str):
+    obj = Greeter(natsd)
+    await obj.initialize()
+    yield obj
+    await obj.finalize()
+
+
+@pytest.fixture(scope="function")
+async def greeter_streaming(set_telemetry_settings, natsd: str):
+    obj = GreeterStreaming(natsd)
+    await obj.initialize()
+    yield obj
+    await obj.finalize()
+
+
+@pytest.fixture(scope="function")
+async def grpc_service(
+    telemetry_grpc: GRPCTelemetry,
+    greeter: Greeter,
+    greeter_streaming: GreeterStreaming,
+):
+    server = telemetry_grpc.init_server()
+    helloworld_pb2_grpc.add_GreeterServicer_to_server(greeter, server)
+    hellostreamingworld_pb2_grpc.add_MultiGreeterServicer_to_server(
+        greeter_streaming, server
+    )
+    port = server.add_insecure_port("[::]:0")
+    await server.start()
+    yield port
+    await server.stop(grace=True)
+
+
+@pytest.fixture(scope="function")
+async def http_service(
+    set_telemetry_settings, telemetry_grpc: GRPCTelemetry, grpc_service: int
+):
+    tracer_provider = get_telemetry("HTTP_SERVICE")
+    await init_telemetry(tracer_provider)
+    app = FastAPI(title="Test API")  # type: ignore
+    set_global_textmap(B3MultiFormat())
+    instrument_app(app, tracer_provider=tracer_provider, excluded_urls=[], metrics=True)
+
+    @app.get("/")
+    async def simple_api():
+        set_info_on_span({"my.data": "is this"})
+        tracer = tracer_provider.get_tracer(__name__)
+        with tracer.start_as_current_span("simple_api_work") as _:
+            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
+            stub = helloworld_pb2_grpc.GreeterStub(channel)
+            response = await stub.SayHello(
+                helloworld_pb2.HelloRequest(name="you"),
+                # This metadata is here to make sure our instrumentor handles correctly
+                # requests with metadata, as it does some manipulation of in on start_client_span
+                # The servicer endpoint does not use this metadata
+                metadata=aio.Metadata.from_tuple((("header1", "value1"),)),
+            )
+        with tracer.start_as_current_span("simple_stream_api_work") as _:
+            channel = telemetry_grpc.init_client(f"localhost:{grpc_service}")
+            stub = hellostreamingworld_pb2_grpc.MultiGreeterStub(channel)
+            async for sresponse in stub.sayHello(
+                helloworld_pb2.HelloRequest(name="you")
+            ):
+                assert sresponse
+        return response.message
+
+    client_base_url = "http://test"
+    client = AsyncClient(app=app, base_url=client_base_url)  # type: ignore
+    yield client
+    await clean_telemetry("HTTP_SERVICE")
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tests/unit/test_errors.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,99 +1,99 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-from unittest.mock import ANY, patch
-
-from nucliadb_telemetry import errors
-
-
-def test_capture_exception() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        ex = Exception("test")
-        errors.capture_exception(ex)
-        mock_sentry_sdk.capture_exception.assert_called_once_with(ex)
-
-
-def test_capture_exception_no_sentry() -> None:
-    with patch.object(errors, "SENTRY", False), patch(
-        "nucliadb_telemetry.errors.sentry_sdk"
-    ) as mock_sentry_sdk:
-        errors.capture_exception(Exception())
-        mock_sentry_sdk.capture_exception.assert_not_called()
-
-
-def test_capture_message() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        errors.capture_message("error_msg", "level", "scope")
-        mock_sentry_sdk.capture_message.assert_called_once_with(
-            "error_msg", "level", "scope"
-        )
-
-
-def test_capture_message_no_sentry() -> None:
-    with patch.object(errors, "SENTRY", False), patch(
-        "nucliadb_telemetry.errors.sentry_sdk"
-    ) as mock_sentry_sdk:
-        errors.capture_message("error_msg", "level", "scope")
-        mock_sentry_sdk.capture_message.assert_not_called()
-
-
-def test_setup_error_handling(monkeypatch):
-    monkeypatch.setenv("sentry_url", "sentry_url")
-    monkeypatch.setenv("logging_integration", "True")
-    monkeypatch.setenv("environment", "environment")
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ), patch("nucliadb_telemetry.errors.LoggingIntegration") as LoggingIntegration:
-        errors.setup_error_handling("1.0.0")
-        mock_sentry_sdk.init.assert_called_once_with(
-            release="1.0.0",
-            environment="environment",
-            dsn="sentry_url",
-            integrations=[ANY],
-            default_integrations=False,
-        )
-        LoggingIntegration.assert_called_once()
-
-
-def test_setup_error_handling_no_sentry(monkeypatch):
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk:
-        errors.setup_error_handling("1.0.0")
-        mock_sentry_sdk.init.assert_not_called()
-
-
-def test_push_scope() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", True
-    ):
-        with errors.push_scope() as scope:
-            scope.set_extra("key", "value")
-        mock_sentry_sdk.push_scope.assert_called_once_with()
-
-
-def test_push_scope_no_sentry() -> None:
-    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
-        errors, "SENTRY", False
-    ):
-        with errors.push_scope() as scope:
-            scope.set_extra("key", "value")
-        mock_sentry_sdk.push_scope.assert_not_called()
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+from unittest.mock import ANY, patch
+
+from nucliadb_telemetry import errors
+
+
+def test_capture_exception() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        ex = Exception("test")
+        errors.capture_exception(ex)
+        mock_sentry_sdk.capture_exception.assert_called_once_with(ex)
+
+
+def test_capture_exception_no_sentry() -> None:
+    with patch.object(errors, "SENTRY", False), patch(
+        "nucliadb_telemetry.errors.sentry_sdk"
+    ) as mock_sentry_sdk:
+        errors.capture_exception(Exception())
+        mock_sentry_sdk.capture_exception.assert_not_called()
+
+
+def test_capture_message() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        errors.capture_message("error_msg", "level", "scope")
+        mock_sentry_sdk.capture_message.assert_called_once_with(
+            "error_msg", "level", "scope"
+        )
+
+
+def test_capture_message_no_sentry() -> None:
+    with patch.object(errors, "SENTRY", False), patch(
+        "nucliadb_telemetry.errors.sentry_sdk"
+    ) as mock_sentry_sdk:
+        errors.capture_message("error_msg", "level", "scope")
+        mock_sentry_sdk.capture_message.assert_not_called()
+
+
+def test_setup_error_handling(monkeypatch):
+    monkeypatch.setenv("sentry_url", "sentry_url")
+    monkeypatch.setenv("logging_integration", "True")
+    monkeypatch.setenv("environment", "environment")
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ), patch("nucliadb_telemetry.errors.LoggingIntegration") as LoggingIntegration:
+        errors.setup_error_handling("1.0.0")
+        mock_sentry_sdk.init.assert_called_once_with(
+            release="1.0.0",
+            environment="environment",
+            dsn="sentry_url",
+            integrations=[ANY],
+            default_integrations=False,
+        )
+        LoggingIntegration.assert_called_once()
+
+
+def test_setup_error_handling_no_sentry(monkeypatch):
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk:
+        errors.setup_error_handling("1.0.0")
+        mock_sentry_sdk.init.assert_not_called()
+
+
+def test_push_scope() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", True
+    ):
+        with errors.push_scope() as scope:
+            scope.set_extra("key", "value")
+        mock_sentry_sdk.push_scope.assert_called_once_with()
+
+
+def test_push_scope_no_sentry() -> None:
+    with patch("nucliadb_telemetry.errors.sentry_sdk") as mock_sentry_sdk, patch.object(
+        errors, "SENTRY", False
+    ):
+        with errors.push_scope() as scope:
+            scope.set_extra("key", "value")
+        mock_sentry_sdk.push_scope.assert_not_called()
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/nucliadb_telemetry/tracerprovider.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,101 +1,101 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from typing import Optional
-
-from opentelemetry.context import Context  # type: ignore
-from opentelemetry.sdk.trace import TracerProvider  # type: ignore
-from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
-from opentelemetry.util._time import _time_ns  # type: ignore
-
-
-class AsyncMultiSpanProcessor(SpanProcessor):
-    """Implementation of class:`SpanProcessor` that forwards all received
-    events to a list of span processors sequentially.
-
-    The underlying span processors are called in sequential order as they were
-    added.
-    """
-
-    def __init__(self):
-        # use a tuple to avoid race conditions when adding a new span and
-        # iterating through it on "on_start" and "on_end".
-        self._span_processors = ()
-        self._lock = asyncio.Lock()
-
-    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
-        """Adds a SpanProcessor to the list handled by this instance."""
-        async with self._lock:
-            self._span_processors += (span_processor,)
-
-    def on_start(
-        self,
-        span: Span,
-        parent_context: Optional[Context] = None,
-    ) -> None:
-        for sp in self._span_processors:
-            sp.on_start(span, parent_context=parent_context)
-
-    def on_end(self, span: ReadableSpan) -> None:
-        for sp in self._span_processors:
-            sp.on_end(span)
-
-    def shutdown(self) -> None:
-        """Sequentially shuts down all underlying span processors."""
-        for sp in self._span_processors:
-            sp.shutdown()
-
-    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:  # type: ignore
-        """Sequentially calls async_force_flush on all underlying
-        :class:`SpanProcessor`
-
-        Args:
-            timeout_millis: The maximum amount of time over all span processors
-                to wait for spans to be exported. In case the first n span
-                processors exceeded the timeout followup span processors will be
-                skipped.
-
-        Returns:
-            True if all span processors flushed their spans within the
-            given timeout, False otherwise.
-        """
-        deadline_ns = _time_ns() + timeout_millis * 1000000
-        for sp in self._span_processors:
-            current_time_ns = _time_ns()
-            if current_time_ns >= deadline_ns:
-                return False
-
-            if not await sp.async_force_flush(
-                (deadline_ns - current_time_ns) // 1000000
-            ):
-                return False
-
-        return True
-
-
-class AsyncTracerProvider(TracerProvider):
-    initialized: bool = False
-    _active_span_processor: AsyncMultiSpanProcessor  # type: ignore
-
-    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
-        await self._active_span_processor.async_add_span_processor(span_processor)
-
-    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:
-        return await self._active_span_processor.async_force_flush(timeout_millis)
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from typing import Optional
+
+from opentelemetry.context import Context  # type: ignore
+from opentelemetry.sdk.trace import TracerProvider  # type: ignore
+from opentelemetry.sdk.trace import ReadableSpan, Span, SpanProcessor  # type: ignore
+from opentelemetry.util._time import _time_ns  # type: ignore
+
+
+class AsyncMultiSpanProcessor(SpanProcessor):
+    """Implementation of class:`SpanProcessor` that forwards all received
+    events to a list of span processors sequentially.
+
+    The underlying span processors are called in sequential order as they were
+    added.
+    """
+
+    def __init__(self):
+        # use a tuple to avoid race conditions when adding a new span and
+        # iterating through it on "on_start" and "on_end".
+        self._span_processors = ()
+        self._lock = asyncio.Lock()
+
+    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
+        """Adds a SpanProcessor to the list handled by this instance."""
+        async with self._lock:
+            self._span_processors += (span_processor,)
+
+    def on_start(
+        self,
+        span: Span,
+        parent_context: Optional[Context] = None,
+    ) -> None:
+        for sp in self._span_processors:
+            sp.on_start(span, parent_context=parent_context)
+
+    def on_end(self, span: ReadableSpan) -> None:
+        for sp in self._span_processors:
+            sp.on_end(span)
+
+    def shutdown(self) -> None:
+        """Sequentially shuts down all underlying span processors."""
+        for sp in self._span_processors:
+            sp.shutdown()
+
+    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:  # type: ignore
+        """Sequentially calls async_force_flush on all underlying
+        :class:`SpanProcessor`
+
+        Args:
+            timeout_millis: The maximum amount of time over all span processors
+                to wait for spans to be exported. In case the first n span
+                processors exceeded the timeout followup span processors will be
+                skipped.
+
+        Returns:
+            True if all span processors flushed their spans within the
+            given timeout, False otherwise.
+        """
+        deadline_ns = _time_ns() + timeout_millis * 1000000
+        for sp in self._span_processors:
+            current_time_ns = _time_ns()
+            if current_time_ns >= deadline_ns:
+                return False
+
+            if not await sp.async_force_flush(
+                (deadline_ns - current_time_ns) // 1000000
+            ):
+                return False
+
+        return True
+
+
+class AsyncTracerProvider(TracerProvider):
+    initialized: bool = False
+    _active_span_processor: AsyncMultiSpanProcessor  # type: ignore
+
+    async def async_add_span_processor(self, span_processor: SpanProcessor) -> None:
+        await self._active_span_processor.async_add_span_processor(span_processor)
+
+    async def async_force_flush(self, timeout_millis: int = 30000) -> bool:
+        return await self._active_span_processor.async_force_flush(timeout_millis)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/setup.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/setup.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-import re
-
-from setuptools import find_packages, setup  # type: ignore
-
-VERSION = open("VERSION").read().strip()
-README = open("README.md").read()
-
-
-def load_reqs(filename):
-    with open(filename) as reqs_file:
-        return [
-            # pin nucliadb-xxx to the same version as nucliadb
-            line.strip() + f"=={VERSION}"
-            if line.startswith("nucliadb-") and "=" not in line
-            else line.strip()
-            for line in reqs_file.readlines()
-            if not (
-                re.match(r"\s*#", line) or re.match("-e", line) or re.match("-r", line)
-            )
-        ]
-
-
-requirements = load_reqs("requirements.txt")
-
-setup(
-    name="nucliadb_telemetry",
-    version=VERSION,
-    author="nucliadb Authors",
-    author_email="nucliadb@nuclia.com",
-    description="NucliaDB Telemetry Library Python process",
-    long_description=README,
-    long_description_content_type="text/markdown",
-    license="MIT",
-    url="https://github.com/nuclia/nucliadb",
-    classifiers=[
-        "Development Status :: 3 - Alpha",
-        "Intended Audience :: Developers",
-        "Intended Audience :: Information Technology",
-        "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
-        "Framework :: AsyncIO",
-        "Programming Language :: Python",
-        "Programming Language :: Python :: 3.9",
-        "Topic :: System :: Monitoring",
-    ],
-    python_requires=">=3.7",
-    include_package_data=True,
-    package_data={"": ["*.txt", "*.md"], "nucliadb_telemetry": ["py.typed"]},
-    packages=find_packages(),
-    install_requires=requirements,
-)
+import re
+
+from setuptools import find_packages, setup  # type: ignore
+
+VERSION = open("VERSION").read().strip()
+README = open("README.md").read()
+
+
+def load_reqs(filename):
+    with open(filename) as reqs_file:
+        return [
+            # pin nucliadb-xxx to the same version as nucliadb
+            line.strip() + f"=={VERSION}"
+            if line.startswith("nucliadb-") and "=" not in line
+            else line.strip()
+            for line in reqs_file.readlines()
+            if not (
+                re.match(r"\s*#", line) or re.match("-e", line) or re.match("-r", line)
+            )
+        ]
+
+
+requirements = load_reqs("requirements.txt")
+
+setup(
+    name="nucliadb_telemetry",
+    version=VERSION,
+    author="nucliadb Authors",
+    author_email="nucliadb@nuclia.com",
+    description="NucliaDB Telemetry Library Python process",
+    long_description=README,
+    long_description_content_type="text/markdown",
+    license="MIT",
+    url="https://github.com/nuclia/nucliadb",
+    classifiers=[
+        "Development Status :: 3 - Alpha",
+        "Intended Audience :: Developers",
+        "Intended Audience :: Information Technology",
+        "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
+        "Framework :: AsyncIO",
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 3.9",
+        "Topic :: System :: Monitoring",
+    ],
+    python_requires=">=3.7",
+    include_package_data=True,
+    package_data={"": ["*.txt", "*.md"], "nucliadb_telemetry": ["py.typed"]},
+    packages=find_packages(),
+    install_requires=requirements,
+)
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/lib.rs`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-#![allow(clippy::bool_assert_comparison)]
-pub mod payload;
-/// This crate contains  the code responsible for sending usage data to Nuclia's server.
-mod sender;
-pub(crate) mod sink;
-
-pub mod blocking;
-pub mod sync;
-/// This environment variable can be set to disable sending telemetry events.
-pub const DISABLE_TELEMETRY_ENV_KEY: &str = "NUCLIADB_DISABLE_TELEMETRY";
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+#![allow(clippy::bool_assert_comparison)]
+pub mod payload;
+/// This crate contains  the code responsible for sending usage data to Nuclia's server.
+mod sender;
+pub(crate) mod sink;
+
+pub mod blocking;
+pub mod sync;
+/// This environment variable can be set to disable sending telemetry events.
+pub const DISABLE_TELEMETRY_ENV_KEY: &str = "NUCLIADB_DISABLE_TELEMETRY";
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/payload.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/payload.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,146 +1,146 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::env;
-use std::time::UNIX_EPOCH;
-
-use serde::{Deserialize, Serialize};
-use uuid::Uuid;
-
-/// Represents the payload of the request sent with telemetry requests.
-#[derive(Debug, Serialize, Deserialize)]
-pub struct TelemetryPayload {
-    /// Client information. See details in `[ClientInformation]`.
-    pub client_information: ClientInformation,
-    pub events: Vec<EventWithTimestamp>,
-    /// Represents the number of events that where drops due to the
-    /// combination of the `TELEMETRY_PUSH_COOLDOWN` and `MAX_EVENT_IN_QUEUE`.
-    pub num_dropped_events: usize,
-}
-
-impl TelemetryPayload {
-    pub fn from_single_event(event: TelemetryEvent) -> TelemetryPayload {
-        TelemetryPayload {
-            client_information: ClientInformation::default(),
-            events: vec![EventWithTimestamp::from(event)],
-            num_dropped_events: 0,
-        }
-    }
-}
-
-#[derive(Debug, Serialize, Deserialize)]
-pub struct EventWithTimestamp {
-    /// Unix time in seconds.
-    pub timestamp: u64,
-    /// Telemetry event.
-    pub r#type: TelemetryEvent,
-}
-
-/// Returns the number of seconds elapsed since UNIX_EPOCH.
-///
-/// If the system clock is set before 1970, returns 0.
-fn unixtime() -> u64 {
-    match UNIX_EPOCH.elapsed() {
-        Ok(duration) => duration.as_secs(),
-        Err(_) => 0u64,
-    }
-}
-
-impl From<TelemetryEvent> for EventWithTimestamp {
-    fn from(event: TelemetryEvent) -> Self {
-        EventWithTimestamp {
-            timestamp: unixtime(),
-            r#type: event,
-        }
-    }
-}
-
-/// Represents a Telemetry Event send to nucliadb's server for usage information.
-#[derive(Debug, Serialize, Deserialize)]
-pub enum TelemetryEvent {
-    /// Create command is called.
-    Create,
-    /// Delete command
-    Delete,
-    /// Garbage Collect command
-    GarbageCollect,
-    /// Serve command is called.
-    Serve(ServeEvent),
-    /// EndCommand (with the return code)
-    EndCommand { return_code: i32 },
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-pub struct ServeEvent {
-    pub has_seed: bool,
-}
-
-#[derive(Clone, Debug, Serialize, Deserialize)]
-pub struct ClientInformation {
-    session_uuid: uuid::Uuid,
-    nucliadb_version: String,
-    os: String,
-    arch: String,
-    hashed_host_username: String,
-    component: Option<String>,
-    kubernetes: bool,
-}
-
-fn hashed_host_username() -> String {
-    let hostname = hostname::get()
-        .map(|hostname| hostname.to_string_lossy().to_string())
-        .unwrap_or_else(|_| "".to_string());
-    let username = username::get_user_name().unwrap_or_else(|_| "".to_string());
-    let hashed_value = format!("{}:{}", hostname, username);
-    let digest = md5::compute(hashed_value.as_bytes());
-    format!("{:x}", digest)
-}
-
-impl Default for ClientInformation {
-    fn default() -> ClientInformation {
-        ClientInformation {
-            session_uuid: Uuid::new_v4(),
-            nucliadb_version: env!("CARGO_PKG_VERSION").to_string(),
-            os: env::consts::OS.to_string(),
-            arch: env::consts::ARCH.to_string(),
-            hashed_host_username: hashed_host_username(),
-            component: Some("Node".to_string()),
-            kubernetes: std::env::var_os("KUBERNETES_SERVICE_HOST").is_some(),
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::env;
+use std::time::UNIX_EPOCH;
+
+use serde::{Deserialize, Serialize};
+use uuid::Uuid;
+
+/// Represents the payload of the request sent with telemetry requests.
+#[derive(Debug, Serialize, Deserialize)]
+pub struct TelemetryPayload {
+    /// Client information. See details in `[ClientInformation]`.
+    pub client_information: ClientInformation,
+    pub events: Vec<EventWithTimestamp>,
+    /// Represents the number of events that where drops due to the
+    /// combination of the `TELEMETRY_PUSH_COOLDOWN` and `MAX_EVENT_IN_QUEUE`.
+    pub num_dropped_events: usize,
+}
+
+impl TelemetryPayload {
+    pub fn from_single_event(event: TelemetryEvent) -> TelemetryPayload {
+        TelemetryPayload {
+            client_information: ClientInformation::default(),
+            events: vec![EventWithTimestamp::from(event)],
+            num_dropped_events: 0,
+        }
+    }
+}
+
+#[derive(Debug, Serialize, Deserialize)]
+pub struct EventWithTimestamp {
+    /// Unix time in seconds.
+    pub timestamp: u64,
+    /// Telemetry event.
+    pub r#type: TelemetryEvent,
+}
+
+/// Returns the number of seconds elapsed since UNIX_EPOCH.
+///
+/// If the system clock is set before 1970, returns 0.
+fn unixtime() -> u64 {
+    match UNIX_EPOCH.elapsed() {
+        Ok(duration) => duration.as_secs(),
+        Err(_) => 0u64,
+    }
+}
+
+impl From<TelemetryEvent> for EventWithTimestamp {
+    fn from(event: TelemetryEvent) -> Self {
+        EventWithTimestamp {
+            timestamp: unixtime(),
+            r#type: event,
+        }
+    }
+}
+
+/// Represents a Telemetry Event send to nucliadb's server for usage information.
+#[derive(Debug, Serialize, Deserialize)]
+pub enum TelemetryEvent {
+    /// Create command is called.
+    Create,
+    /// Delete command
+    Delete,
+    /// Garbage Collect command
+    GarbageCollect,
+    /// Serve command is called.
+    Serve(ServeEvent),
+    /// EndCommand (with the return code)
+    EndCommand { return_code: i32 },
+}
+
+#[derive(Clone, Debug, Serialize, Deserialize)]
+pub struct ServeEvent {
+    pub has_seed: bool,
+}
+
+#[derive(Clone, Debug, Serialize, Deserialize)]
+pub struct ClientInformation {
+    session_uuid: uuid::Uuid,
+    nucliadb_version: String,
+    os: String,
+    arch: String,
+    hashed_host_username: String,
+    component: Option<String>,
+    kubernetes: bool,
+}
+
+fn hashed_host_username() -> String {
+    let hostname = hostname::get()
+        .map(|hostname| hostname.to_string_lossy().to_string())
+        .unwrap_or_else(|_| "".to_string());
+    let username = username::get_user_name().unwrap_or_else(|_| "".to_string());
+    let hashed_value = format!("{}:{}", hostname, username);
+    let digest = md5::compute(hashed_value.as_bytes());
+    format!("{:x}", digest)
+}
+
+impl Default for ClientInformation {
+    fn default() -> ClientInformation {
+        ClientInformation {
+            session_uuid: Uuid::new_v4(),
+            nucliadb_version: env!("CARGO_PKG_VERSION").to_string(),
+            os: env::consts::OS.to_string(),
+            arch: env::consts::ARCH.to_string(),
+            hashed_host_username: hashed_host_username(),
+            component: Some("Node".to_string()),
+            kubernetes: std::env::var_os("KUBERNETES_SERVICE_HOST").is_some(),
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/sender.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/sender.rs`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,401 +1,401 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::mem;
-use std::sync::atomic::{AtomicBool, Ordering};
-use std::sync::Arc;
-use std::time::Duration;
-
-use tokio::sync::mpsc::{Receiver, Sender};
-use tokio::sync::{oneshot, Mutex, RwLock};
-use tokio::task::JoinHandle;
-use tokio::time::Interval;
-use tracing::info;
-
-use crate::payload::{ClientInformation, EventWithTimestamp, TelemetryEvent, TelemetryPayload};
-use crate::sink::{HttpClient, Sink};
-
-/// At most 1 Request per minutes.
-const TELEMETRY_PUSH_COOLDOWN: Duration = Duration::from_secs(60);
-
-/// Upon termination of the program, we send one last telemetry request with pending events.
-/// This duration is the amount of time we wait for at most to send that last telemetry request.
-const LAST_REQUEST_TIMEOUT: Duration = Duration::from_secs(1);
-
-const MAX_NUM_EVENTS_IN_QUEUE: usize = 10;
-
-#[cfg(test)]
-struct ClockButton(Sender<()>);
-
-#[cfg(test)]
-impl ClockButton {
-    async fn tick(&self) {
-        let _ = self.0.send(()).await;
-    }
-}
-
-enum Clock {
-    Periodical(Mutex<Interval>),
-    #[cfg(test)]
-    Manual(Mutex<Receiver<()>>),
-}
-
-impl Clock {
-    pub fn periodical(period: Duration) -> Clock {
-        let interval = tokio::time::interval(period);
-        Clock::Periodical(Mutex::new(interval))
-    }
-
-    #[cfg(test)]
-    pub async fn manual() -> (ClockButton, Clock) {
-        let (tx, rx) = tokio::sync::mpsc::channel(1);
-        let _ = tx.send(()).await;
-        let button = ClockButton(tx);
-        (button, Clock::Manual(Mutex::new(rx)))
-    }
-
-    async fn tick(&self) {
-        match self {
-            Clock::Periodical(interval) => {
-                interval.lock().await.tick().await;
-            }
-            #[cfg(test)]
-            Clock::Manual(channel) => {
-                channel.lock().await.recv().await;
-            }
-        }
-    }
-}
-
-#[derive(Default)]
-struct EventsState {
-    events: Vec<EventWithTimestamp>,
-    num_dropped_events: usize,
-}
-
-impl EventsState {
-    fn drain_events(&mut self) -> EventsState {
-        mem::replace(
-            self,
-            EventsState {
-                events: Vec::new(),
-                num_dropped_events: 0,
-            },
-        )
-    }
-
-    /// Adds an event.
-    /// If the queue is already saturated, (ie. it has reached the len `MAX_NUM_EVENTS_IN_QUEUE`)
-    // Returns true iff it was the first event in the queue.
-    fn push_event(&mut self, event: TelemetryEvent) -> bool {
-        if self.events.len() >= MAX_NUM_EVENTS_IN_QUEUE {
-            self.num_dropped_events += 1;
-            return false;
-        }
-        let events_was_empty = self.events.is_empty();
-        self.events.push(EventWithTimestamp::from(event));
-        events_was_empty
-    }
-}
-
-struct Events {
-    state: RwLock<EventsState>,
-    items_available_tx: Sender<()>,
-    items_available_rx: RwLock<Receiver<()>>,
-}
-
-impl Default for Events {
-    fn default() -> Self {
-        let (items_available_tx, items_available_rx) = tokio::sync::mpsc::channel(1);
-        Events {
-            state: RwLock::new(EventsState::default()),
-            items_available_tx,
-            items_available_rx: RwLock::new(items_available_rx),
-        }
-    }
-}
-
-impl Events {
-    /// Wait for events to be available (if there are pending events, then do not wait)
-    /// and then send them to the PushAPI server.
-    async fn drain_events(&self) -> EventsState {
-        self.items_available_rx.write().await.recv().await;
-        self.state.write().await.drain_events()
-    }
-
-    async fn push_event(&self, event: TelemetryEvent) {
-        let is_first_event = self.state.write().await.push_event(event);
-        if is_first_event {
-            let _ = self.items_available_tx.send(()).await;
-        }
-    }
-}
-
-pub(crate) struct Inner {
-    sink: Option<Box<dyn Sink>>,
-    client_information: ClientInformation,
-    /// This channel is just used to signal there are new items available.
-    events: Events,
-    clock: Clock,
-    is_started: AtomicBool,
-}
-
-impl Inner {
-    pub fn is_disabled(&self) -> bool {
-        self.sink.is_none()
-    }
-
-    async fn create_telemetry_payload(&self) -> TelemetryPayload {
-        let events_state = self.events.drain_events().await;
-        TelemetryPayload {
-            client_information: self.client_information.clone(),
-            events: events_state.events,
-            num_dropped_events: events_state.num_dropped_events,
-        }
-    }
-
-    /// Wait for events to be available (if there are pending events, then do not wait)
-    /// and then send them to the PushAPI server.
-    ///
-    /// If the requests fails, it fails silently.
-    async fn send_pending_events(&self) {
-        if let Some(sink) = self.sink.as_ref() {
-            let payload = self.create_telemetry_payload().await;
-            sink.send_payload(payload).await;
-        }
-    }
-
-    async fn send(&self, event: TelemetryEvent) {
-        if self.is_disabled() {
-            return;
-        }
-        self.events.push_event(event).await;
-    }
-}
-
-pub struct TelemetrySender {
-    pub(crate) inner: Arc<Inner>,
-}
-
-pub enum TelemetryLoopHandle {
-    NoLoop,
-    WithLoop {
-        join_handle: JoinHandle<()>,
-        terminate_command_tx: oneshot::Sender<()>,
-    },
-}
-
-impl TelemetryLoopHandle {
-    /// Terminate telemetry will exit the telemetry loop
-    /// and possibly send the last request, possibly ignoring the
-    /// telemetry cooldown.
-    pub async fn terminate_telemetry(self) {
-        if let Self::WithLoop {
-            join_handle,
-            terminate_command_tx,
-        } = self
-        {
-            let _ = terminate_command_tx.send(());
-            let _ = tokio::time::timeout(LAST_REQUEST_TIMEOUT, join_handle).await;
-        }
-    }
-}
-
-impl TelemetrySender {
-    fn new<S: Sink>(sink_opt: Option<S>, clock: Clock) -> TelemetrySender {
-        let sink_opt: Option<Box<dyn Sink>> = if let Some(sink) = sink_opt {
-            Some(Box::new(sink))
-        } else {
-            None
-        };
-        TelemetrySender {
-            inner: Arc::new(Inner {
-                sink: sink_opt,
-                client_information: ClientInformation::default(),
-                events: Events::default(),
-                clock,
-                is_started: AtomicBool::new(false),
-            }),
-        }
-    }
-
-    pub fn start_loop(&self) -> TelemetryLoopHandle {
-        let (terminate_command_tx, mut terminate_command_rx) = oneshot::channel();
-        if self.inner.is_disabled() {
-            return TelemetryLoopHandle::NoLoop;
-        }
-
-        assert!(
-            self.inner
-                .is_started
-                .compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst)
-                .is_ok(),
-            "The telemetry loop is already started."
-        );
-
-        let inner = self.inner.clone();
-        let join_handle = tokio::task::spawn(async move {
-            // This channel is used to send the command to terminate telemetry.
-            loop {
-                let quit_loop = tokio::select! {
-                    _ = (&mut terminate_command_rx) => { true }
-                    _ = inner.clock.tick() => { false }
-                };
-                inner.send_pending_events().await;
-                if quit_loop {
-                    break;
-                }
-            }
-        });
-        TelemetryLoopHandle::WithLoop {
-            join_handle,
-            terminate_command_tx,
-        }
-    }
-
-    pub async fn send(&self, event: TelemetryEvent) {
-        self.inner.send(event).await;
-    }
-}
-
-/// Check to see if telemetry is enabled.
-pub fn is_telemetry_enabled() -> bool {
-    std::env::var_os(crate::DISABLE_TELEMETRY_ENV_KEY).is_none()
-}
-
-fn create_http_client() -> Option<HttpClient> {
-    // TODO add telemetry URL.
-    let client_opt = if is_telemetry_enabled() {
-        HttpClient::try_new()
-    } else {
-        None
-    };
-    if let Some(client) = client_opt.as_ref() {
-        info!("telemetry to {} is enabled.", client.endpoint());
-    } else {
-        info!("telemetry to nucliadb is disabled.");
-    }
-    client_opt
-}
-
-impl Default for TelemetrySender {
-    fn default() -> Self {
-        let http_client = create_http_client();
-        TelemetrySender::new(http_client, Clock::periodical(TELEMETRY_PUSH_COOLDOWN))
-    }
-}
-
-#[cfg(test)]
-mod tests {
-
-    use std::env;
-
-    use super::*;
-
-    #[ignore]
-    #[tokio::test]
-    async fn test_enabling_and_disabling_telemetry() {
-        // We group the two in a single test to ensure it happens on the same thread.
-        env::set_var(crate::DISABLE_TELEMETRY_ENV_KEY, "");
-        assert_eq!(TelemetrySender::default().inner.is_disabled(), true);
-        env::remove_var(crate::DISABLE_TELEMETRY_ENV_KEY);
-        assert_eq!(TelemetrySender::default().inner.is_disabled(), false);
-    }
-
-    #[tokio::test]
-    async fn test_telemetry_no_wait_for_first_event() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (_clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        let payload_opt = rx.recv().await;
-        assert!(payload_opt.is_some());
-        let payload = payload_opt.unwrap();
-        assert_eq!(payload.events.len(), 1);
-        loop_handler.terminate_telemetry().await;
-    }
-
-    #[tokio::test]
-    async fn test_telemetry_two_events() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 1);
-        }
-        clock_btn.tick().await;
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 1);
-        }
-        loop_handler.terminate_telemetry().await;
-    }
-
-    #[tokio::test]
-    async fn test_telemetry_cooldown_observed() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 1);
-        }
-        tokio::task::yield_now().await;
-        telemetry_sender.send(TelemetryEvent::Create).await;
-
-        let timeout_res = tokio::time::timeout(Duration::from_millis(1), rx.recv()).await;
-        assert!(timeout_res.is_err());
-
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        clock_btn.tick().await;
-        {
-            let payload = rx.recv().await.unwrap();
-            assert_eq!(payload.events.len(), 2);
-        }
-        loop_handler.terminate_telemetry().await;
-    }
-
-    #[tokio::test]
-    async fn test_terminate_telemetry_sends_pending_events() {
-        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
-        let (_clock_btn, clock) = Clock::manual().await;
-        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
-        let loop_handler = telemetry_sender.start_loop();
-        telemetry_sender.send(TelemetryEvent::Create).await;
-        let payload = rx.recv().await.unwrap();
-        assert_eq!(payload.events.len(), 1);
-        telemetry_sender
-            .send(TelemetryEvent::EndCommand { return_code: 2i32 })
-            .await;
-        loop_handler.terminate_telemetry().await;
-        let payload = rx.recv().await.unwrap();
-        assert_eq!(payload.events.len(), 1);
-        assert!(matches!(
-            &payload.events[0].r#type,
-            &TelemetryEvent::EndCommand { .. }
-        ));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::mem;
+use std::sync::atomic::{AtomicBool, Ordering};
+use std::sync::Arc;
+use std::time::Duration;
+
+use tokio::sync::mpsc::{Receiver, Sender};
+use tokio::sync::{oneshot, Mutex, RwLock};
+use tokio::task::JoinHandle;
+use tokio::time::Interval;
+use tracing::info;
+
+use crate::payload::{ClientInformation, EventWithTimestamp, TelemetryEvent, TelemetryPayload};
+use crate::sink::{HttpClient, Sink};
+
+/// At most 1 Request per minutes.
+const TELEMETRY_PUSH_COOLDOWN: Duration = Duration::from_secs(60);
+
+/// Upon termination of the program, we send one last telemetry request with pending events.
+/// This duration is the amount of time we wait for at most to send that last telemetry request.
+const LAST_REQUEST_TIMEOUT: Duration = Duration::from_secs(1);
+
+const MAX_NUM_EVENTS_IN_QUEUE: usize = 10;
+
+#[cfg(test)]
+struct ClockButton(Sender<()>);
+
+#[cfg(test)]
+impl ClockButton {
+    async fn tick(&self) {
+        let _ = self.0.send(()).await;
+    }
+}
+
+enum Clock {
+    Periodical(Mutex<Interval>),
+    #[cfg(test)]
+    Manual(Mutex<Receiver<()>>),
+}
+
+impl Clock {
+    pub fn periodical(period: Duration) -> Clock {
+        let interval = tokio::time::interval(period);
+        Clock::Periodical(Mutex::new(interval))
+    }
+
+    #[cfg(test)]
+    pub async fn manual() -> (ClockButton, Clock) {
+        let (tx, rx) = tokio::sync::mpsc::channel(1);
+        let _ = tx.send(()).await;
+        let button = ClockButton(tx);
+        (button, Clock::Manual(Mutex::new(rx)))
+    }
+
+    async fn tick(&self) {
+        match self {
+            Clock::Periodical(interval) => {
+                interval.lock().await.tick().await;
+            }
+            #[cfg(test)]
+            Clock::Manual(channel) => {
+                channel.lock().await.recv().await;
+            }
+        }
+    }
+}
+
+#[derive(Default)]
+struct EventsState {
+    events: Vec<EventWithTimestamp>,
+    num_dropped_events: usize,
+}
+
+impl EventsState {
+    fn drain_events(&mut self) -> EventsState {
+        mem::replace(
+            self,
+            EventsState {
+                events: Vec::new(),
+                num_dropped_events: 0,
+            },
+        )
+    }
+
+    /// Adds an event.
+    /// If the queue is already saturated, (ie. it has reached the len `MAX_NUM_EVENTS_IN_QUEUE`)
+    // Returns true iff it was the first event in the queue.
+    fn push_event(&mut self, event: TelemetryEvent) -> bool {
+        if self.events.len() >= MAX_NUM_EVENTS_IN_QUEUE {
+            self.num_dropped_events += 1;
+            return false;
+        }
+        let events_was_empty = self.events.is_empty();
+        self.events.push(EventWithTimestamp::from(event));
+        events_was_empty
+    }
+}
+
+struct Events {
+    state: RwLock<EventsState>,
+    items_available_tx: Sender<()>,
+    items_available_rx: RwLock<Receiver<()>>,
+}
+
+impl Default for Events {
+    fn default() -> Self {
+        let (items_available_tx, items_available_rx) = tokio::sync::mpsc::channel(1);
+        Events {
+            state: RwLock::new(EventsState::default()),
+            items_available_tx,
+            items_available_rx: RwLock::new(items_available_rx),
+        }
+    }
+}
+
+impl Events {
+    /// Wait for events to be available (if there are pending events, then do not wait)
+    /// and then send them to the PushAPI server.
+    async fn drain_events(&self) -> EventsState {
+        self.items_available_rx.write().await.recv().await;
+        self.state.write().await.drain_events()
+    }
+
+    async fn push_event(&self, event: TelemetryEvent) {
+        let is_first_event = self.state.write().await.push_event(event);
+        if is_first_event {
+            let _ = self.items_available_tx.send(()).await;
+        }
+    }
+}
+
+pub(crate) struct Inner {
+    sink: Option<Box<dyn Sink>>,
+    client_information: ClientInformation,
+    /// This channel is just used to signal there are new items available.
+    events: Events,
+    clock: Clock,
+    is_started: AtomicBool,
+}
+
+impl Inner {
+    pub fn is_disabled(&self) -> bool {
+        self.sink.is_none()
+    }
+
+    async fn create_telemetry_payload(&self) -> TelemetryPayload {
+        let events_state = self.events.drain_events().await;
+        TelemetryPayload {
+            client_information: self.client_information.clone(),
+            events: events_state.events,
+            num_dropped_events: events_state.num_dropped_events,
+        }
+    }
+
+    /// Wait for events to be available (if there are pending events, then do not wait)
+    /// and then send them to the PushAPI server.
+    ///
+    /// If the requests fails, it fails silently.
+    async fn send_pending_events(&self) {
+        if let Some(sink) = self.sink.as_ref() {
+            let payload = self.create_telemetry_payload().await;
+            sink.send_payload(payload).await;
+        }
+    }
+
+    async fn send(&self, event: TelemetryEvent) {
+        if self.is_disabled() {
+            return;
+        }
+        self.events.push_event(event).await;
+    }
+}
+
+pub struct TelemetrySender {
+    pub(crate) inner: Arc<Inner>,
+}
+
+pub enum TelemetryLoopHandle {
+    NoLoop,
+    WithLoop {
+        join_handle: JoinHandle<()>,
+        terminate_command_tx: oneshot::Sender<()>,
+    },
+}
+
+impl TelemetryLoopHandle {
+    /// Terminate telemetry will exit the telemetry loop
+    /// and possibly send the last request, possibly ignoring the
+    /// telemetry cooldown.
+    pub async fn terminate_telemetry(self) {
+        if let Self::WithLoop {
+            join_handle,
+            terminate_command_tx,
+        } = self
+        {
+            let _ = terminate_command_tx.send(());
+            let _ = tokio::time::timeout(LAST_REQUEST_TIMEOUT, join_handle).await;
+        }
+    }
+}
+
+impl TelemetrySender {
+    fn new<S: Sink>(sink_opt: Option<S>, clock: Clock) -> TelemetrySender {
+        let sink_opt: Option<Box<dyn Sink>> = if let Some(sink) = sink_opt {
+            Some(Box::new(sink))
+        } else {
+            None
+        };
+        TelemetrySender {
+            inner: Arc::new(Inner {
+                sink: sink_opt,
+                client_information: ClientInformation::default(),
+                events: Events::default(),
+                clock,
+                is_started: AtomicBool::new(false),
+            }),
+        }
+    }
+
+    pub fn start_loop(&self) -> TelemetryLoopHandle {
+        let (terminate_command_tx, mut terminate_command_rx) = oneshot::channel();
+        if self.inner.is_disabled() {
+            return TelemetryLoopHandle::NoLoop;
+        }
+
+        assert!(
+            self.inner
+                .is_started
+                .compare_exchange(false, true, Ordering::SeqCst, Ordering::SeqCst)
+                .is_ok(),
+            "The telemetry loop is already started."
+        );
+
+        let inner = self.inner.clone();
+        let join_handle = tokio::task::spawn(async move {
+            // This channel is used to send the command to terminate telemetry.
+            loop {
+                let quit_loop = tokio::select! {
+                    _ = (&mut terminate_command_rx) => { true }
+                    _ = inner.clock.tick() => { false }
+                };
+                inner.send_pending_events().await;
+                if quit_loop {
+                    break;
+                }
+            }
+        });
+        TelemetryLoopHandle::WithLoop {
+            join_handle,
+            terminate_command_tx,
+        }
+    }
+
+    pub async fn send(&self, event: TelemetryEvent) {
+        self.inner.send(event).await;
+    }
+}
+
+/// Check to see if telemetry is enabled.
+pub fn is_telemetry_enabled() -> bool {
+    std::env::var_os(crate::DISABLE_TELEMETRY_ENV_KEY).is_none()
+}
+
+fn create_http_client() -> Option<HttpClient> {
+    // TODO add telemetry URL.
+    let client_opt = if is_telemetry_enabled() {
+        HttpClient::try_new()
+    } else {
+        None
+    };
+    if let Some(client) = client_opt.as_ref() {
+        info!("telemetry to {} is enabled.", client.endpoint());
+    } else {
+        info!("telemetry to nucliadb is disabled.");
+    }
+    client_opt
+}
+
+impl Default for TelemetrySender {
+    fn default() -> Self {
+        let http_client = create_http_client();
+        TelemetrySender::new(http_client, Clock::periodical(TELEMETRY_PUSH_COOLDOWN))
+    }
+}
+
+#[cfg(test)]
+mod tests {
+
+    use std::env;
+
+    use super::*;
+
+    #[ignore]
+    #[tokio::test]
+    async fn test_enabling_and_disabling_telemetry() {
+        // We group the two in a single test to ensure it happens on the same thread.
+        env::set_var(crate::DISABLE_TELEMETRY_ENV_KEY, "");
+        assert_eq!(TelemetrySender::default().inner.is_disabled(), true);
+        env::remove_var(crate::DISABLE_TELEMETRY_ENV_KEY);
+        assert_eq!(TelemetrySender::default().inner.is_disabled(), false);
+    }
+
+    #[tokio::test]
+    async fn test_telemetry_no_wait_for_first_event() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (_clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        let payload_opt = rx.recv().await;
+        assert!(payload_opt.is_some());
+        let payload = payload_opt.unwrap();
+        assert_eq!(payload.events.len(), 1);
+        loop_handler.terminate_telemetry().await;
+    }
+
+    #[tokio::test]
+    async fn test_telemetry_two_events() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 1);
+        }
+        clock_btn.tick().await;
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 1);
+        }
+        loop_handler.terminate_telemetry().await;
+    }
+
+    #[tokio::test]
+    async fn test_telemetry_cooldown_observed() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 1);
+        }
+        tokio::task::yield_now().await;
+        telemetry_sender.send(TelemetryEvent::Create).await;
+
+        let timeout_res = tokio::time::timeout(Duration::from_millis(1), rx.recv()).await;
+        assert!(timeout_res.is_err());
+
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        clock_btn.tick().await;
+        {
+            let payload = rx.recv().await.unwrap();
+            assert_eq!(payload.events.len(), 2);
+        }
+        loop_handler.terminate_telemetry().await;
+    }
+
+    #[tokio::test]
+    async fn test_terminate_telemetry_sends_pending_events() {
+        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
+        let (_clock_btn, clock) = Clock::manual().await;
+        let telemetry_sender = TelemetrySender::new(Some(tx), clock);
+        let loop_handler = telemetry_sender.start_loop();
+        telemetry_sender.send(TelemetryEvent::Create).await;
+        let payload = rx.recv().await.unwrap();
+        assert_eq!(payload.events.len(), 1);
+        telemetry_sender
+            .send(TelemetryEvent::EndCommand { return_code: 2i32 })
+            .await;
+        loop_handler.terminate_telemetry().await;
+        let payload = rx.recv().await.unwrap();
+        assert_eq!(payload.events.len(), 1);
+        assert!(matches!(
+            &payload.events[0].r#type,
+            &TelemetryEvent::EndCommand { .. }
+        ));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_telemetry/src/sync.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_telemetry/src/sync.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use once_cell::sync::OnceCell;
-
-use crate::payload::TelemetryEvent;
-pub use crate::sender::is_telemetry_enabled;
-use crate::sender::{TelemetryLoopHandle, TelemetrySender};
-
-pub fn start_telemetry_loop() -> TelemetryLoopHandle {
-    get_telemetry_sender_singleton().start_loop()
-}
-
-fn get_telemetry_sender_singleton() -> &'static TelemetrySender {
-    static INSTANCE: OnceCell<TelemetrySender> = OnceCell::new();
-    INSTANCE.get_or_init(TelemetrySender::default)
-}
-
-/// Sends a telemetry event to Nuclia's server via HTTP.
-///
-/// Telemetry guarantees to send at most 1 request per minute.
-/// Each requests can ship at most 10 messages.
-///
-/// If this methods is called too often, some events will be dropped.
-///
-/// If the http requests fail, the error will be silent.
-///
-/// We voluntarily use an enum here to make it easier for reader
-/// to audit the type of information that is send home.
-pub async fn send_telemetry_event(event: TelemetryEvent) {
-    get_telemetry_sender_singleton().send(event).await
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use once_cell::sync::OnceCell;
+
+use crate::payload::TelemetryEvent;
+pub use crate::sender::is_telemetry_enabled;
+use crate::sender::{TelemetryLoopHandle, TelemetrySender};
+
+pub fn start_telemetry_loop() -> TelemetryLoopHandle {
+    get_telemetry_sender_singleton().start_loop()
+}
+
+fn get_telemetry_sender_singleton() -> &'static TelemetrySender {
+    static INSTANCE: OnceCell<TelemetrySender> = OnceCell::new();
+    INSTANCE.get_or_init(TelemetrySender::default)
+}
+
+/// Sends a telemetry event to Nuclia's server via HTTP.
+///
+/// Telemetry guarantees to send at most 1 request per minute.
+/// Each requests can ship at most 10 messages.
+///
+/// If this methods is called too often, some events will be dropped.
+///
+/// If the http requests fail, the error will be silent.
+///
+/// We voluntarily use an enum here to make it easier for reader
+/// to audit the type of information that is send home.
+pub async fn send_telemetry_event(event: TelemetryEvent) {
+    get_telemetry_sender_singleton().send(event).await
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/fs_state.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/fs_state.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,212 +1,212 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::fs::{File, OpenOptions};
-use std::io;
-use std::io::{BufReader, BufWriter, Write};
-use std::path::{Path, PathBuf};
-use std::time::SystemTime;
-
-use fs2::FileExt;
-use serde::de::DeserializeOwned;
-use serde::Serialize;
-use thiserror::Error;
-
-pub type FsResult<O> = std::result::Result<O, FsError>;
-
-#[derive(Debug, Error)]
-pub enum FsError {
-    #[error("Serialization error: {0}")]
-    ParsingError(#[from] bincode::Error),
-    #[error("IO error: {0}")]
-    IoError(#[from] std::io::Error),
-}
-
-mod names {
-    pub const LOCK: &str = "lk.lock";
-    pub const STATE: &str = "state.bincode";
-    pub const TEMP: &str = "temp_state.bincode";
-}
-
-#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
-pub struct Version(SystemTime);
-
-fn write_state<S>(path: &Path, state: &S) -> FsResult<()>
-where S: Serialize {
-    let temporal_path = path.join(names::TEMP);
-    let state_path = path.join(names::STATE);
-    let mut file = BufWriter::new(
-        OpenOptions::new()
-            .create(true)
-            .write(true)
-            .truncate(true)
-            .open(&temporal_path)?,
-    );
-    bincode::serialize_into(&mut file, state)?;
-    file.flush()?;
-    std::fs::rename(&temporal_path, state_path)?;
-    Ok(())
-}
-
-fn read_state<S>(path: &Path) -> FsResult<S>
-where S: DeserializeOwned {
-    let mut file = BufReader::new(
-        OpenOptions::new()
-            .read(true)
-            .open(path.join(names::STATE))?,
-    );
-    Ok(bincode::deserialize_from(&mut file)?)
-}
-
-pub fn initialize_disk<S, F>(path: &Path, with: F) -> FsResult<()>
-where
-    F: Fn() -> S,
-    S: Serialize,
-{
-    if !path.join(names::STATE).is_file() {
-        write_state(path, &with())?;
-    }
-    Ok(())
-}
-
-pub fn exclusive_lock(path: &Path) -> FsResult<ELock> {
-    Ok(ELock::new(path)?)
-}
-pub fn shared_lock(path: &Path) -> FsResult<SLock> {
-    Ok(SLock::new(path)?)
-}
-
-pub fn persist_state<S>(lock: &ELock, state: &S) -> FsResult<()>
-where S: Serialize {
-    write_state(lock.as_ref(), state)
-}
-
-pub fn load_state<S>(lock: &Lock) -> FsResult<S>
-where S: DeserializeOwned {
-    read_state(lock.as_ref())
-}
-pub fn crnt_version(lock: &Lock) -> FsResult<Version> {
-    let meta = std::fs::metadata(lock.path.join(names::STATE))?;
-    Ok(Version(meta.modified()?))
-}
-
-/// A Lock that may be exclusive or shared
-/// Useful when the code would work in either case.
-pub struct Lock {
-    path: PathBuf,
-    #[allow(unused)]
-    lock: File,
-}
-impl Lock {
-    fn open_lock(path: &Path) -> io::Result<File> {
-        let file = OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(path.join(names::LOCK))?;
-        Ok(file)
-    }
-    fn exclusive(path: &Path) -> io::Result<Lock> {
-        let path = path.to_path_buf();
-        let lock = Lock::open_lock(&path)?;
-        lock.lock_exclusive()?;
-        Ok(Lock { lock, path })
-    }
-    fn shared(path: &Path) -> io::Result<Lock> {
-        let path = path.to_path_buf();
-        let lock = Lock::open_lock(&path)?;
-        lock.lock_shared()?;
-        Ok(Lock { lock, path })
-    }
-}
-impl AsRef<Path> for Lock {
-    fn as_ref(&self) -> &Path {
-        &self.path
-    }
-}
-
-/// A exclusive lock
-pub struct ELock(Lock);
-impl ELock {
-    pub(super) fn new(path: &Path) -> io::Result<ELock> {
-        Lock::exclusive(path).map(ELock)
-    }
-}
-impl std::ops::Deref for ELock {
-    type Target = Lock;
-    fn deref(&self) -> &Self::Target {
-        &self.0
-    }
-}
-impl AsRef<Path> for ELock {
-    fn as_ref(&self) -> &Path {
-        self.0.as_ref()
-    }
-}
-
-/// A shared lock
-pub struct SLock(Lock);
-impl SLock {
-    pub fn new(path: &Path) -> io::Result<SLock> {
-        Lock::shared(path).map(SLock)
-    }
-}
-impl std::ops::Deref for SLock {
-    type Target = Lock;
-    fn deref(&self) -> &Self::Target {
-        &self.0
-    }
-}
-impl AsRef<Path> for SLock {
-    fn as_ref(&self) -> &Path {
-        self.0.as_ref()
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use tempfile::TempDir;
-
-    use super::*;
-
-    #[derive(Serialize, serde::Deserialize, Default)]
-    struct State {
-        n: usize,
-    }
-
-    #[test]
-    fn test() {
-        let dir = TempDir::new().unwrap();
-        initialize_disk(dir.path(), State::default).unwrap();
-        let lock = exclusive_lock(dir.path()).unwrap();
-        assert!(dir.path().join(names::STATE).is_file());
-        assert!(dir.path().join(names::LOCK).is_file());
-        let v0 = crnt_version(&lock).unwrap();
-        std::mem::drop(lock);
-        let lock = exclusive_lock(dir.path()).unwrap();
-        assert!(dir.path().join(names::STATE).is_file());
-        assert!(dir.path().join(names::LOCK).is_file());
-        assert_eq!(v0, crnt_version(&lock).unwrap());
-        std::thread::sleep(std::time::Duration::from_millis(100));
-        write_state(dir.path(), &State::default()).unwrap();
-        let new_version = crnt_version(&lock).unwrap();
-        assert!(v0 < new_version);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::fs::{File, OpenOptions};
+use std::io;
+use std::io::{BufReader, BufWriter, Write};
+use std::path::{Path, PathBuf};
+use std::time::SystemTime;
+
+use fs2::FileExt;
+use serde::de::DeserializeOwned;
+use serde::Serialize;
+use thiserror::Error;
+
+pub type FsResult<O> = std::result::Result<O, FsError>;
+
+#[derive(Debug, Error)]
+pub enum FsError {
+    #[error("Serialization error: {0}")]
+    ParsingError(#[from] bincode::Error),
+    #[error("IO error: {0}")]
+    IoError(#[from] std::io::Error),
+}
+
+mod names {
+    pub const LOCK: &str = "lk.lock";
+    pub const STATE: &str = "state.bincode";
+    pub const TEMP: &str = "temp_state.bincode";
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord)]
+pub struct Version(SystemTime);
+
+fn write_state<S>(path: &Path, state: &S) -> FsResult<()>
+where S: Serialize {
+    let temporal_path = path.join(names::TEMP);
+    let state_path = path.join(names::STATE);
+    let mut file = BufWriter::new(
+        OpenOptions::new()
+            .create(true)
+            .write(true)
+            .truncate(true)
+            .open(&temporal_path)?,
+    );
+    bincode::serialize_into(&mut file, state)?;
+    file.flush()?;
+    std::fs::rename(&temporal_path, state_path)?;
+    Ok(())
+}
+
+fn read_state<S>(path: &Path) -> FsResult<S>
+where S: DeserializeOwned {
+    let mut file = BufReader::new(
+        OpenOptions::new()
+            .read(true)
+            .open(path.join(names::STATE))?,
+    );
+    Ok(bincode::deserialize_from(&mut file)?)
+}
+
+pub fn initialize_disk<S, F>(path: &Path, with: F) -> FsResult<()>
+where
+    F: Fn() -> S,
+    S: Serialize,
+{
+    if !path.join(names::STATE).is_file() {
+        write_state(path, &with())?;
+    }
+    Ok(())
+}
+
+pub fn exclusive_lock(path: &Path) -> FsResult<ELock> {
+    Ok(ELock::new(path)?)
+}
+pub fn shared_lock(path: &Path) -> FsResult<SLock> {
+    Ok(SLock::new(path)?)
+}
+
+pub fn persist_state<S>(lock: &ELock, state: &S) -> FsResult<()>
+where S: Serialize {
+    write_state(lock.as_ref(), state)
+}
+
+pub fn load_state<S>(lock: &Lock) -> FsResult<S>
+where S: DeserializeOwned {
+    read_state(lock.as_ref())
+}
+pub fn crnt_version(lock: &Lock) -> FsResult<Version> {
+    let meta = std::fs::metadata(lock.path.join(names::STATE))?;
+    Ok(Version(meta.modified()?))
+}
+
+/// A Lock that may be exclusive or shared
+/// Useful when the code would work in either case.
+pub struct Lock {
+    path: PathBuf,
+    #[allow(unused)]
+    lock: File,
+}
+impl Lock {
+    fn open_lock(path: &Path) -> io::Result<File> {
+        let file = OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(path.join(names::LOCK))?;
+        Ok(file)
+    }
+    fn exclusive(path: &Path) -> io::Result<Lock> {
+        let path = path.to_path_buf();
+        let lock = Lock::open_lock(&path)?;
+        lock.lock_exclusive()?;
+        Ok(Lock { lock, path })
+    }
+    fn shared(path: &Path) -> io::Result<Lock> {
+        let path = path.to_path_buf();
+        let lock = Lock::open_lock(&path)?;
+        lock.lock_shared()?;
+        Ok(Lock { lock, path })
+    }
+}
+impl AsRef<Path> for Lock {
+    fn as_ref(&self) -> &Path {
+        &self.path
+    }
+}
+
+/// A exclusive lock
+pub struct ELock(Lock);
+impl ELock {
+    pub(super) fn new(path: &Path) -> io::Result<ELock> {
+        Lock::exclusive(path).map(ELock)
+    }
+}
+impl std::ops::Deref for ELock {
+    type Target = Lock;
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+impl AsRef<Path> for ELock {
+    fn as_ref(&self) -> &Path {
+        self.0.as_ref()
+    }
+}
+
+/// A shared lock
+pub struct SLock(Lock);
+impl SLock {
+    pub fn new(path: &Path) -> io::Result<SLock> {
+        Lock::shared(path).map(SLock)
+    }
+}
+impl std::ops::Deref for SLock {
+    type Target = Lock;
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+impl AsRef<Path> for SLock {
+    fn as_ref(&self) -> &Path {
+        self.0.as_ref()
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use tempfile::TempDir;
+
+    use super::*;
+
+    #[derive(Serialize, serde::Deserialize, Default)]
+    struct State {
+        n: usize,
+    }
+
+    #[test]
+    fn test() {
+        let dir = TempDir::new().unwrap();
+        initialize_disk(dir.path(), State::default).unwrap();
+        let lock = exclusive_lock(dir.path()).unwrap();
+        assert!(dir.path().join(names::STATE).is_file());
+        assert!(dir.path().join(names::LOCK).is_file());
+        let v0 = crnt_version(&lock).unwrap();
+        std::mem::drop(lock);
+        let lock = exclusive_lock(dir.path()).unwrap();
+        assert!(dir.path().join(names::STATE).is_file());
+        assert!(dir.path().join(names::LOCK).is_file());
+        assert_eq!(v0, crnt_version(&lock).unwrap());
+        std::thread::sleep(std::time::Duration::from_millis(100));
+        write_state(dir.path(), &State::default()).unwrap();
+        let new_version = crnt_version(&lock).unwrap();
+        assert!(v0 < new_version);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/paragraphs.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/paragraphs.rs`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use nucliadb_protos::*;
-
-use crate::prelude::*;
-
-pub type ParagraphsReaderPointer = Arc<dyn ParagraphReader>;
-pub type ParagraphsWriterPointer = Arc<RwLock<dyn ParagraphWriter>>;
-
-pub struct ParagraphConfig {
-    pub path: PathBuf,
-}
-
-pub struct ParagraphIterator(Box<dyn Iterator<Item = ParagraphItem> + Send>);
-impl ParagraphIterator {
-    pub fn new<I>(inner: I) -> ParagraphIterator
-    where I: Iterator<Item = ParagraphItem> + Send + 'static {
-        ParagraphIterator(Box::new(inner))
-    }
-}
-impl Iterator for ParagraphIterator {
-    type Item = ParagraphItem;
-    fn next(&mut self) -> Option<Self::Item> {
-        self.0.next()
-    }
-}
-
-pub trait ParagraphReader:
-    ReaderChild<Request = ParagraphSearchRequest, Response = ParagraphSearchResponse>
-{
-    fn iterator(&self, request: &StreamRequest) -> NodeResult<ParagraphIterator>;
-    fn suggest(&self, request: &SuggestRequest) -> NodeResult<Self::Response>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait ParagraphWriter: WriterChild {}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use nucliadb_protos::*;
+
+use crate::prelude::*;
+
+pub type ParagraphsReaderPointer = Arc<dyn ParagraphReader>;
+pub type ParagraphsWriterPointer = Arc<RwLock<dyn ParagraphWriter>>;
+
+pub struct ParagraphConfig {
+    pub path: PathBuf,
+}
+
+pub struct ParagraphIterator(Box<dyn Iterator<Item = ParagraphItem> + Send>);
+impl ParagraphIterator {
+    pub fn new<I>(inner: I) -> ParagraphIterator
+    where I: Iterator<Item = ParagraphItem> + Send + 'static {
+        ParagraphIterator(Box::new(inner))
+    }
+}
+impl Iterator for ParagraphIterator {
+    type Item = ParagraphItem;
+    fn next(&mut self) -> Option<Self::Item> {
+        self.0.next()
+    }
+}
+
+pub trait ParagraphReader:
+    ReaderChild<Request = ParagraphSearchRequest, Response = ParagraphSearchResponse>
+{
+    fn iterator(&self, request: &StreamRequest) -> NodeResult<ParagraphIterator>;
+    fn suggest(&self, request: &SuggestRequest) -> NodeResult<Self::Response>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait ParagraphWriter: WriterChild {}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/relations.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/relations.rs`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use nucliadb_protos::*;
-
-use crate::prelude::*;
-
-pub type RelationsReaderPointer = Arc<dyn RelationReader>;
-pub type RelationsWriterPointer = Arc<RwLock<dyn RelationWriter>>;
-
-#[derive(Clone)]
-pub struct RelationConfig {
-    pub path: PathBuf,
-}
-
-pub trait RelationReader:
-    ReaderChild<Request = RelationSearchRequest, Response = RelationSearchResponse>
-{
-    fn get_edges(&self) -> NodeResult<EdgeList>;
-    fn get_node_types(&self) -> NodeResult<TypeList>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait RelationWriter: WriterChild {
-    fn join_graph(&mut self, graph: &JoinGraph) -> NodeResult<()>;
-    fn delete_nodes(&mut self, graph: &DeleteGraphNodes) -> NodeResult<()>;
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use nucliadb_protos::*;
+
+use crate::prelude::*;
+
+pub type RelationsReaderPointer = Arc<dyn RelationReader>;
+pub type RelationsWriterPointer = Arc<RwLock<dyn RelationWriter>>;
+
+#[derive(Clone)]
+pub struct RelationConfig {
+    pub path: PathBuf,
+}
+
+pub trait RelationReader:
+    ReaderChild<Request = RelationSearchRequest, Response = RelationSearchResponse>
+{
+    fn get_edges(&self) -> NodeResult<EdgeList>;
+    fn get_node_types(&self) -> NodeResult<TypeList>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait RelationWriter: WriterChild {
+    fn join_graph(&mut self, graph: &JoinGraph) -> NodeResult<()>;
+    fn delete_nodes(&mut self, graph: &DeleteGraphNodes) -> NodeResult<()>;
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/texts.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/texts.rs`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use nucliadb_protos::*;
-
-use crate::prelude::*;
-
-pub type TextsReaderPointer = Arc<dyn FieldReader>;
-pub type TextsWriterPointer = Arc<RwLock<dyn FieldWriter>>;
-pub struct TextConfig {
-    pub path: PathBuf,
-}
-
-pub struct DocumentIterator(Box<dyn Iterator<Item = DocumentItem> + Send>);
-impl DocumentIterator {
-    pub fn new<I>(inner: I) -> DocumentIterator
-    where I: Iterator<Item = DocumentItem> + Send + 'static {
-        DocumentIterator(Box::new(inner))
-    }
-}
-impl Iterator for DocumentIterator {
-    type Item = DocumentItem;
-    fn next(&mut self) -> Option<Self::Item> {
-        self.0.next()
-    }
-}
-
-pub trait FieldReader:
-    ReaderChild<Request = DocumentSearchRequest, Response = DocumentSearchResponse>
-{
-    fn iterator(&self, request: &StreamRequest) -> NodeResult<DocumentIterator>;
-    fn count(&self) -> NodeResult<usize>;
-}
-
-pub trait FieldWriter: WriterChild {}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use nucliadb_protos::*;
+
+use crate::prelude::*;
+
+pub type TextsReaderPointer = Arc<dyn FieldReader>;
+pub type TextsWriterPointer = Arc<RwLock<dyn FieldWriter>>;
+pub struct TextConfig {
+    pub path: PathBuf,
+}
+
+pub struct DocumentIterator(Box<dyn Iterator<Item = DocumentItem> + Send>);
+impl DocumentIterator {
+    pub fn new<I>(inner: I) -> DocumentIterator
+    where I: Iterator<Item = DocumentItem> + Send + 'static {
+        DocumentIterator(Box::new(inner))
+    }
+}
+impl Iterator for DocumentIterator {
+    type Item = DocumentItem;
+    fn next(&mut self) -> Option<Self::Item> {
+        self.0.next()
+    }
+}
+
+pub trait FieldReader:
+    ReaderChild<Request = DocumentSearchRequest, Response = DocumentSearchResponse>
+{
+    fn iterator(&self, request: &StreamRequest) -> NodeResult<DocumentIterator>;
+    fn count(&self) -> NodeResult<usize>;
+}
+
+pub trait FieldWriter: WriterChild {}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_core/src/vectors.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_core/src/vectors.rs`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-use std::path::PathBuf;
-use std::sync::{Arc, RwLock};
-
-use nucliadb_protos::*;
-
-use crate::prelude::*;
-
-pub type VectorsReaderPointer = Arc<dyn VectorReader>;
-pub type VectorsWriterPointer = Arc<RwLock<dyn VectorWriter>>;
-
-#[derive(Clone)]
-pub struct VectorConfig {
-    pub similarity: Option<VectorSimilarity>,
-    pub no_results: Option<usize>,
-    pub path: PathBuf,
-    pub vectorset: PathBuf,
-}
-
-pub trait VectorReader:
-    ReaderChild<Request = VectorSearchRequest, Response = VectorSearchResponse>
-{
-    fn count(&self, vectorset: &str) -> NodeResult<usize>;
-}
-
-pub trait VectorWriter: WriterChild {
-    fn list_vectorsets(&self) -> NodeResult<Vec<String>>;
-    fn remove_vectorset(&mut self, setid: &VectorSetId) -> NodeResult<()>;
-    fn add_vectorset(
-        &mut self,
-        setid: &VectorSetId,
-        similarity: VectorSimilarity,
-    ) -> NodeResult<()>;
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+use std::path::PathBuf;
+use std::sync::{Arc, RwLock};
+
+use nucliadb_protos::*;
+
+use crate::prelude::*;
+
+pub type VectorsReaderPointer = Arc<dyn VectorReader>;
+pub type VectorsWriterPointer = Arc<RwLock<dyn VectorWriter>>;
+
+#[derive(Clone)]
+pub struct VectorConfig {
+    pub similarity: Option<VectorSimilarity>,
+    pub no_results: Option<usize>,
+    pub path: PathBuf,
+    pub vectorset: PathBuf,
+}
+
+pub trait VectorReader:
+    ReaderChild<Request = VectorSearchRequest, Response = VectorSearchResponse>
+{
+    fn count(&self, vectorset: &str) -> NodeResult<usize>;
+}
+
+pub trait VectorWriter: WriterChild {
+    fn list_vectorsets(&self) -> NodeResult<Vec<String>>;
+    fn remove_vectorset(&mut self, setid: &VectorSetId) -> NodeResult<()>;
+    fn add_vectorset(
+        &mut self,
+        setid: &VectorSetId,
+        similarity: VectorSimilarity,
+    ) -> NodeResult<()>;
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/Cargo.toml` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/Cargo.toml`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-[package]
-name = "nucliadb_cluster"
-version = "0.1.0"
-authors = ['Bosutech S.L.']
-edition = "2021"
-license = "AGPL-3.0-or-later"
-description = "nucliadb cluster membership"
-repository = "https://github.com/stashify/nucliadb"
-homepage = "https://nuclia.com/"
-documentation = "https://nuclia.com/"
-
-[[bin]]
-name = "cluster_manager"
-path = "src/bin/manager.rs"
-
-[dependencies]
-anyhow = "1.0"
-async-trait = "0.1"
-serde = { version = "1.0", features = ["derive"] }
-serde_json = "1.0.81"
-thiserror = "1.0"
-tokio = { version = "1.7", features = ["full"] }
-tokio-stream = { version = "0.1.6", features = ["sync"] }
-tracing = "0.1"
-uuid = { version = "1.1", features = ["v4"] }
-log = "0.4.14"
-env_logger = "0.9.0"
-chitchat = "0.5.0"
-dockertest = "0.3.0"
-bytes = "1.1.0"
-crc32fast = "1.3.2"
-rand = "0.8.5"
-strum = { version = "0.24.1", features = ["derive"] }
-clap = { version = "4.0.29", features = ["derive", "env"] }
-parse_duration = "2.1.1"
-derive_builder = "0.12.0"
-derive_more = { version = "0.99.17", default-features = false, features = ["display", "deref", "deref_mut"] }
-futures = "0.3.25"
-
-[dev-dependencies]
-serde_test = "1.0.150"
-serial_test = "0.10.0"
-
-[[test]]
-name = "integration"
-path = "tests/integration.rs"
+[package]
+name = "nucliadb_cluster"
+version = "0.1.0"
+authors = ['Bosutech S.L.']
+edition = "2021"
+license = "AGPL-3.0-or-later"
+description = "nucliadb cluster membership"
+repository = "https://github.com/stashify/nucliadb"
+homepage = "https://nuclia.com/"
+documentation = "https://nuclia.com/"
+
+[[bin]]
+name = "cluster_manager"
+path = "src/bin/manager.rs"
+
+[dependencies]
+anyhow = "1.0"
+async-trait = "0.1"
+serde = { version = "1.0", features = ["derive"] }
+serde_json = "1.0.81"
+thiserror = "1.0"
+tokio = { version = "1.7", features = ["full"] }
+tokio-stream = { version = "0.1.6", features = ["sync"] }
+tracing = "0.1"
+uuid = { version = "1.1", features = ["v4"] }
+log = "0.4.14"
+env_logger = "0.9.0"
+chitchat = "0.5.0"
+dockertest = "0.3.0"
+bytes = "1.1.0"
+crc32fast = "1.3.2"
+rand = "0.8.5"
+strum = { version = "0.24.1", features = ["derive"] }
+clap = { version = "4.0.29", features = ["derive", "env"] }
+parse_duration = "2.1.1"
+derive_builder = "0.12.0"
+derive_more = { version = "0.99.17", default-features = false, features = ["display", "deref", "deref_mut"] }
+futures = "0.3.25"
+
+[dev-dependencies]
+serde_test = "1.0.150"
+serial_test = "0.10.0"
+
+[[test]]
+name = "integration"
+path = "tests/integration.rs"
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/src/node.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/src/node.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,482 +1,482 @@
-use std::collections::HashMap;
-use std::fmt;
-use std::net::SocketAddr;
-use std::sync::Arc;
-use std::time::Duration;
-
-use anyhow::anyhow;
-use chitchat::transport::UdpTransport;
-use chitchat::{ChitchatConfig, ChitchatHandle, FailureDetectorConfig, NodeId};
-use derive_builder::Builder;
-use futures::{stream, Stream, StreamExt};
-use serde::{Deserialize, Serialize};
-use strum::{Display as EnumDisplay, EnumString};
-use uuid::Uuid;
-
-use crate::error::Error;
-use crate::key::Key;
-use crate::register::Register;
-
-const NODE_KIND_KEY: Key<NodeType> = Key::new("## nucliadb reserved ## node_type");
-const REGISTER_KEY: Key<Register> = Key::new("## nucliadb reserved ## register");
-
-/// Retrieve the information about a given list of cluster nodes.
-pub async fn cluster_snapshot(nodes: Vec<NodeHandle>) -> Vec<NodeSnapshot> {
-    stream::iter(nodes)
-        .then(|node| async move { node.snapshot().await })
-        .collect()
-        .await
-}
-
-#[non_exhaustive]
-#[derive(
-    Debug, Copy, Clone, PartialEq, Eq, Hash, EnumString, EnumDisplay, Serialize, Deserialize,
-)]
-pub enum NodeType {
-    Io,
-    Search,
-    Ingest,
-    Train,
-    Unknown,
-}
-
-/// Represents a single node instance in a distributed cluster based on scuttlebut protocol.
-///
-/// # Examples
-/// ```rust,no_run
-/// # #[tokio::main]
-/// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
-/// use nucliadb_cluster::{Node, NodeType};
-///
-/// let node = Node::builder()
-///     .register_as(NodeType::Ingest)
-///     .on_local_network("0.0.0.0:4000".parse()?)
-///     .build()?;
-///
-/// // Holds that handle to keep the node alive.
-/// let node = node.start().await?;
-///
-/// # Ok(())
-/// # }
-/// ```
-#[derive(Debug, Builder)]
-#[builder(pattern = "owned", setter(prefix = "with", strip_option, into))]
-pub struct Node {
-    /// The node identifier.
-    #[builder(default = "Uuid::new_v4().to_string()")]
-    id: String,
-    /// The type of the node.
-    #[builder(setter(name = "register_as"))]
-    r#type: NodeType,
-    /// The cluster identifier to join.
-    #[builder(
-        setter(name = "join_cluster"),
-        default = "Node::DEFAULT_CLUSTER_ID.to_string()"
-    )]
-    cluster_id: String,
-    /// A list of known nodes to connect with in order to join the cluster.
-    #[builder(default)]
-    seed_nodes: Vec<String>,
-    /// The address the node will listen to.
-    /// Note that address can be the same as the `public_address` if the node
-    /// joins a local cluster.
-    #[builder(setter(custom))]
-    listen_address: SocketAddr,
-    /// The public address to communicate with the node.
-    #[builder(setter(custom))]
-    public_address: SocketAddr,
-    /// The interval between each node update.
-    /// An update consists to send/receive the node state to/from other cluster nodes.
-    #[builder(default = "Node::DEFAULT_UPDATE_INTERVAL")]
-    update_interval: Duration,
-    /// The initial node state.
-    #[builder(setter(custom), default)]
-    initial_state: Vec<(String, String)>,
-}
-
-impl NodeBuilder {
-    /// Configures the node to join a local cluster network.
-    pub fn on_local_network(mut self, address: SocketAddr) -> Self {
-        self.listen_address = Some(address);
-        self.public_address = Some(address);
-
-        self
-    }
-
-    /// Configures the node to join a public cluster network.
-    pub fn on_public_network(
-        mut self,
-        public_address: SocketAddr,
-        listen_address: SocketAddr,
-    ) -> Self {
-        self.listen_address = Some(listen_address);
-        self.public_address = Some(public_address);
-
-        self
-    }
-
-    /// Inserts the given key/value to the initial node state.
-    ///
-    /// Note that, in case of key duplication, only the last one will be kept in the state.
-    pub fn insert_to_initial_state<T: ToString>(mut self, key: Key<T>, value: T) -> Self {
-        self.initial_state
-            .get_or_insert_with(Vec::default)
-            .push((key.to_string(), value.to_string()));
-
-        self
-    }
-}
-
-impl Node {
-    const DEFAULT_CLUSTER_ID: &str = "nucliadb-cluster";
-    const DEFAULT_UPDATE_INTERVAL: Duration = Duration::from_millis(100);
-
-    /// Returns a builder to create a custom `Node`.
-    pub fn builder() -> NodeBuilder {
-        NodeBuilder::default()
-    }
-
-    /// Starts the node and join the cluster.
-    ///
-    /// The handle returning by this method must be kept in order to keep alive the node
-    /// and retrieve information about the cluster.
-    ///
-    /// # Errors
-    /// This method may fails in the node does not succeed to join the cluster.
-    pub async fn start(mut self) -> Result<NodeHandle, Error> {
-        let node_id = NodeId::new(self.id, self.public_address);
-
-        let configuration = ChitchatConfig {
-            node_id: node_id.clone(),
-            cluster_id: self.cluster_id,
-            gossip_interval: self.update_interval,
-            listen_addr: self.listen_address,
-            seed_nodes: self.seed_nodes,
-            failure_detector_config: FailureDetectorConfig::default(),
-            is_ready_predicate: None,
-        };
-
-        let register = Register::with_keys(
-            self.initial_state
-                .iter()
-                .cloned()
-                .map(|(key, _)| key)
-                .collect(),
-        );
-
-        self.initial_state
-            .push((REGISTER_KEY.to_string(), register.to_string()));
-        self.initial_state
-            .push((NODE_KIND_KEY.to_string(), self.r#type.to_string()));
-
-        let handle = chitchat::spawn_chitchat(configuration, self.initial_state, &UdpTransport)
-            .await
-            .map_err(Error::Start)?;
-
-        Ok(NodeHandle {
-            node_id,
-            handle: Arc::new(handle),
-        })
-    }
-}
-
-/// A handle of the started node.
-///
-/// This type permits to watch the cluster joined by the node and retrieve
-/// information about the cluster nodes as well.
-#[must_use]
-#[derive(Clone)]
-pub struct NodeHandle {
-    node_id: NodeId,
-    handle: Arc<ChitchatHandle>,
-}
-
-impl fmt::Debug for NodeHandle {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        f.debug_struct("NodeHandle")
-            .field("node_id", &self.node_id)
-            .finish()
-    }
-}
-
-impl NodeHandle {
-    /// Returns the node identifier.
-    pub fn id(&self) -> &str {
-        &self.node_id.id
-    }
-
-    /// Update the node state with the given key/value.
-    ///
-    /// Note that the new node state will be propagated only on the next cluster update.
-    pub async fn update_state<T: ToString>(&self, key: Key<T>, value: T) {
-        let chitchat = self.handle.chitchat();
-        let mut chitchat = chitchat.lock().await;
-
-        let state = chitchat.self_node_state();
-        let mut register: Register = state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
-
-        register.insert(key.to_string());
-
-        state.set(REGISTER_KEY, register);
-        state.set(key, value);
-    }
-
-    /// Shuts the node down by notifying and quitting the cluster.
-    pub async fn shutdown(self) -> Result<(), Error> {
-        let handle =
-            Arc::try_unwrap(self.handle).map_err(|_| Error::Shutdown(anyhow!("node is in use")))?;
-
-        handle.shutdown().await.map_err(Error::Shutdown)
-    }
-
-    /// Returns a stream over any changes in the cluster.
-    pub async fn cluster_watcher(&self) -> impl Stream<Item = Vec<NodeHandle>> + 'static {
-        let handle = Arc::clone(&self.handle);
-        let node_id = self.node_id.clone();
-
-        self.handle
-            .chitchat()
-            .lock()
-            .await
-            .ready_nodes_watcher()
-            .map(move |nodes| {
-                nodes
-                    .into_iter()
-                    // `ready_nodes_watcher` does not contains this node so we add it
-                    // to have the complete list of live nodes.
-                    .chain([node_id.clone()])
-                    .map(|node_id| NodeHandle {
-                        node_id,
-                        handle: Arc::clone(&handle),
-                    })
-                    .collect()
-            })
-    }
-
-    /// Returns the current list of live nodes in the cluster.
-    pub async fn live_nodes(&self) -> Vec<NodeHandle> {
-        self.handle
-            .chitchat()
-            .lock()
-            .await
-            .live_nodes()
-            .cloned()
-            // `live_nodes` does not contains this node so we add it
-            // to have the complete list of live nodes.
-            .chain([self.node_id.clone()])
-            .map(|node_id| NodeHandle {
-                node_id,
-                handle: Arc::clone(&self.handle),
-            })
-            .collect()
-    }
-
-    /// Returns a node snapshot.
-    pub async fn snapshot(&self) -> NodeSnapshot {
-        self.handle
-            .with_chitchat(|chitchat| {
-                let state = chitchat.node_state(&self.node_id).unwrap();
-                let register: Register =
-                    state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
-
-                let r#type = state
-                    .get(NODE_KIND_KEY.as_str())
-                    .and_then(|s| s.parse().ok())
-                    .unwrap_or(NodeType::Unknown);
-
-                let state = register
-                    .iter()
-                    .filter_map(|key| {
-                        state
-                            .get(key)
-                            .map(|value| (key.to_string(), value.to_string()))
-                    })
-                    .collect();
-
-                NodeSnapshot {
-                    id: self.node_id.id.to_string(),
-                    address: self.node_id.gossip_public_address,
-                    is_self: chitchat.self_node_id() == &self.node_id,
-                    r#type,
-                    state,
-                }
-            })
-            .await
-    }
-}
-
-/// A snapshot of a cluster node with its state and meta information.
-#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
-pub struct NodeSnapshot {
-    id: String,
-    r#type: NodeType,
-    address: SocketAddr,
-    is_self: bool,
-    #[serde(flatten)]
-    state: HashMap<String, String>,
-}
-
-#[cfg(test)]
-mod tests {
-    use serde_test::{assert_tokens, Token};
-
-    use super::*;
-
-    #[test]
-    fn it_ser_de_node_type() {
-        let data = [
-            (NodeType::Io, "Io"),
-            (NodeType::Search, "Search"),
-            (NodeType::Ingest, "Ingest"),
-            (NodeType::Train, "Train"),
-            (NodeType::Unknown, "Unknown"),
-        ];
-
-        for (node, variant) in data {
-            assert_tokens(
-                &node,
-                &[Token::UnitVariant {
-                    name: "NodeType",
-                    variant,
-                }],
-            );
-        }
-    }
-
-    #[tokio::test]
-    #[serial_test::serial("cluster")]
-    async fn it_registers_user_keys_in_state() -> Result<(), Box<dyn std::error::Error>> {
-        const A_KEY: Key<u32> = Key::new("a-key");
-        const B_KEY: Key<f32> = Key::new("b-key");
-        const C_KEY: Key<char> = Key::new("c-key");
-
-        let node = Node::builder()
-            .on_local_network("0.0.0.0:8080".parse()?)
-            .register_as(NodeType::Io)
-            .insert_to_initial_state(A_KEY, 42)
-            .insert_to_initial_state(B_KEY, 0.0)
-            .build()?;
-
-        let node = node.start().await?;
-
-        let retrieve_registered_keys = || async {
-            node.handle
-                .chitchat()
-                .lock()
-                .await
-                .self_node_state()
-                .get(REGISTER_KEY.as_str())
-                .map(Register::from)
-        };
-
-        assert_eq!(
-            retrieve_registered_keys().await,
-            Some(Register::with_keys(
-                ["a-key".to_string(), "b-key".to_string()]
-                    .into_iter()
-                    .collect()
-            ))
-        );
-
-        node.update_state(C_KEY, 'a').await;
-
-        assert_eq!(
-            retrieve_registered_keys().await,
-            Some(Register::with_keys(
-                [
-                    "a-key".to_string(),
-                    "b-key".to_string(),
-                    "c-key".to_string()
-                ]
-                .into_iter()
-                .collect()
-            ))
-        );
-
-        Ok(())
-    }
-
-    #[tokio::test]
-    #[serial_test::serial("cluster")]
-    async fn it_builds_node_snapshot() -> Result<(), Box<dyn std::error::Error>> {
-        const LOAD_SCORE_KEY: Key<f32> = Key::new("load-score");
-
-        let node = Node::builder()
-            .on_local_network("0.0.0.0:8080".parse()?)
-            .register_as(NodeType::Io)
-            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
-            .build()?;
-
-        let node = node.start().await?;
-        let snapshot = node.snapshot().await;
-
-        assert_eq!(
-            snapshot,
-            NodeSnapshot {
-                id: node.node_id.id,
-                r#type: NodeType::Io,
-                address: node.node_id.gossip_public_address,
-                is_self: true,
-                state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
-                    .into_iter()
-                    .collect(),
-            }
-        );
-
-        Ok(())
-    }
-
-    #[tokio::test]
-    #[serial_test::serial("cluster")]
-    async fn it_builds_cluster_snapshot() -> Result<(), Box<dyn std::error::Error>> {
-        const LOAD_SCORE_KEY: Key<f32> = Key::new("load_score");
-
-        let first_node = Node::builder()
-            .on_local_network("0.0.0.0:8080".parse()?)
-            .register_as(NodeType::Io)
-            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
-            .build()?;
-
-        let first_node = first_node.start().await?;
-
-        let second_node = Node::builder()
-            .on_local_network("0.0.0.0:8081".parse()?)
-            .register_as(NodeType::Ingest)
-            .insert_to_initial_state(LOAD_SCORE_KEY, 40.0)
-            .with_seed_nodes(vec!["0.0.0.0:8080".to_string()])
-            .build()?;
-
-        let second_node = second_node.start().await?;
-
-        tokio::time::sleep(Node::DEFAULT_UPDATE_INTERVAL * 2).await;
-
-        let live_nodes = first_node.live_nodes().await;
-        let snapshots = cluster_snapshot(live_nodes).await;
-
-        assert_eq!(
-            snapshots,
-            vec![
-                NodeSnapshot {
-                    id: second_node.node_id.id,
-                    r#type: NodeType::Ingest,
-                    address: second_node.node_id.gossip_public_address,
-                    is_self: false,
-                    state: [(LOAD_SCORE_KEY.to_string(), 40.to_string())]
-                        .into_iter()
-                        .collect(),
-                },
-                NodeSnapshot {
-                    id: first_node.node_id.id,
-                    r#type: NodeType::Io,
-                    address: first_node.node_id.gossip_public_address,
-                    is_self: true,
-                    state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
-                        .into_iter()
-                        .collect(),
-                },
-            ]
-        );
-
-        Ok(())
-    }
-}
+use std::collections::HashMap;
+use std::fmt;
+use std::net::SocketAddr;
+use std::sync::Arc;
+use std::time::Duration;
+
+use anyhow::anyhow;
+use chitchat::transport::UdpTransport;
+use chitchat::{ChitchatConfig, ChitchatHandle, FailureDetectorConfig, NodeId};
+use derive_builder::Builder;
+use futures::{stream, Stream, StreamExt};
+use serde::{Deserialize, Serialize};
+use strum::{Display as EnumDisplay, EnumString};
+use uuid::Uuid;
+
+use crate::error::Error;
+use crate::key::Key;
+use crate::register::Register;
+
+const NODE_KIND_KEY: Key<NodeType> = Key::new("## nucliadb reserved ## node_type");
+const REGISTER_KEY: Key<Register> = Key::new("## nucliadb reserved ## register");
+
+/// Retrieve the information about a given list of cluster nodes.
+pub async fn cluster_snapshot(nodes: Vec<NodeHandle>) -> Vec<NodeSnapshot> {
+    stream::iter(nodes)
+        .then(|node| async move { node.snapshot().await })
+        .collect()
+        .await
+}
+
+#[non_exhaustive]
+#[derive(
+    Debug, Copy, Clone, PartialEq, Eq, Hash, EnumString, EnumDisplay, Serialize, Deserialize,
+)]
+pub enum NodeType {
+    Io,
+    Search,
+    Ingest,
+    Train,
+    Unknown,
+}
+
+/// Represents a single node instance in a distributed cluster based on scuttlebut protocol.
+///
+/// # Examples
+/// ```rust,no_run
+/// # #[tokio::main]
+/// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
+/// use nucliadb_cluster::{Node, NodeType};
+///
+/// let node = Node::builder()
+///     .register_as(NodeType::Ingest)
+///     .on_local_network("0.0.0.0:4000".parse()?)
+///     .build()?;
+///
+/// // Holds that handle to keep the node alive.
+/// let node = node.start().await?;
+///
+/// # Ok(())
+/// # }
+/// ```
+#[derive(Debug, Builder)]
+#[builder(pattern = "owned", setter(prefix = "with", strip_option, into))]
+pub struct Node {
+    /// The node identifier.
+    #[builder(default = "Uuid::new_v4().to_string()")]
+    id: String,
+    /// The type of the node.
+    #[builder(setter(name = "register_as"))]
+    r#type: NodeType,
+    /// The cluster identifier to join.
+    #[builder(
+        setter(name = "join_cluster"),
+        default = "Node::DEFAULT_CLUSTER_ID.to_string()"
+    )]
+    cluster_id: String,
+    /// A list of known nodes to connect with in order to join the cluster.
+    #[builder(default)]
+    seed_nodes: Vec<String>,
+    /// The address the node will listen to.
+    /// Note that address can be the same as the `public_address` if the node
+    /// joins a local cluster.
+    #[builder(setter(custom))]
+    listen_address: SocketAddr,
+    /// The public address to communicate with the node.
+    #[builder(setter(custom))]
+    public_address: SocketAddr,
+    /// The interval between each node update.
+    /// An update consists to send/receive the node state to/from other cluster nodes.
+    #[builder(default = "Node::DEFAULT_UPDATE_INTERVAL")]
+    update_interval: Duration,
+    /// The initial node state.
+    #[builder(setter(custom), default)]
+    initial_state: Vec<(String, String)>,
+}
+
+impl NodeBuilder {
+    /// Configures the node to join a local cluster network.
+    pub fn on_local_network(mut self, address: SocketAddr) -> Self {
+        self.listen_address = Some(address);
+        self.public_address = Some(address);
+
+        self
+    }
+
+    /// Configures the node to join a public cluster network.
+    pub fn on_public_network(
+        mut self,
+        public_address: SocketAddr,
+        listen_address: SocketAddr,
+    ) -> Self {
+        self.listen_address = Some(listen_address);
+        self.public_address = Some(public_address);
+
+        self
+    }
+
+    /// Inserts the given key/value to the initial node state.
+    ///
+    /// Note that, in case of key duplication, only the last one will be kept in the state.
+    pub fn insert_to_initial_state<T: ToString>(mut self, key: Key<T>, value: T) -> Self {
+        self.initial_state
+            .get_or_insert_with(Vec::default)
+            .push((key.to_string(), value.to_string()));
+
+        self
+    }
+}
+
+impl Node {
+    const DEFAULT_CLUSTER_ID: &str = "nucliadb-cluster";
+    const DEFAULT_UPDATE_INTERVAL: Duration = Duration::from_millis(100);
+
+    /// Returns a builder to create a custom `Node`.
+    pub fn builder() -> NodeBuilder {
+        NodeBuilder::default()
+    }
+
+    /// Starts the node and join the cluster.
+    ///
+    /// The handle returning by this method must be kept in order to keep alive the node
+    /// and retrieve information about the cluster.
+    ///
+    /// # Errors
+    /// This method may fails in the node does not succeed to join the cluster.
+    pub async fn start(mut self) -> Result<NodeHandle, Error> {
+        let node_id = NodeId::new(self.id, self.public_address);
+
+        let configuration = ChitchatConfig {
+            node_id: node_id.clone(),
+            cluster_id: self.cluster_id,
+            gossip_interval: self.update_interval,
+            listen_addr: self.listen_address,
+            seed_nodes: self.seed_nodes,
+            failure_detector_config: FailureDetectorConfig::default(),
+            is_ready_predicate: None,
+        };
+
+        let register = Register::with_keys(
+            self.initial_state
+                .iter()
+                .cloned()
+                .map(|(key, _)| key)
+                .collect(),
+        );
+
+        self.initial_state
+            .push((REGISTER_KEY.to_string(), register.to_string()));
+        self.initial_state
+            .push((NODE_KIND_KEY.to_string(), self.r#type.to_string()));
+
+        let handle = chitchat::spawn_chitchat(configuration, self.initial_state, &UdpTransport)
+            .await
+            .map_err(Error::Start)?;
+
+        Ok(NodeHandle {
+            node_id,
+            handle: Arc::new(handle),
+        })
+    }
+}
+
+/// A handle of the started node.
+///
+/// This type permits to watch the cluster joined by the node and retrieve
+/// information about the cluster nodes as well.
+#[must_use]
+#[derive(Clone)]
+pub struct NodeHandle {
+    node_id: NodeId,
+    handle: Arc<ChitchatHandle>,
+}
+
+impl fmt::Debug for NodeHandle {
+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
+        f.debug_struct("NodeHandle")
+            .field("node_id", &self.node_id)
+            .finish()
+    }
+}
+
+impl NodeHandle {
+    /// Returns the node identifier.
+    pub fn id(&self) -> &str {
+        &self.node_id.id
+    }
+
+    /// Update the node state with the given key/value.
+    ///
+    /// Note that the new node state will be propagated only on the next cluster update.
+    pub async fn update_state<T: ToString>(&self, key: Key<T>, value: T) {
+        let chitchat = self.handle.chitchat();
+        let mut chitchat = chitchat.lock().await;
+
+        let state = chitchat.self_node_state();
+        let mut register: Register = state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
+
+        register.insert(key.to_string());
+
+        state.set(REGISTER_KEY, register);
+        state.set(key, value);
+    }
+
+    /// Shuts the node down by notifying and quitting the cluster.
+    pub async fn shutdown(self) -> Result<(), Error> {
+        let handle =
+            Arc::try_unwrap(self.handle).map_err(|_| Error::Shutdown(anyhow!("node is in use")))?;
+
+        handle.shutdown().await.map_err(Error::Shutdown)
+    }
+
+    /// Returns a stream over any changes in the cluster.
+    pub async fn cluster_watcher(&self) -> impl Stream<Item = Vec<NodeHandle>> + 'static {
+        let handle = Arc::clone(&self.handle);
+        let node_id = self.node_id.clone();
+
+        self.handle
+            .chitchat()
+            .lock()
+            .await
+            .ready_nodes_watcher()
+            .map(move |nodes| {
+                nodes
+                    .into_iter()
+                    // `ready_nodes_watcher` does not contains this node so we add it
+                    // to have the complete list of live nodes.
+                    .chain([node_id.clone()])
+                    .map(|node_id| NodeHandle {
+                        node_id,
+                        handle: Arc::clone(&handle),
+                    })
+                    .collect()
+            })
+    }
+
+    /// Returns the current list of live nodes in the cluster.
+    pub async fn live_nodes(&self) -> Vec<NodeHandle> {
+        self.handle
+            .chitchat()
+            .lock()
+            .await
+            .live_nodes()
+            .cloned()
+            // `live_nodes` does not contains this node so we add it
+            // to have the complete list of live nodes.
+            .chain([self.node_id.clone()])
+            .map(|node_id| NodeHandle {
+                node_id,
+                handle: Arc::clone(&self.handle),
+            })
+            .collect()
+    }
+
+    /// Returns a node snapshot.
+    pub async fn snapshot(&self) -> NodeSnapshot {
+        self.handle
+            .with_chitchat(|chitchat| {
+                let state = chitchat.node_state(&self.node_id).unwrap();
+                let register: Register =
+                    state.get(REGISTER_KEY.as_str()).unwrap_or_default().into();
+
+                let r#type = state
+                    .get(NODE_KIND_KEY.as_str())
+                    .and_then(|s| s.parse().ok())
+                    .unwrap_or(NodeType::Unknown);
+
+                let state = register
+                    .iter()
+                    .filter_map(|key| {
+                        state
+                            .get(key)
+                            .map(|value| (key.to_string(), value.to_string()))
+                    })
+                    .collect();
+
+                NodeSnapshot {
+                    id: self.node_id.id.to_string(),
+                    address: self.node_id.gossip_public_address,
+                    is_self: chitchat.self_node_id() == &self.node_id,
+                    r#type,
+                    state,
+                }
+            })
+            .await
+    }
+}
+
+/// A snapshot of a cluster node with its state and meta information.
+#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
+pub struct NodeSnapshot {
+    id: String,
+    r#type: NodeType,
+    address: SocketAddr,
+    is_self: bool,
+    #[serde(flatten)]
+    state: HashMap<String, String>,
+}
+
+#[cfg(test)]
+mod tests {
+    use serde_test::{assert_tokens, Token};
+
+    use super::*;
+
+    #[test]
+    fn it_ser_de_node_type() {
+        let data = [
+            (NodeType::Io, "Io"),
+            (NodeType::Search, "Search"),
+            (NodeType::Ingest, "Ingest"),
+            (NodeType::Train, "Train"),
+            (NodeType::Unknown, "Unknown"),
+        ];
+
+        for (node, variant) in data {
+            assert_tokens(
+                &node,
+                &[Token::UnitVariant {
+                    name: "NodeType",
+                    variant,
+                }],
+            );
+        }
+    }
+
+    #[tokio::test]
+    #[serial_test::serial("cluster")]
+    async fn it_registers_user_keys_in_state() -> Result<(), Box<dyn std::error::Error>> {
+        const A_KEY: Key<u32> = Key::new("a-key");
+        const B_KEY: Key<f32> = Key::new("b-key");
+        const C_KEY: Key<char> = Key::new("c-key");
+
+        let node = Node::builder()
+            .on_local_network("0.0.0.0:8080".parse()?)
+            .register_as(NodeType::Io)
+            .insert_to_initial_state(A_KEY, 42)
+            .insert_to_initial_state(B_KEY, 0.0)
+            .build()?;
+
+        let node = node.start().await?;
+
+        let retrieve_registered_keys = || async {
+            node.handle
+                .chitchat()
+                .lock()
+                .await
+                .self_node_state()
+                .get(REGISTER_KEY.as_str())
+                .map(Register::from)
+        };
+
+        assert_eq!(
+            retrieve_registered_keys().await,
+            Some(Register::with_keys(
+                ["a-key".to_string(), "b-key".to_string()]
+                    .into_iter()
+                    .collect()
+            ))
+        );
+
+        node.update_state(C_KEY, 'a').await;
+
+        assert_eq!(
+            retrieve_registered_keys().await,
+            Some(Register::with_keys(
+                [
+                    "a-key".to_string(),
+                    "b-key".to_string(),
+                    "c-key".to_string()
+                ]
+                .into_iter()
+                .collect()
+            ))
+        );
+
+        Ok(())
+    }
+
+    #[tokio::test]
+    #[serial_test::serial("cluster")]
+    async fn it_builds_node_snapshot() -> Result<(), Box<dyn std::error::Error>> {
+        const LOAD_SCORE_KEY: Key<f32> = Key::new("load-score");
+
+        let node = Node::builder()
+            .on_local_network("0.0.0.0:8080".parse()?)
+            .register_as(NodeType::Io)
+            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
+            .build()?;
+
+        let node = node.start().await?;
+        let snapshot = node.snapshot().await;
+
+        assert_eq!(
+            snapshot,
+            NodeSnapshot {
+                id: node.node_id.id,
+                r#type: NodeType::Io,
+                address: node.node_id.gossip_public_address,
+                is_self: true,
+                state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
+                    .into_iter()
+                    .collect(),
+            }
+        );
+
+        Ok(())
+    }
+
+    #[tokio::test]
+    #[serial_test::serial("cluster")]
+    async fn it_builds_cluster_snapshot() -> Result<(), Box<dyn std::error::Error>> {
+        const LOAD_SCORE_KEY: Key<f32> = Key::new("load_score");
+
+        let first_node = Node::builder()
+            .on_local_network("0.0.0.0:8080".parse()?)
+            .register_as(NodeType::Io)
+            .insert_to_initial_state(LOAD_SCORE_KEY, 100.0)
+            .build()?;
+
+        let first_node = first_node.start().await?;
+
+        let second_node = Node::builder()
+            .on_local_network("0.0.0.0:8081".parse()?)
+            .register_as(NodeType::Ingest)
+            .insert_to_initial_state(LOAD_SCORE_KEY, 40.0)
+            .with_seed_nodes(vec!["0.0.0.0:8080".to_string()])
+            .build()?;
+
+        let second_node = second_node.start().await?;
+
+        tokio::time::sleep(Node::DEFAULT_UPDATE_INTERVAL * 2).await;
+
+        let live_nodes = first_node.live_nodes().await;
+        let snapshots = cluster_snapshot(live_nodes).await;
+
+        assert_eq!(
+            snapshots,
+            vec![
+                NodeSnapshot {
+                    id: second_node.node_id.id,
+                    r#type: NodeType::Ingest,
+                    address: second_node.node_id.gossip_public_address,
+                    is_self: false,
+                    state: [(LOAD_SCORE_KEY.to_string(), 40.to_string())]
+                        .into_iter()
+                        .collect(),
+                },
+                NodeSnapshot {
+                    id: first_node.node_id.id,
+                    r#type: NodeType::Io,
+                    address: first_node.node_id.gossip_public_address,
+                    is_self: true,
+                    state: [(LOAD_SCORE_KEY.to_string(), 100.to_string())]
+                        .into_iter()
+                        .collect(),
+                },
+            ]
+        );
+
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/tests/cluster_reader.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/tests/cluster_reader.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-#just for local tests
-from nucliadb_cluster_rust import Member
-import asyncio, socket
-import json
-from codecs import StreamReader, StreamWriter
-from typing import Optional, List
-
-class UnSockReader:
-    socket_reader: Optional[asyncio.Task] = None
-    
-    def __init__(self, path):
-        self.sock_path = path
-        self.socket_reader = None
-    
-    async def handle_connection(self, reader: StreamReader, _):
-        print("Handle new connection via unix socket")
-        while True:
-            try:
-                update_readed = await reader.read(512)
-                if len(update_readed) == 0:
-                    print("connection closed by cluster manager")
-                    break
-                json_data: List[Member] = json.loads(update_readed.decode("utf8").replace("'", '"'))
-                print(json.dumps(json_data, indent=2, sort_keys=True))
-            except IOError as e:
-                print(f"error while reading from unix socket: {e}")
-
-    def run_server(self):
-        self.socket_reader = asyncio.start_unix_server(self.handle_connection, self.sock_path)
-        asyncio.create_task(self.socket_reader, name="socket_reader")
-        loop = asyncio.get_event_loop()
-        loop.run_forever()
-
-if __name__ == "__main__":
-    reader = UnSockReader("/tmp/rust_python.sock")
+#just for local tests
+from nucliadb_cluster_rust import Member
+import asyncio, socket
+import json
+from codecs import StreamReader, StreamWriter
+from typing import Optional, List
+
+class UnSockReader:
+    socket_reader: Optional[asyncio.Task] = None
+    
+    def __init__(self, path):
+        self.sock_path = path
+        self.socket_reader = None
+    
+    async def handle_connection(self, reader: StreamReader, _):
+        print("Handle new connection via unix socket")
+        while True:
+            try:
+                update_readed = await reader.read(512)
+                if len(update_readed) == 0:
+                    print("connection closed by cluster manager")
+                    break
+                json_data: List[Member] = json.loads(update_readed.decode("utf8").replace("'", '"'))
+                print(json.dumps(json_data, indent=2, sort_keys=True))
+            except IOError as e:
+                print(f"error while reading from unix socket: {e}")
+
+    def run_server(self):
+        self.socket_reader = asyncio.start_unix_server(self.handle_connection, self.sock_path)
+        asyncio.create_task(self.socket_reader, name="socket_reader")
+        loop = asyncio.get_event_loop()
+        loop.run_forever()
+
+if __name__ == "__main__":
+    reader = UnSockReader("/tmp/rust_python.sock")
     reader.run_server()
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/tests/integration.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/tests/integration.rs`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,219 +1,219 @@
-use std::net::{IpAddr, Ipv4Addr, SocketAddr, TcpListener};
-use std::str::FromStr;
-use std::sync::atomic::{AtomicUsize, Ordering};
-use std::time::Duration;
-use std::{env, io};
-
-use bytes::BytesMut;
-use dockertest::{Composition, DockerTest, StartPolicy};
-use log::error;
-use nucliadb_cluster::{Node, NodeHandle, NodeSnapshot, NodeType};
-use tokio::net::UnixStream;
-use tokio::sync::mpsc;
-use tokio_stream::StreamExt;
-use uuid::Uuid;
-
-const SEED_NODE: &str = "0.0.0.0:4040";
-const UPDATE_INTERVAL: Duration = Duration::from_millis(250);
-
-pub async fn create_seed_node() -> anyhow::Result<NodeHandle> {
-    let node = Node::builder()
-        .register_as(NodeType::Io)
-        .on_local_network(SocketAddr::from_str(SEED_NODE).unwrap())
-        .with_seed_nodes(vec![SEED_NODE.to_string()])
-        .with_update_interval(UPDATE_INTERVAL)
-        .build()?;
-
-    let node = node.start().await?;
-
-    Ok(node)
-}
-
-pub fn find_available_port() -> anyhow::Result<u16> {
-    let socket = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1)), 0);
-    let listener = TcpListener::bind(socket)?;
-    let port = listener.local_addr()?.port();
-
-    Ok(port)
-}
-
-pub async fn create_node_for_test_with_id(
-    id: String,
-    seed_node: String,
-) -> anyhow::Result<NodeHandle> {
-    let port = find_available_port()?;
-
-    eprintln!("port: {port}");
-
-    let node = Node::builder()
-        .with_id(id)
-        .register_as(NodeType::Io)
-        .on_local_network(SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), port))
-        .with_seed_nodes(vec![seed_node])
-        .with_update_interval(UPDATE_INTERVAL)
-        .build()?;
-
-    let node = node.start().await?;
-
-    Ok(node)
-}
-
-/// Creates a local cluster listening on a random port.
-pub async fn create_node_for_test(seed_node: String) -> anyhow::Result<NodeHandle> {
-    let peer_uuid = Uuid::new_v4().to_string();
-    let node = create_node_for_test_with_id(peer_uuid, seed_node).await?;
-
-    Ok(node)
-}
-
-pub fn setup_logging_for_tests() {
-    use std::sync::Once;
-    static INIT: Once = Once::new();
-    INIT.call_once(|| {
-        env_logger::builder().format_timestamp(None).init();
-    });
-}
-
-#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
-#[serial_test::serial("cluster")]
-async fn test_cluster_single_node() {
-    setup_logging_for_tests();
-    // create seed node
-    let node = create_seed_node().await.unwrap();
-
-    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
-
-    let live_nodes = node.live_nodes().await;
-
-    assert_eq!(live_nodes.len(), 1);
-}
-
-#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
-#[serial_test::serial("cluster")]
-async fn test_cluster_two_nodes() {
-    setup_logging_for_tests();
-    // create seed node
-    let first_node = create_seed_node().await.unwrap();
-    let mut cluster_watcher = first_node.cluster_watcher().await;
-
-    // add node to cluster
-    let second_node = create_node_for_test(SEED_NODE.to_string()).await.unwrap();
-
-    // allow nodes start and communicate
-    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
-
-    match tokio::time::timeout(UPDATE_INTERVAL, cluster_watcher.next()).await {
-        Ok(Some(live_nodes)) => {
-            let live_nodes = live_nodes
-                .into_iter()
-                .map(|node| node.id().to_string())
-                .collect::<Vec<_>>();
-
-            assert_eq!(live_nodes.len(), 2);
-            assert!(live_nodes.contains(&first_node.id().to_string()));
-            assert!(live_nodes.contains(&second_node.id().to_string()));
-        }
-        Ok(None) => {
-            panic!("no changes in cluster");
-        }
-        Err(e) => {
-            panic!("timeout while waiting cluster changes: {e}");
-        }
-    }
-}
-
-enum NodeOperation {
-    Add,
-    Delete,
-}
-
-struct TestClusterState {
-    nodes: AtomicUsize,
-}
-
-impl TestClusterState {
-    fn new() -> Self {
-        Self {
-            nodes: AtomicUsize::new(0),
-        }
-    }
-
-    pub(crate) fn change_state(&self, ops: &NodeOperation) {
-        match ops {
-            NodeOperation::Add => self.nodes.fetch_add(1, Ordering::AcqRel),
-            NodeOperation::Delete => self.nodes.fetch_sub(1, Ordering::AcqRel),
-        };
-    }
-
-    pub(crate) fn nodes(&self) -> usize {
-        self.nodes.load(Ordering::Relaxed)
-    }
-}
-
-#[ignore = "ignore"]
-#[allow(unused_assignments)]
-#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
-async fn test_integration_3_nodes_with_monitor() {
-    let registry = env::var("IMAGE_REPOSITORY").unwrap();
-    let sock_path = env::var("SOCKET_PATH").unwrap();
-    let mut docker = DockerTest::new();
-
-    let unix_stream = UnixStream::connect(sock_path.clone()).await.unwrap();
-    let (tx, mut rx) = mpsc::channel::<()>(2);
-    let state = TestClusterState::new();
-    let mut operation = NodeOperation::Add;
-
-    tokio::spawn(async move {
-        loop {
-            if unix_stream.readable().await.is_ok() {
-                let mut buffer = BytesMut::with_capacity(512);
-                match unix_stream.try_read(&mut buffer) {
-                    Ok(bytes_read) => {
-                        assert_ne!(
-                            bytes_read, 0,
-                            "0 bytes read from socket. Connection closed by writer"
-                        );
-                        let update = serde_json::from_slice::<Vec<NodeSnapshot>>(&buffer).unwrap();
-                        state.change_state(&operation);
-                        assert_eq!(update.len(), state.nodes());
-                        if state.nodes() == 2 {
-                            tx.send(()).await.unwrap();
-                            operation = NodeOperation::Delete;
-                            break; // TODO: when dockertest crate will implement stop/pause delete
-                                   // this break
-                        }
-                    }
-                    Err(ref e) if e.kind() == io::ErrorKind::WouldBlock => continue,
-                    Err(e) => {
-                        error!("error during reading from unix socket: {e}");
-                        break;
-                    }
-                }
-            }
-        }
-    });
-
-    let mut watcher_node = Composition::with_repository(registry.clone())
-        .with_container_name("node_with_watcher")
-        .with_start_policy(StartPolicy::Strict);
-    watcher_node.bind_mount(sock_path.clone(), sock_path.clone());
-
-    let node2 = Composition::with_repository(registry.clone())
-        .with_container_name("node2")
-        .with_start_policy(StartPolicy::Strict);
-
-    let node3 = Composition::with_repository(registry)
-        .with_container_name("node3")
-        .with_start_policy(StartPolicy::Strict);
-
-    docker.add_composition(watcher_node);
-    docker.add_composition(node2);
-    docker.add_composition(node3);
-    docker.run(|_ops| async move {
-        // wait until all containers started and all nodes will be in cluster
-        assert_eq!(Some(()), rx.recv().await);
-
-        // TODO: when dockertest crate will implement stop/pause for containers
-        // sequentially turn off the nodes from the cluster
-    });
-}
+use std::net::{IpAddr, Ipv4Addr, SocketAddr, TcpListener};
+use std::str::FromStr;
+use std::sync::atomic::{AtomicUsize, Ordering};
+use std::time::Duration;
+use std::{env, io};
+
+use bytes::BytesMut;
+use dockertest::{Composition, DockerTest, StartPolicy};
+use log::error;
+use nucliadb_cluster::{Node, NodeHandle, NodeSnapshot, NodeType};
+use tokio::net::UnixStream;
+use tokio::sync::mpsc;
+use tokio_stream::StreamExt;
+use uuid::Uuid;
+
+const SEED_NODE: &str = "0.0.0.0:4040";
+const UPDATE_INTERVAL: Duration = Duration::from_millis(250);
+
+pub async fn create_seed_node() -> anyhow::Result<NodeHandle> {
+    let node = Node::builder()
+        .register_as(NodeType::Io)
+        .on_local_network(SocketAddr::from_str(SEED_NODE).unwrap())
+        .with_seed_nodes(vec![SEED_NODE.to_string()])
+        .with_update_interval(UPDATE_INTERVAL)
+        .build()?;
+
+    let node = node.start().await?;
+
+    Ok(node)
+}
+
+pub fn find_available_port() -> anyhow::Result<u16> {
+    let socket = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(127, 0, 0, 1)), 0);
+    let listener = TcpListener::bind(socket)?;
+    let port = listener.local_addr()?.port();
+
+    Ok(port)
+}
+
+pub async fn create_node_for_test_with_id(
+    id: String,
+    seed_node: String,
+) -> anyhow::Result<NodeHandle> {
+    let port = find_available_port()?;
+
+    eprintln!("port: {port}");
+
+    let node = Node::builder()
+        .with_id(id)
+        .register_as(NodeType::Io)
+        .on_local_network(SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), port))
+        .with_seed_nodes(vec![seed_node])
+        .with_update_interval(UPDATE_INTERVAL)
+        .build()?;
+
+    let node = node.start().await?;
+
+    Ok(node)
+}
+
+/// Creates a local cluster listening on a random port.
+pub async fn create_node_for_test(seed_node: String) -> anyhow::Result<NodeHandle> {
+    let peer_uuid = Uuid::new_v4().to_string();
+    let node = create_node_for_test_with_id(peer_uuid, seed_node).await?;
+
+    Ok(node)
+}
+
+pub fn setup_logging_for_tests() {
+    use std::sync::Once;
+    static INIT: Once = Once::new();
+    INIT.call_once(|| {
+        env_logger::builder().format_timestamp(None).init();
+    });
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+#[serial_test::serial("cluster")]
+async fn test_cluster_single_node() {
+    setup_logging_for_tests();
+    // create seed node
+    let node = create_seed_node().await.unwrap();
+
+    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
+
+    let live_nodes = node.live_nodes().await;
+
+    assert_eq!(live_nodes.len(), 1);
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
+#[serial_test::serial("cluster")]
+async fn test_cluster_two_nodes() {
+    setup_logging_for_tests();
+    // create seed node
+    let first_node = create_seed_node().await.unwrap();
+    let mut cluster_watcher = first_node.cluster_watcher().await;
+
+    // add node to cluster
+    let second_node = create_node_for_test(SEED_NODE.to_string()).await.unwrap();
+
+    // allow nodes start and communicate
+    tokio::time::sleep(UPDATE_INTERVAL * 2).await;
+
+    match tokio::time::timeout(UPDATE_INTERVAL, cluster_watcher.next()).await {
+        Ok(Some(live_nodes)) => {
+            let live_nodes = live_nodes
+                .into_iter()
+                .map(|node| node.id().to_string())
+                .collect::<Vec<_>>();
+
+            assert_eq!(live_nodes.len(), 2);
+            assert!(live_nodes.contains(&first_node.id().to_string()));
+            assert!(live_nodes.contains(&second_node.id().to_string()));
+        }
+        Ok(None) => {
+            panic!("no changes in cluster");
+        }
+        Err(e) => {
+            panic!("timeout while waiting cluster changes: {e}");
+        }
+    }
+}
+
+enum NodeOperation {
+    Add,
+    Delete,
+}
+
+struct TestClusterState {
+    nodes: AtomicUsize,
+}
+
+impl TestClusterState {
+    fn new() -> Self {
+        Self {
+            nodes: AtomicUsize::new(0),
+        }
+    }
+
+    pub(crate) fn change_state(&self, ops: &NodeOperation) {
+        match ops {
+            NodeOperation::Add => self.nodes.fetch_add(1, Ordering::AcqRel),
+            NodeOperation::Delete => self.nodes.fetch_sub(1, Ordering::AcqRel),
+        };
+    }
+
+    pub(crate) fn nodes(&self) -> usize {
+        self.nodes.load(Ordering::Relaxed)
+    }
+}
+
+#[ignore = "ignore"]
+#[allow(unused_assignments)]
+#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
+async fn test_integration_3_nodes_with_monitor() {
+    let registry = env::var("IMAGE_REPOSITORY").unwrap();
+    let sock_path = env::var("SOCKET_PATH").unwrap();
+    let mut docker = DockerTest::new();
+
+    let unix_stream = UnixStream::connect(sock_path.clone()).await.unwrap();
+    let (tx, mut rx) = mpsc::channel::<()>(2);
+    let state = TestClusterState::new();
+    let mut operation = NodeOperation::Add;
+
+    tokio::spawn(async move {
+        loop {
+            if unix_stream.readable().await.is_ok() {
+                let mut buffer = BytesMut::with_capacity(512);
+                match unix_stream.try_read(&mut buffer) {
+                    Ok(bytes_read) => {
+                        assert_ne!(
+                            bytes_read, 0,
+                            "0 bytes read from socket. Connection closed by writer"
+                        );
+                        let update = serde_json::from_slice::<Vec<NodeSnapshot>>(&buffer).unwrap();
+                        state.change_state(&operation);
+                        assert_eq!(update.len(), state.nodes());
+                        if state.nodes() == 2 {
+                            tx.send(()).await.unwrap();
+                            operation = NodeOperation::Delete;
+                            break; // TODO: when dockertest crate will implement stop/pause delete
+                                   // this break
+                        }
+                    }
+                    Err(ref e) if e.kind() == io::ErrorKind::WouldBlock => continue,
+                    Err(e) => {
+                        error!("error during reading from unix socket: {e}");
+                        break;
+                    }
+                }
+            }
+        }
+    });
+
+    let mut watcher_node = Composition::with_repository(registry.clone())
+        .with_container_name("node_with_watcher")
+        .with_start_policy(StartPolicy::Strict);
+    watcher_node.bind_mount(sock_path.clone(), sock_path.clone());
+
+    let node2 = Composition::with_repository(registry.clone())
+        .with_container_name("node2")
+        .with_start_policy(StartPolicy::Strict);
+
+    let node3 = Composition::with_repository(registry)
+        .with_container_name("node3")
+        .with_start_policy(StartPolicy::Strict);
+
+    docker.add_composition(watcher_node);
+    docker.add_composition(node2);
+    docker.add_composition(node3);
+    docker.run(|_ops| async move {
+        // wait until all containers started and all nodes will be in cluster
+        assert_eq!(Some(()), rx.recv().await);
+
+        // TODO: when dockertest crate will implement stop/pause for containers
+        // sequentially turn off the nodes from the cluster
+    });
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_cluster/tests/test2.py` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_cluster/tests/test2.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,92 +1,92 @@
-import asyncio
-import json
-import binascii
-from sys import byteorder
-from typing import List, Optional
-
-# from nucliadb_ingest import logger
-# from nucliadb_ingest.orm.node import (
-#    ClusterMember,
-#    DefinedNodesNucliaDBSearch,
-#    chitchat_update_node,
-# )
-# from nucliadb_ingest.sentry import SENTRY
-# from nucliadb_search.settings import settings
-
-# if SENTRY:
-#    from sentry_sdk import capture_exception
-
-
-def start_chitchat():
-    chitchat = ChitchatNucliaDBSearch("0.0.0.0", 31337)
-    asyncio.run(chitchat.start())
-
-    return chitchat
-
-
-class ChitchatNucliaDBSearch:
-    chitchat_update_srv: Optional[asyncio.Task] = None
-
-    def __init__(self, host: str, port: int):
-        self.host = host
-        self.port = port
-        self.chitchat_update_srv = None
-
-    async def start(self):
-        print("enter chitchat.start()")
-        self.chitchat_update_srv = await asyncio.start_server(
-            self.socket_reader, host=self.host, port=self.port
-        )
-        print("tcp server created ")
-        async with self.chitchat_update_srv:
-            print("awaiting connections from rust part of cluster")
-            await asyncio.create_task(self.chitchat_update_srv.serve_forever())
-
-    async def socket_reader(
-        self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter
-    ):
-        while True:
-            try:
-                print("wait data in socket")
-                mgr_message = await reader.read(
-                    4096
-                )  # TODO: add message types enum with proper deserialization
-                if len(mgr_message) == 0:
-                    print("empty message received")
-                    continue
-                if len(mgr_message) == 4:
-                    print("check message received: {}".format(mgr_message.hex()))
-                    hash = binascii.crc32(mgr_message)
-                    print(f"calculated hash: {hash}")
-                    response = hash.to_bytes(4, byteorder="big")
-                    print(f"Hash response: {response}")
-                    writer.write(response)
-                    await writer.drain()
-                    continue
-                else:
-                    print(f"update message received: {mgr_message}")
-                    members: List[ClusterMember] = json.loads(
-                        mgr_message.decode("utf8").replace("'", '"')
-                    )
-                    print(f"updated members: {members}")
-                    if len(members) != 0:
-                        await chitchat_update_node(members)
-                        writer.write(len(members))
-                        await writer.drain()
-                    else:
-                        print("connection closed by writer")
-                        break
-            except IOError as e:
-                print(f"exception: {e}")
-                # if SENTRY:
-                #    capture_exception(e)
-                # logger.exception(f"error while reading update from unix socket: {e}")
-
-    async def close(self):
-        self.chitchat_update_srv.cancel()
-
-
-if __name__ == "__main__":
-    chitchat = start_chitchat()
-    loop = asyncio.get_event_loop()
-    loop.run_forever()
+import asyncio
+import json
+import binascii
+from sys import byteorder
+from typing import List, Optional
+
+# from nucliadb_ingest import logger
+# from nucliadb_ingest.orm.node import (
+#    ClusterMember,
+#    DefinedNodesNucliaDBSearch,
+#    chitchat_update_node,
+# )
+# from nucliadb_ingest.sentry import SENTRY
+# from nucliadb_search.settings import settings
+
+# if SENTRY:
+#    from sentry_sdk import capture_exception
+
+
+def start_chitchat():
+    chitchat = ChitchatNucliaDBSearch("0.0.0.0", 31337)
+    asyncio.run(chitchat.start())
+
+    return chitchat
+
+
+class ChitchatNucliaDBSearch:
+    chitchat_update_srv: Optional[asyncio.Task] = None
+
+    def __init__(self, host: str, port: int):
+        self.host = host
+        self.port = port
+        self.chitchat_update_srv = None
+
+    async def start(self):
+        print("enter chitchat.start()")
+        self.chitchat_update_srv = await asyncio.start_server(
+            self.socket_reader, host=self.host, port=self.port
+        )
+        print("tcp server created ")
+        async with self.chitchat_update_srv:
+            print("awaiting connections from rust part of cluster")
+            await asyncio.create_task(self.chitchat_update_srv.serve_forever())
+
+    async def socket_reader(
+        self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter
+    ):
+        while True:
+            try:
+                print("wait data in socket")
+                mgr_message = await reader.read(
+                    4096
+                )  # TODO: add message types enum with proper deserialization
+                if len(mgr_message) == 0:
+                    print("empty message received")
+                    continue
+                if len(mgr_message) == 4:
+                    print("check message received: {}".format(mgr_message.hex()))
+                    hash = binascii.crc32(mgr_message)
+                    print(f"calculated hash: {hash}")
+                    response = hash.to_bytes(4, byteorder="big")
+                    print(f"Hash response: {response}")
+                    writer.write(response)
+                    await writer.drain()
+                    continue
+                else:
+                    print(f"update message received: {mgr_message}")
+                    members: List[ClusterMember] = json.loads(
+                        mgr_message.decode("utf8").replace("'", '"')
+                    )
+                    print(f"updated members: {members}")
+                    if len(members) != 0:
+                        await chitchat_update_node(members)
+                        writer.write(len(members))
+                        await writer.drain()
+                    else:
+                        print("connection closed by writer")
+                        break
+            except IOError as e:
+                print(f"exception: {e}")
+                # if SENTRY:
+                #    capture_exception(e)
+                # logger.exception(f"error while reading update from unix socket: {e}")
+
+    async def close(self):
+        self.chitchat_update_srv.cancel()
+
+
+if __name__ == "__main__":
+    chitchat = start_chitchat()
+    loop = asyncio.get_event_loop()
+    loop.run_forever()
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/Cargo.toml` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/Cargo.toml`

 * *Files identical despite different names*

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/disk_hnsw.rs`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,293 +1,293 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-// Node:
-// -> Layers segment.
-// -> Indexing segment.
-// \Indexing segment:
-// Per layer in the hnsw in reverse order the start of
-// its adjacency list.
-// \Layer segment:
-// -> N: number of connexions.
-// -> List per connexion of T tuples (node, edge), where:
-// -> node: usize, in little endian.
-// -> edge: f32, in little endian.
-// Hnsw:
-// -> Node segment.
-// -> Indexing segment.
-// -> Entry point segment.
-// \Entry point segment:
-// -> Layer, usize in little endian.
-// -> Node, usize in little endian.
-// \Node segment (serialized as explained above).
-// \Indexing segment:
-// Per layer in the hnsw:
-// -> The byte where it ends.
-//
-//
-
-use std::collections::HashMap;
-use std::io;
-
-use super::ops_hnsw::{Hnsw, Layer};
-use super::ram_hnsw::{Edge, EntryPoint, RAMHnsw};
-use super::Address;
-use crate::data_types::usize_utils::*;
-
-const EDGE_LEN: usize = 4;
-const NODE_LEN: usize = USIZE_LEN;
-const CNX_LEN: usize = NODE_LEN + EDGE_LEN;
-
-fn f32_from_le_bytes(buf: &[u8]) -> f32 {
-    let mut temp = [0; 4];
-    temp.copy_from_slice(buf);
-    f32::from_le_bytes(temp)
-}
-
-pub struct DiskLayer<'a> {
-    hnsw: &'a [u8],
-    layer: usize,
-}
-
-impl<'a> Layer for &'a DiskLayer<'a> {
-    type EdgeIt = EdgeIter<'a>;
-    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
-        let node = DiskHnsw::get_node(self.hnsw, node);
-        DiskHnsw::get_out_edges(node, self.layer)
-    }
-}
-
-impl<'a> Layer for DiskLayer<'a> {
-    type EdgeIt = EdgeIter<'a>;
-    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
-        let node = DiskHnsw::get_node(self.hnsw, node);
-        DiskHnsw::get_out_edges(node, self.layer)
-    }
-}
-
-impl<'a> Hnsw for &'a [u8] {
-    type L = DiskLayer<'a>;
-    fn get_entry_point(&self) -> Option<EntryPoint> {
-        DiskHnsw::get_entry_point(self)
-    }
-    fn get_layer(&self, i: usize) -> Self::L {
-        DiskLayer {
-            hnsw: self,
-            layer: i,
-        }
-    }
-}
-
-pub struct EdgeIter<'a> {
-    crnt: usize,
-    buf: &'a [u8],
-}
-impl<'a> Iterator for EdgeIter<'a> {
-    type Item = (Address, Edge);
-    fn next(&mut self) -> Option<Self::Item> {
-        if self.buf.len() == self.crnt {
-            None
-        } else {
-            let buf = self.buf;
-            let mut crnt = self.crnt;
-            let node = usize_from_slice_le(&buf[crnt..(crnt + NODE_LEN)]);
-            crnt += USIZE_LEN;
-            let edge = f32_from_le_bytes(&buf[crnt..(crnt + EDGE_LEN)]);
-            crnt += EDGE_LEN;
-            self.crnt = crnt;
-            Some((Address(node), Edge { dist: edge }))
-        }
-    }
-}
-
-pub struct DiskHnsw;
-impl DiskHnsw {
-    fn serialize_node<W>(
-        mut buf: W,
-        offset: usize,
-        node: usize,
-        hnsw: &RAMHnsw,
-    ) -> io::Result<usize>
-    where
-        W: io::Write,
-    {
-        let node = Address(node);
-        let mut length = offset;
-        let mut indexing = HashMap::new();
-        for layer in 0..hnsw.no_layers() {
-            let no_edges = hnsw.get_layer(layer).no_out_edges(node);
-            indexing.insert(layer, length);
-            buf.write_all(&no_edges.to_le_bytes())?;
-            length += USIZE_LEN;
-            for (cnx, edge) in hnsw.get_layer(layer).get_out_edges(node) {
-                buf.write_all(&cnx.0.to_le_bytes())?;
-                buf.write_all(&edge.dist.to_le_bytes())?;
-                length += CNX_LEN;
-            }
-        }
-        for layer in (0..hnsw.no_layers()).rev() {
-            buf.write_all(&indexing[&layer].to_le_bytes())?;
-        }
-        length += hnsw.no_layers() * USIZE_LEN;
-        buf.flush()?;
-        Ok(length)
-    }
-
-    // node must be serialized using DiskNode, may have trailing bytes at the start.
-    fn get_out_edges(node: &[u8], layer: usize) -> EdgeIter {
-        // layer + 1 since the layers are stored in reverse order.
-        // [l3, l2, l1, l0, end] Since we have the position of end, the layer i is
-        // i + 1 positions to its left.
-        let pos = node.len() - ((layer + 1) * USIZE_LEN);
-        let cnx_start = usize_from_slice_le(&node[pos..(pos + USIZE_LEN)]);
-        let no_cnx = usize_from_slice_le(&node[cnx_start..(cnx_start + USIZE_LEN)]);
-        let cnx_start = cnx_start + USIZE_LEN;
-        let cnx_end = cnx_start + (no_cnx * CNX_LEN);
-        EdgeIter {
-            crnt: 0,
-            buf: &node[cnx_start..cnx_end],
-        }
-    }
-    pub fn serialize_into<W: io::Write>(
-        mut buf: W,
-        no_nodes: usize,
-        hnsw: RAMHnsw,
-    ) -> io::Result<()> {
-        if let Some(entry_point) = hnsw.entry_point {
-            let mut length = 0;
-            let mut nodes_end = vec![];
-            for node in 0..no_nodes {
-                length = DiskHnsw::serialize_node(&mut buf, length, node, &hnsw)?;
-                nodes_end.push(length)
-            }
-            for ends_at in nodes_end.into_iter().rev() {
-                buf.write_all(&ends_at.to_le_bytes())?;
-                length += USIZE_LEN;
-            }
-            let EntryPoint { node, layer } = entry_point;
-            buf.write_all(&layer.to_le_bytes())?;
-            buf.write_all(&node.0.to_le_bytes())?;
-            let _length = length + 2 * USIZE_LEN;
-            buf.flush()?;
-        }
-        Ok(())
-    }
-    // hnsw must be serialized using DiskHnsw, may have trailing bytes at the start.
-    pub fn get_entry_point(hnsw: &[u8]) -> Option<EntryPoint> {
-        if !hnsw.is_empty() {
-            let node_start = hnsw.len() - USIZE_LEN;
-            let layer_start = node_start - USIZE_LEN;
-            let node = usize_from_slice_le(&hnsw[node_start..(node_start + USIZE_LEN)]);
-            let layer = usize_from_slice_le(&hnsw[layer_start..(layer_start + USIZE_LEN)]);
-            Some(EntryPoint {
-                node: Address(node),
-                layer,
-            })
-        } else {
-            None
-        }
-    }
-    // hnsw must be serialized using MHnsw, may have trailing bytes at the start.
-    // The returned node will have trailing bytes at the start.
-    pub fn get_node(hnsw: &[u8], node: usize) -> &[u8] {
-        let indexing_end = hnsw.len() - (2 * USIZE_LEN);
-        // node + 1 since the layers are stored in reverse order.
-        // [n3, n2, n1, n0, end] Since we have the position of end, the node i is
-        // i + 1 positions to its left.
-        let pos = indexing_end - ((node + 1) * USIZE_LEN);
-        let node_end = usize_from_slice_le(&hnsw[pos..(pos + USIZE_LEN)]);
-        &hnsw[..node_end]
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use crate::data_point::ram_hnsw::RAMLayer;
-    fn layer_check<L: Layer>(buf: L, no_nodes: usize, cnx: &[Vec<(Address, Edge)>]) {
-        let no_cnx = vec![];
-        for i in 0..no_nodes {
-            let expected = cnx.get(i).unwrap_or(&no_cnx);
-            let got: Vec<_> = buf.get_out_edges(Address(i)).collect();
-            assert_eq!(expected, &got);
-        }
-    }
-    #[test]
-    fn empty_hnsw() {
-        let hnsw = RAMHnsw::new();
-        let mut buf = vec![];
-        DiskHnsw::serialize_into(&mut buf, 0, hnsw).unwrap();
-        let ep = DiskHnsw::get_entry_point(&buf);
-        assert_eq!(ep, None);
-    }
-
-    #[test]
-    fn hnsw_test() {
-        let no_nodes = 3;
-        let cnx0 = vec![
-            vec![(Address(1), Edge { dist: 1.0 })],
-            vec![(Address(2), Edge { dist: 2.0 })],
-            vec![(Address(3), Edge { dist: 3.0 })],
-        ];
-        let layer0 = RAMLayer {
-            out: cnx0
-                .iter()
-                .enumerate()
-                .map(|(i, c)| (Address(i), c.clone()))
-                .collect(),
-        };
-        let cnx1 = vec![
-            vec![(Address(1), Edge { dist: 4.0 })],
-            vec![(Address(2), Edge { dist: 5.0 })],
-        ];
-        let layer1 = RAMLayer {
-            out: cnx1
-                .iter()
-                .enumerate()
-                .map(|(i, c)| (Address(i), c.clone()))
-                .collect(),
-        };
-        let cnx2 = vec![vec![(Address(1), Edge { dist: 6.0 })]];
-        let layer2 = RAMLayer {
-            out: cnx2
-                .iter()
-                .enumerate()
-                .map(|(i, c)| (Address(i), c.clone()))
-                .collect(),
-        };
-        let entry_point = EntryPoint {
-            node: Address(0),
-            layer: 2,
-        };
-        let mut hnsw = RAMHnsw::new();
-        hnsw.entry_point = Some(entry_point);
-        hnsw.layers = vec![layer0, layer1, layer2];
-        let mut buf = vec![];
-        DiskHnsw::serialize_into(&mut buf, no_nodes, hnsw).unwrap();
-        let ep = DiskHnsw::get_entry_point(&buf).unwrap();
-        assert_eq!(ep, entry_point);
-        let layer0 = buf.as_slice().get_layer(0);
-        layer_check(layer0, no_nodes, &cnx0);
-        let layer1 = buf.as_slice().get_layer(1);
-        layer_check(layer1, no_nodes, &cnx1);
-        let layer2 = buf.as_slice().get_layer(2);
-        layer_check(layer2, no_nodes, &cnx2);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+// Node:
+// -> Layers segment.
+// -> Indexing segment.
+// \Indexing segment:
+// Per layer in the hnsw in reverse order the start of
+// its adjacency list.
+// \Layer segment:
+// -> N: number of connexions.
+// -> List per connexion of T tuples (node, edge), where:
+// -> node: usize, in little endian.
+// -> edge: f32, in little endian.
+// Hnsw:
+// -> Node segment.
+// -> Indexing segment.
+// -> Entry point segment.
+// \Entry point segment:
+// -> Layer, usize in little endian.
+// -> Node, usize in little endian.
+// \Node segment (serialized as explained above).
+// \Indexing segment:
+// Per layer in the hnsw:
+// -> The byte where it ends.
+//
+//
+
+use std::collections::HashMap;
+use std::io;
+
+use super::ops_hnsw::{Hnsw, Layer};
+use super::ram_hnsw::{Edge, EntryPoint, RAMHnsw};
+use super::Address;
+use crate::data_types::usize_utils::*;
+
+const EDGE_LEN: usize = 4;
+const NODE_LEN: usize = USIZE_LEN;
+const CNX_LEN: usize = NODE_LEN + EDGE_LEN;
+
+fn f32_from_le_bytes(buf: &[u8]) -> f32 {
+    let mut temp = [0; 4];
+    temp.copy_from_slice(buf);
+    f32::from_le_bytes(temp)
+}
+
+pub struct DiskLayer<'a> {
+    hnsw: &'a [u8],
+    layer: usize,
+}
+
+impl<'a> Layer for &'a DiskLayer<'a> {
+    type EdgeIt = EdgeIter<'a>;
+    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
+        let node = DiskHnsw::get_node(self.hnsw, node);
+        DiskHnsw::get_out_edges(node, self.layer)
+    }
+}
+
+impl<'a> Layer for DiskLayer<'a> {
+    type EdgeIt = EdgeIter<'a>;
+    fn get_out_edges(&self, Address(node): Address) -> Self::EdgeIt {
+        let node = DiskHnsw::get_node(self.hnsw, node);
+        DiskHnsw::get_out_edges(node, self.layer)
+    }
+}
+
+impl<'a> Hnsw for &'a [u8] {
+    type L = DiskLayer<'a>;
+    fn get_entry_point(&self) -> Option<EntryPoint> {
+        DiskHnsw::get_entry_point(self)
+    }
+    fn get_layer(&self, i: usize) -> Self::L {
+        DiskLayer {
+            hnsw: self,
+            layer: i,
+        }
+    }
+}
+
+pub struct EdgeIter<'a> {
+    crnt: usize,
+    buf: &'a [u8],
+}
+impl<'a> Iterator for EdgeIter<'a> {
+    type Item = (Address, Edge);
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.buf.len() == self.crnt {
+            None
+        } else {
+            let buf = self.buf;
+            let mut crnt = self.crnt;
+            let node = usize_from_slice_le(&buf[crnt..(crnt + NODE_LEN)]);
+            crnt += USIZE_LEN;
+            let edge = f32_from_le_bytes(&buf[crnt..(crnt + EDGE_LEN)]);
+            crnt += EDGE_LEN;
+            self.crnt = crnt;
+            Some((Address(node), Edge { dist: edge }))
+        }
+    }
+}
+
+pub struct DiskHnsw;
+impl DiskHnsw {
+    fn serialize_node<W>(
+        mut buf: W,
+        offset: usize,
+        node: usize,
+        hnsw: &RAMHnsw,
+    ) -> io::Result<usize>
+    where
+        W: io::Write,
+    {
+        let node = Address(node);
+        let mut length = offset;
+        let mut indexing = HashMap::new();
+        for layer in 0..hnsw.no_layers() {
+            let no_edges = hnsw.get_layer(layer).no_out_edges(node);
+            indexing.insert(layer, length);
+            buf.write_all(&no_edges.to_le_bytes())?;
+            length += USIZE_LEN;
+            for (cnx, edge) in hnsw.get_layer(layer).get_out_edges(node) {
+                buf.write_all(&cnx.0.to_le_bytes())?;
+                buf.write_all(&edge.dist.to_le_bytes())?;
+                length += CNX_LEN;
+            }
+        }
+        for layer in (0..hnsw.no_layers()).rev() {
+            buf.write_all(&indexing[&layer].to_le_bytes())?;
+        }
+        length += hnsw.no_layers() * USIZE_LEN;
+        buf.flush()?;
+        Ok(length)
+    }
+
+    // node must be serialized using DiskNode, may have trailing bytes at the start.
+    fn get_out_edges(node: &[u8], layer: usize) -> EdgeIter {
+        // layer + 1 since the layers are stored in reverse order.
+        // [l3, l2, l1, l0, end] Since we have the position of end, the layer i is
+        // i + 1 positions to its left.
+        let pos = node.len() - ((layer + 1) * USIZE_LEN);
+        let cnx_start = usize_from_slice_le(&node[pos..(pos + USIZE_LEN)]);
+        let no_cnx = usize_from_slice_le(&node[cnx_start..(cnx_start + USIZE_LEN)]);
+        let cnx_start = cnx_start + USIZE_LEN;
+        let cnx_end = cnx_start + (no_cnx * CNX_LEN);
+        EdgeIter {
+            crnt: 0,
+            buf: &node[cnx_start..cnx_end],
+        }
+    }
+    pub fn serialize_into<W: io::Write>(
+        mut buf: W,
+        no_nodes: usize,
+        hnsw: RAMHnsw,
+    ) -> io::Result<()> {
+        if let Some(entry_point) = hnsw.entry_point {
+            let mut length = 0;
+            let mut nodes_end = vec![];
+            for node in 0..no_nodes {
+                length = DiskHnsw::serialize_node(&mut buf, length, node, &hnsw)?;
+                nodes_end.push(length)
+            }
+            for ends_at in nodes_end.into_iter().rev() {
+                buf.write_all(&ends_at.to_le_bytes())?;
+                length += USIZE_LEN;
+            }
+            let EntryPoint { node, layer } = entry_point;
+            buf.write_all(&layer.to_le_bytes())?;
+            buf.write_all(&node.0.to_le_bytes())?;
+            let _length = length + 2 * USIZE_LEN;
+            buf.flush()?;
+        }
+        Ok(())
+    }
+    // hnsw must be serialized using DiskHnsw, may have trailing bytes at the start.
+    pub fn get_entry_point(hnsw: &[u8]) -> Option<EntryPoint> {
+        if !hnsw.is_empty() {
+            let node_start = hnsw.len() - USIZE_LEN;
+            let layer_start = node_start - USIZE_LEN;
+            let node = usize_from_slice_le(&hnsw[node_start..(node_start + USIZE_LEN)]);
+            let layer = usize_from_slice_le(&hnsw[layer_start..(layer_start + USIZE_LEN)]);
+            Some(EntryPoint {
+                node: Address(node),
+                layer,
+            })
+        } else {
+            None
+        }
+    }
+    // hnsw must be serialized using MHnsw, may have trailing bytes at the start.
+    // The returned node will have trailing bytes at the start.
+    pub fn get_node(hnsw: &[u8], node: usize) -> &[u8] {
+        let indexing_end = hnsw.len() - (2 * USIZE_LEN);
+        // node + 1 since the layers are stored in reverse order.
+        // [n3, n2, n1, n0, end] Since we have the position of end, the node i is
+        // i + 1 positions to its left.
+        let pos = indexing_end - ((node + 1) * USIZE_LEN);
+        let node_end = usize_from_slice_le(&hnsw[pos..(pos + USIZE_LEN)]);
+        &hnsw[..node_end]
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::data_point::ram_hnsw::RAMLayer;
+    fn layer_check<L: Layer>(buf: L, no_nodes: usize, cnx: &[Vec<(Address, Edge)>]) {
+        let no_cnx = vec![];
+        for i in 0..no_nodes {
+            let expected = cnx.get(i).unwrap_or(&no_cnx);
+            let got: Vec<_> = buf.get_out_edges(Address(i)).collect();
+            assert_eq!(expected, &got);
+        }
+    }
+    #[test]
+    fn empty_hnsw() {
+        let hnsw = RAMHnsw::new();
+        let mut buf = vec![];
+        DiskHnsw::serialize_into(&mut buf, 0, hnsw).unwrap();
+        let ep = DiskHnsw::get_entry_point(&buf);
+        assert_eq!(ep, None);
+    }
+
+    #[test]
+    fn hnsw_test() {
+        let no_nodes = 3;
+        let cnx0 = vec![
+            vec![(Address(1), Edge { dist: 1.0 })],
+            vec![(Address(2), Edge { dist: 2.0 })],
+            vec![(Address(3), Edge { dist: 3.0 })],
+        ];
+        let layer0 = RAMLayer {
+            out: cnx0
+                .iter()
+                .enumerate()
+                .map(|(i, c)| (Address(i), c.clone()))
+                .collect(),
+        };
+        let cnx1 = vec![
+            vec![(Address(1), Edge { dist: 4.0 })],
+            vec![(Address(2), Edge { dist: 5.0 })],
+        ];
+        let layer1 = RAMLayer {
+            out: cnx1
+                .iter()
+                .enumerate()
+                .map(|(i, c)| (Address(i), c.clone()))
+                .collect(),
+        };
+        let cnx2 = vec![vec![(Address(1), Edge { dist: 6.0 })]];
+        let layer2 = RAMLayer {
+            out: cnx2
+                .iter()
+                .enumerate()
+                .map(|(i, c)| (Address(i), c.clone()))
+                .collect(),
+        };
+        let entry_point = EntryPoint {
+            node: Address(0),
+            layer: 2,
+        };
+        let mut hnsw = RAMHnsw::new();
+        hnsw.entry_point = Some(entry_point);
+        hnsw.layers = vec![layer0, layer1, layer2];
+        let mut buf = vec![];
+        DiskHnsw::serialize_into(&mut buf, no_nodes, hnsw).unwrap();
+        let ep = DiskHnsw::get_entry_point(&buf).unwrap();
+        assert_eq!(ep, entry_point);
+        let layer0 = buf.as_slice().get_layer(0);
+        layer_check(layer0, no_nodes, &cnx0);
+        let layer1 = buf.as_slice().get_layer(1);
+        layer_check(layer1, no_nodes, &cnx1);
+        let layer2 = buf.as_slice().get_layer(2);
+        layer_check(layer2, no_nodes, &cnx2);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/mod.rs`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,527 +1,527 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod disk_hnsw;
-pub mod node;
-pub mod ops_hnsw;
-pub mod ram_hnsw;
-#[cfg(test)]
-mod tests;
-
-use std::time::SystemTime;
-use std::{fs, io, path};
-
-use disk_hnsw::DiskHnsw;
-use io::{BufWriter, Write};
-use key_value::Slot;
-use memmap2::Mmap;
-use node::Node;
-pub use ops_hnsw::DataRetriever;
-use ops_hnsw::HnswOps;
-use ram_hnsw::RAMHnsw;
-use serde::{Deserialize, Serialize};
-pub use uuid::Uuid as DpId;
-
-use crate::data_types::{key_value, trie, trie_ram, vector, DeleteLog};
-use crate::formula::Formula;
-use crate::VectorR;
-
-mod file_names {
-    pub const NODES: &str = "nodes.kv";
-    pub const HNSW: &str = "index.hnsw";
-    pub const JOURNAL: &str = "journal.json";
-}
-
-pub struct NoDLog;
-impl DeleteLog for NoDLog {
-    fn is_deleted(&self, _: &[u8]) -> bool {
-        false
-    }
-}
-
-#[derive(Default, Debug, Clone, Copy, Serialize, Deserialize)]
-pub enum Similarity {
-    Dot,
-    #[default]
-    Cosine,
-}
-impl Similarity {
-    pub fn compute(&self, x: &[u8], y: &[u8]) -> f32 {
-        match self {
-            Similarity::Cosine => vector::cosine_similarity(x, y),
-            Similarity::Dot => vector::dot_similarity(x, y),
-        }
-    }
-}
-
-#[derive(Clone, Copy, Serialize, Deserialize, Debug)]
-pub struct Journal {
-    uid: DpId,
-    nodes: usize,
-    ctime: SystemTime,
-}
-impl Journal {
-    pub fn id(&self) -> DpId {
-        self.uid
-    }
-    pub fn no_nodes(&self) -> usize {
-        self.nodes
-    }
-    pub fn time(&self) -> SystemTime {
-        self.ctime
-    }
-    pub fn update_time(&mut self, time: SystemTime) {
-        self.ctime = time;
-    }
-}
-
-#[derive(
-    Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize, Default,
-)]
-pub struct Address(usize);
-impl Address {
-    #[cfg(test)]
-    pub const fn dummy() -> Address {
-        Address(0)
-    }
-}
-
-pub struct Retriever<'a, Dlog> {
-    similarity: Similarity,
-    no_nodes: usize,
-    temp: &'a [u8],
-    nodes: &'a Mmap,
-    delete_log: &'a Dlog,
-}
-impl<'a, Dlog: DeleteLog> Retriever<'a, Dlog> {
-    pub fn new(
-        temp: &'a [u8],
-        nodes: &'a Mmap,
-        delete_log: &'a Dlog,
-        similarity: Similarity,
-    ) -> Retriever<'a, Dlog> {
-        Retriever {
-            temp,
-            nodes,
-            delete_log,
-            similarity,
-            no_nodes: key_value::get_no_elems(nodes),
-        }
-    }
-    fn find_node(&self, Address(x): Address) -> &[u8] {
-        if x == self.no_nodes {
-            self.temp
-        } else {
-            key_value::get_value(Node, self.nodes, x)
-        }
-    }
-}
-
-impl<'a, Dlog: DeleteLog> DataRetriever for Retriever<'a, Dlog> {
-    fn get_vector(&self, x @ Address(addr): Address) -> &[u8] {
-        if addr == self.no_nodes {
-            self.temp
-        } else {
-            let x = self.find_node(x);
-            Node::vector(x)
-        }
-    }
-    fn is_deleted(&self, x @ Address(addr): Address) -> bool {
-        if addr == self.no_nodes {
-            false
-        } else {
-            let x = self.find_node(x);
-            let key = Node::key(x);
-            self.delete_log.is_deleted(key)
-        }
-    }
-    fn has_label(&self, Address(x): Address, label: &[u8]) -> bool {
-        if x == self.no_nodes {
-            false
-        } else {
-            let x = key_value::get_value(Node, self.nodes, x);
-            Node::has_label(x, label)
-        }
-    }
-    fn similarity(&self, x @ Address(a0): Address, y @ Address(a1): Address) -> f32 {
-        if a0 == self.no_nodes {
-            let y = self.find_node(y);
-            let y = Node::vector(y);
-            self.similarity.compute(self.temp, y)
-        } else if a1 == self.no_nodes {
-            let x = self.find_node(x);
-            let x = Node::vector(x);
-            self.similarity.compute(self.temp, x)
-        } else {
-            let x = self.find_node(x);
-            let y = self.find_node(y);
-            let x = Node::vector(x);
-            let y = Node::vector(y);
-            self.similarity.compute(x, y)
-        }
-    }
-}
-
-#[derive(Clone, Debug)]
-pub struct LabelDictionary(Vec<u8>);
-impl Default for LabelDictionary {
-    fn default() -> Self {
-        LabelDictionary::new(vec![])
-    }
-}
-impl LabelDictionary {
-    pub fn new(mut labels: Vec<String>) -> LabelDictionary {
-        labels.sort();
-        let ram_trie = trie_ram::create_trie(&labels);
-        LabelDictionary(trie::serialize(ram_trie))
-    }
-}
-#[derive(Clone, Debug)]
-pub struct Elem {
-    pub key: Vec<u8>,
-    pub vector: Vec<u8>,
-    pub metadata: Option<Vec<u8>>,
-    pub labels: LabelDictionary,
-}
-impl Elem {
-    pub fn new(
-        key: String,
-        vector: Vec<f32>,
-        labels: LabelDictionary,
-        metadata: Option<Vec<u8>>,
-    ) -> Elem {
-        Elem {
-            labels,
-            metadata,
-            key: key.as_bytes().to_vec(),
-            vector: vector::encode_vector(&vector),
-        }
-    }
-}
-
-impl key_value::KVElem for Elem {
-    fn serialized_len(&self) -> usize {
-        Node::serialized_len(
-            &self.key,
-            &self.vector,
-            &self.labels.0,
-            self.metadata.as_ref(),
-        )
-    }
-    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()> {
-        Node::serialize_into(
-            w,
-            self.key,
-            self.vector,
-            self.labels.0,
-            self.metadata.as_ref(),
-        )
-    }
-}
-
-#[derive(Debug, Clone)]
-pub struct Neighbour {
-    score: f32,
-    node: Vec<u8>,
-}
-impl Eq for Neighbour {}
-impl std::hash::Hash for Neighbour {
-    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
-        self.node.hash(state);
-    }
-}
-impl Ord for Neighbour {
-    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
-        self.node.cmp(&other.node)
-    }
-}
-impl PartialOrd for Neighbour {
-    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
-        self.node.partial_cmp(&other.node)
-    }
-}
-impl PartialEq for Neighbour {
-    fn eq(&self, other: &Self) -> bool {
-        self.node == other.node
-    }
-}
-
-impl Neighbour {
-    #[cfg(test)]
-    pub fn dummy_neighbour(node: &[u8], score: f32) -> Neighbour {
-        Neighbour {
-            score,
-            node: node.to_vec(),
-        }
-    }
-    fn new(Address(addr): Address, data: &[u8], score: f32) -> Neighbour {
-        let node = key_value::get_value(Node, data, addr);
-        let (exact, _) = Node.read_exact(node);
-        Neighbour {
-            score,
-            node: exact.to_vec(),
-        }
-    }
-    pub fn score(&self) -> f32 {
-        self.score
-    }
-    pub fn id(&self) -> &[u8] {
-        Node.get_key(&self.node)
-    }
-    pub fn labels(&self) -> Vec<String> {
-        Node::labels(&self.node)
-    }
-    pub fn metadata(&self) -> Option<&[u8]> {
-        let metadata = Node::metadata(&self.node);
-        if metadata.is_empty() {
-            None
-        } else {
-            Some(metadata)
-        }
-    }
-}
-
-pub struct DataPoint {
-    journal: Journal,
-    nodes: Mmap,
-    index: Mmap,
-}
-
-impl AsRef<DataPoint> for DataPoint {
-    fn as_ref(&self) -> &DataPoint {
-        self
-    }
-}
-
-impl DataPoint {
-    pub fn get_id(&self) -> DpId {
-        self.journal.uid
-    }
-    pub fn meta(&self) -> Journal {
-        self.journal
-    }
-    pub fn get_keys<Dlog: DeleteLog>(&self, delete_log: &Dlog) -> Vec<String> {
-        key_value::get_keys(Node, &self.nodes)
-            .filter(|k| !delete_log.is_deleted(k))
-            .map(String::from_utf8_lossy)
-            .map(|s| s.to_string())
-            .collect()
-    }
-    pub fn search<Dlog: DeleteLog>(
-        &self,
-        delete_log: &Dlog,
-        query: &[f32],
-        filter: &Formula,
-        with_duplicates: bool,
-        results: usize,
-        similarity: Similarity,
-    ) -> impl Iterator<Item = Neighbour> + '_ {
-        let encoded_query = vector::encode_vector(query);
-        let tracker = Retriever::new(&encoded_query, &self.nodes, delete_log, similarity);
-        let ops = HnswOps::new(&tracker);
-        let neighbours = ops.search(
-            Address(self.journal.nodes),
-            self.index.as_ref(),
-            results,
-            filter,
-            with_duplicates,
-        );
-        neighbours
-            .into_iter()
-            .map(|(address, dist)| (Neighbour::new(address, &self.nodes, dist)))
-            .take(results)
-    }
-    pub fn merge<Dlog>(
-        dir: &path::Path,
-        operants: &[(Dlog, DpId)],
-        similarity: Similarity,
-    ) -> VectorR<DataPoint>
-    where
-        Dlog: DeleteLog,
-    {
-        let uid = DpId::new_v4().to_string();
-        let id = dir.join(&uid);
-        fs::create_dir(&id)?;
-        let mut nodes = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::NODES))?;
-        let mut journalf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::JOURNAL))?;
-        let mut hnswf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::HNSW))?;
-        let operants = operants
-            .iter()
-            .map(|(dlog, dp_id)| DataPoint::open(dir, *dp_id).map(|v| (dlog, v)))
-            .collect::<VectorR<Vec<_>>>()?;
-        let node_producers = operants
-            .iter()
-            .map(|dp| ((dp.0, Node), dp.1.nodes.as_ref()));
-        {
-            let mut node_buffer = BufWriter::new(&mut nodes);
-            key_value::merge(&mut node_buffer, node_producers.collect())?;
-            node_buffer.flush()?;
-        }
-
-        let nodes = unsafe { Mmap::map(&nodes)? };
-        let no_nodes = key_value::get_no_elems(&nodes);
-        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity);
-        let mut ops = HnswOps::new(&tracker);
-        let mut index = RAMHnsw::new();
-        for id in 0..no_nodes {
-            ops.insert(Address(id), &mut index)
-        }
-
-        {
-            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
-            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
-            hnswf_buffer.flush()?;
-        }
-
-        let index = unsafe { Mmap::map(&hnswf)? };
-
-        let journal = Journal {
-            nodes: no_nodes,
-            uid: DpId::parse_str(&uid).unwrap(),
-            ctime: SystemTime::now(),
-        };
-
-        {
-            let mut journalf_buffer = BufWriter::new(&mut journalf);
-            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
-            journalf_buffer.flush()?;
-        }
-
-        // Mark it as a Datapoint in progress, since it needs to be commited.
-
-        Ok(DataPoint {
-            journal,
-            nodes,
-            index,
-        })
-    }
-    pub fn delete(dir: &path::Path, uid: DpId) -> VectorR<()> {
-        let uid = uid.to_string();
-        let id = dir.join(uid);
-        fs::remove_dir_all(id)?;
-        Ok(())
-    }
-    pub fn open(dir: &path::Path, uid: DpId) -> VectorR<DataPoint> {
-        let uid = uid.to_string();
-        let id = dir.join(uid);
-        let nodes = fs::OpenOptions::new()
-            .read(true)
-            .open(id.join(file_names::NODES))?;
-        let journal = fs::OpenOptions::new()
-            .read(true)
-            .open(id.join(file_names::JOURNAL))?;
-        let hnswf = fs::OpenOptions::new()
-            .read(true)
-            .open(id.join(file_names::HNSW))?;
-
-        let nodes = unsafe { Mmap::map(&nodes)? };
-        let index = unsafe { Mmap::map(&hnswf)? };
-        let journal: Journal = serde_json::from_reader(journal)?;
-        Ok(DataPoint {
-            journal,
-            nodes,
-            index,
-        })
-    }
-    pub fn new(
-        dir: &path::Path,
-        mut elems: Vec<Elem>,
-        time: Option<SystemTime>,
-        similarity: Similarity,
-    ) -> VectorR<DataPoint> {
-        let uid = DpId::new_v4().to_string();
-        let id = dir.join(&uid);
-        fs::create_dir(&id)?;
-        let mut nodesf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::NODES))?;
-        let mut journalf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::JOURNAL))?;
-        let mut hnswf = fs::OpenOptions::new()
-            .read(true)
-            .write(true)
-            .create(true)
-            .open(id.join(file_names::HNSW))?;
-
-        elems.sort_by(|a, b| a.key.cmp(&b.key));
-        elems.dedup_by(|a, b| a.key.cmp(&b.key).is_eq());
-        {
-            // Serializing nodes on disk
-            // Nodes are stored on disk and mmaped.
-            let mut nodesf_buffer = BufWriter::new(&mut nodesf);
-            key_value::create_key_value(&mut nodesf_buffer, elems)?;
-            nodesf_buffer.flush()?;
-        }
-        let nodes = unsafe { Mmap::map(&nodesf)? };
-        let no_nodes = key_value::get_no_elems(&nodes);
-
-        // Creating the HNSW using the mmaped nodes
-        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity);
-        let mut ops = HnswOps::new(&tracker);
-        let mut index = RAMHnsw::new();
-        for id in 0..no_nodes {
-            ops.insert(Address(id), &mut index)
-        }
-
-        {
-            // The HNSW is on RAM
-            // Serializing the HNSW into disk
-            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
-            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
-            hnswf_buffer.flush()?;
-        }
-        let index = unsafe { Mmap::map(&hnswf)? };
-
-        let journal = Journal {
-            nodes: no_nodes,
-            uid: DpId::parse_str(&uid).unwrap(),
-            ctime: time.unwrap_or_else(SystemTime::now),
-        };
-        {
-            // Saving the journal
-            let mut journalf_buffer = BufWriter::new(&mut journalf);
-            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
-            journalf_buffer.flush()?;
-        }
-
-        Ok(DataPoint {
-            journal,
-            nodes,
-            index,
-        })
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod disk_hnsw;
+pub mod node;
+pub mod ops_hnsw;
+pub mod ram_hnsw;
+#[cfg(test)]
+mod tests;
+
+use std::time::SystemTime;
+use std::{fs, io, path};
+
+use disk_hnsw::DiskHnsw;
+use io::{BufWriter, Write};
+use key_value::Slot;
+use memmap2::Mmap;
+use node::Node;
+pub use ops_hnsw::DataRetriever;
+use ops_hnsw::HnswOps;
+use ram_hnsw::RAMHnsw;
+use serde::{Deserialize, Serialize};
+pub use uuid::Uuid as DpId;
+
+use crate::data_types::{key_value, trie, trie_ram, vector, DeleteLog};
+use crate::formula::Formula;
+use crate::VectorR;
+
+mod file_names {
+    pub const NODES: &str = "nodes.kv";
+    pub const HNSW: &str = "index.hnsw";
+    pub const JOURNAL: &str = "journal.json";
+}
+
+pub struct NoDLog;
+impl DeleteLog for NoDLog {
+    fn is_deleted(&self, _: &[u8]) -> bool {
+        false
+    }
+}
+
+#[derive(Default, Debug, Clone, Copy, Serialize, Deserialize)]
+pub enum Similarity {
+    Dot,
+    #[default]
+    Cosine,
+}
+impl Similarity {
+    pub fn compute(&self, x: &[u8], y: &[u8]) -> f32 {
+        match self {
+            Similarity::Cosine => vector::cosine_similarity(x, y),
+            Similarity::Dot => vector::dot_similarity(x, y),
+        }
+    }
+}
+
+#[derive(Clone, Copy, Serialize, Deserialize, Debug)]
+pub struct Journal {
+    uid: DpId,
+    nodes: usize,
+    ctime: SystemTime,
+}
+impl Journal {
+    pub fn id(&self) -> DpId {
+        self.uid
+    }
+    pub fn no_nodes(&self) -> usize {
+        self.nodes
+    }
+    pub fn time(&self) -> SystemTime {
+        self.ctime
+    }
+    pub fn update_time(&mut self, time: SystemTime) {
+        self.ctime = time;
+    }
+}
+
+#[derive(
+    Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize, Default,
+)]
+pub struct Address(usize);
+impl Address {
+    #[cfg(test)]
+    pub const fn dummy() -> Address {
+        Address(0)
+    }
+}
+
+pub struct Retriever<'a, Dlog> {
+    similarity: Similarity,
+    no_nodes: usize,
+    temp: &'a [u8],
+    nodes: &'a Mmap,
+    delete_log: &'a Dlog,
+}
+impl<'a, Dlog: DeleteLog> Retriever<'a, Dlog> {
+    pub fn new(
+        temp: &'a [u8],
+        nodes: &'a Mmap,
+        delete_log: &'a Dlog,
+        similarity: Similarity,
+    ) -> Retriever<'a, Dlog> {
+        Retriever {
+            temp,
+            nodes,
+            delete_log,
+            similarity,
+            no_nodes: key_value::get_no_elems(nodes),
+        }
+    }
+    fn find_node(&self, Address(x): Address) -> &[u8] {
+        if x == self.no_nodes {
+            self.temp
+        } else {
+            key_value::get_value(Node, self.nodes, x)
+        }
+    }
+}
+
+impl<'a, Dlog: DeleteLog> DataRetriever for Retriever<'a, Dlog> {
+    fn get_vector(&self, x @ Address(addr): Address) -> &[u8] {
+        if addr == self.no_nodes {
+            self.temp
+        } else {
+            let x = self.find_node(x);
+            Node::vector(x)
+        }
+    }
+    fn is_deleted(&self, x @ Address(addr): Address) -> bool {
+        if addr == self.no_nodes {
+            false
+        } else {
+            let x = self.find_node(x);
+            let key = Node::key(x);
+            self.delete_log.is_deleted(key)
+        }
+    }
+    fn has_label(&self, Address(x): Address, label: &[u8]) -> bool {
+        if x == self.no_nodes {
+            false
+        } else {
+            let x = key_value::get_value(Node, self.nodes, x);
+            Node::has_label(x, label)
+        }
+    }
+    fn similarity(&self, x @ Address(a0): Address, y @ Address(a1): Address) -> f32 {
+        if a0 == self.no_nodes {
+            let y = self.find_node(y);
+            let y = Node::vector(y);
+            self.similarity.compute(self.temp, y)
+        } else if a1 == self.no_nodes {
+            let x = self.find_node(x);
+            let x = Node::vector(x);
+            self.similarity.compute(self.temp, x)
+        } else {
+            let x = self.find_node(x);
+            let y = self.find_node(y);
+            let x = Node::vector(x);
+            let y = Node::vector(y);
+            self.similarity.compute(x, y)
+        }
+    }
+}
+
+#[derive(Clone, Debug)]
+pub struct LabelDictionary(Vec<u8>);
+impl Default for LabelDictionary {
+    fn default() -> Self {
+        LabelDictionary::new(vec![])
+    }
+}
+impl LabelDictionary {
+    pub fn new(mut labels: Vec<String>) -> LabelDictionary {
+        labels.sort();
+        let ram_trie = trie_ram::create_trie(&labels);
+        LabelDictionary(trie::serialize(ram_trie))
+    }
+}
+#[derive(Clone, Debug)]
+pub struct Elem {
+    pub key: Vec<u8>,
+    pub vector: Vec<u8>,
+    pub metadata: Option<Vec<u8>>,
+    pub labels: LabelDictionary,
+}
+impl Elem {
+    pub fn new(
+        key: String,
+        vector: Vec<f32>,
+        labels: LabelDictionary,
+        metadata: Option<Vec<u8>>,
+    ) -> Elem {
+        Elem {
+            labels,
+            metadata,
+            key: key.as_bytes().to_vec(),
+            vector: vector::encode_vector(&vector),
+        }
+    }
+}
+
+impl key_value::KVElem for Elem {
+    fn serialized_len(&self) -> usize {
+        Node::serialized_len(
+            &self.key,
+            &self.vector,
+            &self.labels.0,
+            self.metadata.as_ref(),
+        )
+    }
+    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()> {
+        Node::serialize_into(
+            w,
+            self.key,
+            self.vector,
+            self.labels.0,
+            self.metadata.as_ref(),
+        )
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct Neighbour {
+    score: f32,
+    node: Vec<u8>,
+}
+impl Eq for Neighbour {}
+impl std::hash::Hash for Neighbour {
+    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
+        self.node.hash(state);
+    }
+}
+impl Ord for Neighbour {
+    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
+        self.node.cmp(&other.node)
+    }
+}
+impl PartialOrd for Neighbour {
+    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
+        self.node.partial_cmp(&other.node)
+    }
+}
+impl PartialEq for Neighbour {
+    fn eq(&self, other: &Self) -> bool {
+        self.node == other.node
+    }
+}
+
+impl Neighbour {
+    #[cfg(test)]
+    pub fn dummy_neighbour(node: &[u8], score: f32) -> Neighbour {
+        Neighbour {
+            score,
+            node: node.to_vec(),
+        }
+    }
+    fn new(Address(addr): Address, data: &[u8], score: f32) -> Neighbour {
+        let node = key_value::get_value(Node, data, addr);
+        let (exact, _) = Node.read_exact(node);
+        Neighbour {
+            score,
+            node: exact.to_vec(),
+        }
+    }
+    pub fn score(&self) -> f32 {
+        self.score
+    }
+    pub fn id(&self) -> &[u8] {
+        Node.get_key(&self.node)
+    }
+    pub fn labels(&self) -> Vec<String> {
+        Node::labels(&self.node)
+    }
+    pub fn metadata(&self) -> Option<&[u8]> {
+        let metadata = Node::metadata(&self.node);
+        if metadata.is_empty() {
+            None
+        } else {
+            Some(metadata)
+        }
+    }
+}
+
+pub struct DataPoint {
+    journal: Journal,
+    nodes: Mmap,
+    index: Mmap,
+}
+
+impl AsRef<DataPoint> for DataPoint {
+    fn as_ref(&self) -> &DataPoint {
+        self
+    }
+}
+
+impl DataPoint {
+    pub fn get_id(&self) -> DpId {
+        self.journal.uid
+    }
+    pub fn meta(&self) -> Journal {
+        self.journal
+    }
+    pub fn get_keys<Dlog: DeleteLog>(&self, delete_log: &Dlog) -> Vec<String> {
+        key_value::get_keys(Node, &self.nodes)
+            .filter(|k| !delete_log.is_deleted(k))
+            .map(String::from_utf8_lossy)
+            .map(|s| s.to_string())
+            .collect()
+    }
+    pub fn search<Dlog: DeleteLog>(
+        &self,
+        delete_log: &Dlog,
+        query: &[f32],
+        filter: &Formula,
+        with_duplicates: bool,
+        results: usize,
+        similarity: Similarity,
+    ) -> impl Iterator<Item = Neighbour> + '_ {
+        let encoded_query = vector::encode_vector(query);
+        let tracker = Retriever::new(&encoded_query, &self.nodes, delete_log, similarity);
+        let ops = HnswOps::new(&tracker);
+        let neighbours = ops.search(
+            Address(self.journal.nodes),
+            self.index.as_ref(),
+            results,
+            filter,
+            with_duplicates,
+        );
+        neighbours
+            .into_iter()
+            .map(|(address, dist)| (Neighbour::new(address, &self.nodes, dist)))
+            .take(results)
+    }
+    pub fn merge<Dlog>(
+        dir: &path::Path,
+        operants: &[(Dlog, DpId)],
+        similarity: Similarity,
+    ) -> VectorR<DataPoint>
+    where
+        Dlog: DeleteLog,
+    {
+        let uid = DpId::new_v4().to_string();
+        let id = dir.join(&uid);
+        fs::create_dir(&id)?;
+        let mut nodes = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::NODES))?;
+        let mut journalf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::JOURNAL))?;
+        let mut hnswf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::HNSW))?;
+        let operants = operants
+            .iter()
+            .map(|(dlog, dp_id)| DataPoint::open(dir, *dp_id).map(|v| (dlog, v)))
+            .collect::<VectorR<Vec<_>>>()?;
+        let node_producers = operants
+            .iter()
+            .map(|dp| ((dp.0, Node), dp.1.nodes.as_ref()));
+        {
+            let mut node_buffer = BufWriter::new(&mut nodes);
+            key_value::merge(&mut node_buffer, node_producers.collect())?;
+            node_buffer.flush()?;
+        }
+
+        let nodes = unsafe { Mmap::map(&nodes)? };
+        let no_nodes = key_value::get_no_elems(&nodes);
+        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity);
+        let mut ops = HnswOps::new(&tracker);
+        let mut index = RAMHnsw::new();
+        for id in 0..no_nodes {
+            ops.insert(Address(id), &mut index)
+        }
+
+        {
+            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
+            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
+            hnswf_buffer.flush()?;
+        }
+
+        let index = unsafe { Mmap::map(&hnswf)? };
+
+        let journal = Journal {
+            nodes: no_nodes,
+            uid: DpId::parse_str(&uid).unwrap(),
+            ctime: SystemTime::now(),
+        };
+
+        {
+            let mut journalf_buffer = BufWriter::new(&mut journalf);
+            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
+            journalf_buffer.flush()?;
+        }
+
+        // Mark it as a Datapoint in progress, since it needs to be commited.
+
+        Ok(DataPoint {
+            journal,
+            nodes,
+            index,
+        })
+    }
+    pub fn delete(dir: &path::Path, uid: DpId) -> VectorR<()> {
+        let uid = uid.to_string();
+        let id = dir.join(uid);
+        fs::remove_dir_all(id)?;
+        Ok(())
+    }
+    pub fn open(dir: &path::Path, uid: DpId) -> VectorR<DataPoint> {
+        let uid = uid.to_string();
+        let id = dir.join(uid);
+        let nodes = fs::OpenOptions::new()
+            .read(true)
+            .open(id.join(file_names::NODES))?;
+        let journal = fs::OpenOptions::new()
+            .read(true)
+            .open(id.join(file_names::JOURNAL))?;
+        let hnswf = fs::OpenOptions::new()
+            .read(true)
+            .open(id.join(file_names::HNSW))?;
+
+        let nodes = unsafe { Mmap::map(&nodes)? };
+        let index = unsafe { Mmap::map(&hnswf)? };
+        let journal: Journal = serde_json::from_reader(journal)?;
+        Ok(DataPoint {
+            journal,
+            nodes,
+            index,
+        })
+    }
+    pub fn new(
+        dir: &path::Path,
+        mut elems: Vec<Elem>,
+        time: Option<SystemTime>,
+        similarity: Similarity,
+    ) -> VectorR<DataPoint> {
+        let uid = DpId::new_v4().to_string();
+        let id = dir.join(&uid);
+        fs::create_dir(&id)?;
+        let mut nodesf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::NODES))?;
+        let mut journalf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::JOURNAL))?;
+        let mut hnswf = fs::OpenOptions::new()
+            .read(true)
+            .write(true)
+            .create(true)
+            .open(id.join(file_names::HNSW))?;
+
+        elems.sort_by(|a, b| a.key.cmp(&b.key));
+        elems.dedup_by(|a, b| a.key.cmp(&b.key).is_eq());
+        {
+            // Serializing nodes on disk
+            // Nodes are stored on disk and mmaped.
+            let mut nodesf_buffer = BufWriter::new(&mut nodesf);
+            key_value::create_key_value(&mut nodesf_buffer, elems)?;
+            nodesf_buffer.flush()?;
+        }
+        let nodes = unsafe { Mmap::map(&nodesf)? };
+        let no_nodes = key_value::get_no_elems(&nodes);
+
+        // Creating the HNSW using the mmaped nodes
+        let tracker = Retriever::new(&[], &nodes, &NoDLog, similarity);
+        let mut ops = HnswOps::new(&tracker);
+        let mut index = RAMHnsw::new();
+        for id in 0..no_nodes {
+            ops.insert(Address(id), &mut index)
+        }
+
+        {
+            // The HNSW is on RAM
+            // Serializing the HNSW into disk
+            let mut hnswf_buffer = BufWriter::new(&mut hnswf);
+            DiskHnsw::serialize_into(&mut hnswf_buffer, no_nodes, index)?;
+            hnswf_buffer.flush()?;
+        }
+        let index = unsafe { Mmap::map(&hnswf)? };
+
+        let journal = Journal {
+            nodes: no_nodes,
+            uid: DpId::parse_str(&uid).unwrap(),
+            ctime: time.unwrap_or_else(SystemTime::now),
+        };
+        {
+            // Saving the journal
+            let mut journalf_buffer = BufWriter::new(&mut journalf);
+            journalf_buffer.write_all(&serde_json::to_vec(&journal)?)?;
+            journalf_buffer.flush()?;
+        }
+
+        Ok(DataPoint {
+            journal,
+            nodes,
+            index,
+        })
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/node.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/node.rs`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,284 +1,284 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io;
-
-use crate::data_types::key_value::Slot;
-use crate::data_types::trie;
-use crate::data_types::usize_utils::*;
-
-// Nodes are the main element of the system. The following data is stored inside them:
-// -> vector: Vec<u8> used for building a hnsw index with them (is a serialized Vec<f32>).
-// -> key: String assigned by the user to identify the node.
-// Once a node is persisted by the system it will be dentified by a pointer. This pointer represents
-// the start of the serialized node in the file (or other element) it is serialized into. Therefore
-// nodes are used by the system in their serialized form, which is:
-// len: number of bytes representing this node. (usize in little endian)
-// vector_start: byte where the vector segment starts. (usize in little endian)
-// key_start: byte where the key segment starts. (usize in little endian)
-// label_start: byte where the label segment starts. (usize in little endian)
-// [Block of metadata, may be empty]: Everything between the header and the content is consider metadata.
-// vector segment:
-// - len: size of the segment. (usize in little endian)
-// - value: the vector.
-// string segment:
-// - len: size of the segment. (usize in little endian)
-// - value: the serialized string.
-// label segment: trie
-
-const LEN: (usize, usize) = (0, USIZE_LEN);
-const VECTOR_START: (usize, usize) = (LEN.1, LEN.1 + USIZE_LEN);
-const KEY_START: (usize, usize) = (VECTOR_START.1, VECTOR_START.1 + USIZE_LEN);
-const LABEL_START: (usize, usize) = (KEY_START.1, KEY_START.1 + USIZE_LEN);
-const HEADER_LEN: usize = 4 * USIZE_LEN;
-
-#[derive(Clone, Copy)]
-pub struct Node;
-impl Node {
-    pub fn serialized_len<S, V, T, M>(key: S, vector: V, trie: T, metadata: Option<M>) -> usize
-    where
-        S: AsRef<[u8]>,
-        V: AsRef<[u8]>,
-        T: AsRef<[u8]>,
-        M: AsRef<[u8]>,
-    {
-        let skey = key.as_ref();
-        let svector = vector.as_ref();
-        let strie = trie.as_ref();
-        let svector_len = svector.len() + USIZE_LEN;
-        let skey_len = skey.len() + USIZE_LEN;
-        let slabels_len = strie.len();
-        let metadata_len = metadata.map(|m| m.as_ref().len()).unwrap_or_default();
-        HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len
-    }
-    pub fn serialize<S, V, T, M>(key: S, vector: V, labels: T, metadata: Option<M>) -> Vec<u8>
-    where
-        S: AsRef<[u8]>,
-        V: AsRef<[u8]>,
-        T: AsRef<[u8]>,
-        M: AsRef<[u8]>,
-    {
-        let mut buf = vec![];
-        Node::serialize_into(&mut buf, key, vector, labels, metadata).unwrap();
-        buf
-    }
-    // labels must be sorted.
-    pub fn serialize_into<W, S, V, T, M>(
-        mut w: W,
-        key: S,
-        vector: V,
-        trie: T,
-        metadata: Option<M>,
-    ) -> io::Result<()>
-    where
-        W: io::Write,
-        S: AsRef<[u8]>,
-        V: AsRef<[u8]>,
-        T: AsRef<[u8]>,
-        M: AsRef<[u8]>,
-    {
-        let skey = key.as_ref();
-        let svector = vector.as_ref();
-        let strie = trie.as_ref();
-
-        // Reading lens
-        let svector_len = svector.len() + USIZE_LEN;
-        let skey_len = skey.len() + USIZE_LEN;
-        let slabels_len = strie.len();
-        let metadata_len = metadata
-            .as_ref()
-            .map(|m| m.as_ref().len())
-            .unwrap_or_default();
-
-        // Pointer computations
-        let len = HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len;
-        let vector_start = HEADER_LEN + metadata_len;
-        let key_start = vector_start + svector_len;
-        let labels_start = key_start + skey_len;
-
-        // Write pointers
-        w.write_all(&len.to_le_bytes())?;
-        w.write_all(&vector_start.to_le_bytes())?;
-        w.write_all(&key_start.to_le_bytes())?;
-        w.write_all(&labels_start.to_le_bytes())?;
-        // Metadata segment
-        metadata.map_or(Ok(()), |m| w.write_all(m.as_ref()))?;
-        // Values
-        w.write_all(&svector.len().to_le_bytes())?;
-        w.write_all(svector)?;
-        w.write_all(&skey.len().to_le_bytes())?;
-        w.write_all(skey)?;
-        w.write_all(strie)?;
-        w.flush()
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn metadata(x: &[u8]) -> &[u8] {
-        // The metadata starts just after the header ends.
-        let metadata_start = LABEL_START.1;
-        // The metadata ends when the vector segment starts.
-        let metadata_end = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
-        &x[metadata_start..metadata_end]
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    // This function will decompress the trie data structure that contains the
-    // labels. Use only if you need all the labels.
-    pub fn labels(x: &[u8]) -> Vec<String> {
-        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
-        trie::decompress(&x[xlabel_ptr..])
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn key(x: &[u8]) -> &[u8] {
-        let xkey_ptr = usize_from_slice_le(&x[KEY_START.0..KEY_START.1]);
-        let xkey_len = usize_from_slice_le(&x[xkey_ptr..(xkey_ptr + USIZE_LEN)]);
-        let xkey_start = xkey_ptr + USIZE_LEN;
-        &x[xkey_start..(xkey_start + xkey_len)]
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn vector(x: &[u8]) -> &[u8] {
-        let xvec_ptr = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
-        let xvec_len = usize_from_slice_le(&x[xvec_ptr..(xvec_ptr + USIZE_LEN)]);
-        let xvec_start = xvec_ptr + USIZE_LEN;
-        &x[xvec_start..(xvec_start + xvec_len)]
-    }
-    // x must be serialized using Node, may have trailing bytes.
-    pub fn has_label(x: &[u8], label: &[u8]) -> bool {
-        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
-        trie::has_word(&x[xlabel_ptr..], label)
-    }
-}
-impl Slot for Node {
-    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
-        let xkey = self.get_key(x);
-        xkey.cmp(key)
-    }
-    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
-        Self::key(x)
-    }
-    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
-        let len = usize_from_slice_le(&x[LEN.0..LEN.1]);
-        (&x[0..len], &x[len..])
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use crate::data_types::{trie_ram, vector};
-    lazy_static::lazy_static! {
-        static ref NO_LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&NO_LABELS));
-        static ref LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&LABELS));
-    }
-    const NO_LABELS: [&[u8]; 0] = [];
-    const LABELS: [&[u8]; 3] = [b"L1", b"L2", b"L3"];
-    const NO_METADATA: Option<&[u8]> = None;
-
-    #[test]
-    fn create_test() {
-        let key = b"NODE1";
-        let vector = vector::encode_vector(&[12.; 1000]);
-        let mut buf = Vec::new();
-        Node::serialize_into(&mut buf, key, &vector, NO_LABELS_TRIE.clone(), NO_METADATA).unwrap();
-        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
-        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
-        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
-        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
-        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
-        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
-        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
-        let metadata = Node::metadata(&buf);
-        assert_eq!(metadata.len(), 0);
-        assert_eq!(len, buf.len());
-        assert_eq!(vector_len, vector.len());
-        assert_eq!(key_len, key.len());
-        assert_eq!(&buf[svector], &vector);
-        assert_eq!(&buf[skey], key.as_slice());
-        assert_eq!(Node::vector(&buf), &vector);
-        assert_eq!(Node::key(&buf), key);
-
-        let key = b"NODE2";
-        let metadata = b"THIS ARE THE METADATA CONTENTS";
-        let vector = vector::encode_vector(&[13.; 1000]);
-        let mut buf = Vec::new();
-        Node::serialize_into(&mut buf, key, &vector, LABELS_TRIE.clone(), Some(metadata)).unwrap();
-        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
-        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
-        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
-        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
-        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
-        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
-        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
-        let smetadata = Node::metadata(&buf);
-        assert_eq!(smetadata, metadata.as_slice());
-        assert_eq!(len, buf.len());
-        assert_eq!(vector_len, vector.len());
-        assert_eq!(key_len, key.len());
-        assert_eq!(&buf[svector], &vector);
-        assert_eq!(&buf[skey], key.as_slice());
-        assert_eq!(Node::vector(&buf), &vector);
-        assert_eq!(Node::key(&buf), key);
-        assert!(LABELS.iter().all(|l| Node::has_label(&buf, l)));
-    }
-
-    #[test]
-    fn look_up_test() {
-        let mut buf = Vec::new();
-        let key1 = b"NODE1";
-        let metadata1 = b"The node 1 has metadata";
-        let vector1 = vector::encode_vector(&[12.; 1000]);
-        let node1 = buf.len();
-        Node::serialize_into(
-            &mut buf,
-            key1,
-            &vector1,
-            NO_LABELS_TRIE.clone(),
-            Some(&metadata1),
-        )
-        .unwrap();
-        let key2 = b"NODE2";
-        let metadata2 = b"Tuns out node 2 also has metadata";
-        let vector2 = vector::encode_vector(&[15.; 1000]);
-        let node2 = buf.len();
-        Node::serialize_into(
-            &mut buf,
-            key2,
-            &vector2,
-            NO_LABELS_TRIE.clone(),
-            Some(&metadata2),
-        )
-        .unwrap();
-        assert_eq!(Node::key(&buf[node1..]), key1);
-        assert_eq!(Node::key(&buf[node2..]), key2);
-        assert_eq!(Node::vector(&buf[node1..]), vector1);
-        assert_eq!(Node::vector(&buf[node2..]), vector2);
-        assert!(Node.keep_in_merge(&buf[node1..]));
-        assert!(Node.keep_in_merge(&buf[node2..]));
-        assert_eq!(Node.cmp_slot(&buf[node1..], &buf[node2..]), key1.cmp(key2));
-        assert_eq!(Node::metadata(&buf[node1..]), metadata1);
-        assert_eq!(Node::metadata(&buf[node2..]), metadata2);
-        assert_eq!(
-            Node.read_exact(&buf[node1..]),
-            (&buf[node1..node2], &buf[node2..])
-        );
-        assert_eq!(
-            Node.read_exact(&buf[node2..]),
-            (&buf[node2..], [].as_slice())
-        );
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io;
+
+use crate::data_types::key_value::Slot;
+use crate::data_types::trie;
+use crate::data_types::usize_utils::*;
+
+// Nodes are the main element of the system. The following data is stored inside them:
+// -> vector: Vec<u8> used for building a hnsw index with them (is a serialized Vec<f32>).
+// -> key: String assigned by the user to identify the node.
+// Once a node is persisted by the system it will be dentified by a pointer. This pointer represents
+// the start of the serialized node in the file (or other element) it is serialized into. Therefore
+// nodes are used by the system in their serialized form, which is:
+// len: number of bytes representing this node. (usize in little endian)
+// vector_start: byte where the vector segment starts. (usize in little endian)
+// key_start: byte where the key segment starts. (usize in little endian)
+// label_start: byte where the label segment starts. (usize in little endian)
+// [Block of metadata, may be empty]: Everything between the header and the content is consider metadata.
+// vector segment:
+// - len: size of the segment. (usize in little endian)
+// - value: the vector.
+// string segment:
+// - len: size of the segment. (usize in little endian)
+// - value: the serialized string.
+// label segment: trie
+
+const LEN: (usize, usize) = (0, USIZE_LEN);
+const VECTOR_START: (usize, usize) = (LEN.1, LEN.1 + USIZE_LEN);
+const KEY_START: (usize, usize) = (VECTOR_START.1, VECTOR_START.1 + USIZE_LEN);
+const LABEL_START: (usize, usize) = (KEY_START.1, KEY_START.1 + USIZE_LEN);
+const HEADER_LEN: usize = 4 * USIZE_LEN;
+
+#[derive(Clone, Copy)]
+pub struct Node;
+impl Node {
+    pub fn serialized_len<S, V, T, M>(key: S, vector: V, trie: T, metadata: Option<M>) -> usize
+    where
+        S: AsRef<[u8]>,
+        V: AsRef<[u8]>,
+        T: AsRef<[u8]>,
+        M: AsRef<[u8]>,
+    {
+        let skey = key.as_ref();
+        let svector = vector.as_ref();
+        let strie = trie.as_ref();
+        let svector_len = svector.len() + USIZE_LEN;
+        let skey_len = skey.len() + USIZE_LEN;
+        let slabels_len = strie.len();
+        let metadata_len = metadata.map(|m| m.as_ref().len()).unwrap_or_default();
+        HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len
+    }
+    pub fn serialize<S, V, T, M>(key: S, vector: V, labels: T, metadata: Option<M>) -> Vec<u8>
+    where
+        S: AsRef<[u8]>,
+        V: AsRef<[u8]>,
+        T: AsRef<[u8]>,
+        M: AsRef<[u8]>,
+    {
+        let mut buf = vec![];
+        Node::serialize_into(&mut buf, key, vector, labels, metadata).unwrap();
+        buf
+    }
+    // labels must be sorted.
+    pub fn serialize_into<W, S, V, T, M>(
+        mut w: W,
+        key: S,
+        vector: V,
+        trie: T,
+        metadata: Option<M>,
+    ) -> io::Result<()>
+    where
+        W: io::Write,
+        S: AsRef<[u8]>,
+        V: AsRef<[u8]>,
+        T: AsRef<[u8]>,
+        M: AsRef<[u8]>,
+    {
+        let skey = key.as_ref();
+        let svector = vector.as_ref();
+        let strie = trie.as_ref();
+
+        // Reading lens
+        let svector_len = svector.len() + USIZE_LEN;
+        let skey_len = skey.len() + USIZE_LEN;
+        let slabels_len = strie.len();
+        let metadata_len = metadata
+            .as_ref()
+            .map(|m| m.as_ref().len())
+            .unwrap_or_default();
+
+        // Pointer computations
+        let len = HEADER_LEN + svector_len + skey_len + slabels_len + metadata_len;
+        let vector_start = HEADER_LEN + metadata_len;
+        let key_start = vector_start + svector_len;
+        let labels_start = key_start + skey_len;
+
+        // Write pointers
+        w.write_all(&len.to_le_bytes())?;
+        w.write_all(&vector_start.to_le_bytes())?;
+        w.write_all(&key_start.to_le_bytes())?;
+        w.write_all(&labels_start.to_le_bytes())?;
+        // Metadata segment
+        metadata.map_or(Ok(()), |m| w.write_all(m.as_ref()))?;
+        // Values
+        w.write_all(&svector.len().to_le_bytes())?;
+        w.write_all(svector)?;
+        w.write_all(&skey.len().to_le_bytes())?;
+        w.write_all(skey)?;
+        w.write_all(strie)?;
+        w.flush()
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn metadata(x: &[u8]) -> &[u8] {
+        // The metadata starts just after the header ends.
+        let metadata_start = LABEL_START.1;
+        // The metadata ends when the vector segment starts.
+        let metadata_end = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
+        &x[metadata_start..metadata_end]
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    // This function will decompress the trie data structure that contains the
+    // labels. Use only if you need all the labels.
+    pub fn labels(x: &[u8]) -> Vec<String> {
+        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
+        trie::decompress(&x[xlabel_ptr..])
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn key(x: &[u8]) -> &[u8] {
+        let xkey_ptr = usize_from_slice_le(&x[KEY_START.0..KEY_START.1]);
+        let xkey_len = usize_from_slice_le(&x[xkey_ptr..(xkey_ptr + USIZE_LEN)]);
+        let xkey_start = xkey_ptr + USIZE_LEN;
+        &x[xkey_start..(xkey_start + xkey_len)]
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn vector(x: &[u8]) -> &[u8] {
+        let xvec_ptr = usize_from_slice_le(&x[VECTOR_START.0..VECTOR_START.1]);
+        let xvec_len = usize_from_slice_le(&x[xvec_ptr..(xvec_ptr + USIZE_LEN)]);
+        let xvec_start = xvec_ptr + USIZE_LEN;
+        &x[xvec_start..(xvec_start + xvec_len)]
+    }
+    // x must be serialized using Node, may have trailing bytes.
+    pub fn has_label(x: &[u8], label: &[u8]) -> bool {
+        let xlabel_ptr = usize_from_slice_le(&x[LABEL_START.0..LABEL_START.1]);
+        trie::has_word(&x[xlabel_ptr..], label)
+    }
+}
+impl Slot for Node {
+    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
+        let xkey = self.get_key(x);
+        xkey.cmp(key)
+    }
+    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
+        Self::key(x)
+    }
+    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
+        let len = usize_from_slice_le(&x[LEN.0..LEN.1]);
+        (&x[0..len], &x[len..])
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::data_types::{trie_ram, vector};
+    lazy_static::lazy_static! {
+        static ref NO_LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&NO_LABELS));
+        static ref LABELS_TRIE: Vec<u8> = trie::serialize(trie_ram::create_trie(&LABELS));
+    }
+    const NO_LABELS: [&[u8]; 0] = [];
+    const LABELS: [&[u8]; 3] = [b"L1", b"L2", b"L3"];
+    const NO_METADATA: Option<&[u8]> = None;
+
+    #[test]
+    fn create_test() {
+        let key = b"NODE1";
+        let vector = vector::encode_vector(&[12.; 1000]);
+        let mut buf = Vec::new();
+        Node::serialize_into(&mut buf, key, &vector, NO_LABELS_TRIE.clone(), NO_METADATA).unwrap();
+        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
+        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
+        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
+        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
+        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
+        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
+        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
+        let metadata = Node::metadata(&buf);
+        assert_eq!(metadata.len(), 0);
+        assert_eq!(len, buf.len());
+        assert_eq!(vector_len, vector.len());
+        assert_eq!(key_len, key.len());
+        assert_eq!(&buf[svector], &vector);
+        assert_eq!(&buf[skey], key.as_slice());
+        assert_eq!(Node::vector(&buf), &vector);
+        assert_eq!(Node::key(&buf), key);
+
+        let key = b"NODE2";
+        let metadata = b"THIS ARE THE METADATA CONTENTS";
+        let vector = vector::encode_vector(&[13.; 1000]);
+        let mut buf = Vec::new();
+        Node::serialize_into(&mut buf, key, &vector, LABELS_TRIE.clone(), Some(metadata)).unwrap();
+        let len = usize_from_slice_le(&buf[LEN.0..LEN.1]);
+        let vector_start = usize_from_slice_le(&buf[VECTOR_START.0..VECTOR_START.1]);
+        let key_start = usize_from_slice_le(&buf[KEY_START.0..KEY_START.1]);
+        let vector_len = usize_from_slice_le(&buf[vector_start..(vector_start + USIZE_LEN)]);
+        let key_len = usize_from_slice_le(&buf[key_start..(key_start + USIZE_LEN)]);
+        let svector = (vector_start + USIZE_LEN)..(vector_start + USIZE_LEN + vector_len);
+        let skey = (key_start + USIZE_LEN)..(key_start + USIZE_LEN + key_len);
+        let smetadata = Node::metadata(&buf);
+        assert_eq!(smetadata, metadata.as_slice());
+        assert_eq!(len, buf.len());
+        assert_eq!(vector_len, vector.len());
+        assert_eq!(key_len, key.len());
+        assert_eq!(&buf[svector], &vector);
+        assert_eq!(&buf[skey], key.as_slice());
+        assert_eq!(Node::vector(&buf), &vector);
+        assert_eq!(Node::key(&buf), key);
+        assert!(LABELS.iter().all(|l| Node::has_label(&buf, l)));
+    }
+
+    #[test]
+    fn look_up_test() {
+        let mut buf = Vec::new();
+        let key1 = b"NODE1";
+        let metadata1 = b"The node 1 has metadata";
+        let vector1 = vector::encode_vector(&[12.; 1000]);
+        let node1 = buf.len();
+        Node::serialize_into(
+            &mut buf,
+            key1,
+            &vector1,
+            NO_LABELS_TRIE.clone(),
+            Some(&metadata1),
+        )
+        .unwrap();
+        let key2 = b"NODE2";
+        let metadata2 = b"Tuns out node 2 also has metadata";
+        let vector2 = vector::encode_vector(&[15.; 1000]);
+        let node2 = buf.len();
+        Node::serialize_into(
+            &mut buf,
+            key2,
+            &vector2,
+            NO_LABELS_TRIE.clone(),
+            Some(&metadata2),
+        )
+        .unwrap();
+        assert_eq!(Node::key(&buf[node1..]), key1);
+        assert_eq!(Node::key(&buf[node2..]), key2);
+        assert_eq!(Node::vector(&buf[node1..]), vector1);
+        assert_eq!(Node::vector(&buf[node2..]), vector2);
+        assert!(Node.keep_in_merge(&buf[node1..]));
+        assert!(Node.keep_in_merge(&buf[node2..]));
+        assert_eq!(Node.cmp_slot(&buf[node1..], &buf[node2..]), key1.cmp(key2));
+        assert_eq!(Node::metadata(&buf[node1..]), metadata1);
+        assert_eq!(Node::metadata(&buf[node2..]), metadata2);
+        assert_eq!(
+            Node.read_exact(&buf[node1..]),
+            (&buf[node1..node2], &buf[node2..])
+        );
+        assert_eq!(
+            Node.read_exact(&buf[node2..]),
+            (&buf[node2..], [].as_slice())
+        );
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point/ram_hnsw.rs`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashMap;
-
-use ops_hnsw::{Hnsw, Layer};
-use serde::{Deserialize, Serialize};
-
-use super::*;
-
-const NO_EDGES: [(Address, Edge); 0] = [];
-
-#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
-pub struct EntryPoint {
-    pub node: Address,
-    pub layer: usize,
-}
-
-#[derive(Clone, Copy, Debug, PartialEq, PartialOrd, Serialize, Deserialize)]
-pub struct Edge {
-    pub dist: f32,
-}
-
-#[derive(Default, Clone)]
-pub struct RAMLayer {
-    pub out: HashMap<Address, Vec<(Address, Edge)>>,
-}
-
-impl RAMLayer {
-    fn out_edges(&self, node: Address) -> std::iter::Copied<std::slice::Iter<'_, (Address, Edge)>> {
-        self.out
-            .get(&node)
-            .map_or_else(|| NO_EDGES.iter().copied(), |out| out.iter().copied())
-    }
-    pub fn new() -> RAMLayer {
-        RAMLayer::default()
-    }
-    pub fn add_node(&mut self, node: Address) {
-        self.out.entry(node).or_insert_with(Vec::new);
-    }
-    pub fn add_edge(&mut self, from: Address, edge: Edge, to: Address) {
-        if let Some(edges) = self.out.get_mut(&from) {
-            edges.push((to, edge))
-        }
-    }
-    pub fn take_out_edges(&mut self, x: Address) -> Vec<(Address, Edge)> {
-        self.out.get_mut(&x).map(std::mem::take).unwrap_or_default()
-    }
-    pub fn no_out_edges(&self, node: Address) -> usize {
-        self.out.get(&node).map_or(0, |v| v.len())
-    }
-    pub fn first(&self) -> Option<Address> {
-        self.out.keys().next().cloned()
-    }
-    pub fn is_empty(&self) -> bool {
-        self.out.len() == 0
-    }
-    #[cfg(test)]
-    #[allow(unused)]
-    pub fn no_nodes(&self) -> usize {
-        self.out.len()
-    }
-}
-
-#[derive(Default, Clone)]
-pub struct RAMHnsw {
-    pub entry_point: Option<EntryPoint>,
-    pub layers: Vec<RAMLayer>,
-}
-impl RAMHnsw {
-    pub fn new() -> RAMHnsw {
-        Self::default()
-    }
-    pub fn increase_layers_with(&mut self, x: Address, level: usize) -> &mut Self {
-        while self.layers.len() <= level {
-            let mut new_layer = RAMLayer::new();
-            new_layer.add_node(x);
-            self.layers.push(new_layer);
-        }
-        self
-    }
-    pub fn remove_empty_layers(&mut self) -> &mut Self {
-        while self.layers.last().map(|l| l.is_empty()).unwrap_or_default() {
-            self.layers.pop();
-        }
-        self
-    }
-    pub fn update_entry_point(&mut self) -> &mut Self {
-        self.remove_empty_layers();
-        self.entry_point = self
-            .layers
-            .iter()
-            .enumerate()
-            .last()
-            .and_then(|(index, l)| l.first().map(|node| (node, index)))
-            .map(|(node, layer)| EntryPoint { node, layer });
-        self
-    }
-    pub fn no_layers(&self) -> usize {
-        self.layers.len()
-    }
-}
-
-impl<'a> Layer for &'a RAMLayer {
-    type EdgeIt = std::iter::Copied<std::slice::Iter<'a, (Address, Edge)>>;
-    fn get_out_edges(&self, node: Address) -> Self::EdgeIt {
-        self.out_edges(node)
-    }
-}
-
-impl<'a> Hnsw for &'a RAMHnsw {
-    type L = &'a RAMLayer;
-    fn get_entry_point(&self) -> Option<EntryPoint> {
-        self.entry_point
-    }
-    fn get_layer(&self, i: usize) -> Self::L {
-        &self.layers[i]
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashMap;
+
+use ops_hnsw::{Hnsw, Layer};
+use serde::{Deserialize, Serialize};
+
+use super::*;
+
+const NO_EDGES: [(Address, Edge); 0] = [];
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
+pub struct EntryPoint {
+    pub node: Address,
+    pub layer: usize,
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, PartialOrd, Serialize, Deserialize)]
+pub struct Edge {
+    pub dist: f32,
+}
+
+#[derive(Default, Clone)]
+pub struct RAMLayer {
+    pub out: HashMap<Address, Vec<(Address, Edge)>>,
+}
+
+impl RAMLayer {
+    fn out_edges(&self, node: Address) -> std::iter::Copied<std::slice::Iter<'_, (Address, Edge)>> {
+        self.out
+            .get(&node)
+            .map_or_else(|| NO_EDGES.iter().copied(), |out| out.iter().copied())
+    }
+    pub fn new() -> RAMLayer {
+        RAMLayer::default()
+    }
+    pub fn add_node(&mut self, node: Address) {
+        self.out.entry(node).or_insert_with(Vec::new);
+    }
+    pub fn add_edge(&mut self, from: Address, edge: Edge, to: Address) {
+        if let Some(edges) = self.out.get_mut(&from) {
+            edges.push((to, edge))
+        }
+    }
+    pub fn take_out_edges(&mut self, x: Address) -> Vec<(Address, Edge)> {
+        self.out.get_mut(&x).map(std::mem::take).unwrap_or_default()
+    }
+    pub fn no_out_edges(&self, node: Address) -> usize {
+        self.out.get(&node).map_or(0, |v| v.len())
+    }
+    pub fn first(&self) -> Option<Address> {
+        self.out.keys().next().cloned()
+    }
+    pub fn is_empty(&self) -> bool {
+        self.out.len() == 0
+    }
+    #[cfg(test)]
+    #[allow(unused)]
+    pub fn no_nodes(&self) -> usize {
+        self.out.len()
+    }
+}
+
+#[derive(Default, Clone)]
+pub struct RAMHnsw {
+    pub entry_point: Option<EntryPoint>,
+    pub layers: Vec<RAMLayer>,
+}
+impl RAMHnsw {
+    pub fn new() -> RAMHnsw {
+        Self::default()
+    }
+    pub fn increase_layers_with(&mut self, x: Address, level: usize) -> &mut Self {
+        while self.layers.len() <= level {
+            let mut new_layer = RAMLayer::new();
+            new_layer.add_node(x);
+            self.layers.push(new_layer);
+        }
+        self
+    }
+    pub fn remove_empty_layers(&mut self) -> &mut Self {
+        while self.layers.last().map(|l| l.is_empty()).unwrap_or_default() {
+            self.layers.pop();
+        }
+        self
+    }
+    pub fn update_entry_point(&mut self) -> &mut Self {
+        self.remove_empty_layers();
+        self.entry_point = self
+            .layers
+            .iter()
+            .enumerate()
+            .last()
+            .and_then(|(index, l)| l.first().map(|node| (node, index)))
+            .map(|(node, layer)| EntryPoint { node, layer });
+        self
+    }
+    pub fn no_layers(&self) -> usize {
+        self.layers.len()
+    }
+}
+
+impl<'a> Layer for &'a RAMLayer {
+    type EdgeIt = std::iter::Copied<std::slice::Iter<'a, (Address, Edge)>>;
+    fn get_out_edges(&self, node: Address) -> Self::EdgeIt {
+        self.out_edges(node)
+    }
+}
+
+impl<'a> Hnsw for &'a RAMHnsw {
+    type L = &'a RAMLayer;
+    fn get_entry_point(&self) -> Option<EntryPoint> {
+        self.entry_point
+    }
+    fn get_layer(&self, i: usize) -> Self::L {
+        &self.layers[i]
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/merge_worker.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,128 +1,128 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::path::PathBuf;
-use std::sync::MutexGuard;
-use std::time::Duration;
-
-use nucliadb_core::fs_state;
-use nucliadb_core::tracing::*;
-
-use super::merger::{MergeQuery, MergeRequest};
-use super::work_flag::MergerWriterSync;
-use super::State;
-use crate::data_point::{DataPoint, DpId, Similarity};
-use crate::data_point_provider::merger;
-use crate::VectorR;
-
-const SLEEP_TIME: Duration = Duration::from_millis(100);
-pub(crate) struct Worker {
-    location: PathBuf,
-    work_flag: MergerWriterSync,
-    similarity: Similarity,
-}
-impl MergeQuery for Worker {
-    fn do_work(&self) -> VectorR<()> {
-        self.work()
-    }
-}
-impl Worker {
-    pub(crate) fn request(
-        location: PathBuf,
-        work_flag: MergerWriterSync,
-        similarity: Similarity,
-    ) -> MergeRequest {
-        Box::new(Worker {
-            similarity,
-            location,
-            work_flag,
-        })
-    }
-    fn merge_report<It>(&self, old: It, new: DpId) -> String
-    where It: Iterator<Item = DpId> {
-        use std::fmt::Write;
-        let mut msg = String::new();
-        for (id, dp_id) in old.enumerate() {
-            writeln!(msg, "  ({id}) {dp_id}").unwrap();
-        }
-        write!(msg, "==> {new}").unwrap();
-        msg
-    }
-    fn notify_merger(&self) {
-        let worker = Worker::request(
-            self.location.clone(),
-            self.work_flag.clone(),
-            self.similarity,
-        );
-        merger::send_merge_request(worker);
-    }
-    fn try_to_work_or_delay(&self) -> MutexGuard<'_, ()> {
-        loop {
-            match self.work_flag.try_to_start_working() {
-                Ok(lock) => break lock,
-                Err(_) => {
-                    info!("Merge delayed at: {:?}", self.location);
-                    std::thread::sleep(SLEEP_TIME);
-                }
-            }
-        }
-    }
-    fn work(&self) -> VectorR<()> {
-        let work_flag = self.try_to_work_or_delay();
-
-        let subscriber = self.location.as_path();
-        info!("{subscriber:?} is ready to perform a merge");
-        let lock = fs_state::shared_lock(subscriber)?;
-        let state: State = fs_state::load_state(&lock)?;
-        std::mem::drop(lock);
-
-        let Some(work) = state.current_work_unit().map(|work|
-            work
-            .iter()
-            .rev()
-            .map(|journal| (state.delete_log(*journal), journal.id()))
-            .collect::<Vec<_>>()
-        ) else { return Ok(());};
-        let new_dp = DataPoint::merge(subscriber, &work, self.similarity)?;
-        let ids: Vec<_> = work.into_iter().map(|(_, v)| v).collect();
-        std::mem::drop(state);
-
-        let report = self.merge_report(ids.iter().copied(), new_dp.meta().id());
-
-        let lock = fs_state::exclusive_lock(subscriber)?;
-        let mut state: State = fs_state::load_state(&lock)?;
-        let creates_work = state.replace_work_unit(new_dp);
-        fs_state::persist_state(&lock, &state)?;
-        std::mem::drop(lock);
-        info!("Merge on {subscriber:?}:\n{report}");
-        if creates_work {
-            self.notify_merger();
-        }
-
-        info!("Removing deprecated datapoints");
-        ids.into_iter()
-            .map(|dp| (subscriber, dp, DataPoint::delete(subscriber, dp)))
-            .filter(|(.., r)| r.is_err())
-            .for_each(|(s, id, ..)| info!("Error while deleting {s:?}/{id}"));
-        std::mem::drop(work_flag);
-
-        info!("Merge request completed");
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::path::PathBuf;
+use std::sync::MutexGuard;
+use std::time::Duration;
+
+use nucliadb_core::fs_state;
+use nucliadb_core::tracing::*;
+
+use super::merger::{MergeQuery, MergeRequest};
+use super::work_flag::MergerWriterSync;
+use super::State;
+use crate::data_point::{DataPoint, DpId, Similarity};
+use crate::data_point_provider::merger;
+use crate::VectorR;
+
+const SLEEP_TIME: Duration = Duration::from_millis(100);
+pub(crate) struct Worker {
+    location: PathBuf,
+    work_flag: MergerWriterSync,
+    similarity: Similarity,
+}
+impl MergeQuery for Worker {
+    fn do_work(&self) -> VectorR<()> {
+        self.work()
+    }
+}
+impl Worker {
+    pub(crate) fn request(
+        location: PathBuf,
+        work_flag: MergerWriterSync,
+        similarity: Similarity,
+    ) -> MergeRequest {
+        Box::new(Worker {
+            similarity,
+            location,
+            work_flag,
+        })
+    }
+    fn merge_report<It>(&self, old: It, new: DpId) -> String
+    where It: Iterator<Item = DpId> {
+        use std::fmt::Write;
+        let mut msg = String::new();
+        for (id, dp_id) in old.enumerate() {
+            writeln!(msg, "  ({id}) {dp_id}").unwrap();
+        }
+        write!(msg, "==> {new}").unwrap();
+        msg
+    }
+    fn notify_merger(&self) {
+        let worker = Worker::request(
+            self.location.clone(),
+            self.work_flag.clone(),
+            self.similarity,
+        );
+        merger::send_merge_request(worker);
+    }
+    fn try_to_work_or_delay(&self) -> MutexGuard<'_, ()> {
+        loop {
+            match self.work_flag.try_to_start_working() {
+                Ok(lock) => break lock,
+                Err(_) => {
+                    info!("Merge delayed at: {:?}", self.location);
+                    std::thread::sleep(SLEEP_TIME);
+                }
+            }
+        }
+    }
+    fn work(&self) -> VectorR<()> {
+        let work_flag = self.try_to_work_or_delay();
+
+        let subscriber = self.location.as_path();
+        info!("{subscriber:?} is ready to perform a merge");
+        let lock = fs_state::shared_lock(subscriber)?;
+        let state: State = fs_state::load_state(&lock)?;
+        std::mem::drop(lock);
+
+        let Some(work) = state.current_work_unit().map(|work|
+            work
+            .iter()
+            .rev()
+            .map(|journal| (state.delete_log(*journal), journal.id()))
+            .collect::<Vec<_>>()
+        ) else { return Ok(());};
+        let new_dp = DataPoint::merge(subscriber, &work, self.similarity)?;
+        let ids: Vec<_> = work.into_iter().map(|(_, v)| v).collect();
+        std::mem::drop(state);
+
+        let report = self.merge_report(ids.iter().copied(), new_dp.meta().id());
+
+        let lock = fs_state::exclusive_lock(subscriber)?;
+        let mut state: State = fs_state::load_state(&lock)?;
+        let creates_work = state.replace_work_unit(new_dp);
+        fs_state::persist_state(&lock, &state)?;
+        std::mem::drop(lock);
+        info!("Merge on {subscriber:?}:\n{report}");
+        if creates_work {
+            self.notify_merger();
+        }
+
+        info!("Removing deprecated datapoints");
+        ids.into_iter()
+            .map(|dp| (subscriber, dp, DataPoint::delete(subscriber, dp)))
+            .filter(|(.., r)| r.is_err())
+            .for_each(|(s, id, ..)| info!("Error while deleting {s:?}/{id}"));
+        std::mem::drop(work_flag);
+
+        info!("Merge request completed");
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/merger.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,84 +1,84 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::sync::mpsc::{self, Receiver, Sender};
-use std::sync::Once;
-
-use nucliadb_core::tracing;
-
-use crate::{VectorErr, VectorR};
-
-pub type MergeRequest = Box<dyn MergeQuery>;
-pub type MergeTxn = Sender<MergeRequest>;
-
-pub trait MergeQuery: Send {
-    fn do_work(&self) -> VectorR<()>;
-}
-
-#[derive(Clone)]
-struct MergerHandle(MergeTxn);
-impl MergerHandle {
-    pub fn send(&self, request: MergeRequest) {
-        let Err(e) = self.0.send(request) else { return };
-        tracing::info!("Error sending merge request, {e}");
-    }
-}
-
-static mut MERGER_NOTIFIER: Option<MergerHandle> = None;
-static MERGER_NOTIFIER_SET: Once = Once::new();
-
-pub fn send_merge_request(request: MergeRequest) {
-    // It is always safe to read from MERGER_NOTIFIER since
-    // it can only be writen through MERGER_NOTIFIER_SET and is not exposed in the public interface.
-    // MERGER_NOTIFIER_SET is protected by the type Once so we avoid concurrency problems.
-    match unsafe { &MERGER_NOTIFIER } {
-        Some(merger) => merger.send(request),
-        None => tracing::warn!("Merge requests are being sent without a merger intalled"),
-    }
-}
-
-pub struct Merger {
-    rtxn: Receiver<MergeRequest>,
-}
-
-impl Merger {
-    pub fn install_global() -> VectorR<impl FnOnce()> {
-        let mut status = Err(VectorErr::MergerAlreadyInitialized);
-        MERGER_NOTIFIER_SET.call_once(|| unsafe {
-            let (stxn, rtxn) = mpsc::channel();
-            let handler = MergerHandle(stxn);
-            // It is safe to initialize MERGER_NOTIFIER
-            // since the setter can only be called once.
-            MERGER_NOTIFIER = Some(handler);
-            status = Ok(|| Merger { rtxn }.run());
-        });
-        status
-    }
-    fn run(self) {
-        loop {
-            match self.rtxn.recv() {
-                Err(err) => tracing::info!("channel error {}", err),
-                Ok(query) => match query.do_work() {
-                    Ok(()) => (),
-                    Err(err) => tracing::info!("error merging: {err}"),
-                },
-            }
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::sync::mpsc::{self, Receiver, Sender};
+use std::sync::Once;
+
+use nucliadb_core::tracing;
+
+use crate::{VectorErr, VectorR};
+
+pub type MergeRequest = Box<dyn MergeQuery>;
+pub type MergeTxn = Sender<MergeRequest>;
+
+pub trait MergeQuery: Send {
+    fn do_work(&self) -> VectorR<()>;
+}
+
+#[derive(Clone)]
+struct MergerHandle(MergeTxn);
+impl MergerHandle {
+    pub fn send(&self, request: MergeRequest) {
+        let Err(e) = self.0.send(request) else { return };
+        tracing::info!("Error sending merge request, {e}");
+    }
+}
+
+static mut MERGER_NOTIFIER: Option<MergerHandle> = None;
+static MERGER_NOTIFIER_SET: Once = Once::new();
+
+pub fn send_merge_request(request: MergeRequest) {
+    // It is always safe to read from MERGER_NOTIFIER since
+    // it can only be writen through MERGER_NOTIFIER_SET and is not exposed in the public interface.
+    // MERGER_NOTIFIER_SET is protected by the type Once so we avoid concurrency problems.
+    match unsafe { &MERGER_NOTIFIER } {
+        Some(merger) => merger.send(request),
+        None => tracing::warn!("Merge requests are being sent without a merger intalled"),
+    }
+}
+
+pub struct Merger {
+    rtxn: Receiver<MergeRequest>,
+}
+
+impl Merger {
+    pub fn install_global() -> VectorR<impl FnOnce()> {
+        let mut status = Err(VectorErr::MergerAlreadyInitialized);
+        MERGER_NOTIFIER_SET.call_once(|| unsafe {
+            let (stxn, rtxn) = mpsc::channel();
+            let handler = MergerHandle(stxn);
+            // It is safe to initialize MERGER_NOTIFIER
+            // since the setter can only be called once.
+            MERGER_NOTIFIER = Some(handler);
+            status = Ok(|| Merger { rtxn }.run());
+        });
+        status
+    }
+    fn run(self) {
+        loop {
+            match self.rtxn.recv() {
+                Err(err) => tracing::info!("channel error {}", err),
+                Ok(query) => match query.do_work() {
+                    Ok(()) => (),
+                    Err(err) => tracing::info!("error merging: {err}"),
+                },
+            }
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/mod.rs`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,255 +1,255 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-mod merge_worker;
-mod merger;
-mod state;
-mod work_flag;
-use std::fs::File;
-use std::io::{BufReader, BufWriter, Write};
-use std::mem;
-use std::path::{Path, PathBuf};
-use std::sync::{RwLock, RwLockReadGuard, RwLockWriteGuard};
-use std::time::SystemTime;
-
-pub use merger::Merger;
-use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
-use nucliadb_core::tracing::*;
-use serde::{Deserialize, Serialize};
-use state::*;
-use work_flag::MergerWriterSync;
-
-pub use crate::data_point::Neighbour;
-use crate::data_point::{DataPoint, DpId, Similarity};
-use crate::data_point_provider::merge_worker::Worker;
-use crate::formula::Formula;
-use crate::VectorR;
-pub type TemporalMark = SystemTime;
-
-const METADATA: &str = "metadata.json";
-
-pub trait SearchRequest {
-    fn get_query(&self) -> &[f32];
-    fn get_filter(&self) -> &Formula;
-    fn no_results(&self) -> usize;
-    fn with_duplicates(&self) -> bool;
-}
-
-#[derive(Clone, Copy, Debug)]
-pub enum IndexCheck {
-    None,
-    Sanity,
-}
-
-#[derive(Debug, Default, Serialize, Deserialize)]
-pub struct IndexMetadata {
-    #[serde(default)]
-    pub similarity: Similarity,
-}
-impl IndexMetadata {
-    pub fn write(&self, path: &Path) -> VectorR<()> {
-        let mut writer = BufWriter::new(File::create(path.join(METADATA))?);
-        serde_json::to_writer(&mut writer, self)?;
-        Ok(writer.flush()?)
-    }
-    pub fn open(path: &Path) -> VectorR<Option<IndexMetadata>> {
-        let path = &path.join(METADATA);
-        if !path.is_file() {
-            return Ok(None);
-        }
-        let mut reader = BufReader::new(File::open(path)?);
-        Ok(Some(serde_json::from_reader(&mut reader)?))
-    }
-}
-
-pub struct Index {
-    metadata: IndexMetadata,
-    work_flag: MergerWriterSync,
-    state: RwLock<State>,
-    date: RwLock<Version>,
-    location: PathBuf,
-}
-impl Index {
-    fn read_state(&self) -> RwLockReadGuard<'_, State> {
-        self.state.read().unwrap_or_else(|e| e.into_inner())
-    }
-    fn write_state(&self) -> RwLockWriteGuard<'_, State> {
-        self.state.write().unwrap_or_else(|e| e.into_inner())
-    }
-    fn read_date(&self) -> RwLockReadGuard<'_, Version> {
-        self.date.read().unwrap_or_else(|e| e.into_inner())
-    }
-    fn write_date(&self) -> RwLockWriteGuard<'_, Version> {
-        self.date.write().unwrap_or_else(|e| e.into_inner())
-    }
-    fn update(&self, lock: &Lock) -> VectorR<()> {
-        let disk_v = fs_state::crnt_version(lock)?;
-        let date = self.read_date();
-        if disk_v > *date {
-            mem::drop(date);
-            let new_state = fs_state::load_state(lock)?;
-            let mut state = self.write_state();
-            let mut date = self.write_date();
-            *state = new_state;
-            *date = disk_v;
-            mem::drop(date);
-            mem::drop(state);
-        }
-        Ok(())
-    }
-    fn notify_merger(&self) {
-        let worker = Worker::request(
-            self.location.clone(),
-            self.work_flag.clone(),
-            self.metadata.similarity,
-        );
-        merger::send_merge_request(worker);
-    }
-    pub fn open(path: &Path, with_check: IndexCheck) -> VectorR<Index> {
-        let lock = fs_state::shared_lock(path)?;
-        let state = fs_state::load_state::<State>(&lock)?;
-        let date = fs_state::crnt_version(&lock)?;
-        let metadata = IndexMetadata::open(path)?.map(Ok).unwrap_or_else(|| {
-            // Old indexes may not have this file so in that case the
-            // metadata file they should have is created.
-            let metadata = IndexMetadata::default();
-            metadata.write(path).map(|_| metadata)
-        })?;
-        let index = Index {
-            metadata,
-            work_flag: MergerWriterSync::new(),
-            state: RwLock::new(state),
-            date: RwLock::new(date),
-            location: path.to_path_buf(),
-        };
-        if let IndexCheck::Sanity = with_check {
-            let mut state = index.write_state();
-            let merge_work = state.work_stack_len();
-            (0..merge_work).for_each(|_| index.notify_merger());
-        }
-        Ok(index)
-    }
-    pub fn new(path: &Path, metadata: IndexMetadata) -> VectorR<Index> {
-        std::fs::create_dir_all(path)?;
-        fs_state::initialize_disk(path, State::new)?;
-        metadata.write(path)?;
-        let lock = fs_state::shared_lock(path)?;
-        let state = fs_state::load_state::<State>(&lock)?;
-        let date = fs_state::crnt_version(&lock)?;
-        let index = Index {
-            metadata,
-            work_flag: MergerWriterSync::new(),
-            state: RwLock::new(state),
-            date: RwLock::new(date),
-            location: path.to_path_buf(),
-        };
-        Ok(index)
-    }
-    pub fn delete(&self, prefix: impl AsRef<str>, temporal_mark: SystemTime, _: &ELock) {
-        let mut state = self.write_state();
-        state.remove(prefix.as_ref(), temporal_mark);
-    }
-    pub fn get_keys(&self, _: &Lock) -> VectorR<Vec<String>> {
-        self.read_state().keys(&self.location)
-    }
-    pub fn search(&self, request: &dyn SearchRequest, _: &Lock) -> VectorR<Vec<Neighbour>> {
-        self.read_state()
-            .search(&self.location, request, self.metadata.similarity)
-    }
-    pub fn no_nodes(&self, _: &Lock) -> usize {
-        self.read_state().no_nodes()
-    }
-    pub fn collect_garbage(&self, _: &Lock) -> VectorR<()> {
-        use std::collections::HashSet;
-        let work_flag = self.work_flag.try_to_start_working()?;
-        let state = self.read_state();
-        let in_use_dp: HashSet<_> = state.dpid_iter().collect();
-        for dir_entry in std::fs::read_dir(&self.location)? {
-            let entry = dir_entry?;
-            let path = entry.path();
-            let name = entry.file_name().to_string_lossy().to_string();
-            if path.is_file() {
-                continue;
-            }
-            let Ok(dpid) = DpId::parse_str(&name) else {
-                info!("Unknown item {path:?} found");
-                continue;
-            };
-            if !in_use_dp.contains(&dpid) {
-                info!("found garbage {name}");
-                let Err(err)  = DataPoint::delete(&self.location, dpid) else { continue };
-                warn!("{name} is garbage and could not be deleted because of {err}");
-            }
-        }
-        std::mem::drop(work_flag);
-        Ok(())
-    }
-    pub fn add(&self, dp: DataPoint, _: &ELock) {
-        let mut state = self.write_state();
-        if state.add(dp) {
-            self.notify_merger()
-        }
-    }
-    pub fn commit(&self, lock: ELock) -> VectorR<()> {
-        let state = self.read_state();
-        let mut date = self.write_date();
-        fs_state::persist_state::<State>(&lock, &state)?;
-        *date = fs_state::crnt_version(&lock)?;
-        Ok(())
-    }
-    pub fn get_elock(&self) -> VectorR<ELock> {
-        let lock = fs_state::exclusive_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn get_slock(&self) -> VectorR<SLock> {
-        let lock = fs_state::shared_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn location(&self) -> &Path {
-        &self.location
-    }
-    pub fn metadata(&self) -> &IndexMetadata {
-        &self.metadata
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use nucliadb_core::NodeResult;
-
-    use super::*;
-    use crate::data_point::Similarity;
-    #[test]
-    fn garbage_collection_test() -> NodeResult<()> {
-        let dir = tempfile::tempdir()?;
-        let index = Index::new(dir.path(), IndexMetadata::default())?;
-        let empty_no_entries = std::fs::read_dir(dir.path())?.count();
-        for _ in 0..10 {
-            DataPoint::new(dir.path(), vec![], None, Similarity::Cosine).unwrap();
-        }
-        let lock = index.get_slock()?;
-        index.collect_garbage(&lock)?;
-        let no_entries = std::fs::read_dir(dir.path())?.count();
-        assert_eq!(no_entries, empty_no_entries);
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+mod merge_worker;
+mod merger;
+mod state;
+mod work_flag;
+use std::fs::File;
+use std::io::{BufReader, BufWriter, Write};
+use std::mem;
+use std::path::{Path, PathBuf};
+use std::sync::{RwLock, RwLockReadGuard, RwLockWriteGuard};
+use std::time::SystemTime;
+
+pub use merger::Merger;
+use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
+use nucliadb_core::tracing::*;
+use serde::{Deserialize, Serialize};
+use state::*;
+use work_flag::MergerWriterSync;
+
+pub use crate::data_point::Neighbour;
+use crate::data_point::{DataPoint, DpId, Similarity};
+use crate::data_point_provider::merge_worker::Worker;
+use crate::formula::Formula;
+use crate::VectorR;
+pub type TemporalMark = SystemTime;
+
+const METADATA: &str = "metadata.json";
+
+pub trait SearchRequest {
+    fn get_query(&self) -> &[f32];
+    fn get_filter(&self) -> &Formula;
+    fn no_results(&self) -> usize;
+    fn with_duplicates(&self) -> bool;
+}
+
+#[derive(Clone, Copy, Debug)]
+pub enum IndexCheck {
+    None,
+    Sanity,
+}
+
+#[derive(Debug, Default, Serialize, Deserialize)]
+pub struct IndexMetadata {
+    #[serde(default)]
+    pub similarity: Similarity,
+}
+impl IndexMetadata {
+    pub fn write(&self, path: &Path) -> VectorR<()> {
+        let mut writer = BufWriter::new(File::create(path.join(METADATA))?);
+        serde_json::to_writer(&mut writer, self)?;
+        Ok(writer.flush()?)
+    }
+    pub fn open(path: &Path) -> VectorR<Option<IndexMetadata>> {
+        let path = &path.join(METADATA);
+        if !path.is_file() {
+            return Ok(None);
+        }
+        let mut reader = BufReader::new(File::open(path)?);
+        Ok(Some(serde_json::from_reader(&mut reader)?))
+    }
+}
+
+pub struct Index {
+    metadata: IndexMetadata,
+    work_flag: MergerWriterSync,
+    state: RwLock<State>,
+    date: RwLock<Version>,
+    location: PathBuf,
+}
+impl Index {
+    fn read_state(&self) -> RwLockReadGuard<'_, State> {
+        self.state.read().unwrap_or_else(|e| e.into_inner())
+    }
+    fn write_state(&self) -> RwLockWriteGuard<'_, State> {
+        self.state.write().unwrap_or_else(|e| e.into_inner())
+    }
+    fn read_date(&self) -> RwLockReadGuard<'_, Version> {
+        self.date.read().unwrap_or_else(|e| e.into_inner())
+    }
+    fn write_date(&self) -> RwLockWriteGuard<'_, Version> {
+        self.date.write().unwrap_or_else(|e| e.into_inner())
+    }
+    fn update(&self, lock: &Lock) -> VectorR<()> {
+        let disk_v = fs_state::crnt_version(lock)?;
+        let date = self.read_date();
+        if disk_v > *date {
+            mem::drop(date);
+            let new_state = fs_state::load_state(lock)?;
+            let mut state = self.write_state();
+            let mut date = self.write_date();
+            *state = new_state;
+            *date = disk_v;
+            mem::drop(date);
+            mem::drop(state);
+        }
+        Ok(())
+    }
+    fn notify_merger(&self) {
+        let worker = Worker::request(
+            self.location.clone(),
+            self.work_flag.clone(),
+            self.metadata.similarity,
+        );
+        merger::send_merge_request(worker);
+    }
+    pub fn open(path: &Path, with_check: IndexCheck) -> VectorR<Index> {
+        let lock = fs_state::shared_lock(path)?;
+        let state = fs_state::load_state::<State>(&lock)?;
+        let date = fs_state::crnt_version(&lock)?;
+        let metadata = IndexMetadata::open(path)?.map(Ok).unwrap_or_else(|| {
+            // Old indexes may not have this file so in that case the
+            // metadata file they should have is created.
+            let metadata = IndexMetadata::default();
+            metadata.write(path).map(|_| metadata)
+        })?;
+        let index = Index {
+            metadata,
+            work_flag: MergerWriterSync::new(),
+            state: RwLock::new(state),
+            date: RwLock::new(date),
+            location: path.to_path_buf(),
+        };
+        if let IndexCheck::Sanity = with_check {
+            let mut state = index.write_state();
+            let merge_work = state.work_stack_len();
+            (0..merge_work).for_each(|_| index.notify_merger());
+        }
+        Ok(index)
+    }
+    pub fn new(path: &Path, metadata: IndexMetadata) -> VectorR<Index> {
+        std::fs::create_dir_all(path)?;
+        fs_state::initialize_disk(path, State::new)?;
+        metadata.write(path)?;
+        let lock = fs_state::shared_lock(path)?;
+        let state = fs_state::load_state::<State>(&lock)?;
+        let date = fs_state::crnt_version(&lock)?;
+        let index = Index {
+            metadata,
+            work_flag: MergerWriterSync::new(),
+            state: RwLock::new(state),
+            date: RwLock::new(date),
+            location: path.to_path_buf(),
+        };
+        Ok(index)
+    }
+    pub fn delete(&self, prefix: impl AsRef<str>, temporal_mark: SystemTime, _: &ELock) {
+        let mut state = self.write_state();
+        state.remove(prefix.as_ref(), temporal_mark);
+    }
+    pub fn get_keys(&self, _: &Lock) -> VectorR<Vec<String>> {
+        self.read_state().keys(&self.location)
+    }
+    pub fn search(&self, request: &dyn SearchRequest, _: &Lock) -> VectorR<Vec<Neighbour>> {
+        self.read_state()
+            .search(&self.location, request, self.metadata.similarity)
+    }
+    pub fn no_nodes(&self, _: &Lock) -> usize {
+        self.read_state().no_nodes()
+    }
+    pub fn collect_garbage(&self, _: &Lock) -> VectorR<()> {
+        use std::collections::HashSet;
+        let work_flag = self.work_flag.try_to_start_working()?;
+        let state = self.read_state();
+        let in_use_dp: HashSet<_> = state.dpid_iter().collect();
+        for dir_entry in std::fs::read_dir(&self.location)? {
+            let entry = dir_entry?;
+            let path = entry.path();
+            let name = entry.file_name().to_string_lossy().to_string();
+            if path.is_file() {
+                continue;
+            }
+            let Ok(dpid) = DpId::parse_str(&name) else {
+                info!("Unknown item {path:?} found");
+                continue;
+            };
+            if !in_use_dp.contains(&dpid) {
+                info!("found garbage {name}");
+                let Err(err)  = DataPoint::delete(&self.location, dpid) else { continue };
+                warn!("{name} is garbage and could not be deleted because of {err}");
+            }
+        }
+        std::mem::drop(work_flag);
+        Ok(())
+    }
+    pub fn add(&self, dp: DataPoint, _: &ELock) {
+        let mut state = self.write_state();
+        if state.add(dp) {
+            self.notify_merger()
+        }
+    }
+    pub fn commit(&self, lock: ELock) -> VectorR<()> {
+        let state = self.read_state();
+        let mut date = self.write_date();
+        fs_state::persist_state::<State>(&lock, &state)?;
+        *date = fs_state::crnt_version(&lock)?;
+        Ok(())
+    }
+    pub fn get_elock(&self) -> VectorR<ELock> {
+        let lock = fs_state::exclusive_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn get_slock(&self) -> VectorR<SLock> {
+        let lock = fs_state::shared_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn location(&self) -> &Path {
+        &self.location
+    }
+    pub fn metadata(&self) -> &IndexMetadata {
+        &self.metadata
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use nucliadb_core::NodeResult;
+
+    use super::*;
+    use crate::data_point::Similarity;
+    #[test]
+    fn garbage_collection_test() -> NodeResult<()> {
+        let dir = tempfile::tempdir()?;
+        let index = Index::new(dir.path(), IndexMetadata::default())?;
+        let empty_no_entries = std::fs::read_dir(dir.path())?.count();
+        for _ in 0..10 {
+            DataPoint::new(dir.path(), vec![], None, Similarity::Cosine).unwrap();
+        }
+        let lock = index.get_slock()?;
+        index.collect_garbage(&lock)?;
+        let no_entries = std::fs::read_dir(dir.path())?.count();
+        assert_eq!(no_entries, empty_no_entries);
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_point_provider/work_flag.rs`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::sync::{Arc, Mutex, MutexGuard, TryLockError};
-
-use crate::{VectorErr, VectorR};
-
-#[derive(Clone)]
-pub struct MergerWriterSync(Arc<Mutex<()>>);
-impl MergerWriterSync {
-    pub fn new() -> MergerWriterSync {
-        MergerWriterSync(Arc::new(Mutex::new(())))
-    }
-    pub fn try_to_start_working(&self) -> VectorR<MutexGuard<'_, ()>> {
-        match self.0.try_lock() {
-            Ok(lock) => Ok(lock),
-            Err(TryLockError::Poisoned(poisoned)) => Ok(poisoned.into_inner()),
-            Err(TryLockError::WouldBlock) => Err(VectorErr::WorkDelayed),
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::sync::{Arc, Mutex, MutexGuard, TryLockError};
+
+use crate::{VectorErr, VectorR};
+
+#[derive(Clone)]
+pub struct MergerWriterSync(Arc<Mutex<()>>);
+impl MergerWriterSync {
+    pub fn new() -> MergerWriterSync {
+        MergerWriterSync(Arc::new(Mutex::new(())))
+    }
+    pub fn try_to_start_working(&self) -> VectorR<MutexGuard<'_, ()>> {
+        match self.0.try_lock() {
+            Ok(lock) => Ok(lock),
+            Err(TryLockError::Poisoned(poisoned)) => Ok(poisoned.into_inner()),
+            Err(TryLockError::WouldBlock) => Err(VectorErr::WorkDelayed),
+        }
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/dtrie_ram.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::collections::HashMap;
-use std::time::SystemTime;
-
-use serde::{Deserialize, Serialize};
-
-#[derive(Default, Clone, Serialize, Deserialize)]
-pub struct DTrie {
-    value: Option<SystemTime>,
-    go_table: HashMap<u8, Box<DTrie>>,
-}
-impl DTrie {
-    fn inner_get(&self, key: &[u8], current: Option<SystemTime>) -> Option<SystemTime> {
-        let current = std::cmp::max(current, self.value);
-        let [head, tail @ ..] = key else { return current };
-        let Some(node) = self.go_table.get(head) else { return current};
-        node.inner_get(tail, current)
-    }
-    fn inner_prune(&mut self, time: SystemTime) -> bool {
-        self.value = self.value.filter(|v| *v > time);
-        self.go_table = std::mem::take(&mut self.go_table)
-            .into_iter()
-            .map(|(k, mut v)| (v.inner_prune(time), k, v))
-            .filter(|v| !v.0)
-            .map(|v| (v.1, v.2))
-            .collect();
-        self.value.is_none() && self.go_table.is_empty()
-    }
-    pub fn new() -> DTrie {
-        DTrie::default()
-    }
-    pub fn insert(&mut self, key: &[u8], value: SystemTime) {
-        match key {
-            [] => {
-                self.value = Some(value);
-                self.go_table.clear();
-            }
-            [head, tail @ ..] => {
-                self.go_table
-                    .entry(*head)
-                    .or_insert_with(|| Box::new(DTrie::new()))
-                    .as_mut()
-                    .insert(tail, value);
-            }
-        }
-    }
-    pub fn get(&self, key: &[u8]) -> Option<SystemTime> {
-        self.inner_get(key, None)
-    }
-    pub fn prune(&mut self, time: SystemTime) {
-        self.inner_prune(time);
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::time::Duration;
-
-    use super::*;
-
-    const KEY: &str = "key";
-    const N0: &str = "key_0";
-    const N1: &str = "key_1";
-    const N2: &str = "key_2";
-
-    #[test]
-    fn insert_search() {
-        let tplus0 = SystemTime::now();
-        let tplus1 = tplus0 + Duration::from_secs(1);
-        let tplus2 = tplus0 + Duration::from_secs(2);
-        let tplus3 = tplus0 + Duration::from_secs(3);
-
-        // Time matches the prefix order
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-
-        // Prefixes overwrite previous values
-        let mut trie = DTrie::new();
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-
-        let mut trie = DTrie::new();
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N2.as_bytes(), tplus3);
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-
-        let mut trie = DTrie::new();
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N2.as_bytes(), tplus0);
-        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus0));
-    }
-    #[test]
-    fn prune() {
-        let tplus0 = SystemTime::now();
-        let tplus1 = tplus0 + Duration::from_secs(1);
-        let tplus2 = tplus0 + Duration::from_secs(2);
-        let tplus3 = tplus0 + Duration::from_secs(3);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus0);
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus1);
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
-        assert_eq!(trie.get(N0.as_bytes()), None);
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus2);
-        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
-        assert_eq!(trie.get(N1.as_bytes()), None);
-        assert_eq!(trie.get(N0.as_bytes()), None);
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-
-        let mut trie = DTrie::new();
-        trie.insert(KEY.as_bytes(), tplus0);
-        trie.insert(N0.as_bytes(), tplus1);
-        trie.insert(N1.as_bytes(), tplus2);
-        trie.insert(N2.as_bytes(), tplus3);
-        trie.prune(tplus3);
-        assert_eq!(trie.get(N2.as_bytes()), None);
-        assert_eq!(trie.get(N1.as_bytes()), None);
-        assert_eq!(trie.get(N0.as_bytes()), None);
-        assert_eq!(trie.get(KEY.as_bytes()), None);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::collections::HashMap;
+use std::time::SystemTime;
+
+use serde::{Deserialize, Serialize};
+
+#[derive(Default, Clone, Serialize, Deserialize)]
+pub struct DTrie {
+    value: Option<SystemTime>,
+    go_table: HashMap<u8, Box<DTrie>>,
+}
+impl DTrie {
+    fn inner_get(&self, key: &[u8], current: Option<SystemTime>) -> Option<SystemTime> {
+        let current = std::cmp::max(current, self.value);
+        let [head, tail @ ..] = key else { return current };
+        let Some(node) = self.go_table.get(head) else { return current};
+        node.inner_get(tail, current)
+    }
+    fn inner_prune(&mut self, time: SystemTime) -> bool {
+        self.value = self.value.filter(|v| *v > time);
+        self.go_table = std::mem::take(&mut self.go_table)
+            .into_iter()
+            .map(|(k, mut v)| (v.inner_prune(time), k, v))
+            .filter(|v| !v.0)
+            .map(|v| (v.1, v.2))
+            .collect();
+        self.value.is_none() && self.go_table.is_empty()
+    }
+    pub fn new() -> DTrie {
+        DTrie::default()
+    }
+    pub fn insert(&mut self, key: &[u8], value: SystemTime) {
+        match key {
+            [] => {
+                self.value = Some(value);
+                self.go_table.clear();
+            }
+            [head, tail @ ..] => {
+                self.go_table
+                    .entry(*head)
+                    .or_insert_with(|| Box::new(DTrie::new()))
+                    .as_mut()
+                    .insert(tail, value);
+            }
+        }
+    }
+    pub fn get(&self, key: &[u8]) -> Option<SystemTime> {
+        self.inner_get(key, None)
+    }
+    pub fn prune(&mut self, time: SystemTime) {
+        self.inner_prune(time);
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::time::Duration;
+
+    use super::*;
+
+    const KEY: &str = "key";
+    const N0: &str = "key_0";
+    const N1: &str = "key_1";
+    const N2: &str = "key_2";
+
+    #[test]
+    fn insert_search() {
+        let tplus0 = SystemTime::now();
+        let tplus1 = tplus0 + Duration::from_secs(1);
+        let tplus2 = tplus0 + Duration::from_secs(2);
+        let tplus3 = tplus0 + Duration::from_secs(3);
+
+        // Time matches the prefix order
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+
+        // Prefixes overwrite previous values
+        let mut trie = DTrie::new();
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+
+        let mut trie = DTrie::new();
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N2.as_bytes(), tplus3);
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+
+        let mut trie = DTrie::new();
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N2.as_bytes(), tplus0);
+        assert_eq!(trie.get(KEY.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus0));
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus0));
+    }
+    #[test]
+    fn prune() {
+        let tplus0 = SystemTime::now();
+        let tplus1 = tplus0 + Duration::from_secs(1);
+        let tplus2 = tplus0 + Duration::from_secs(2);
+        let tplus3 = tplus0 + Duration::from_secs(3);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus0);
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N0.as_bytes()), Some(tplus1));
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus1);
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(N1.as_bytes()), Some(tplus2));
+        assert_eq!(trie.get(N0.as_bytes()), None);
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus2);
+        assert_eq!(trie.get(N2.as_bytes()), Some(tplus3));
+        assert_eq!(trie.get(N1.as_bytes()), None);
+        assert_eq!(trie.get(N0.as_bytes()), None);
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+
+        let mut trie = DTrie::new();
+        trie.insert(KEY.as_bytes(), tplus0);
+        trie.insert(N0.as_bytes(), tplus1);
+        trie.insert(N1.as_bytes(), tplus2);
+        trie.insert(N2.as_bytes(), tplus3);
+        trie.prune(tplus3);
+        assert_eq!(trie.get(N2.as_bytes()), None);
+        assert_eq!(trie.get(N1.as_bytes()), None);
+        assert_eq!(trie.get(N0.as_bytes()), None);
+        assert_eq!(trie.get(KEY.as_bytes()), None);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/key_value.rs`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,384 +1,384 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::cmp::Ordering;
-use std::io::{self, Seek, SeekFrom, Write};
-
-use super::usize_utils::*;
-
-// A key-value store schema.
-// The data is arrange in a way such that the following operations can be performed:
-// -> Given a key, find its value. O(log(n))
-// -> Given an id, find its value. O(1).
-// N = Address size in bytes.
-// Serialization schema:
-//
-// [ number of values: N bytes in little endian.   ] --> Header
-// [ sequence of pointers sorted by the keys.      ]
-// [ slots: in sequence.                           ]
-//
-// A pointer in idx is the start of a value, stored N bytes in little endian)
-// It might not be obvious why the list of pointers is needed. It is because the slots may be of
-// variable length, which would render a binary search imposible. Idx elements do have a fixed
-// length (N bytes).
-
-const N: usize = USIZE_LEN;
-const HEADER_LEN: usize = N;
-const POINTER_LEN: usize = N;
-pub type Pointer = usize;
-pub type HeaderE = usize;
-
-// Given an index i, the start of its element on idx is:
-//              [i*IDXE_LEN + HEADER_LEN]
-pub fn get_pointer(x: &[u8], i: usize) -> Pointer {
-    let start = (i * POINTER_LEN) + HEADER_LEN;
-    let end = start + POINTER_LEN;
-    let mut buff = [0; POINTER_LEN];
-    buff.copy_from_slice(&x[start..end]);
-    usize::from_le_bytes(buff)
-}
-
-// O(1)
-pub fn get_no_elems(x: &[u8]) -> HeaderE {
-    usize_from_slice_le(&x[..HEADER_LEN])
-}
-
-pub trait Slot {
-    // Is a mistake to assume that x does not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
-    // Slot.
-    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8];
-    // Is a mistake to assume that x does not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
-    // Slot.
-    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering;
-    // The function should split x at i, being i the index where
-    // x[0],..,x[i] is a valid representation of a slot value.
-    // i is ensured to exists.
-    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]);
-    // Is a mistake to assume that x does not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
-    // Slot.
-    fn keep_in_merge(&self, _: &[u8]) -> bool {
-        true
-    }
-    // Is a mistake to assume that x and y do not have trailing bytes at the end.
-    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid Slot.
-    // Is safe to assume that there is an j such that y[0],.., y[k] is a valid Slot.
-    fn cmp_slot(&self, x: &[u8], y: &[u8]) -> Ordering {
-        let y_key = self.get_key(y);
-        self.cmp_keys(x, y_key)
-    }
-}
-
-pub trait KVElem {
-    fn serialized_len(&self) -> usize;
-    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()>;
-}
-
-impl<T: AsRef<[u8]>> KVElem for T {
-    fn serialized_len(&self) -> usize {
-        self.as_ref().len()
-    }
-    fn serialize_into<W: io::Write>(self, mut w: W) -> io::Result<()> {
-        w.write_all(self.as_ref())
-    }
-}
-
-#[allow(unused)]
-pub fn new_key_value<S>(slots: Vec<S>) -> Vec<u8>
-where S: KVElem {
-    let mut buf = vec![];
-    create_key_value(&mut buf, slots).unwrap();
-    buf
-}
-// Slots are expected to be sorted by their key.
-pub fn create_key_value<S, W>(mut at: W, slots: Vec<S>) -> io::Result<()>
-where
-    S: KVElem,
-    W: Write,
-{
-    let no_values: [u8; POINTER_LEN] = slots.len().to_le_bytes();
-    at.write_all(&no_values)?;
-    slots.iter().try_fold::<_, _, io::Result<_>>(
-        HEADER_LEN + (slots.len() * POINTER_LEN),
-        |pos, slot| {
-            let pbytes: [u8; POINTER_LEN] = pos.to_le_bytes();
-            at.write_all(&pbytes)?;
-            Ok(pos + slot.serialized_len())
-        },
-    )?;
-    slots
-        .into_iter()
-        .try_for_each(|slot| slot.serialize_into(&mut at))
-}
-
-// O(log n) where n is the number of slots in src.
-#[allow(unused)]
-pub fn search_by_key<S: Slot>(interface: S, src: &[u8], key: &[u8]) -> Option<usize> {
-    let number_of_values = get_no_elems(src);
-    let mut start = 0;
-    let mut end = number_of_values;
-    let mut found = None;
-    while start < end && found.is_none() {
-        let m = start + ((end - start) / 2);
-        let slot_start = get_pointer(src, m);
-        let slot = &src[slot_start..];
-        match interface.cmp_keys(slot, key) {
-            Ordering::Equal => {
-                found = Some(m);
-            }
-            Ordering::Less => {
-                start = m + 1;
-            }
-            Ordering::Greater => {
-                end = m;
-            }
-        }
-    }
-    found
-}
-
-// O(1)
-pub fn get_value<S: Slot>(interface: S, src: &[u8], id: usize) -> &[u8] {
-    let pointer = get_pointer(src, id);
-    interface.read_exact(&src[pointer..]).0
-}
-
-// Returns all the keys stored at the serialized key-value 'x'
-// O(1)
-pub fn get_keys<'a, S: Slot + Copy + 'a>(
-    interface: S,
-    x: &'a [u8],
-) -> impl Iterator<Item = &'a [u8]> {
-    (0..get_no_elems(x))
-        .map(move |i| get_value(interface, x, i))
-        .map(move |v| interface.get_key(v))
-}
-
-fn transfer_elem<S, R>(
-    interface: S,
-    at: &mut R,
-    from: &[u8],
-    id: Pointer,
-    writen_elems: usize,
-    crnt_length: usize,
-) -> io::Result<usize>
-where
-    S: Slot,
-    R: Write + Seek,
-{
-    let idx_slot = (HEADER_LEN + (writen_elems * POINTER_LEN)) as u64;
-    let value = get_value::<S>(interface, from, id);
-    at.seek(SeekFrom::Start(idx_slot))?;
-    at.write_all(&crnt_length.to_le_bytes())?;
-    at.seek(SeekFrom::Start(crnt_length as u64))?;
-    at.write_all(value)?;
-    Ok(crnt_length + value.len())
-}
-fn get_metrics<S: Slot>(interface: S, source: &[u8]) -> (usize, usize) {
-    let len = get_no_elems(source);
-    let mut value_space = 0;
-    let mut no_elems = 0;
-    for id in 0..len {
-        let ptr = get_pointer(source, id);
-        let (elem, _) = interface.read_exact(&source[ptr..]);
-        if interface.keep_in_merge(elem) {
-            value_space += elem.len();
-            no_elems += 1;
-        }
-    }
-    (no_elems, value_space)
-}
-
-// Merge algorithm for n key-value stores.
-// WARNING: In case of keys duplicatied keys it favors the contents of the first slot.
-// Returns the number of elements merged into the file.
-pub fn merge<S, R>(recepient: &mut R, producers: Vec<(S, &[u8])>) -> io::Result<usize>
-where
-    S: Slot + Copy,
-    R: Write + Seek,
-{
-    let lens = producers
-        .iter()
-        .copied()
-        .map(|(_, data)| data)
-        .map(get_no_elems)
-        .collect::<Vec<_>>();
-
-    // The number of elements that will remain at the merged file
-    // needs to be computed so the space is reserved.
-    let (no_elems, value_space) = producers
-        .iter()
-        .copied()
-        .map(|(interface, p)| get_metrics::<S>(interface, p))
-        .fold((0, 0), |(ne, vs), (ne_p, vs_p)| (ne + ne_p, vs + vs_p));
-
-    // Reserve space
-    let total_space = HEADER_LEN + (POINTER_LEN * no_elems) + value_space;
-    for _ in 0..total_space {
-        recepient.write_all(&[0])?;
-    }
-
-    // Merge loop
-    let mut writen_elems = 0;
-    let mut crnt_length = HEADER_LEN + (POINTER_LEN * no_elems);
-    let mut ids = vec![0usize; producers.len()];
-
-    while ids
-        .iter()
-        .copied()
-        .zip(lens.iter().copied())
-        .any(|(id, len)| id < len)
-    {
-        let min_data = producers
-            .iter()
-            .copied()
-            .zip(ids.iter().copied())
-            .zip(lens.iter().copied())
-            .filter(|((_, x_id), x_len)| *x_id < *x_len)
-            .map(|((x, x_id), _)| (x, x_id, get_pointer(x.1, x_id)))
-            .filter(|((interface, x), _, x_ptr)| interface.keep_in_merge(&x[*x_ptr..]))
-            .min_by(|(x, _, x_ptr), (y, _, y_ptr)| x.0.cmp_slot(&x.1[*x_ptr..], &y.1[*y_ptr..]));
-        producers
-            .iter()
-            .copied()
-            .zip(ids.iter_mut())
-            .zip(lens.iter().copied())
-            .filter(|((_, x_id), x_len)| **x_id < *x_len)
-            .map(|((x, x_id), _)| (x, get_pointer(x.1, *x_id), x_id))
-            .for_each(|((interface, x), x_ptr, x_id)| {
-                let is_equal = min_data.map(|(min, _, min_ptr)| {
-                    interface.cmp_slot(&x[x_ptr..], &min.1[min_ptr..]).is_eq()
-                });
-                if !interface.keep_in_merge(&x[x_ptr..]) {
-                    *x_id += 1;
-                } else if is_equal.unwrap_or_default() {
-                    *x_id += 1
-                }
-            });
-        if let Some(((interface, min), min_id, _)) = min_data {
-            crnt_length =
-                transfer_elem(interface, recepient, min, min_id, writen_elems, crnt_length)?;
-            writen_elems += 1;
-        }
-    }
-    // Write the number of elements
-    recepient.seek(SeekFrom::Start(0))?;
-    recepient.write_all(&writen_elems.to_le_bytes())?;
-    recepient.seek(SeekFrom::Start(0)).unwrap();
-    recepient.flush()?;
-    Ok(writen_elems)
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    // u32 numbers in big-endian (so cmp is faster)
-    #[derive(Clone, Copy)]
-    pub struct TElem;
-    impl Slot for TElem {
-        fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
-            &x[0..4]
-        }
-        fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering {
-            x[0..4].cmp(&key[0..4])
-        }
-        fn read_exact<'a>(&self, x: &'a [u8]) -> (&'a [u8], &'a [u8]) {
-            x.split_at(4)
-        }
-    }
-
-    fn store_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
-        let no_values = get_no_elems(buf);
-        assert_eq!(no_values, expected.len());
-        for (i, item) in expected.iter().enumerate() {
-            let value_ptr = get_pointer(buf, i);
-            let (exact, _) = TElem.read_exact(&buf[value_ptr..]);
-            assert_eq!(exact, item.as_ref());
-        }
-    }
-
-    fn retrieval_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
-        let interface = TElem;
-        let mut expected_keys = vec![];
-        for i in 0..expected.len() {
-            let id =
-                search_by_key(interface, buf, interface.get_key(expected[i].as_ref())).unwrap();
-            let value = get_value(interface, buf, id);
-            let key = interface.get_key(value);
-            let (head, tail) = interface.read_exact(value);
-            expected_keys.push(key);
-            assert_eq!(id, i);
-            assert_eq!(value, head);
-            assert_eq!(tail, &[] as &[u8]);
-            assert_eq!(value, expected[id].as_ref());
-        }
-        let got_keys = get_keys(interface, buf).collect::<Vec<_>>();
-        assert_eq!(expected_keys, got_keys);
-    }
-    #[test]
-    fn store_test() {
-        let elems: [u32; 5] = [0, 1, 2, 3, 4];
-        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
-        let mut buf = Vec::new();
-        create_key_value(&mut buf, encoded.clone()).unwrap();
-        store_checks(&encoded, &buf);
-    }
-    #[test]
-    fn retrieval_test() {
-        let elems: [u32; 5] = [0, 1, 2, 3, 4];
-        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
-        let mut buf = Vec::new();
-        create_key_value(&mut buf, encoded.clone()).unwrap();
-        store_checks(&encoded, &buf);
-        retrieval_checks(&encoded, &buf);
-    }
-    #[test]
-    fn merge_test() {
-        use std::io::Read;
-        let v0: Vec<_> = [0u32, 2, 4].iter().map(|x| x.to_be_bytes()).collect();
-        let v1: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
-        let v2: Vec<_> = [8u32, 9, 10, 11].iter().map(|x| x.to_be_bytes()).collect();
-        let v3: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
-        let expected: Vec<_> = [0u32, 1, 2, 4, 5, 7, 8, 9, 10, 11]
-            .iter()
-            .map(|x| x.to_be_bytes())
-            .collect();
-        let mut v0_store = vec![];
-        let mut v1_store = vec![];
-        let mut v2_store = vec![];
-        let mut v3_store = vec![];
-        create_key_value(&mut v0_store, v0).unwrap();
-        create_key_value(&mut v1_store, v1).unwrap();
-        create_key_value(&mut v2_store, v2).unwrap();
-        create_key_value(&mut v3_store, v3).unwrap();
-        let mut file = tempfile::tempfile().unwrap();
-        let elems: Vec<_> = [&v0_store, &v1_store, &v2_store, &v3_store]
-            .into_iter()
-            .map(|e| (TElem, e.as_slice()))
-            .collect();
-        merge(&mut file, elems).unwrap();
-        let mut buf = vec![];
-        file.read_to_end(&mut buf).unwrap();
-        store_checks(&expected, &buf);
-        retrieval_checks(&expected, &buf);
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::cmp::Ordering;
+use std::io::{self, Seek, SeekFrom, Write};
+
+use super::usize_utils::*;
+
+// A key-value store schema.
+// The data is arrange in a way such that the following operations can be performed:
+// -> Given a key, find its value. O(log(n))
+// -> Given an id, find its value. O(1).
+// N = Address size in bytes.
+// Serialization schema:
+//
+// [ number of values: N bytes in little endian.   ] --> Header
+// [ sequence of pointers sorted by the keys.      ]
+// [ slots: in sequence.                           ]
+//
+// A pointer in idx is the start of a value, stored N bytes in little endian)
+// It might not be obvious why the list of pointers is needed. It is because the slots may be of
+// variable length, which would render a binary search imposible. Idx elements do have a fixed
+// length (N bytes).
+
+const N: usize = USIZE_LEN;
+const HEADER_LEN: usize = N;
+const POINTER_LEN: usize = N;
+pub type Pointer = usize;
+pub type HeaderE = usize;
+
+// Given an index i, the start of its element on idx is:
+//              [i*IDXE_LEN + HEADER_LEN]
+pub fn get_pointer(x: &[u8], i: usize) -> Pointer {
+    let start = (i * POINTER_LEN) + HEADER_LEN;
+    let end = start + POINTER_LEN;
+    let mut buff = [0; POINTER_LEN];
+    buff.copy_from_slice(&x[start..end]);
+    usize::from_le_bytes(buff)
+}
+
+// O(1)
+pub fn get_no_elems(x: &[u8]) -> HeaderE {
+    usize_from_slice_le(&x[..HEADER_LEN])
+}
+
+pub trait Slot {
+    // Is a mistake to assume that x does not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
+    // Slot.
+    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8];
+    // Is a mistake to assume that x does not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
+    // Slot.
+    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering;
+    // The function should split x at i, being i the index where
+    // x[0],..,x[i] is a valid representation of a slot value.
+    // i is ensured to exists.
+    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]);
+    // Is a mistake to assume that x does not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid
+    // Slot.
+    fn keep_in_merge(&self, _: &[u8]) -> bool {
+        true
+    }
+    // Is a mistake to assume that x and y do not have trailing bytes at the end.
+    // Is safe to assume that there is an i such that x[0],.., x[i] is a valid Slot.
+    // Is safe to assume that there is an j such that y[0],.., y[k] is a valid Slot.
+    fn cmp_slot(&self, x: &[u8], y: &[u8]) -> Ordering {
+        let y_key = self.get_key(y);
+        self.cmp_keys(x, y_key)
+    }
+}
+
+pub trait KVElem {
+    fn serialized_len(&self) -> usize;
+    fn serialize_into<W: io::Write>(self, w: W) -> io::Result<()>;
+}
+
+impl<T: AsRef<[u8]>> KVElem for T {
+    fn serialized_len(&self) -> usize {
+        self.as_ref().len()
+    }
+    fn serialize_into<W: io::Write>(self, mut w: W) -> io::Result<()> {
+        w.write_all(self.as_ref())
+    }
+}
+
+#[allow(unused)]
+pub fn new_key_value<S>(slots: Vec<S>) -> Vec<u8>
+where S: KVElem {
+    let mut buf = vec![];
+    create_key_value(&mut buf, slots).unwrap();
+    buf
+}
+// Slots are expected to be sorted by their key.
+pub fn create_key_value<S, W>(mut at: W, slots: Vec<S>) -> io::Result<()>
+where
+    S: KVElem,
+    W: Write,
+{
+    let no_values: [u8; POINTER_LEN] = slots.len().to_le_bytes();
+    at.write_all(&no_values)?;
+    slots.iter().try_fold::<_, _, io::Result<_>>(
+        HEADER_LEN + (slots.len() * POINTER_LEN),
+        |pos, slot| {
+            let pbytes: [u8; POINTER_LEN] = pos.to_le_bytes();
+            at.write_all(&pbytes)?;
+            Ok(pos + slot.serialized_len())
+        },
+    )?;
+    slots
+        .into_iter()
+        .try_for_each(|slot| slot.serialize_into(&mut at))
+}
+
+// O(log n) where n is the number of slots in src.
+#[allow(unused)]
+pub fn search_by_key<S: Slot>(interface: S, src: &[u8], key: &[u8]) -> Option<usize> {
+    let number_of_values = get_no_elems(src);
+    let mut start = 0;
+    let mut end = number_of_values;
+    let mut found = None;
+    while start < end && found.is_none() {
+        let m = start + ((end - start) / 2);
+        let slot_start = get_pointer(src, m);
+        let slot = &src[slot_start..];
+        match interface.cmp_keys(slot, key) {
+            Ordering::Equal => {
+                found = Some(m);
+            }
+            Ordering::Less => {
+                start = m + 1;
+            }
+            Ordering::Greater => {
+                end = m;
+            }
+        }
+    }
+    found
+}
+
+// O(1)
+pub fn get_value<S: Slot>(interface: S, src: &[u8], id: usize) -> &[u8] {
+    let pointer = get_pointer(src, id);
+    interface.read_exact(&src[pointer..]).0
+}
+
+// Returns all the keys stored at the serialized key-value 'x'
+// O(1)
+pub fn get_keys<'a, S: Slot + Copy + 'a>(
+    interface: S,
+    x: &'a [u8],
+) -> impl Iterator<Item = &'a [u8]> {
+    (0..get_no_elems(x))
+        .map(move |i| get_value(interface, x, i))
+        .map(move |v| interface.get_key(v))
+}
+
+fn transfer_elem<S, R>(
+    interface: S,
+    at: &mut R,
+    from: &[u8],
+    id: Pointer,
+    writen_elems: usize,
+    crnt_length: usize,
+) -> io::Result<usize>
+where
+    S: Slot,
+    R: Write + Seek,
+{
+    let idx_slot = (HEADER_LEN + (writen_elems * POINTER_LEN)) as u64;
+    let value = get_value::<S>(interface, from, id);
+    at.seek(SeekFrom::Start(idx_slot))?;
+    at.write_all(&crnt_length.to_le_bytes())?;
+    at.seek(SeekFrom::Start(crnt_length as u64))?;
+    at.write_all(value)?;
+    Ok(crnt_length + value.len())
+}
+fn get_metrics<S: Slot>(interface: S, source: &[u8]) -> (usize, usize) {
+    let len = get_no_elems(source);
+    let mut value_space = 0;
+    let mut no_elems = 0;
+    for id in 0..len {
+        let ptr = get_pointer(source, id);
+        let (elem, _) = interface.read_exact(&source[ptr..]);
+        if interface.keep_in_merge(elem) {
+            value_space += elem.len();
+            no_elems += 1;
+        }
+    }
+    (no_elems, value_space)
+}
+
+// Merge algorithm for n key-value stores.
+// WARNING: In case of keys duplicatied keys it favors the contents of the first slot.
+// Returns the number of elements merged into the file.
+pub fn merge<S, R>(recepient: &mut R, producers: Vec<(S, &[u8])>) -> io::Result<usize>
+where
+    S: Slot + Copy,
+    R: Write + Seek,
+{
+    let lens = producers
+        .iter()
+        .copied()
+        .map(|(_, data)| data)
+        .map(get_no_elems)
+        .collect::<Vec<_>>();
+
+    // The number of elements that will remain at the merged file
+    // needs to be computed so the space is reserved.
+    let (no_elems, value_space) = producers
+        .iter()
+        .copied()
+        .map(|(interface, p)| get_metrics::<S>(interface, p))
+        .fold((0, 0), |(ne, vs), (ne_p, vs_p)| (ne + ne_p, vs + vs_p));
+
+    // Reserve space
+    let total_space = HEADER_LEN + (POINTER_LEN * no_elems) + value_space;
+    for _ in 0..total_space {
+        recepient.write_all(&[0])?;
+    }
+
+    // Merge loop
+    let mut writen_elems = 0;
+    let mut crnt_length = HEADER_LEN + (POINTER_LEN * no_elems);
+    let mut ids = vec![0usize; producers.len()];
+
+    while ids
+        .iter()
+        .copied()
+        .zip(lens.iter().copied())
+        .any(|(id, len)| id < len)
+    {
+        let min_data = producers
+            .iter()
+            .copied()
+            .zip(ids.iter().copied())
+            .zip(lens.iter().copied())
+            .filter(|((_, x_id), x_len)| *x_id < *x_len)
+            .map(|((x, x_id), _)| (x, x_id, get_pointer(x.1, x_id)))
+            .filter(|((interface, x), _, x_ptr)| interface.keep_in_merge(&x[*x_ptr..]))
+            .min_by(|(x, _, x_ptr), (y, _, y_ptr)| x.0.cmp_slot(&x.1[*x_ptr..], &y.1[*y_ptr..]));
+        producers
+            .iter()
+            .copied()
+            .zip(ids.iter_mut())
+            .zip(lens.iter().copied())
+            .filter(|((_, x_id), x_len)| **x_id < *x_len)
+            .map(|((x, x_id), _)| (x, get_pointer(x.1, *x_id), x_id))
+            .for_each(|((interface, x), x_ptr, x_id)| {
+                let is_equal = min_data.map(|(min, _, min_ptr)| {
+                    interface.cmp_slot(&x[x_ptr..], &min.1[min_ptr..]).is_eq()
+                });
+                if !interface.keep_in_merge(&x[x_ptr..]) {
+                    *x_id += 1;
+                } else if is_equal.unwrap_or_default() {
+                    *x_id += 1
+                }
+            });
+        if let Some(((interface, min), min_id, _)) = min_data {
+            crnt_length =
+                transfer_elem(interface, recepient, min, min_id, writen_elems, crnt_length)?;
+            writen_elems += 1;
+        }
+    }
+    // Write the number of elements
+    recepient.seek(SeekFrom::Start(0))?;
+    recepient.write_all(&writen_elems.to_le_bytes())?;
+    recepient.seek(SeekFrom::Start(0)).unwrap();
+    recepient.flush()?;
+    Ok(writen_elems)
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    // u32 numbers in big-endian (so cmp is faster)
+    #[derive(Clone, Copy)]
+    pub struct TElem;
+    impl Slot for TElem {
+        fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
+            &x[0..4]
+        }
+        fn cmp_keys(&self, x: &[u8], key: &[u8]) -> Ordering {
+            x[0..4].cmp(&key[0..4])
+        }
+        fn read_exact<'a>(&self, x: &'a [u8]) -> (&'a [u8], &'a [u8]) {
+            x.split_at(4)
+        }
+    }
+
+    fn store_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
+        let no_values = get_no_elems(buf);
+        assert_eq!(no_values, expected.len());
+        for (i, item) in expected.iter().enumerate() {
+            let value_ptr = get_pointer(buf, i);
+            let (exact, _) = TElem.read_exact(&buf[value_ptr..]);
+            assert_eq!(exact, item.as_ref());
+        }
+    }
+
+    fn retrieval_checks(expected: &[impl AsRef<[u8]>], buf: &[u8]) {
+        let interface = TElem;
+        let mut expected_keys = vec![];
+        for i in 0..expected.len() {
+            let id =
+                search_by_key(interface, buf, interface.get_key(expected[i].as_ref())).unwrap();
+            let value = get_value(interface, buf, id);
+            let key = interface.get_key(value);
+            let (head, tail) = interface.read_exact(value);
+            expected_keys.push(key);
+            assert_eq!(id, i);
+            assert_eq!(value, head);
+            assert_eq!(tail, &[] as &[u8]);
+            assert_eq!(value, expected[id].as_ref());
+        }
+        let got_keys = get_keys(interface, buf).collect::<Vec<_>>();
+        assert_eq!(expected_keys, got_keys);
+    }
+    #[test]
+    fn store_test() {
+        let elems: [u32; 5] = [0, 1, 2, 3, 4];
+        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
+        let mut buf = Vec::new();
+        create_key_value(&mut buf, encoded.clone()).unwrap();
+        store_checks(&encoded, &buf);
+    }
+    #[test]
+    fn retrieval_test() {
+        let elems: [u32; 5] = [0, 1, 2, 3, 4];
+        let encoded: Vec<_> = elems.iter().map(|x| x.to_be_bytes()).collect();
+        let mut buf = Vec::new();
+        create_key_value(&mut buf, encoded.clone()).unwrap();
+        store_checks(&encoded, &buf);
+        retrieval_checks(&encoded, &buf);
+    }
+    #[test]
+    fn merge_test() {
+        use std::io::Read;
+        let v0: Vec<_> = [0u32, 2, 4].iter().map(|x| x.to_be_bytes()).collect();
+        let v1: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
+        let v2: Vec<_> = [8u32, 9, 10, 11].iter().map(|x| x.to_be_bytes()).collect();
+        let v3: Vec<_> = [1u32, 2, 5, 7].iter().map(|x| x.to_be_bytes()).collect();
+        let expected: Vec<_> = [0u32, 1, 2, 4, 5, 7, 8, 9, 10, 11]
+            .iter()
+            .map(|x| x.to_be_bytes())
+            .collect();
+        let mut v0_store = vec![];
+        let mut v1_store = vec![];
+        let mut v2_store = vec![];
+        let mut v3_store = vec![];
+        create_key_value(&mut v0_store, v0).unwrap();
+        create_key_value(&mut v1_store, v1).unwrap();
+        create_key_value(&mut v2_store, v2).unwrap();
+        create_key_value(&mut v3_store, v3).unwrap();
+        let mut file = tempfile::tempfile().unwrap();
+        let elems: Vec<_> = [&v0_store, &v1_store, &v2_store, &v3_store]
+            .into_iter()
+            .map(|e| (TElem, e.as_slice()))
+            .collect();
+        merge(&mut file, elems).unwrap();
+        let mut buf = vec![];
+        file.read_to_end(&mut buf).unwrap();
+        store_checks(&expected, &buf);
+        retrieval_checks(&expected, &buf);
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/mod.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod dtrie_ram;
-pub mod key_value;
-pub mod trie;
-pub mod trie_ram;
-pub mod vector;
-
-pub mod usize_utils {
-    pub const USIZE_LEN: usize = (usize::BITS / 8) as usize;
-    pub fn usize_from_slice_le(v: &[u8]) -> usize {
-        let mut buff = [0; USIZE_LEN];
-        buff.copy_from_slice(v);
-        usize::from_le_bytes(buff)
-    }
-}
-
-pub trait DeleteLog: std::marker::Sync {
-    fn is_deleted(&self, _: &[u8]) -> bool;
-}
-
-impl<'a, D: DeleteLog> DeleteLog for &'a D {
-    fn is_deleted(&self, x: &[u8]) -> bool {
-        D::is_deleted(self, x)
-    }
-}
-
-impl DeleteLog for dtrie_ram::DTrie {
-    fn is_deleted(&self, key: &[u8]) -> bool {
-        self.get(key).is_some()
-    }
-}
-
-impl<Dl: DeleteLog, S: key_value::Slot> key_value::Slot for (Dl, S) {
-    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
-        self.1.get_key(x)
-    }
-    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
-        self.1.cmp_keys(x, key)
-    }
-    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
-        self.1.read_exact(x)
-    }
-    fn keep_in_merge(&self, x: &[u8]) -> bool {
-        let key = self.1.get_key(x);
-        !self.0.is_deleted(key)
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod dtrie_ram;
+pub mod key_value;
+pub mod trie;
+pub mod trie_ram;
+pub mod vector;
+
+pub mod usize_utils {
+    pub const USIZE_LEN: usize = (usize::BITS / 8) as usize;
+    pub fn usize_from_slice_le(v: &[u8]) -> usize {
+        let mut buff = [0; USIZE_LEN];
+        buff.copy_from_slice(v);
+        usize::from_le_bytes(buff)
+    }
+}
+
+pub trait DeleteLog: std::marker::Sync {
+    fn is_deleted(&self, _: &[u8]) -> bool;
+}
+
+impl<'a, D: DeleteLog> DeleteLog for &'a D {
+    fn is_deleted(&self, x: &[u8]) -> bool {
+        D::is_deleted(self, x)
+    }
+}
+
+impl DeleteLog for dtrie_ram::DTrie {
+    fn is_deleted(&self, key: &[u8]) -> bool {
+        self.get(key).is_some()
+    }
+}
+
+impl<Dl: DeleteLog, S: key_value::Slot> key_value::Slot for (Dl, S) {
+    fn get_key<'a>(&self, x: &'a [u8]) -> &'a [u8] {
+        self.1.get_key(x)
+    }
+    fn cmp_keys(&self, x: &[u8], key: &[u8]) -> std::cmp::Ordering {
+        self.1.cmp_keys(x, key)
+    }
+    fn read_exact<'a>(&self, x: &'a [u8]) -> (/* head */ &'a [u8], /* tail */ &'a [u8]) {
+        self.1.read_exact(x)
+    }
+    fn keep_in_merge(&self, x: &[u8]) -> bool {
+        let key = self.1.get_key(x);
+        !self.0.is_deleted(key)
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/trie.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/trie.rs`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,180 +1,180 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io;
-
-use super::trie_ram::*;
-use super::usize_utils::*;
-
-const ADJ_HEADER: usize = 1 + USIZE_LEN;
-const EDGE_LEN: usize = 1 + USIZE_LEN;
-
-const IS_FINAL: (usize, usize) = (0, 1);
-const LENGTH: (usize, usize) = (IS_FINAL.1, IS_FINAL.1 + USIZE_LEN);
-const TABLE: usize = LENGTH.1;
-
-fn get_node_ptr(trie: &[u8], node: usize) -> usize {
-    let start = trie.len() - ((node + 1) * USIZE_LEN);
-    usize_from_slice_le(&trie[start..(start + USIZE_LEN)])
-}
-
-// A serialized trie is an value section and a index section and the length.
-// -> len: usize little endian
-// Value section:
-// -> Is final: 1 byte, if the value stored is 1 then is final.
-// -> no_connexions:  usize little endian.
-// per connexion:
-// -> 1 byte for the edge label
-// -> 1 usize little endian for the targeted node.
-// Index section: 1 usize in little per node in order, containing the
-// address of the value section.
-pub fn serialize_into<W: io::Write>(mut buf: W, trie: Trie) -> io::Result<()> {
-    use std::collections::HashMap;
-    let no_nodes = trie.len();
-    let len = serialized_len(&trie);
-    let mut indexing = HashMap::new();
-    let mut byte_offset = 0;
-    buf.write_all(&len.to_le_bytes())?;
-    byte_offset += USIZE_LEN;
-    for (node, (is_final, adjacency)) in trie.into_iter().enumerate() {
-        indexing.insert(node, byte_offset);
-        buf.write_all(&[u8::from(is_final)])?;
-        buf.write_all(&adjacency.len().to_le_bytes())?;
-        byte_offset += ADJ_HEADER;
-        for (edge, node) in adjacency {
-            buf.write_all(&edge.to_le_bytes())?;
-            buf.write_all(&node.to_le_bytes())?;
-            byte_offset += EDGE_LEN;
-        }
-    }
-    for node in (0..no_nodes).rev() {
-        let is_in = indexing[&node];
-        buf.write_all(&is_in.to_le_bytes())?;
-        byte_offset += USIZE_LEN;
-    }
-    buf.flush()
-}
-pub fn serialized_len(trie: &Trie) -> usize {
-    USIZE_LEN
-        + trie
-            .iter()
-            .map(|(_, table)| EDGE_LEN * table.len())
-            .map(|table_len| table_len + ADJ_HEADER)
-            .map(|value| value + USIZE_LEN)
-            .sum::<usize>()
-}
-pub fn serialize(trie: Trie) -> Vec<u8> {
-    let mut buf = vec![];
-    serialize_into(&mut buf, trie).unwrap();
-    buf
-}
-pub fn has_word(trie: &[u8], word: &[u8]) -> bool {
-    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
-    search(&trie[0..len], 0, word)
-}
-
-pub fn decompress(trie: &[u8]) -> Vec<String> {
-    let mut collector = vec![];
-    let mut current = vec![];
-    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
-    decompress_labels(&trie[0..len], 0, &mut collector, &mut current);
-    collector
-}
-
-fn decompress_labels(trie: &[u8], node: usize, collector: &mut Vec<String>, current: &mut Vec<u8>) {
-    let node_ptr = get_node_ptr(trie, node);
-    if trie[node_ptr] == 1 {
-        let label = String::from_utf8_lossy(current).to_string();
-        collector.push(label);
-    }
-    let offset = &trie[node_ptr..];
-    let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
-    let adjacency = &offset[TABLE..];
-    let mut i = 0;
-    while i < length {
-        let position = i * EDGE_LEN;
-        let number_s = position + 1;
-        let number_e = number_s + USIZE_LEN;
-        let new_byte = adjacency[position];
-        let new_node = usize_from_slice_le(&adjacency[number_s..number_e]);
-        current.push(new_byte);
-        decompress_labels(trie, new_node, collector, current);
-        current.pop();
-        i += 1;
-    }
-}
-
-fn search(trie: &[u8], node: usize, word: &[u8]) -> bool {
-    let node_ptr = get_node_ptr(trie, node);
-    match word {
-        [] => trie[node_ptr] == 1,
-        [head, tail @ ..] => {
-            let offset = &trie[node_ptr..];
-            let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
-            let adjacency = &offset[TABLE..];
-            let mut i = 0;
-            let mut goes_to = None;
-            while i < length && goes_to.is_none() {
-                let position = i * EDGE_LEN;
-                if *head == adjacency[position] {
-                    let number_s = position + 1;
-                    let number_e = number_s + USIZE_LEN;
-                    goes_to = Some(usize_from_slice_le(&adjacency[number_s..number_e]));
-                }
-                i += 1;
-            }
-            match goes_to {
-                Some(new_node) => search(trie, new_node, tail),
-                None => false,
-            }
-        }
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-    #[test]
-    fn create_and_search_test() {
-        let dictionary = [
-            b"WORD1".as_slice(),
-            b"WORD2".as_slice(),
-            b"WORD3".as_slice(),
-            b"ORD1".as_slice(),
-            b"BAD".as_slice(),
-            b"GOOD".as_slice(),
-        ];
-        let not_in_dictionary = [
-            b"WO1D1".as_slice(),
-            b"LORD".as_slice(),
-            b"BAF".as_slice(),
-            b"WOR".as_slice(),
-        ];
-
-        let trie = create_trie(&dictionary);
-        let trie = serialize(trie);
-        let labels = super::decompress(&trie);
-        assert!(dictionary.iter().all(|w| has_word(&trie, w)));
-        assert!(not_in_dictionary.iter().all(|w| !has_word(&trie, w)));
-
-        assert_eq!(labels.len(), dictionary.len());
-        assert!(labels.iter().all(|w| dictionary.contains(&w.as_bytes())));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io;
+
+use super::trie_ram::*;
+use super::usize_utils::*;
+
+const ADJ_HEADER: usize = 1 + USIZE_LEN;
+const EDGE_LEN: usize = 1 + USIZE_LEN;
+
+const IS_FINAL: (usize, usize) = (0, 1);
+const LENGTH: (usize, usize) = (IS_FINAL.1, IS_FINAL.1 + USIZE_LEN);
+const TABLE: usize = LENGTH.1;
+
+fn get_node_ptr(trie: &[u8], node: usize) -> usize {
+    let start = trie.len() - ((node + 1) * USIZE_LEN);
+    usize_from_slice_le(&trie[start..(start + USIZE_LEN)])
+}
+
+// A serialized trie is an value section and a index section and the length.
+// -> len: usize little endian
+// Value section:
+// -> Is final: 1 byte, if the value stored is 1 then is final.
+// -> no_connexions:  usize little endian.
+// per connexion:
+// -> 1 byte for the edge label
+// -> 1 usize little endian for the targeted node.
+// Index section: 1 usize in little per node in order, containing the
+// address of the value section.
+pub fn serialize_into<W: io::Write>(mut buf: W, trie: Trie) -> io::Result<()> {
+    use std::collections::HashMap;
+    let no_nodes = trie.len();
+    let len = serialized_len(&trie);
+    let mut indexing = HashMap::new();
+    let mut byte_offset = 0;
+    buf.write_all(&len.to_le_bytes())?;
+    byte_offset += USIZE_LEN;
+    for (node, (is_final, adjacency)) in trie.into_iter().enumerate() {
+        indexing.insert(node, byte_offset);
+        buf.write_all(&[u8::from(is_final)])?;
+        buf.write_all(&adjacency.len().to_le_bytes())?;
+        byte_offset += ADJ_HEADER;
+        for (edge, node) in adjacency {
+            buf.write_all(&edge.to_le_bytes())?;
+            buf.write_all(&node.to_le_bytes())?;
+            byte_offset += EDGE_LEN;
+        }
+    }
+    for node in (0..no_nodes).rev() {
+        let is_in = indexing[&node];
+        buf.write_all(&is_in.to_le_bytes())?;
+        byte_offset += USIZE_LEN;
+    }
+    buf.flush()
+}
+pub fn serialized_len(trie: &Trie) -> usize {
+    USIZE_LEN
+        + trie
+            .iter()
+            .map(|(_, table)| EDGE_LEN * table.len())
+            .map(|table_len| table_len + ADJ_HEADER)
+            .map(|value| value + USIZE_LEN)
+            .sum::<usize>()
+}
+pub fn serialize(trie: Trie) -> Vec<u8> {
+    let mut buf = vec![];
+    serialize_into(&mut buf, trie).unwrap();
+    buf
+}
+pub fn has_word(trie: &[u8], word: &[u8]) -> bool {
+    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
+    search(&trie[0..len], 0, word)
+}
+
+pub fn decompress(trie: &[u8]) -> Vec<String> {
+    let mut collector = vec![];
+    let mut current = vec![];
+    let len = usize_from_slice_le(&trie[0..USIZE_LEN]);
+    decompress_labels(&trie[0..len], 0, &mut collector, &mut current);
+    collector
+}
+
+fn decompress_labels(trie: &[u8], node: usize, collector: &mut Vec<String>, current: &mut Vec<u8>) {
+    let node_ptr = get_node_ptr(trie, node);
+    if trie[node_ptr] == 1 {
+        let label = String::from_utf8_lossy(current).to_string();
+        collector.push(label);
+    }
+    let offset = &trie[node_ptr..];
+    let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
+    let adjacency = &offset[TABLE..];
+    let mut i = 0;
+    while i < length {
+        let position = i * EDGE_LEN;
+        let number_s = position + 1;
+        let number_e = number_s + USIZE_LEN;
+        let new_byte = adjacency[position];
+        let new_node = usize_from_slice_le(&adjacency[number_s..number_e]);
+        current.push(new_byte);
+        decompress_labels(trie, new_node, collector, current);
+        current.pop();
+        i += 1;
+    }
+}
+
+fn search(trie: &[u8], node: usize, word: &[u8]) -> bool {
+    let node_ptr = get_node_ptr(trie, node);
+    match word {
+        [] => trie[node_ptr] == 1,
+        [head, tail @ ..] => {
+            let offset = &trie[node_ptr..];
+            let length = usize_from_slice_le(&offset[LENGTH.0..LENGTH.1]);
+            let adjacency = &offset[TABLE..];
+            let mut i = 0;
+            let mut goes_to = None;
+            while i < length && goes_to.is_none() {
+                let position = i * EDGE_LEN;
+                if *head == adjacency[position] {
+                    let number_s = position + 1;
+                    let number_e = number_s + USIZE_LEN;
+                    goes_to = Some(usize_from_slice_le(&adjacency[number_s..number_e]));
+                }
+                i += 1;
+            }
+            match goes_to {
+                Some(new_node) => search(trie, new_node, tail),
+                None => false,
+            }
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    #[test]
+    fn create_and_search_test() {
+        let dictionary = [
+            b"WORD1".as_slice(),
+            b"WORD2".as_slice(),
+            b"WORD3".as_slice(),
+            b"ORD1".as_slice(),
+            b"BAD".as_slice(),
+            b"GOOD".as_slice(),
+        ];
+        let not_in_dictionary = [
+            b"WO1D1".as_slice(),
+            b"LORD".as_slice(),
+            b"BAF".as_slice(),
+            b"WOR".as_slice(),
+        ];
+
+        let trie = create_trie(&dictionary);
+        let trie = serialize(trie);
+        let labels = super::decompress(&trie);
+        assert!(dictionary.iter().all(|w| has_word(&trie, w)));
+        assert!(not_in_dictionary.iter().all(|w| !has_word(&trie, w)));
+
+        assert_eq!(labels.len(), dictionary.len());
+        assert!(labels.iter().all(|w| dictionary.contains(&w.as_bytes())));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/data_types/vector.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/data_types/vector.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,140 +1,140 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use std::io::{Read, Write};
-type Len = u64;
-type Unit = f32;
-type Dist = f32;
-
-fn encode_length(mut buff: Vec<u8>, vec: &[Unit]) -> Vec<u8> {
-    let len = vec.len() as Len;
-    buff.write_all(&len.to_le_bytes()).unwrap();
-    buff.flush().unwrap();
-    buff
-}
-fn encode_unit(mut buff: Vec<u8>, unit: Unit) -> Vec<u8> {
-    buff.write_all(&unit.to_le_bytes()).unwrap();
-    buff.flush().unwrap();
-    buff
-}
-
-pub fn cosine_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
-    let mut buff_x = [0; 8];
-    let mut buff_y = [0; 8];
-    x.read_exact(&mut buff_x).unwrap();
-    y.read_exact(&mut buff_y).unwrap();
-    let len_x = Len::from_le_bytes(buff_x);
-    let len_y = Len::from_le_bytes(buff_y);
-    assert_eq!(len_x, len_y);
-    let len = len_x;
-    let mut buff_x = [0; 4];
-    let mut buff_y = [0; 4];
-    let mut sum = 0.0;
-    let mut dem_x = 0.0;
-    let mut dem_y = 0.0;
-    for _ in 0..len {
-        x.read_exact(&mut buff_x).unwrap();
-        y.read_exact(&mut buff_y).unwrap();
-        let x_value = Unit::from_le_bytes(buff_x);
-        let y_value = Unit::from_le_bytes(buff_y);
-        sum += x_value * y_value;
-        dem_x += x_value * x_value;
-        dem_y += y_value * y_value;
-    }
-    sum / (f32::sqrt(dem_x) * f32::sqrt(dem_y))
-}
-
-pub fn dot_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
-    let mut buff_x = [0; 8];
-    let mut buff_y = [0; 8];
-    x.read_exact(&mut buff_x).unwrap();
-    y.read_exact(&mut buff_y).unwrap();
-    let len_x = Len::from_le_bytes(buff_x);
-    let len_y = Len::from_le_bytes(buff_y);
-    assert_eq!(len_x, len_y);
-    let len = len_x;
-    let mut buff_x = [0; 4];
-    let mut buff_y = [0; 4];
-    let mut sum = 0.0;
-    for _ in 0..len {
-        x.read_exact(&mut buff_x).unwrap();
-        y.read_exact(&mut buff_y).unwrap();
-        let x_value = Unit::from_le_bytes(buff_x);
-        let y_value = Unit::from_le_bytes(buff_y);
-        sum += x_value * y_value;
-    }
-    sum
-}
-
-pub fn encode_vector(vec: &[Unit]) -> Vec<u8> {
-    vec.iter()
-        .cloned()
-        .fold(encode_length(vec![], vec), encode_unit)
-}
-
-#[cfg(test)]
-mod test {
-    use super::*;
-    fn naive_cosine_similatiry(a: &[f32], b: &[f32]) -> f32 {
-        let ab: f32 = a
-            .iter()
-            .cloned()
-            .zip(b.iter().cloned())
-            .map(|(a, b)| a * b)
-            .sum();
-        let aa: f32 = a.iter().cloned().map(|a| a * a).sum();
-        let bb: f32 = b.iter().cloned().map(|b| b * b).sum();
-        ab / (f32::sqrt(aa) * f32::sqrt(bb))
-    }
-
-    fn naive_dot_similatiry(a: &[f32], b: &[f32]) -> f32 {
-        a.iter()
-            .cloned()
-            .zip(b.iter().cloned())
-            .map(|(a, b)| a * b)
-            .sum()
-    }
-
-    #[test]
-    fn cosine_test() {
-        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
-        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
-        let v0_r = encode_vector(&v0);
-        let v1_r = encode_vector(&v1);
-        assert_eq!(
-            naive_cosine_similatiry(&v0, &v1),
-            cosine_similarity(&v0_r, &v1_r)
-        );
-        assert_eq!(
-            naive_cosine_similatiry(&v0, &v0),
-            cosine_similarity(&v0_r, &v0_r)
-        );
-    }
-
-    #[test]
-    fn dot_test() {
-        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
-        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
-        let v0_r = encode_vector(&v0);
-        let v1_r = encode_vector(&v1);
-        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
-        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io::{Read, Write};
+type Len = u64;
+type Unit = f32;
+type Dist = f32;
+
+fn encode_length(mut buff: Vec<u8>, vec: &[Unit]) -> Vec<u8> {
+    let len = vec.len() as Len;
+    buff.write_all(&len.to_le_bytes()).unwrap();
+    buff.flush().unwrap();
+    buff
+}
+fn encode_unit(mut buff: Vec<u8>, unit: Unit) -> Vec<u8> {
+    buff.write_all(&unit.to_le_bytes()).unwrap();
+    buff.flush().unwrap();
+    buff
+}
+
+pub fn cosine_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
+    let mut buff_x = [0; 8];
+    let mut buff_y = [0; 8];
+    x.read_exact(&mut buff_x).unwrap();
+    y.read_exact(&mut buff_y).unwrap();
+    let len_x = Len::from_le_bytes(buff_x);
+    let len_y = Len::from_le_bytes(buff_y);
+    assert_eq!(len_x, len_y);
+    let len = len_x;
+    let mut buff_x = [0; 4];
+    let mut buff_y = [0; 4];
+    let mut sum = 0.0;
+    let mut dem_x = 0.0;
+    let mut dem_y = 0.0;
+    for _ in 0..len {
+        x.read_exact(&mut buff_x).unwrap();
+        y.read_exact(&mut buff_y).unwrap();
+        let x_value = Unit::from_le_bytes(buff_x);
+        let y_value = Unit::from_le_bytes(buff_y);
+        sum += x_value * y_value;
+        dem_x += x_value * x_value;
+        dem_y += y_value * y_value;
+    }
+    sum / (f32::sqrt(dem_x) * f32::sqrt(dem_y))
+}
+
+pub fn dot_similarity(mut x: &[u8], mut y: &[u8]) -> Dist {
+    let mut buff_x = [0; 8];
+    let mut buff_y = [0; 8];
+    x.read_exact(&mut buff_x).unwrap();
+    y.read_exact(&mut buff_y).unwrap();
+    let len_x = Len::from_le_bytes(buff_x);
+    let len_y = Len::from_le_bytes(buff_y);
+    assert_eq!(len_x, len_y);
+    let len = len_x;
+    let mut buff_x = [0; 4];
+    let mut buff_y = [0; 4];
+    let mut sum = 0.0;
+    for _ in 0..len {
+        x.read_exact(&mut buff_x).unwrap();
+        y.read_exact(&mut buff_y).unwrap();
+        let x_value = Unit::from_le_bytes(buff_x);
+        let y_value = Unit::from_le_bytes(buff_y);
+        sum += x_value * y_value;
+    }
+    sum
+}
+
+pub fn encode_vector(vec: &[Unit]) -> Vec<u8> {
+    vec.iter()
+        .cloned()
+        .fold(encode_length(vec![], vec), encode_unit)
+}
+
+#[cfg(test)]
+mod test {
+    use super::*;
+    fn naive_cosine_similatiry(a: &[f32], b: &[f32]) -> f32 {
+        let ab: f32 = a
+            .iter()
+            .cloned()
+            .zip(b.iter().cloned())
+            .map(|(a, b)| a * b)
+            .sum();
+        let aa: f32 = a.iter().cloned().map(|a| a * a).sum();
+        let bb: f32 = b.iter().cloned().map(|b| b * b).sum();
+        ab / (f32::sqrt(aa) * f32::sqrt(bb))
+    }
+
+    fn naive_dot_similatiry(a: &[f32], b: &[f32]) -> f32 {
+        a.iter()
+            .cloned()
+            .zip(b.iter().cloned())
+            .map(|(a, b)| a * b)
+            .sum()
+    }
+
+    #[test]
+    fn cosine_test() {
+        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
+        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
+        let v0_r = encode_vector(&v0);
+        let v1_r = encode_vector(&v1);
+        assert_eq!(
+            naive_cosine_similatiry(&v0, &v1),
+            cosine_similarity(&v0_r, &v1_r)
+        );
+        assert_eq!(
+            naive_cosine_similatiry(&v0, &v0),
+            cosine_similarity(&v0_r, &v0_r)
+        );
+    }
+
+    #[test]
+    fn dot_test() {
+        let v0: Vec<_> = (0..758).map(|i| (i * 2) as f32).collect();
+        let v1: Vec<_> = (0..758).map(|i| ((i * 2) + 1) as f32).collect();
+        let v0_r = encode_vector(&v0);
+        let v1_r = encode_vector(&v1);
+        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
+        assert_eq!(naive_dot_similatiry(&v0, &v1), dot_similarity(&v0_r, &v1_r));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/formula/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/formula/mod.rs`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,162 +1,162 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-use crate::data_point::{Address, DataRetriever};
-
-/// Is a singleton clause formed by label. It will be satisfied only if the address
-/// is applied to has the label.
-#[derive(Debug, Clone)]
-pub struct LabelClause {
-    value: String,
-}
-impl LabelClause {
-    pub fn new(labels: String) -> LabelClause {
-        LabelClause { value: labels }
-    }
-    fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
-        retriever.has_label(x, self.value.as_bytes())
-    }
-}
-
-/// Is a clause formed by the conjuction of several LabelClauses. Additionally this
-/// clause has a threshold that specifies the minimum number of LabelClauses that have to
-/// succeed in order for the overall conjuction to be satisfied.
-#[derive(Debug, Clone)]
-pub struct CompoundClause {
-    threshold: usize,
-    labels: Vec<LabelClause>,
-}
-impl CompoundClause {
-    pub fn new(threshold: usize, labels: Vec<LabelClause>) -> CompoundClause {
-        CompoundClause { threshold, labels }
-    }
-    fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
-        let number_of_subqueries = self.labels.len();
-        let mut threshold = self.threshold;
-        let mut i = 0;
-        while threshold > 0 && i <= number_of_subqueries {
-            let is_valid = self.labels[i].run(x, retriever);
-            threshold -= is_valid as usize;
-            i += 1;
-        }
-        threshold == 0
-    }
-}
-
-/// Wrapper that unifies the different types of clauses a formula may have.
-#[derive(Debug, Clone)]
-pub enum Clause {
-    Label(LabelClause),
-    Compound(CompoundClause),
-}
-
-impl Clause {
-    fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
-        match self {
-            Clause::Compound(q) => q.run(x, retriever),
-            Clause::Label(q) => q.run(x, retriever),
-        }
-    }
-}
-
-impl From<LabelClause> for Clause {
-    fn from(value: LabelClause) -> Self {
-        Clause::Label(value)
-    }
-}
-
-impl From<CompoundClause> for Clause {
-    fn from(value: CompoundClause) -> Self {
-        Clause::Compound(value)
-    }
-}
-
-/// Formulas are boolean expressions in conjuctive normal form, but for labels.
-/// The clauses in a formula are connected by intersections, and they are formed
-/// by strings. Once applied to a given address, the formula becomes a boolean
-/// expression that evaluates to whether the address is valid or not.
-#[derive(Debug, Clone, Default)]
-pub struct Formula {
-    clauses: Vec<Clause>,
-}
-impl Formula {
-    pub fn new() -> Formula {
-        Formula::default()
-    }
-    pub fn extend<C>(&mut self, clause: C)
-    where Clause: From<C> {
-        self.clauses.push(clause.into())
-    }
-    pub fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
-        self.clauses.iter().all(|q| q.run(x, retriever))
-    }
-}
-
-#[cfg(test)]
-mod tests {
-    use std::collections::HashSet;
-
-    use super::*;
-    use crate::data_point::Address;
-    struct DummyRetriever {
-        labels: HashSet<&'static [u8]>,
-    }
-    impl DataRetriever for DummyRetriever {
-        fn has_label(&self, _: Address, label: &[u8]) -> bool {
-            self.labels.contains(label)
-        }
-        fn is_deleted(&self, _: Address) -> bool {
-            panic!("Not meant to be used")
-        }
-        fn similarity(&self, _: Address, _: Address) -> f32 {
-            panic!("Not meant to be used")
-        }
-        fn get_vector(&self, _: Address) -> &[u8] {
-            panic!("Not meant to be used")
-        }
-    }
-    #[test]
-    fn test_query() {
-        const L1: &str = "Label1";
-        const L2: &str = "Label2";
-        const L3: &str = "Label3";
-        const ADDRESS: Address = Address::dummy();
-        let retriever = DummyRetriever {
-            labels: [L1.as_bytes(), L3.as_bytes()].into_iter().collect(),
-        };
-        let mut formula = Formula::new();
-        formula.extend(LabelClause::new(L1.to_string()));
-        formula.extend(LabelClause::new(L3.to_string()));
-        assert!(formula.run(ADDRESS, &retriever));
-
-        let mut formula = Formula::new();
-        formula.extend(LabelClause::new(L1.to_string()));
-        formula.extend(LabelClause::new(L2.to_string()));
-        assert!(!formula.run(ADDRESS, &retriever));
-
-        let mut formula = Formula::new();
-        let inner = vec![
-            LabelClause::new(L1.to_string()),
-            LabelClause::new(L2.to_string()),
-        ];
-        formula.extend(CompoundClause::new(1, inner));
-        assert!(formula.run(ADDRESS, &retriever));
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use crate::data_point::{Address, DataRetriever};
+
+/// Is a singleton clause formed by label. It will be satisfied only if the address
+/// is applied to has the label.
+#[derive(Debug, Clone)]
+pub struct LabelClause {
+    value: String,
+}
+impl LabelClause {
+    pub fn new(labels: String) -> LabelClause {
+        LabelClause { value: labels }
+    }
+    fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
+        retriever.has_label(x, self.value.as_bytes())
+    }
+}
+
+/// Is a clause formed by the conjuction of several LabelClauses. Additionally this
+/// clause has a threshold that specifies the minimum number of LabelClauses that have to
+/// succeed in order for the overall conjuction to be satisfied.
+#[derive(Debug, Clone)]
+pub struct CompoundClause {
+    threshold: usize,
+    labels: Vec<LabelClause>,
+}
+impl CompoundClause {
+    pub fn new(threshold: usize, labels: Vec<LabelClause>) -> CompoundClause {
+        CompoundClause { threshold, labels }
+    }
+    fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
+        let number_of_subqueries = self.labels.len();
+        let mut threshold = self.threshold;
+        let mut i = 0;
+        while threshold > 0 && i <= number_of_subqueries {
+            let is_valid = self.labels[i].run(x, retriever);
+            threshold -= is_valid as usize;
+            i += 1;
+        }
+        threshold == 0
+    }
+}
+
+/// Wrapper that unifies the different types of clauses a formula may have.
+#[derive(Debug, Clone)]
+pub enum Clause {
+    Label(LabelClause),
+    Compound(CompoundClause),
+}
+
+impl Clause {
+    fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
+        match self {
+            Clause::Compound(q) => q.run(x, retriever),
+            Clause::Label(q) => q.run(x, retriever),
+        }
+    }
+}
+
+impl From<LabelClause> for Clause {
+    fn from(value: LabelClause) -> Self {
+        Clause::Label(value)
+    }
+}
+
+impl From<CompoundClause> for Clause {
+    fn from(value: CompoundClause) -> Self {
+        Clause::Compound(value)
+    }
+}
+
+/// Formulas are boolean expressions in conjuctive normal form, but for labels.
+/// The clauses in a formula are connected by intersections, and they are formed
+/// by strings. Once applied to a given address, the formula becomes a boolean
+/// expression that evaluates to whether the address is valid or not.
+#[derive(Debug, Clone, Default)]
+pub struct Formula {
+    clauses: Vec<Clause>,
+}
+impl Formula {
+    pub fn new() -> Formula {
+        Formula::default()
+    }
+    pub fn extend<C>(&mut self, clause: C)
+    where Clause: From<C> {
+        self.clauses.push(clause.into())
+    }
+    pub fn run<D: DataRetriever>(&self, x: Address, retriever: &D) -> bool {
+        self.clauses.iter().all(|q| q.run(x, retriever))
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use std::collections::HashSet;
+
+    use super::*;
+    use crate::data_point::Address;
+    struct DummyRetriever {
+        labels: HashSet<&'static [u8]>,
+    }
+    impl DataRetriever for DummyRetriever {
+        fn has_label(&self, _: Address, label: &[u8]) -> bool {
+            self.labels.contains(label)
+        }
+        fn is_deleted(&self, _: Address) -> bool {
+            panic!("Not meant to be used")
+        }
+        fn similarity(&self, _: Address, _: Address) -> f32 {
+            panic!("Not meant to be used")
+        }
+        fn get_vector(&self, _: Address) -> &[u8] {
+            panic!("Not meant to be used")
+        }
+    }
+    #[test]
+    fn test_query() {
+        const L1: &str = "Label1";
+        const L2: &str = "Label2";
+        const L3: &str = "Label3";
+        const ADDRESS: Address = Address::dummy();
+        let retriever = DummyRetriever {
+            labels: [L1.as_bytes(), L3.as_bytes()].into_iter().collect(),
+        };
+        let mut formula = Formula::new();
+        formula.extend(LabelClause::new(L1.to_string()));
+        formula.extend(LabelClause::new(L3.to_string()));
+        assert!(formula.run(ADDRESS, &retriever));
+
+        let mut formula = Formula::new();
+        formula.extend(LabelClause::new(L1.to_string()));
+        formula.extend(LabelClause::new(L2.to_string()));
+        assert!(!formula.run(ADDRESS, &retriever));
+
+        let mut formula = Formula::new();
+        let inner = vec![
+            LabelClause::new(L1.to_string()),
+            LabelClause::new(L2.to_string()),
+        ];
+        formula.extend(CompoundClause::new(1, inner));
+        assert!(formula.run(ADDRESS, &retriever));
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/indexset/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/indexset/mod.rs`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,114 +1,114 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-mod state;
-use std::path::{Path, PathBuf};
-use std::sync::RwLock;
-
-use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
-use state::State;
-
-use crate::data_point::Similarity;
-use crate::data_point_provider::{Index, IndexCheck};
-use crate::VectorR;
-pub trait IndexKeyCollector {
-    fn add_key(&mut self, key: String);
-}
-
-pub struct IndexSet {
-    state: RwLock<State>,
-    date: RwLock<Version>,
-    location: PathBuf,
-}
-impl IndexSet {
-    pub fn new(path: &Path, with_check: IndexCheck) -> VectorR<IndexSet> {
-        if !path.exists() {
-            std::fs::create_dir_all(path)?;
-        }
-        fs_state::initialize_disk(path, || State::new(path.to_path_buf()))?;
-        let lock = fs_state::shared_lock(path)?;
-        let state = fs_state::load_state::<State>(&lock)?;
-        let date = fs_state::crnt_version(&lock)?;
-        if let IndexCheck::Sanity = with_check {
-            state.do_sanity_checks()?;
-        }
-        let index = IndexSet {
-            state: RwLock::new(state),
-            date: RwLock::new(date),
-            location: path.to_path_buf(),
-        };
-        Ok(index)
-    }
-    pub fn remove_index(&mut self, index: &str, _: &ELock) -> VectorR<()> {
-        let mut write = self.state.write().unwrap();
-        write.remove_index(index)
-    }
-    pub fn get_or_create<'a, S>(
-        &'a mut self,
-        index: S,
-        similarity: Similarity,
-        _: &ELock,
-    ) -> VectorR<Index>
-    where
-        S: Into<std::borrow::Cow<'a, str>>,
-    {
-        let mut write = self.state.write().unwrap();
-        write.get_or_create(index, similarity)
-    }
-    fn update(&self, lock: &fs_state::Lock) -> VectorR<()> {
-        let disk_v = fs_state::crnt_version(lock)?;
-        let date = *self.date.read().unwrap();
-        if disk_v > date {
-            let new_state = fs_state::load_state(lock)?;
-            let mut state = self.state.write().unwrap();
-            let mut date = self.date.write().unwrap();
-            *state = new_state;
-            *date = disk_v;
-        }
-        Ok(())
-    }
-    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C, _: &Lock) {
-        let read = self.state.read().unwrap();
-        read.index_keys(c);
-    }
-    pub fn get(&self, index: &str, _: &Lock) -> VectorR<Option<Index>> {
-        let read = self.state.read().unwrap();
-        read.get(index)
-    }
-    pub fn get_elock(&self) -> VectorR<ELock> {
-        let lock = fs_state::exclusive_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn get_slock(&self) -> VectorR<SLock> {
-        let lock = fs_state::shared_lock(&self.location)?;
-        self.update(&lock)?;
-        Ok(lock)
-    }
-    pub fn get_location(&self) -> &Path {
-        &self.location
-    }
-    pub fn commit(&self, lock: ELock) -> VectorR<()> {
-        let state = self.state.read().unwrap();
-        let mut date = self.date.write().unwrap();
-        fs_state::persist_state::<State>(&lock, &state)?;
-        *date = fs_state::crnt_version(&lock)?;
-        Ok(())
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+mod state;
+use std::path::{Path, PathBuf};
+use std::sync::RwLock;
+
+use nucliadb_core::fs_state::{self, ELock, Lock, SLock, Version};
+use state::State;
+
+use crate::data_point::Similarity;
+use crate::data_point_provider::{Index, IndexCheck};
+use crate::VectorR;
+pub trait IndexKeyCollector {
+    fn add_key(&mut self, key: String);
+}
+
+pub struct IndexSet {
+    state: RwLock<State>,
+    date: RwLock<Version>,
+    location: PathBuf,
+}
+impl IndexSet {
+    pub fn new(path: &Path, with_check: IndexCheck) -> VectorR<IndexSet> {
+        if !path.exists() {
+            std::fs::create_dir_all(path)?;
+        }
+        fs_state::initialize_disk(path, || State::new(path.to_path_buf()))?;
+        let lock = fs_state::shared_lock(path)?;
+        let state = fs_state::load_state::<State>(&lock)?;
+        let date = fs_state::crnt_version(&lock)?;
+        if let IndexCheck::Sanity = with_check {
+            state.do_sanity_checks()?;
+        }
+        let index = IndexSet {
+            state: RwLock::new(state),
+            date: RwLock::new(date),
+            location: path.to_path_buf(),
+        };
+        Ok(index)
+    }
+    pub fn remove_index(&mut self, index: &str, _: &ELock) -> VectorR<()> {
+        let mut write = self.state.write().unwrap();
+        write.remove_index(index)
+    }
+    pub fn get_or_create<'a, S>(
+        &'a mut self,
+        index: S,
+        similarity: Similarity,
+        _: &ELock,
+    ) -> VectorR<Index>
+    where
+        S: Into<std::borrow::Cow<'a, str>>,
+    {
+        let mut write = self.state.write().unwrap();
+        write.get_or_create(index, similarity)
+    }
+    fn update(&self, lock: &fs_state::Lock) -> VectorR<()> {
+        let disk_v = fs_state::crnt_version(lock)?;
+        let date = *self.date.read().unwrap();
+        if disk_v > date {
+            let new_state = fs_state::load_state(lock)?;
+            let mut state = self.state.write().unwrap();
+            let mut date = self.date.write().unwrap();
+            *state = new_state;
+            *date = disk_v;
+        }
+        Ok(())
+    }
+    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C, _: &Lock) {
+        let read = self.state.read().unwrap();
+        read.index_keys(c);
+    }
+    pub fn get(&self, index: &str, _: &Lock) -> VectorR<Option<Index>> {
+        let read = self.state.read().unwrap();
+        read.get(index)
+    }
+    pub fn get_elock(&self) -> VectorR<ELock> {
+        let lock = fs_state::exclusive_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn get_slock(&self) -> VectorR<SLock> {
+        let lock = fs_state::shared_lock(&self.location)?;
+        self.update(&lock)?;
+        Ok(lock)
+    }
+    pub fn get_location(&self) -> &Path {
+        &self.location
+    }
+    pub fn commit(&self, lock: ELock) -> VectorR<()> {
+        let state = self.state.read().unwrap();
+        let mut date = self.date.write().unwrap();
+        fs_state::persist_state::<State>(&lock, &state)?;
+        *date = fs_state::crnt_version(&lock)?;
+        Ok(())
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/indexset/state.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/indexset/state.rs`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,119 +1,119 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-use std::borrow::Cow;
-use std::collections::HashSet;
-use std::path::PathBuf;
-
-use serde::{Deserialize, Serialize};
-
-use super::IndexKeyCollector;
-use crate::data_point::Similarity;
-use crate::data_point_provider::{Index, IndexCheck, IndexMetadata};
-use crate::VectorR;
-#[derive(Serialize, Deserialize)]
-pub struct State {
-    location: PathBuf,
-    indexes: HashSet<String>,
-}
-impl State {
-    pub fn new(at: PathBuf) -> State {
-        State {
-            location: at,
-            indexes: HashSet::default(),
-        }
-    }
-    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C) {
-        self.indexes.iter().cloned().for_each(|s| c.add_key(s));
-    }
-    pub fn do_sanity_checks(&self) -> VectorR<()> {
-        for index in &self.indexes {
-            let index_path = self.location.join(index);
-            Index::open(&index_path, IndexCheck::Sanity)?;
-        }
-        Ok(())
-    }
-    pub fn remove_index(&mut self, index: &str) -> VectorR<()> {
-        if self.indexes.remove(index) {
-            let index_path = self.location.join(index);
-            std::fs::remove_dir_all(index_path)?;
-        }
-        Ok(())
-    }
-    pub fn get(&self, index: &str) -> VectorR<Option<Index>> {
-        if self.indexes.contains(index) {
-            let location = self.location.join(index);
-            Some(Index::open(&location, IndexCheck::None)).transpose()
-        } else {
-            Ok(None)
-        }
-    }
-    pub fn get_or_create<'a, S>(&mut self, index: S, similarity: Similarity) -> VectorR<Index>
-    where S: Into<Cow<'a, str>> {
-        let index: Cow<_> = index.into();
-        if self.indexes.contains(index.as_ref()) {
-            let index = index.as_ref();
-            let location = self.location.join(index);
-            Index::open(&location, IndexCheck::None)
-        } else {
-            let index = index.to_string();
-            let location = self.location.join(&index);
-            self.indexes.insert(index);
-            Index::new(&location, IndexMetadata { similarity })
-        }
-    }
-}
-
-#[cfg(test)]
-mod test {
-    use tempfile::TempDir;
-
-    use super::*;
-    #[test]
-    fn basic_functionality_test() {
-        let dir = TempDir::new().unwrap();
-        let mut vectorset = State::new(dir.path().to_path_buf());
-        let _index1 = vectorset
-            .get_or_create("Index1".to_string(), Similarity::Cosine)
-            .unwrap();
-        let _index2 = vectorset
-            .get_or_create("Index2".to_string(), Similarity::Cosine)
-            .unwrap();
-        let _index3 = vectorset
-            .get_or_create("Index3".to_string(), Similarity::Cosine)
-            .unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_some());
-        assert!(vectorset.get("Index2").unwrap().is_some());
-        assert!(vectorset.get("Index3").unwrap().is_some());
-        assert!(vectorset.get("Index4").unwrap().is_none());
-        vectorset.do_sanity_checks().unwrap();
-        vectorset.remove_index("Index1").unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_none());
-        assert!(vectorset.get("Index2").unwrap().is_some());
-        assert!(vectorset.get("Index3").unwrap().is_some());
-        vectorset.remove_index("Index2").unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_none());
-        assert!(vectorset.get("Index2").unwrap().is_none());
-        assert!(vectorset.get("Index3").unwrap().is_some());
-        vectorset.remove_index("Index3").unwrap();
-        assert!(vectorset.get("Index1").unwrap().is_none());
-        assert!(vectorset.get("Index2").unwrap().is_none());
-        assert!(vectorset.get("Index3").unwrap().is_none());
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+use std::borrow::Cow;
+use std::collections::HashSet;
+use std::path::PathBuf;
+
+use serde::{Deserialize, Serialize};
+
+use super::IndexKeyCollector;
+use crate::data_point::Similarity;
+use crate::data_point_provider::{Index, IndexCheck, IndexMetadata};
+use crate::VectorR;
+#[derive(Serialize, Deserialize)]
+pub struct State {
+    location: PathBuf,
+    indexes: HashSet<String>,
+}
+impl State {
+    pub fn new(at: PathBuf) -> State {
+        State {
+            location: at,
+            indexes: HashSet::default(),
+        }
+    }
+    pub fn index_keys<C: IndexKeyCollector>(&self, c: &mut C) {
+        self.indexes.iter().cloned().for_each(|s| c.add_key(s));
+    }
+    pub fn do_sanity_checks(&self) -> VectorR<()> {
+        for index in &self.indexes {
+            let index_path = self.location.join(index);
+            Index::open(&index_path, IndexCheck::Sanity)?;
+        }
+        Ok(())
+    }
+    pub fn remove_index(&mut self, index: &str) -> VectorR<()> {
+        if self.indexes.remove(index) {
+            let index_path = self.location.join(index);
+            std::fs::remove_dir_all(index_path)?;
+        }
+        Ok(())
+    }
+    pub fn get(&self, index: &str) -> VectorR<Option<Index>> {
+        if self.indexes.contains(index) {
+            let location = self.location.join(index);
+            Some(Index::open(&location, IndexCheck::None)).transpose()
+        } else {
+            Ok(None)
+        }
+    }
+    pub fn get_or_create<'a, S>(&mut self, index: S, similarity: Similarity) -> VectorR<Index>
+    where S: Into<Cow<'a, str>> {
+        let index: Cow<_> = index.into();
+        if self.indexes.contains(index.as_ref()) {
+            let index = index.as_ref();
+            let location = self.location.join(index);
+            Index::open(&location, IndexCheck::None)
+        } else {
+            let index = index.to_string();
+            let location = self.location.join(&index);
+            self.indexes.insert(index);
+            Index::new(&location, IndexMetadata { similarity })
+        }
+    }
+}
+
+#[cfg(test)]
+mod test {
+    use tempfile::TempDir;
+
+    use super::*;
+    #[test]
+    fn basic_functionality_test() {
+        let dir = TempDir::new().unwrap();
+        let mut vectorset = State::new(dir.path().to_path_buf());
+        let _index1 = vectorset
+            .get_or_create("Index1".to_string(), Similarity::Cosine)
+            .unwrap();
+        let _index2 = vectorset
+            .get_or_create("Index2".to_string(), Similarity::Cosine)
+            .unwrap();
+        let _index3 = vectorset
+            .get_or_create("Index3".to_string(), Similarity::Cosine)
+            .unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_some());
+        assert!(vectorset.get("Index2").unwrap().is_some());
+        assert!(vectorset.get("Index3").unwrap().is_some());
+        assert!(vectorset.get("Index4").unwrap().is_none());
+        vectorset.do_sanity_checks().unwrap();
+        vectorset.remove_index("Index1").unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_none());
+        assert!(vectorset.get("Index2").unwrap().is_some());
+        assert!(vectorset.get("Index3").unwrap().is_some());
+        vectorset.remove_index("Index2").unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_none());
+        assert!(vectorset.get("Index2").unwrap().is_none());
+        assert!(vectorset.get("Index3").unwrap().is_some());
+        vectorset.remove_index("Index3").unwrap();
+        assert!(vectorset.get("Index1").unwrap().is_none());
+        assert!(vectorset.get("Index2").unwrap().is_none());
+        assert!(vectorset.get("Index3").unwrap().is_none());
+    }
+}
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/lib.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_vectors/src/lib.rs`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,47 +1,47 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod data_point;
-pub mod data_point_provider;
-mod data_types;
-pub mod formula;
-pub mod indexset;
-pub mod service;
-
-use thiserror::Error;
-#[derive(Debug, Error)]
-pub enum VectorErr {
-    #[error("json error: {0}")]
-    SJ(#[from] serde_json::Error),
-    #[error("IO error: {0}")]
-    IoErr(#[from] std::io::Error),
-    #[error("Error in fs: {0}")]
-    FsError(#[from] nucliadb_core::fs_state::FsError),
-    #[error("Garbage collection delayed")]
-    WorkDelayed,
-    #[error("Several writers are open at the same time ")]
-    MultipleWriters,
-    #[error("Merger is already initialized")]
-    MergerAlreadyInitialized,
-    #[error("Can not merge zero datapoints")]
-    EmptyMerge,
-}
-
-pub type VectorR<O> = Result<O, VectorErr>;
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+pub mod data_point;
+pub mod data_point_provider;
+mod data_types;
+pub mod formula;
+pub mod indexset;
+pub mod service;
+
+use thiserror::Error;
+#[derive(Debug, Error)]
+pub enum VectorErr {
+    #[error("json error: {0}")]
+    SJ(#[from] serde_json::Error),
+    #[error("IO error: {0}")]
+    IoErr(#[from] std::io::Error),
+    #[error("Error in fs: {0}")]
+    FsError(#[from] nucliadb_core::fs_state::FsError),
+    #[error("Garbage collection delayed")]
+    WorkDelayed,
+    #[error("Several writers are open at the same time ")]
+    MultipleWriters,
+    #[error("Merger is already initialized")]
+    MergerAlreadyInitialized,
+    #[error("Can not merge zero datapoints")]
+    EmptyMerge,
+}
+
+pub type VectorR<O> = Result<O, VectorErr>;
```

### Comparing `nucliadb_node_binding-0.7.5/local_dependencies/nucliadb_vectors/src/service/mod.rs` & `nucliadb_node_binding-0.7.6/local_dependencies/nucliadb_ftp/src/error.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,37 +1,32 @@
-// Copyright (C) 2021 Bosutech XXI S.L.
-//
-// nucliadb is offered under the AGPL v3.0 and as commercial software.
-// For commercial licensing, contact us at info@nuclia.com.
-//
-// AGPL:
-// This program is free software: you can redistribute it and/or modify
-// it under the terms of the GNU Affero General Public License as
-// published by the Free Software Foundation, either version 3 of the
-// License, or (at your option) any later version.
-//
-// This program is distributed in the hope that it will be useful,
-// but WITHOUT ANY WARRANTY; without even the implied warranty of
-// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-// GNU Affero General Public License for more details.
-//
-// You should have received a copy of the GNU Affero General Public License
-// along with this program. If not, see <http://www.gnu.org/licenses/>.
-//
-
-pub mod reader;
-pub mod writer;
-
-use nucliadb_core::protos::VectorSimilarity as GrpcSimilarity;
-pub use reader::*;
-pub use writer::*;
-
-use crate::data_point::Similarity;
-
-impl From<GrpcSimilarity> for Similarity {
-    fn from(value: GrpcSimilarity) -> Self {
-        match value {
-            GrpcSimilarity::Cosine => Similarity::Cosine,
-            GrpcSimilarity::Dot => Similarity::Dot,
-        }
-    }
-}
+// Copyright (C) 2021 Bosutech XXI S.L.
+//
+// nucliadb is offered under the AGPL v3.0 and as commercial software.
+// For commercial licensing, contact us at info@nuclia.com.
+//
+// AGPL:
+// This program is free software: you can redistribute it and/or modify
+// it under the terms of the GNU Affero General Public License as
+// published by the Free Software Foundation, either version 3 of the
+// License, or (at your option) any later version.
+//
+// This program is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+// GNU Affero General Public License for more details.
+//
+// You should have received a copy of the GNU Affero General Public License
+// along with this program. If not, see <http://www.gnu.org/licenses/>.
+//
+
+use std::io;
+
+use thiserror::Error;
+
+/// The error that may occur when publishing/receiving files/directories.
+#[derive(Debug, Error)]
+pub enum Error {
+    #[error("invalid path '{0}': {1}")]
+    InvalidPath(String, String),
+    #[error(transparent)]
+    IoError(#[from] io::Error),
+}
```

### Comparing `nucliadb_node_binding-0.7.5/Cargo.toml` & `nucliadb_node_binding-0.7.6/Cargo.toml`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "nucliadb_node_binding"
-version = "0.7.5"
+version = "0.7.6"
 edition = "2021"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 [lib]
 name = "nucliadb_node_binding"
 crate-type = ["cdylib"]
 
@@ -19,12 +19,12 @@
 log = "0.4"
 bincode = "1.3.3"
 
 openssl = { version = "0.10", features = ["vendored"] }
 prost = "0.10"
 prost-types = "0.10"
 tracing = { version = "0.1.29" }
-tracing-subscriber = { version = "0.3.11", features = [
-    "env-filter",
-    "registry",
-    "std",
+tracing-subscriber = { version = "0.3.11", features = [
+    "env-filter",
+    "registry",
+    "std",
 ] }
```

### Comparing `nucliadb_node_binding-0.7.5/CHANGELOG.md` & `nucliadb_node_binding-0.7.6/CHANGELOG.md`

 * *Files 25% similar despite different names*

```diff
@@ -1,108 +1,112 @@
-# Changelog
-## 0.7.5
-
-- Fully remove stop-words from paragraphs
-## 0.7.4
-
-- Vectors returns the labels of the nearest neighbors 
-## 0.7.3
-
-- Unbounded prefix search on relations
-## 0.7.2
-
-- Unbounded prefix search on relations
-## 0.7.1
-
-- load shard on join graph.
-
-## 0.7.0
-
-- Add `relation_types` and `relation_edges`
-
-## 0.6.0
-
-- Be able to set vector similarity on shards and vectorsets
-
-## 0.5.9
-
-- Fix vector deletion 
-
-## 0.5.8
-
-- Store node metadata
-## 0.5.7
-
-- move merger out of the rayon threadpool
-## 0.5.6
-
-- increase rayon number of threads
-
-## 0.5.5
-
-- sort by creation/modification date
-
-## 0.5.4
-
-- versions.json fix
-
-## 0.5.3
-
-- Node refactor
-
-## 0.5.2
-
-- Telemetry enabled
-
-## 0.5.0
-
-- Implement knowledge graph search
-
-## 0.4.11
-
-- Adding advanced_query to searches
-
-## 0.4.10
-
-- Adding `with_status` filter to document searches
-
-## 0.4.9
-
-- Removing deprecated indexes
-
-## 0.4.8
-
-## 0.4.7
-
-- Binding name update
-
-## 0.4.6
-
-- Binding vectorset and relations update
-
-## 0.4.5
-
-- `only_faceted` flag
-
-## 0.4.5
-
-- Fixed filter search
-
-## 0.4.4
-
-- relations index
-
-## 0.4.3
-- relations index
-
-## 0.4.2
-
-- Lazy loading fix
-- Parallel search enabled
-
-## 0.4.1
-
-- Fix `create` function in `nucliadb_relations` index
-
-## 0.4.0
-
-- Add `clean_and_upgrade_shard` function
+# Changelog
+## 0.7.6
+
+- Properly compute total search results for document and paragaph
+
+## 0.7.5
+
+- Fully remove stop-words from paragraphs
+## 0.7.4
+
+- Vectors returns the labels of the nearest neighbors 
+## 0.7.3
+
+- Unbounded prefix search on relations
+## 0.7.2
+
+- Unbounded prefix search on relations
+## 0.7.1
+
+- load shard on join graph.
+
+## 0.7.0
+
+- Add `relation_types` and `relation_edges`
+
+## 0.6.0
+
+- Be able to set vector similarity on shards and vectorsets
+
+## 0.5.9
+
+- Fix vector deletion 
+
+## 0.5.8
+
+- Store node metadata
+## 0.5.7
+
+- move merger out of the rayon threadpool
+## 0.5.6
+
+- increase rayon number of threads
+
+## 0.5.5
+
+- sort by creation/modification date
+
+## 0.5.4
+
+- versions.json fix
+
+## 0.5.3
+
+- Node refactor
+
+## 0.5.2
+
+- Telemetry enabled
+
+## 0.5.0
+
+- Implement knowledge graph search
+
+## 0.4.11
+
+- Adding advanced_query to searches
+
+## 0.4.10
+
+- Adding `with_status` filter to document searches
+
+## 0.4.9
+
+- Removing deprecated indexes
+
+## 0.4.8
+
+## 0.4.7
+
+- Binding name update
+
+## 0.4.6
+
+- Binding vectorset and relations update
+
+## 0.4.5
+
+- `only_faceted` flag
+
+## 0.4.5
+
+- Fixed filter search
+
+## 0.4.4
+
+- relations index
+
+## 0.4.3
+- relations index
+
+## 0.4.2
+
+- Lazy loading fix
+- Parallel search enabled
+
+## 0.4.1
+
+- Fix `create` function in `nucliadb_relations` index
+
+## 0.4.0
+
+- Add `clean_and_upgrade_shard` function
```

### Comparing `nucliadb_node_binding-0.7.5/pyproject.toml` & `nucliadb_node_binding-0.7.6/pyproject.toml`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-[build-system]
-requires = ["maturin>=0.12,<0.13"]
-build-backend = "maturin"
-
-[project]
-name = "nucliadb_node_binding"
-requires-python = ">=3.6"
-classifiers = [
-    "Programming Language :: Rust",
-    "Programming Language :: Python :: Implementation :: CPython",
-    "Programming Language :: Python :: Implementation :: PyPy",
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+[build-system]
+requires = ["maturin>=0.12,<0.13"]
+build-backend = "maturin"
+
+[project]
+name = "nucliadb_node_binding"
+requires-python = ">=3.6"
+classifiers = [
+    "Programming Language :: Rust",
+    "Programming Language :: Python :: Implementation :: CPython",
+    "Programming Language :: Python :: Implementation :: PyPy",
 ]
```

### Comparing `nucliadb_node_binding-0.7.5/test.py` & `nucliadb_node_binding-0.7.6/test.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,56 +1,56 @@
-# Copyright (C) 2021 Bosutech XXI S.L.
-#
-# nucliadb is offered under the AGPL v3.0 and as commercial software.
-# For commercial licensing, contact us at info@nuclia.com.
-#
-# AGPL:
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero General Public License as
-# published by the Free Software Foundation, either version 3 of the
-# License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
-# GNU Affero General Public License for more details.
-#
-# You should have received a copy of the GNU Affero General Public License
-# along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-import asyncio
-from datetime import datetime
-
-import nucliadb_node_binding  # type: ignore
-from nucliadb_protos.nodereader_pb2 import SearchRequest, SearchResponse
-from nucliadb_protos.noderesources_pb2 import Resource
-from nucliadb_protos.nodewriter_pb2 import ShardCreated
-
-
-async def main():
-    writer = nucliadb_node_binding.NodeWriter.new()
-    reader = nucliadb_node_binding.NodeReader.new()
-    shard = await writer.new_shard("test-kbid")
-    pb = ShardCreated()
-    pb.ParseFromString(bytearray(shard))
-
-    resourcepb = Resource()
-    resourcepb.resource.uuid = "001"
-    resourcepb.resource.shard_id = pb.id
-    resourcepb.texts["field1"].text = "My lovely text"
-    resourcepb.status = Resource.ResourceStatus.PROCESSED
-    resourcepb.shard_id = pb.id
-    resourcepb.metadata.created.FromDatetime(datetime.now())
-    resourcepb.metadata.modified.FromDatetime(datetime.now())
-    await writer.set_resource(resourcepb.SerializeToString())
-
-    searchpb = SearchRequest()
-    searchpb.shard = pb.id
-    searchpb.body = "text"
-    pbresult = await reader.search(searchpb.SerializeToString())
-    pb = SearchResponse()
-    pb.ParseFromString(bytearray(pbresult))
-
-    print(pb)
-
-
-asyncio.run(main())
+# Copyright (C) 2021 Bosutech XXI S.L.
+#
+# nucliadb is offered under the AGPL v3.0 and as commercial software.
+# For commercial licensing, contact us at info@nuclia.com.
+#
+# AGPL:
+# This program is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Affero General Public License as
+# published by the Free Software Foundation, either version 3 of the
+# License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Affero General Public License for more details.
+#
+# You should have received a copy of the GNU Affero General Public License
+# along with this program. If not, see <http://www.gnu.org/licenses/>.
+
+import asyncio
+from datetime import datetime
+
+import nucliadb_node_binding  # type: ignore
+from nucliadb_protos.nodereader_pb2 import SearchRequest, SearchResponse
+from nucliadb_protos.noderesources_pb2 import Resource
+from nucliadb_protos.nodewriter_pb2 import ShardCreated
+
+
+async def main():
+    writer = nucliadb_node_binding.NodeWriter.new()
+    reader = nucliadb_node_binding.NodeReader.new()
+    shard = await writer.new_shard("test-kbid")
+    pb = ShardCreated()
+    pb.ParseFromString(bytearray(shard))
+
+    resourcepb = Resource()
+    resourcepb.resource.uuid = "001"
+    resourcepb.resource.shard_id = pb.id
+    resourcepb.texts["field1"].text = "My lovely text"
+    resourcepb.status = Resource.ResourceStatus.PROCESSED
+    resourcepb.shard_id = pb.id
+    resourcepb.metadata.created.FromDatetime(datetime.now())
+    resourcepb.metadata.modified.FromDatetime(datetime.now())
+    await writer.set_resource(resourcepb.SerializeToString())
+
+    searchpb = SearchRequest()
+    searchpb.shard = pb.id
+    searchpb.body = "text"
+    pbresult = await reader.search(searchpb.SerializeToString())
+    pb = SearchResponse()
+    pb.ParseFromString(bytearray(pbresult))
+
+    print(pb)
+
+
+asyncio.run(main())
```

