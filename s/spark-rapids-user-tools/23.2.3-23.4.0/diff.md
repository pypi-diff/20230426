# Comparing `tmp/spark_rapids_user_tools-23.2.3-117_5cb8c90-py3-none-any.whl.zip` & `tmp/spark_rapids_user_tools-23.4.0-127_7804c08-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,66 +1,70 @@
-Zip file size: 237029 bytes, number of entries: 64
--rw-r--r--  2.0 unx      648 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/__init__.py
--rw-r--r--  2.0 unx    10120 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/cost_estimator.py
--rw-r--r--  2.0 unx    27693 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/dataproc_utils.py
--rw-r--r--  2.0 unx    15306 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/dataproc_wrapper.py
--rw-r--r--  2.0 unx    10514 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/diag.py
--rw-r--r--  2.0 unx     5344 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/diag_dataproc.py
--rw-r--r--  2.0 unx    63036 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/rapids_models.py
--rw-r--r--  2.0 unx     8782 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/utilities.py
--rw-r--r--  2.0 unx      921 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/csp/__init__.py
--rw-r--r--  2.0 unx     1671 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/csp/csp.py
--rw-r--r--  2.0 unx     5196 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/csp/dataproc.py
--rw-r--r--  2.0 unx     1154 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/diag_scripts/hello_world.py
--rw-r--r--  2.0 unx     1276 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/diag_scripts/perf.py
--rw-r--r--  2.0 unx     1273 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/resources/bootstrap-conf.yaml
--rw-r--r--  2.0 unx   684199 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/resources/gcloud-catalog.json
--rw-r--r--  2.0 unx     1416 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/resources/profiling-conf.yaml
--rw-r--r--  2.0 unx     2554 b- defN 23-Apr-03 14:15 spark_rapids_dataproc_tools/resources/qualification-conf.yaml
--rw-r--r--  2.0 unx      750 b- defN 23-Apr-03 14:15 spark_rapids_pytools/__init__.py
--rw-r--r--  2.0 unx      992 b- defN 23-Apr-03 14:15 spark_rapids_pytools/build.py
--rw-r--r--  2.0 unx     1094 b- defN 23-Apr-03 14:15 spark_rapids_pytools/wrapper.py
--rw-r--r--  2.0 unx      646 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/__init__.py
--rw-r--r--  2.0 unx    12735 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/databricks_aws.py
--rw-r--r--  2.0 unx     1268 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/databricks_aws_job.py
--rw-r--r--  2.0 unx    23065 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/dataproc.py
--rw-r--r--  2.0 unx     1426 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/dataproc_job.py
--rw-r--r--  2.0 unx    20734 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/emr.py
--rw-r--r--  2.0 unx    11269 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/emr_job.py
--rw-r--r--  2.0 unx     4939 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/gstorage.py
--rw-r--r--  2.0 unx     4055 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/s3storage.py
--rw-r--r--  2.0 unx    45141 b- defN 23-Apr-03 14:15 spark_rapids_pytools/cloud_api/sp_types.py
--rw-r--r--  2.0 unx      658 b- defN 23-Apr-03 14:15 spark_rapids_pytools/common/__init__.py
--rw-r--r--  2.0 unx      978 b- defN 23-Apr-03 14:15 spark_rapids_pytools/common/exceptions.py
--rw-r--r--  2.0 unx     5432 b- defN 23-Apr-03 14:15 spark_rapids_pytools/common/prop_manager.py
--rw-r--r--  2.0 unx    13912 b- defN 23-Apr-03 14:15 spark_rapids_pytools/common/sys_storage.py
--rw-r--r--  2.0 unx    12420 b- defN 23-Apr-03 14:15 spark_rapids_pytools/common/utilities.py
--rw-r--r--  2.0 unx      659 b- defN 23-Apr-03 14:15 spark_rapids_pytools/pricing/__init__.py
--rw-r--r--  2.0 unx     3522 b- defN 23-Apr-03 14:15 spark_rapids_pytools/pricing/databricks_pricing.py
--rw-r--r--  2.0 unx     4247 b- defN 23-Apr-03 14:15 spark_rapids_pytools/pricing/dataproc_pricing.py
--rw-r--r--  2.0 unx     4717 b- defN 23-Apr-03 14:15 spark_rapids_pytools/pricing/emr_pricing.py
--rw-r--r--  2.0 unx     6199 b- defN 23-Apr-03 14:15 spark_rapids_pytools/pricing/price_provider.py
--rw-r--r--  2.0 unx      666 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/__init__.py
--rw-r--r--  2.0 unx     8004 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/bootstrap.py
--rw-r--r--  2.0 unx    12902 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/profiling.py
--rw-r--r--  2.0 unx    40902 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/qualification.py
--rw-r--r--  2.0 unx     6864 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/rapids_job.py
--rw-r--r--  2.0 unx    32973 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/rapids_tool.py
--rw-r--r--  2.0 unx     6566 b- defN 23-Apr-03 14:15 spark_rapids_pytools/rapids/tool_ctxt.py
--rw-r--r--  2.0 unx     1295 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/bootstrap-conf.yaml
--rw-r--r--  2.0 unx    30566 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/databricks-premium-catalog.json
--rw-r--r--  2.0 unx    10358 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/databricks_aws-configs.json
--rw-r--r--  2.0 unx     7071 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/dataproc-configs.json
--rw-r--r--  2.0 unx     7855 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/emr-configs.json
--rw-r--r--  2.0 unx     1460 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/profiling-conf.yaml
--rw-r--r--  2.0 unx     4197 b- defN 23-Apr-03 14:15 spark_rapids_pytools/resources/qualification-conf.yaml
--rw-r--r--  2.0 unx      630 b- defN 23-Apr-03 14:15 spark_rapids_pytools/wrappers/__init__.py
--rw-r--r--  2.0 unx     8104 b- defN 23-Apr-03 14:15 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
--rw-r--r--  2.0 unx    14685 b- defN 23-Apr-03 14:15 spark_rapids_pytools/wrappers/dataproc_wrapper.py
--rw-r--r--  2.0 unx    15570 b- defN 23-Apr-03 14:15 spark_rapids_pytools/wrappers/emr_wrapper.py
--rw-r--r--  2.0 unx    18745 b- defN 23-Apr-03 14:15 spark_rapids_user_tools-23.2.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     2966 b- defN 23-Apr-03 14:15 spark_rapids_user_tools-23.2.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-03 14:15 spark_rapids_user_tools-23.2.3.dist-info/WHEEL
--rw-r--r--  2.0 unx      152 b- defN 23-Apr-03 14:15 spark_rapids_user_tools-23.2.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       49 b- defN 23-Apr-03 14:15 spark_rapids_user_tools-23.2.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6507 b- defN 23-Apr-03 14:15 spark_rapids_user_tools-23.2.3.dist-info/RECORD
-64 files, 1261140 bytes uncompressed, 226331 bytes compressed:  82.1%
+Zip file size: 241734 bytes, number of entries: 68
+-rw-r--r--  2.0 unx      648 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/__init__.py
+-rw-r--r--  2.0 unx    10120 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/cost_estimator.py
+-rw-r--r--  2.0 unx    27693 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/dataproc_utils.py
+-rw-r--r--  2.0 unx    15306 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/dataproc_wrapper.py
+-rw-r--r--  2.0 unx    10514 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag.py
+-rw-r--r--  2.0 unx     5344 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag_dataproc.py
+-rw-r--r--  2.0 unx    63046 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/rapids_models.py
+-rw-r--r--  2.0 unx     8782 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/utilities.py
+-rw-r--r--  2.0 unx      921 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/csp/__init__.py
+-rw-r--r--  2.0 unx     1671 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/csp/csp.py
+-rw-r--r--  2.0 unx     5196 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/csp/dataproc.py
+-rw-r--r--  2.0 unx     1154 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag_scripts/hello_world.py
+-rw-r--r--  2.0 unx     1276 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag_scripts/perf.py
+-rw-r--r--  2.0 unx     1273 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/bootstrap-conf.yaml
+-rw-r--r--  2.0 unx   684199 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/gcloud-catalog.json
+-rw-r--r--  2.0 unx     1416 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/profiling-conf.yaml
+-rw-r--r--  2.0 unx     2554 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/qualification-conf.yaml
+-rw-r--r--  2.0 unx      750 b- defN 23-Apr-26 15:42 spark_rapids_pytools/__init__.py
+-rw-r--r--  2.0 unx      992 b- defN 23-Apr-26 15:42 spark_rapids_pytools/build.py
+-rw-r--r--  2.0 unx     1094 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrapper.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/__init__.py
+-rw-r--r--  2.0 unx    12837 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/databricks_aws.py
+-rw-r--r--  2.0 unx     1268 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/databricks_aws_job.py
+-rw-r--r--  2.0 unx    24216 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/dataproc.py
+-rw-r--r--  2.0 unx     1426 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/dataproc_job.py
+-rw-r--r--  2.0 unx    21270 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/emr.py
+-rw-r--r--  2.0 unx    11269 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/emr_job.py
+-rw-r--r--  2.0 unx     4939 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/gstorage.py
+-rw-r--r--  2.0 unx     4055 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/s3storage.py
+-rw-r--r--  2.0 unx    47222 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/sp_types.py
+-rw-r--r--  2.0 unx      658 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/__init__.py
+-rw-r--r--  2.0 unx      978 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/exceptions.py
+-rw-r--r--  2.0 unx     5432 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/prop_manager.py
+-rw-r--r--  2.0 unx    14712 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/sys_storage.py
+-rw-r--r--  2.0 unx    13037 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/utilities.py
+-rw-r--r--  2.0 unx      659 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/__init__.py
+-rw-r--r--  2.0 unx     3522 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/databricks_pricing.py
+-rw-r--r--  2.0 unx     4247 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/dataproc_pricing.py
+-rw-r--r--  2.0 unx     4717 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/emr_pricing.py
+-rw-r--r--  2.0 unx     6400 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/price_provider.py
+-rw-r--r--  2.0 unx      666 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/__init__.py
+-rw-r--r--  2.0 unx     8004 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/bootstrap.py
+-rw-r--r--  2.0 unx    12902 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/profiling.py
+-rw-r--r--  2.0 unx    41708 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/qualification.py
+-rw-r--r--  2.0 unx     6864 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/rapids_job.py
+-rw-r--r--  2.0 unx    33530 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/rapids_tool.py
+-rw-r--r--  2.0 unx     6576 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/tool_ctxt.py
+-rw-r--r--  2.0 unx     1295 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/bootstrap-conf.yaml
+-rw-r--r--  2.0 unx    30566 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/databricks-premium-catalog.json
+-rw-r--r--  2.0 unx    10448 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/databricks_aws-configs.json
+-rw-r--r--  2.0 unx     8047 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/dataproc-configs.json
+-rw-r--r--  2.0 unx     8994 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/emr-configs.json
+-rw-r--r--  2.0 unx     1460 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/profiling-conf.yaml
+-rw-r--r--  2.0 unx     4252 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/qualification-conf.yaml
+-rw-r--r--  2.0 unx      671 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      518 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
+-rw-r--r--  2.0 unx      855 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      537 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
+-rw-r--r--  2.0 unx      630 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/__init__.py
+-rw-r--r--  2.0 unx     8104 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
+-rw-r--r--  2.0 unx    14685 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/dataproc_wrapper.py
+-rw-r--r--  2.0 unx    15570 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/emr_wrapper.py
+-rw-r--r--  2.0 unx    18745 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2939 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx      152 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       49 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7009 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/RECORD
+68 files, 1273327 bytes uncompressed, 230176 bytes compressed:  81.9%
```

## zipnote {}

```diff
@@ -156,38 +156,50 @@
 
 Filename: spark_rapids_pytools/resources/profiling-conf.yaml
 Comment: 
 
 Filename: spark_rapids_pytools/resources/qualification-conf.yaml
 Comment: 
 
+Filename: spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
+Comment: 
+
+Filename: spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
+Comment: 
+
+Filename: spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
+Comment: 
+
+Filename: spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
+Comment: 
+
 Filename: spark_rapids_pytools/wrappers/__init__.py
 Comment: 
 
 Filename: spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
 Comment: 
 
 Filename: spark_rapids_pytools/wrappers/dataproc_wrapper.py
 Comment: 
 
 Filename: spark_rapids_pytools/wrappers/emr_wrapper.py
 Comment: 
 
-Filename: spark_rapids_user_tools-23.2.3.dist-info/LICENSE
+Filename: spark_rapids_user_tools-23.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: spark_rapids_user_tools-23.2.3.dist-info/METADATA
+Filename: spark_rapids_user_tools-23.4.0.dist-info/METADATA
 Comment: 
 
-Filename: spark_rapids_user_tools-23.2.3.dist-info/WHEEL
+Filename: spark_rapids_user_tools-23.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: spark_rapids_user_tools-23.2.3.dist-info/entry_points.txt
+Filename: spark_rapids_user_tools-23.4.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-23.2.3.dist-info/top_level.txt
+Filename: spark_rapids_user_tools-23.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-23.2.3.dist-info/RECORD
+Filename: spark_rapids_user_tools-23.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spark_rapids_dataproc_tools/rapids_models.py

```diff
@@ -1,8 +1,8 @@
-# Copyright (c) 2022, NVIDIA CORPORATION.
+# Copyright (c) 2022-2023, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -888,15 +888,15 @@
         if gpu_cluster_props_path is None:
             # The argument is not set, then the dataproc properties is going to be same as
             # the CPU cluster.
             self.gpu_cluster_proxy = self.cpu_cluster_proxy
             props_origin_msg = f'the submission cluster on which the RAPIDS tool is running [{self.cluster}]'
             if cpu_cluster_props_path is not None:
                 props_origin_msg = f'the original CPU cluster properties loaded from {cpu_cluster_props_path}'
-            self.ctxt.loginfo(f'The GPU cluster is the same as {props_origin_msg}. '
+            self.ctxt.loginfo(f'Estimating the GPU cluster based on {props_origin_msg}. '
                               'To update the configuration of the GPU cluster, make sure to pass the '
                               'properties file to the CLI arguments.')
         else:
             self.gpu_cluster_proxy = construct_offline_prop_container(cluster_type='gpu',
                                                                       config_file_path=gpu_cluster_props_path)
 
     def _init_cluster_dataproc_props(self):
```

## spark_rapids_pytools/__init__.py

```diff
@@ -12,9 +12,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """init file of the spark_rapids_pytools package."""
 
 from spark_rapids_pytools.build import get_version
 
-VERSION = '23.02.3'
+VERSION = '23.04.0'
 __version__ = get_version(VERSION)
```

## spark_rapids_pytools/cloud_api/databricks_aws.py

```diff
@@ -153,14 +153,17 @@
     """
 
     def _set_fields_from_props(self):
         super()._set_fields_from_props()
         self.uuid = self.props.get_value('cluster_id')
         self.state = ClusterState.fromstring(self.props.get_value('state'))
 
+    def _set_name_from_props(self) -> None:
+        self.name = self.props.get_value('cluster_name')
+
     def _init_nodes(self):
         # assume that only one master node
         master_nodes_from_conf = self.props.get_value_silent('driver')
         worker_nodes_from_conf = self.props.get_value_silent('executors')
         num_workers = self.props.get_value_silent('num_workers')
         if num_workers is None:
             num_workers = 0
```

## spark_rapids_pytools/cloud_api/dataproc.py

```diff
@@ -365,32 +365,36 @@
         return res_arr
 
     def _set_fields_from_props(self):
         super()._set_fields_from_props()
         self.uuid = self.props.get_value('clusterUuid')
         self.state = ClusterState.fromstring(self.props.get_value('status', 'state'))
 
+    def _set_name_from_props(self) -> None:
+        self.name = self.props.get_value('clusterName')
+
     def _init_nodes(self):
         # assume that only one master node
         master_nodes_from_conf = self.props.get_value('config', 'masterConfig', 'instanceNames')
-        worker_nodes_from_conf = self.props.get_value('config', 'workerConfig', 'instanceNames')
-        # create workers array
+        raw_worker_prop = self.props.get_value_silent('config', 'workerConfig')
         worker_nodes: list = []
-        raw_worker_prop = self.props.get_value('config', 'workerConfig')
-        for worker_node in worker_nodes_from_conf:
-            worker_props = {
-                'name': worker_node,
-                'props': JSONPropertiesContainer(prop_arg=raw_worker_prop, file_load=False),
-                # set the node zone based on the wrapper defined zone
-                'zone': self.zone
-            }
-            worker = DataprocNode.create_worker_node().set_fields_from_dict(worker_props)
-            # TODO for optimization, we should set HW props for 1 worker
-            worker.fetch_and_set_hw_info(self.cli)
-            worker_nodes.append(worker)
+        if raw_worker_prop:
+            worker_nodes_from_conf = self.props.get_value('config', 'workerConfig', 'instanceNames')
+            # create workers array
+            for worker_node in worker_nodes_from_conf:
+                worker_props = {
+                    'name': worker_node,
+                    'props': JSONPropertiesContainer(prop_arg=raw_worker_prop, file_load=False),
+                    # set the node zone based on the wrapper defined zone
+                    'zone': self.zone
+                }
+                worker = DataprocNode.create_worker_node().set_fields_from_dict(worker_props)
+                # TODO for optimization, we should set HW props for 1 worker
+                worker.fetch_and_set_hw_info(self.cli)
+                worker_nodes.append(worker)
         raw_master_props = self.props.get_value('config', 'masterConfig')
         master_props = {
             'name': master_nodes_from_conf[0],
             'props': JSONPropertiesContainer(prop_arg=raw_master_props, file_load=False),
             # set the node zone based on the wrapper defined zone
             'zone': self.zone
         }
@@ -459,14 +463,38 @@
             k_prefix = 'spark:'
             return {key[len(k_prefix):]: value for (key, value) in sw_props.items() if key.startswith(k_prefix)}
         return {}
 
     def get_tmp_storage(self) -> str:
         return self._get_temp_gs_storage()
 
+    def get_image_version(self) -> str:
+        return self.props.get_value_silent('config', 'softwareConfig', 'imageVersion')
+
+    def _set_render_args_create_template(self) -> dict:
+        worker_node = self.get_worker_node()
+        gpu_per_machine, gpu_device = self.get_gpu_per_worker()
+        # map the gpu device to the equivalent accepted argument
+        gpu_device_hash = {
+            'T4': 'nvidia-tesla-t4',
+            'L4': 'nvidia-l4'
+        }
+        return {
+            'CLUSTER_NAME': self.get_name(),
+            'REGION': self.region,
+            'ZONE': self.zone,
+            'IMAGE': self.get_image_version(),
+            'MASTER_MACHINE': self.get_master_node().instance_type,
+            'WORKERS_COUNT': self.get_workers_count(),
+            'WORKERS_MACHINE': worker_node.instance_type,
+            'LOCAL_SSD': 2,
+            'GPU_DEVICE': gpu_device_hash.get(gpu_device),
+            'GPU_PER_WORKER': gpu_per_machine
+        }
+
 
 @dataclass
 class DataprocSavingsEstimator(SavingsEstimator):
     """
     A class that calculates the savings based on Dataproc price provider
     """
     def __calculate_group_cost(self, cluster_inst: ClusterGetAccessor, node_type: SparkNodeType):
```

## spark_rapids_pytools/cloud_api/emr.py

```diff
@@ -414,16 +414,17 @@
 
     def _set_fields_from_props(self):
         super()._set_fields_from_props()
         self.uuid = self.props.get_value('Id')
         self.state = ClusterState.fromstring(self.props.get_value('Status', 'State'))
         self.zone = self.props.get_value('Ec2InstanceAttributes',
                                          'Ec2AvailabilityZone')
-        if self.name is None:
-            self.name = self.props.get_value('Name')
+
+    def _set_name_from_props(self) -> None:
+        self.name = self.props.get_value('Name')
 
     def is_cluster_running(self) -> bool:
         acceptable_init_states = [
             ClusterState.RUNNING,
             ClusterState.STARTING,
             ClusterState.BOOTSTRAPPING,
             ClusterState.WAITING
@@ -438,14 +439,28 @@
                 curr_spark_props = conf_item['Properties']
                 res.update(curr_spark_props)
         return res
 
     def get_tmp_storage(self) -> str:
         raise NotImplementedError
 
+    def get_image_version(self) -> str:
+        return self.props.get_value('ReleaseLabel')
+
+    def _set_render_args_create_template(self) -> dict:
+        worker_node = self.get_worker_node()
+        return {
+            'CLUSTER_NAME': self.get_name(),
+            'ZONE': self.zone,
+            'IMAGE': self.get_image_version(),
+            'MASTER_MACHINE': self.get_master_node().instance_type,
+            'WORKERS_COUNT': self.get_workers_count(),
+            'WORKERS_MACHINE': worker_node.instance_type
+        }
+
 
 @dataclass
 class EmrSavingsEstimator(SavingsEstimator):
     """
     A class that calculates the savings based on an EMR price provider
     """
```

## spark_rapids_pytools/cloud_api/sp_types.py

```diff
@@ -21,15 +21,15 @@
 from enum import Enum
 from logging import Logger
 from typing import cast, Type, Any, List, Union, Optional, Callable
 
 from spark_rapids_pytools.common.prop_manager import AbstractPropertiesContainer, JSONPropertiesContainer, \
     get_elem_non_safe
 from spark_rapids_pytools.common.sys_storage import StorageDriver, FSUtil
-from spark_rapids_pytools.common.utilities import ToolLogging, SysCmd, Utils
+from spark_rapids_pytools.common.utilities import ToolLogging, SysCmd, Utils, TemplateGenerator
 
 
 class EnumeratedType(str, Enum):
     """Abstract representation of enumerated values"""
 
     @classmethod
     def tostring(cls, value: Union[Enum, str]) -> str:
@@ -79,22 +79,24 @@
     """List of supported GPU devices"""
     T4 = 't4'
     V100 = 'v100'
     K80 = 'k80'
     A100 = 'a100'
     P100 = 'P100'
     P4 = 'P4'
+    L4 = 'l4'
 
     @classmethod
     def get_default_gpu(cls):
         return cls.T4
 
     def get_gpu_mem(self) -> list:
         memory_hash = {
             self.T4: [16384],
+            self.L4: [24576],
             self.A100: [40960, 81920],
             self.P4: [8192],
             self.K80: [12288],
             self.V100: [16384],
             self.P100: [16384]
         }
         return memory_hash.get(self)
@@ -588,15 +590,15 @@
     storage: StorageDriver = field(default=None, init=False)
     ctxt: dict = field(default_factory=dict, init=False)
     configs: JSONPropertiesContainer = field(default=None, init=False)
     logger: Logger = field(default=ToolLogging.get_and_setup_logger('rapids.tools.csp'), init=False)
 
     @classmethod
     def list_supported_gpus(cls):
-        return [GpuDevice.T4, GpuDevice.A100]
+        return [GpuDevice.T4, GpuDevice.A100, GpuDevice.L4]
 
     def load_from_config_parser(self, conf_file, **prop_args) -> dict:
         res = None
         try:
             parser_obj = configparser.ConfigParser()
             parser_obj.read(conf_file)
             res = {}
@@ -847,14 +849,25 @@
     region: str = field(default=None, init=False)
     zone: str = field(default=None, init=False)
     state: ClusterState = field(default=ClusterState.RUNNING, init=False)
     nodes: dict = field(default_factory=dict, init=False)
     props: AbstractPropertiesContainer = field(default=None, init=False)
     logger: Logger = field(default=ToolLogging.get_and_setup_logger('rapids.tools.cluster'), init=False)
 
+    @staticmethod
+    def _verify_workers_exist(has_no_workers_cb: Callable[[], bool]):
+        """
+        Specifies how to handle cluster definitions that have no workers
+        :param has_no_workers_cb: A callback that returns True if the cluster does not have any
+               workers
+        """
+        if has_no_workers_cb():
+            raise RuntimeError('Invalid cluster: The cluster has no worker nodes.\n\t'
+                               'It is recommended to define a with (1 master, N workers).')
+
     def __post_init__(self):
         self.cli = self.platform.cli
         self.region = self.cli.get_region()
 
     def _init_connection(self, cluster_id: str = None,
                          props: str = None) -> dict:
         name = cluster_id
@@ -882,16 +895,21 @@
     def _process_loaded_props(self) -> None:
         """
         After loading the raw properties, perform any necessary processing to clean up the
         properties.
         """
         return None
 
+    def _set_name_from_props(self) -> None:
+        pass
+
     def _set_fields_from_props(self):
         self._process_loaded_props()
+        if not self.name:
+            self._set_name_from_props()
 
     def _init_nodes(self):
         pass
 
     def set_connection(self,
                        cluster_id: str = None,
                        props: str = None):
@@ -900,14 +918,16 @@
         :param cluster_id: the argument to be used to fetch the cluster
         :param props: optional argument that includes dictionary of the platform cluster's description.
         :return: a cluster
         """
         pre_init_args = self._init_connection(cluster_id, props)
         self.set_fields_from_dict(pre_init_args)
         self._init_nodes()
+        # Verify that the cluster has defined workers
+        self._verify_workers_exist(lambda: not self.nodes.get(SparkNodeType.WORKER))
         return self
 
     def is_cluster_running(self) -> bool:
         return self.state == ClusterState.RUNNING
 
     def get_eventlogs_from_config(self) -> List[str]:
         res_arr = []
@@ -954,14 +974,16 @@
         raise NotImplementedError
 
     def migrate_from_cluster(self, orig_cluster):
         self.name = orig_cluster.name
         self.uuid = orig_cluster.uuid
         self.zone = orig_cluster.zone
         self.state = orig_cluster.state
+        # we need to copy the props in case we need to read a property
+        self.props = orig_cluster.props
         self._build_migrated_cluster(orig_cluster)
 
     def find_matches_for_node(self) -> (dict, dict):
         """
         Maps the CPU instance types to GPU types
         :return: a map converting CPU machines to GPU ones and a map
                 containing the supported GPUs.
@@ -1009,14 +1031,36 @@
 
     def get_name(self) -> str:
         return self.name
 
     def get_tmp_storage(self) -> str:
         raise NotImplementedError
 
+    def _set_render_args_create_template(self) -> dict:
+        raise NotImplementedError
+
+    def generate_create_script(self) -> str:
+        platform_name = CloudPlatform.pretty_print(self.platform.type_id)
+        template_path = Utils.resource_path(f'templates/{platform_name}-create_gpu_cluster_script.ms')
+        render_args = self._set_render_args_create_template()
+        return TemplateGenerator.render_template_file(template_path, render_args)
+
+    def _set_render_args_bootstrap_template(self, overridden_args: dict = None) -> dict:
+        res = {}
+        if overridden_args:
+            res.update(overridden_args)
+        res.setdefault('CLUSTER_NAME', self.get_name())
+        return res
+
+    def generate_bootstrap_script(self, overridden_args: dict = None) -> str:
+        platform_name = CloudPlatform.pretty_print(self.platform.type_id)
+        template_path = Utils.resource_path(f'templates/{platform_name}-run_bootstrap.ms')
+        render_args = self._set_render_args_bootstrap_template(overridden_args)
+        return TemplateGenerator.render_template_file(template_path, render_args)
+
 
 @dataclass
 class ClusterReshape(ClusterGetAccessor):
     """
     A class that handles reshaping of the given cluster.
     It takes argument a cluster object and callable methods that defines
     the way each cluster property is being reshaped.
```

## spark_rapids_pytools/common/sys_storage.py

```diff
@@ -19,21 +19,21 @@
 import os
 import pathlib
 import re
 import shutil
 import ssl
 import urllib
 from dataclasses import dataclass
-from email.utils import formatdate, parsedate_to_datetime
 from itertools import islice
 from shutil import rmtree
 from typing import List
 
 import certifi
-import requests
+from fastcore.all import urlsave
+from fastprogress.fastprogress import progress_bar
 
 from spark_rapids_pytools.common.exceptions import StorageException
 from spark_rapids_pytools.common.utilities import Utils
 
 
 class FSUtil:
     """Implementation of storage functionality for local disk."""
@@ -121,46 +121,71 @@
         context = ssl.create_default_context(cafile=certifi.where())
         with urllib.request.urlopen(src_url, context=context) as resp:
             with open(dest_file, 'wb') as f:
                 shutil.copyfileobj(resp, f)
         return dest_file
 
     @classmethod
+    def fast_download_url(cls, url: str, fpath: str, timeout=None, pbar_enabled=True) -> str:
+        """
+        Download the given url and display a progress bar
+        """
+        pbar = progress_bar([])
+
+        def progress_bar_cb(count=1, bsize=1, total_size=None):
+            pbar.total = total_size
+            pbar.update(count * bsize)
+
+        return urlsave(url, fpath, reporthook=progress_bar_cb if pbar_enabled else None, timeout=timeout)
+
+    @classmethod
     def cache_from_url(cls,
                        src_url: str,
-                       cache_file: str) -> bool:
+                       cache_file: str,
+                       file_checks: dict = None) -> bool:
         """
         download a resource from given URL as a destination cache_file
         :param src_url: HTTP url containing the resource
         :param cache_file: the file where the resource is saved. It is assumed that this the file
+        :param file_checks: a dictionary that contains the criteria to check that the file is the
+               same.
         :return: true if the file is re-downloaded. False, if the cached file is not modified.
         """
-        # use cache by checking modification time of the resource and the file if it already exists
-        headers = {}
-        if os.path.exists(cache_file):
-            mtime = os.path.getmtime(cache_file)
-            headers['If-Modified-Since'] = formatdate(mtime, usegmt=True)
-        r = requests.get(src_url, headers=headers, stream=True, timeout=100)
-        r.raise_for_status()
-        if r.status_code == requests.codes.not_modified:  # pylint: disable=no-member
-            # no need to download the file
-            return False
-        if r.status_code == requests.codes.ok:  # pylint: disable=no-member
-            with open(cache_file, 'wb') as f:
-                for chunk in r.iter_content(chunk_size=4 * 1048576):
-                    f.write(chunk)
-            # Another alternative that is not suitable for large files
-            # with open(cache_file, 'wb') as f:
-            #     shutil.copyfileobj(r.raw, f)
-            if last_modified := r.headers.get('last-modified'):
-                new_mtime = parsedate_to_datetime(last_modified).timestamp()
-                os.utime(cache_file, times=(datetime.datetime.now().timestamp(), new_mtime))
+
+        def check_cached_file(fpath: str, checks_args: dict) -> bool:
+            if not os.path.exists(fpath):
+                return False
+            if not checks_args:
+                return True
+            expected_size = checks_args.get('size')
+            if expected_size:
+                if expected_size != os.path.getsize(fpath):
+                    return False
+            expiration_time_s = checks_args.get('cacheExpirationSecs')
+            if expiration_time_s:
+                modified_time = os.path.getmtime(fpath)
+                diff_time = int(datetime.datetime.now().timestamp() - modified_time)
+                if diff_time > expiration_time_s:
+                    return False
+            # TODO verify using hashing
             return True
-        # TODO Should we raise exception if the request is neither?
-        return False
+
+        curr_time_stamp = datetime.datetime.now().timestamp()
+        if check_cached_file(cache_file, file_checks):
+            # the file already exists and matches the validation
+            # update the access-time and return True
+            # update modified time and access time
+            return False
+        # download the file
+        cls.fast_download_url(src_url, cache_file)
+        # update modified time and access time
+        os.utime(cache_file, times=(curr_time_stamp, curr_time_stamp))
+        if not check_cached_file(cache_file, file_checks):
+            raise RuntimeError(f'Failed downloading resource {src_url}')
+        return True
 
     @classmethod
     def get_home_directory(cls) -> str:
         return os.path.expanduser('~')
 
     @classmethod
     def expand_path(cls, file_path) -> str:
```

## spark_rapids_pytools/common/utilities.py

```diff
@@ -26,16 +26,20 @@
 import urllib
 from dataclasses import dataclass, field
 from logging import Logger
 from shutil import which
 from typing import Callable, Any
 
 import certifi
+import chevron
 from bs4 import BeautifulSoup
 from packaging.version import Version
+from pygments import highlight
+from pygments.formatters import get_formatter_by_name
+from pygments.lexers import get_lexer_by_name
 
 from spark_rapids_pytools import get_version
 
 
 class Utils:
     """Utility class used to enclose common helpers and utilities."""
 
@@ -237,14 +241,26 @@
             # TODO: set the formatter and handler for file logging
             # fh.setLevel(log_level)
             # fh.setFormatter(ExtraLogFormatter())
             logger.addHandler(fh)
         return logger
 
 
+class TemplateGenerator:
+    """A class to manage templates and content generation"""
+    @classmethod
+    def render_template_file(cls, fpath: string, template_args: dict) -> str:
+        with open(fpath, 'r', encoding='UTF-8') as f:
+            return chevron.render(f, data=template_args)
+
+    @classmethod
+    def highlight_bash_code(cls, bash_script: str) -> str:
+        return highlight(bash_script, get_lexer_by_name('Bash'), get_formatter_by_name('terminal'))
+
+
 @dataclass
 class SysCmd:
     """
     Run command and check return code, capture output etc.
     """
     cmd: Any = None
     cmd_input: str = None
```

## spark_rapids_pytools/pricing/price_provider.py

```diff
@@ -46,16 +46,19 @@
             self._generate_cache_files()
         else:
             self.logger.info('The catalog files are loaded from the cache: %s',
                              Utils.gen_joined_str('; ', self.get_cached_files()))
 
     def _generate_cache_files(self):
         # resource_urls and cache_files should have the same keys
+        cache_checks = {'cacheExpirationSecs': self.cache_expiration_secs}
         for file_key, resource_url in self.resource_urls.items():
-            files_updated = FSUtil.cache_from_url(resource_url, self.cache_files[file_key])
+            files_updated = FSUtil.cache_from_url(resource_url,
+                                                  self.cache_files[file_key],
+                                                  file_checks=cache_checks)
             self.logger.info('The catalog file %s is %s',
                              self.cache_files[file_key],
                              'updated' if files_updated else 'not modified, using the cached content')
 
     def __post_init__(self):
         self.logger = ToolLogging.get_and_setup_logger(f'rapids.tools.price.{self.name}')
         self.cache_directory = Utils.get_rapids_tools_env('CACHE_FOLDER')
```

## spark_rapids_pytools/rapids/qualification.py

```diff
@@ -20,15 +20,15 @@
 from typing import Any, List, Callable
 
 import pandas as pd
 from tabulate import tabulate
 
 from spark_rapids_pytools.cloud_api.sp_types import EnumeratedType, ClusterReshape, DeployMode
 from spark_rapids_pytools.common.sys_storage import FSUtil
-from spark_rapids_pytools.common.utilities import Utils
+from spark_rapids_pytools.common.utilities import Utils, TemplateGenerator
 from spark_rapids_pytools.pricing.price_provider import SavingsEstimator
 from spark_rapids_pytools.rapids.rapids_job import RapidsJobPropContainer
 from spark_rapids_pytools.rapids.rapids_tool import RapidsJarTool
 
 
 class QualFilterApp(EnumeratedType):
     """Values used to filter out the applications in the qualification report"""
@@ -369,16 +369,18 @@
         mask = all_rows[speed_up_col].isin(recommended_vals)
         if selected_cols is None:
             return all_rows.loc[mask]
         return all_rows.loc[mask, selected_cols]
 
     def __remap_columns_and_prune(self, all_rows) -> pd.DataFrame:
         cols_subset = self.ctxt.get_value('toolOutput', 'csv', 'summaryReport', 'columns')
+        # for backward compatibility, filter out non-existing columns
+        existing_cols_subset = [col for col in cols_subset if col in all_rows.columns]
         cols_map = self.ctxt.get_value('toolOutput', 'csv', 'summaryReport', 'mapColumns')
-        subset_data = all_rows.loc[:, cols_subset]
+        subset_data = all_rows.loc[:, existing_cols_subset]
         if cols_map:
             for col_rename in cols_map:
                 subset_data.columns = subset_data.columns.str.replace(col_rename,
                                                                       cols_map.get(col_rename),
                                                                       regex=False)
 
         # for TCO, group by app name and average durations, then recalculate Estimated GPU Speedup
@@ -549,16 +551,16 @@
                 for s_range in savings_ranges.values():
                     if s_range.get('lowerBound') <= est_savings < s_range.get('upperBound'):
                         savings_recommendations = s_range.get('title')
                         break
 
             # For TCO, calculating annual cost savings based on job frequency
             job_frequency = 30  # default frequency is daily
-            if 'Job Frequency(monthly)' in df_row:
-                job_frequency = df_row['Job Frequency(monthly)']
+            if 'Estimated Job Frequency (monthly)' in df_row:
+                job_frequency = df_row['Estimated Job Frequency (monthly)']
             annual_cost_savings = job_frequency * 12 * (cpu_cost - gpu_cost)
 
             return pd.Series([savings_recommendations, cpu_cost, gpu_cost,
                               est_savings, job_frequency, annual_cost_savings])
 
         def get_cost_per_row(df_row, reshape_col: str) -> pd.Series:
             nonlocal saving_estimator_cache
@@ -720,37 +722,45 @@
                                                            remote_work_dir,
                                                            exclude_pattern=exclude_folder)
 
     def _init_rapids_arg_list(self) -> List[str]:
         # TODO: Make sure we add this argument only for jar versions 23.02+
         return ['--platform', self.ctxt.get_platform_name().replace('_', '-')]
 
-    def _generate_section_content(self, sec_conf: dict) -> List[str]:
+    def _generate_section_lines(self, sec_conf: dict) -> List[str]:
         if sec_conf.get('sectionID') == 'initializationScript':
             # format the initialization scripts
-            res = [Utils.gen_report_sec_header(sec_conf.get('sectionName'))]
             reshaped_gpu_cluster = ClusterReshape(self.ctxt.get_ctxt('gpuClusterProxy'))
             gpu_per_machine, gpu_device = reshaped_gpu_cluster.get_gpu_per_worker()
             fill_map = {
                 0: self.ctxt.platform.cli.get_region(),
                 1: [gpu_device.lower(), gpu_per_machine]
             }
-            headers = sec_conf['content'].get('header')
-            if headers:
-                res.extend(headers)
+            res = []
             # TODO: improve the display of code snippets by using module pygments (for bash)
             #   module code can be used for python snippets
             for ind, l_str in enumerate(sec_conf['content'].get('lines')):
                 if ind in fill_map:
                     rep_var = fill_map.get(ind)
                     new_value = l_str.format(*rep_var) if isinstance(rep_var, list) else l_str.format(rep_var)
                     res.append(new_value)
                 else:
                     res.append(l_str)
             return res
+        if sec_conf.get('sectionID') == 'gpuClusterCreationScript':
+            gpu_cluster = self.ctxt.get_ctxt('gpuClusterProxy')
+            script_content = gpu_cluster.generate_create_script()
+            highlighted_code = TemplateGenerator.highlight_bash_code(script_content)
+            return ['```bash', highlighted_code, '```']
+        if sec_conf.get('sectionID') == 'runUserToolsBootstrap':
+            gpu_cluster = self.ctxt.get_ctxt('gpuClusterProxy')
+            override_args = {'CLUSTER_NAME': '$CLUSTER_NAME'}
+            script_content = gpu_cluster.generate_bootstrap_script(overridden_args=override_args)
+            highlighted_code = TemplateGenerator.highlight_bash_code(script_content)
+            return ['```bash', highlighted_code, '```', '']
         return super()._generate_section_content(sec_conf)
 
 
 @dataclass
 class QualificationAsRemote(Qualification):
     """
     Qualification tool running on Remote cluster development.
```

## spark_rapids_pytools/rapids/rapids_tool.py

```diff
@@ -20,15 +20,16 @@
 import tarfile
 import time
 from concurrent.futures import ThreadPoolExecutor
 from dataclasses import dataclass, field
 from logging import Logger
 from typing import Any, Callable, Dict, List
 
-from spark_rapids_pytools.cloud_api.sp_types import CloudPlatform, get_platform, ClusterBase, DeployMode
+from spark_rapids_pytools.cloud_api.sp_types import CloudPlatform, get_platform, \
+    ClusterBase, DeployMode
 from spark_rapids_pytools.common.sys_storage import FSUtil
 from spark_rapids_pytools.common.utilities import ToolLogging, Utils
 from spark_rapids_pytools.rapids.rapids_job import RapidsJobPropContainer
 from spark_rapids_pytools.rapids.tool_ctxt import ToolContext
 
 
 @dataclass
@@ -277,18 +278,32 @@
 
     def _report_tool_full_location(self) -> str:
         pass
 
     def _report_results_are_empty(self):
         return [f'The {self.pretty_name()} tool did not generate any output. Nothing to display.']
 
+    def _generate_section_lines(self, sec_conf: dict) -> List[str]:
+        all_lines = sec_conf['content'].get('lines')
+        if all_lines:
+            return all_lines
+        return None
+
     def _generate_section_content(self, sec_conf: dict) -> List[str]:
-        rep_lines = [Utils.gen_report_sec_header(sec_conf.get('sectionName'), title_width=20)]
-        for sec_line in sec_conf['content'].get('lines'):
-            rep_lines.append(sec_line)
+        sec_title = sec_conf.get('sectionName')
+        rep_lines = []
+        if sec_title:
+            rep_lines.append(Utils.gen_report_sec_header(sec_title, title_width=20))
+        if sec_conf.get('content'):
+            headers = sec_conf['content'].get('header')
+            if headers:
+                rep_lines.extend(headers)
+            all_lines = self._generate_section_lines(sec_conf)
+            if all_lines:
+                rep_lines.extend(all_lines)
         return rep_lines
 
     def _generate_platform_report_sections(self) -> List[str]:
         section_arr = self.ctxt.platform.configs.get_value_silent('wrapperReporting',
                                                                   self.name,
                                                                   'sections')
         if section_arr:
@@ -418,15 +433,16 @@
             Downloads the specified URL and saves it to disk
             """
             start_time = time.monotonic()
             self.logger.info('Checking dependency %s', dep['name'])
             dest_folder = self.ctxt.get_cache_folder()
             resource_file_name = FSUtil.get_resource_name(dep['uri'])
             resource_file = FSUtil.build_path(dest_folder, resource_file_name)
-            is_created = FSUtil.cache_from_url(dep['uri'], resource_file)
+            file_check_dict = {'size': dep['size']}
+            is_created = FSUtil.cache_from_url(dep['uri'], resource_file, file_checks=file_check_dict)
             if is_created:
                 self.logger.info('The dependency %s has been downloaded into %s', dep['uri'],
                                  resource_file)
                 # check if we need to decompress files
             if dep['type'] == 'archive':
                 destination_path = self.ctxt.get_local_work_dir()
                 with tarfile.open(resource_file, mode='r:*') as tar:
```

## spark_rapids_pytools/rapids/tool_ctxt.py

```diff
@@ -41,15 +41,15 @@
         self.platform = self.platform_cls(ctxt_args=self.platform_opts)
 
     def __create_and_set_uuid(self):
         self.uuid = Utils.gen_uuid_with_ts(suffix_len=8)
 
     def __create_and_set_cache_folder(self):
         # get the cache folder from environment variables or set it to default
-        cache_folder = Utils.get_rapids_tools_env('CACHE_FOLDER', '/tmp/rapids_user_tools_cache')
+        cache_folder = Utils.get_rapids_tools_env('CACHE_FOLDER', '/var/tmp/spark_rapids_user_tools_cache')
         # make sure the environment is set
         Utils.set_rapids_tools_env('CACHE_FOLDER', cache_folder)
         FSUtil.make_dirs(cache_folder)
         self.set_local('cacheFolder', cache_folder)
 
     def get_cache_folder(self) -> str:
         return self.get_local('cacheFolder')
```

## spark_rapids_pytools/resources/databricks_aws-configs.json

### Pretty-printed

 * *Similarity: 0.9958333333333333%*

 * *Differences: {"'dependencies'": "{'deployMode': {'LOCAL': {0: {'size': 299350810}, 1: {'size': 962685}, 2: "*

 * *                   "{'size': 280645251}}}}"}*

```diff
@@ -5,28 +5,31 @@
     "dependencies": {
         "deployMode": {
             "LOCAL": [
                 {
                     "name": "Apache Spark",
                     "relativePath": "jars/*",
                     "sha512": "769db39a560a95fd88b58ed3e9e7d1e92fb68ee406689fb4d30c033cb5911e05c1942dcc70e5ec4585df84e80aabbc272b9386a208debda89522efff1335c8ff",
+                    "size": 299350810,
                     "type": "archive",
                     "uri": "https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
                 },
                 {
                     "md5": "59907e790ce713441955015d79f670bc",
                     "name": "Hadoop AWS",
                     "sha1": "a65839fbf1869f81a1632e09f415e586922e4f80",
+                    "size": 962685,
                     "type": "jar",
                     "uri": "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
                 },
                 {
                     "md5": "8a22f2d30b7e8eee9ea44f04fb13b35a",
                     "name": "AWS Java SDK Bundled",
                     "sha1": "02deec3a0ad83d13d032b1812421b23d7a961eea",
+                    "size": 280645251,
                     "type": "jar",
                     "uri": "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar"
                 }
             ]
         }
     },
     "environment": {
```

## spark_rapids_pytools/resources/dataproc-configs.json

### Pretty-printed

 * *Similarity: 0.982638888888889%*

 * *Differences: {"'dependencies'": "{'deployMode': {'LOCAL': {0: {'size': 299350810}, 1: {'size': 36497606}}}}",*

 * * "'wrapperReporting'": "{'qualification': {'sections': {insert: [(1, OrderedDict([('sectionID', "*

 * *                       "'gpuClusterCreationScript'), ('content', OrderedDict([('header', ['', 'To "*

 * *                       "create a GPU cluster, run the following script:', ''])]))])), (2, "*

 * *                       "OrderedDict([('sectionID', 'runUserToolsBootstrap'), ('content', "*

 * *                       "OrderedDic […]*

```diff
@@ -6,21 +6,23 @@
     "dependencies": {
         "deployMode": {
             "LOCAL": [
                 {
                     "name": "Apache Spark",
                     "relativePath": "jars/*",
                     "sha512": "769db39a560a95fd88b58ed3e9e7d1e92fb68ee406689fb4d30c033cb5911e05c1942dcc70e5ec4585df84e80aabbc272b9386a208debda89522efff1335c8ff",
+                    "size": 299350810,
                     "type": "archive",
                     "uri": "https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
                 },
                 {
                     "md5": "7639d38fff9f88fe80d7e4d9d47fb946",
                     "name": "GCS Connector Hadoop3",
                     "sha1": "519e60640b7ffdbb00dbed20b852c2a406ddaff9",
+                    "size": 36497606,
                     "type": "jar",
                     "uri": "https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.11/gcs-connector-hadoop3-2.2.11-shaded.jar"
                 }
             ]
         }
     },
     "environment": {
@@ -250,12 +252,38 @@
                         "lines": [
                             "    --initialization-actions=gs://goog-dataproc-initialization-actions-{}/spark-rapids/spark-rapids.sh \\",
                             "    --worker-accelerator type=nvidia-tesla-{},count={}"
                         ]
                     },
                     "sectionID": "initializationScript",
                     "sectionName": "Initialization Scripts"
+                },
+                {
+                    "content": {
+                        "header": [
+                            "",
+                            "To create a GPU cluster, run the following script:",
+                            ""
+                        ]
+                    },
+                    "sectionID": "gpuClusterCreationScript"
+                },
+                {
+                    "content": {
+                        "header": [
+                            "",
+                            "Once the cluster is created, run the Bootstrap tool to provide optimized",
+                            "RAPIDS Accelerator for Apache Spark configs based on GPU cluster shape.",
+                            "Notes:",
+                            "    - Overriding the Apache Spark default configurations on the cluster",
+                            "      requires SSH access.",
+                            "    - If SSH access is unavailable, you can still dump the recommended",
+                            "      configurations by enabling the `dry_run` flag.",
+                            ""
+                        ]
+                    },
+                    "sectionID": "runUserToolsBootstrap"
                 }
             ]
         }
     }
 }
```

## spark_rapids_pytools/resources/emr-configs.json

### Pretty-printed

 * *Similarity: 0.8298611111111112%*

 * *Differences: {"'dependencies'": "{'deployMode': {'LOCAL': {0: {'size': 299350810}, 1: {'size': 962685}, 2: "*

 * *                   "{'size': 280645251}}}}",*

 * * "'wrapperReporting'": "OrderedDict([('qualification', OrderedDict([('sections', "*

 * *                       "[OrderedDict([('sectionID', 'gpuClusterCreationScript'), ('sectionName', "*

 * *                       "'Initialization Scripts'), ('content', OrderedDict([('header', ['', 'To "*

 * *                       "create a GPU cluster, run the following script:', ''])]))]), "*

 * *     […]*

```diff
@@ -5,28 +5,31 @@
     "dependencies": {
         "deployMode": {
             "LOCAL": [
                 {
                     "name": "Apache Spark",
                     "relativePath": "jars/*",
                     "sha512": "769db39a560a95fd88b58ed3e9e7d1e92fb68ee406689fb4d30c033cb5911e05c1942dcc70e5ec4585df84e80aabbc272b9386a208debda89522efff1335c8ff",
+                    "size": 299350810,
                     "type": "archive",
                     "uri": "https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
                 },
                 {
                     "md5": "59907e790ce713441955015d79f670bc",
                     "name": "Hadoop AWS",
                     "sha1": "a65839fbf1869f81a1632e09f415e586922e4f80",
+                    "size": 962685,
                     "type": "jar",
                     "uri": "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
                 },
                 {
                     "md5": "8a22f2d30b7e8eee9ea44f04fb13b35a",
                     "name": "AWS Java SDK Bundled",
                     "sha1": "02deec3a0ad83d13d032b1812421b23d7a961eea",
+                    "size": 280645251,
                     "type": "jar",
                     "uri": "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar"
                 }
             ]
         }
     },
     "environment": {
@@ -254,9 +257,42 @@
                     },
                     "localFile": "aws_ec2_catalog_ec2_us-west-2.json",
                     "onlineURL": "https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/20230307163705/us-west-2/index.json",
                     "resourceKey": "ec2-catalog"
                 }
             ]
         }
+    },
+    "wrapperReporting": {
+        "qualification": {
+            "sections": [
+                {
+                    "content": {
+                        "header": [
+                            "",
+                            "To create a GPU cluster, run the following script:",
+                            ""
+                        ]
+                    },
+                    "sectionID": "gpuClusterCreationScript",
+                    "sectionName": "Initialization Scripts"
+                },
+                {
+                    "content": {
+                        "header": [
+                            "",
+                            "Once the cluster is created, run the Bootstrap tool to provide optimized",
+                            "RAPIDS Accelerator for Apache Spark configs based on GPU cluster shape.",
+                            "Notes:",
+                            "    - Overriding the Apache Spark default configurations on the cluster",
+                            "      requires SSH access.",
+                            "    - If SSH access is unavailable, you can still dump the recommended",
+                            "      configurations by enabling the `dry_run` flag.",
+                            ""
+                        ]
+                    },
+                    "sectionID": "runUserToolsBootstrap"
+                }
+            ]
+        }
     }
 }
```

## spark_rapids_pytools/resources/qualification-conf.yaml

```diff
@@ -7,14 +7,15 @@
       columns:
         - App Name
         - App ID
         - Recommendation
         - Estimated GPU Speedup
         - Estimated GPU Duration
         - App Duration
+        - Estimated Job Frequency (monthly)
       mapColumns:
         Recommendation: 'Speedup Based Recommendation'
       recommendations:
         speedUp:
           columnName: 'Speedup Based Recommendation'
           selectedRecommendations:
             - 'Strongly Recommended'
@@ -82,15 +83,15 @@
     cleanUp: true
     fileName: qualification_summary.csv
     costColumns:
       - 'Savings Based Recommendation'
       - 'Estimated App Cost'
       - 'Estimated GPU Cost'
       - 'Estimated GPU Savings(%)'
-      - 'Job Frequency(monthly)'
+      - 'Estimated Job Frequency (monthly)'
       - 'Annual Cost Savings'
     savingColumn: 'Estimated GPU Savings(%)'
     speedupRecommendColumn: 'Speedup Based Recommendation'
     savingRecommendColumn: 'Savings Based Recommendation'
     summaryColumns:
       - 'App ID'
       - 'App Name'
```

## Comparing `spark_rapids_user_tools-23.2.3.dist-info/LICENSE` & `spark_rapids_user_tools-23.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `spark_rapids_user_tools-23.2.3.dist-info/METADATA` & `spark_rapids_user_tools-23.4.0.dist-info/METADATA`

 * *Files 21% similar despite different names*

```diff
@@ -1,29 +1,33 @@
 Metadata-Version: 2.1
 Name: spark-rapids-user-tools
-Version: 23.2.3
+Version: 23.4.0
 Summary: A simple wrapper process around cloud service providers to run tools for the RAPIDS Accelerator for Apache Spark.
 Author-email: Raza Jafri <raza.jafri@gmail.com>, Ahmed Hussein <a@ahussein.me>
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
+Requires-Dist: chevron (==0.14.0)
+Requires-Dist: fastprogress (==1.0.3)
+Requires-Dist: fastcore (==1.5.29)
 Requires-Dist: fire (==0.4.0)
 Requires-Dist: pandas (==1.4.3)
 Requires-Dist: pyYAML (==6.0)
 Requires-Dist: tabulate (==0.8.10)
 Requires-Dist: importlib-resources (==5.10.2)
 Requires-Dist: requests (==2.28.2)
 Requires-Dist: packaging (==23.0)
 Requires-Dist: certifi (==2022.12.7)
 Requires-Dist: idna (==3.4)
 Requires-Dist: urllib3 (==1.26.14)
 Requires-Dist: beautifulsoup4 (==4.11.2)
+Requires-Dist: pygments (==2.15.0)
 
 # spark-rapids-user-tools
 
 User tools to help with the adoption, installation, execution, and tuning of RAPIDS Accelerator for Apache Spark.
 
 The wrapper improves end-user experience within the following dimensions:
 1. Qualification: Educate the CPU customer on the cost savings and acceleration potential of RAPIDS Accelerator for
@@ -67,16 +71,7 @@
 3. Make sure to install CSP SDK if you plan to run the tool wrapper.
 
 ## Usage and supported platforms
 
 Please refer to [spark-rapids-user-tools guide](https://github.com/NVIDIA/spark-rapids-tools/blob/main/user_tools/docs/index.md) for details on how to use the tools
 and the platform.
 
-## Changelog
-
-### [22.10.2] - 10-28-2022
-   
-- Support to handle tools jar arguments in the user tools wrapper
- 
-### [22.10.1] - 10-26-2022
-  
-- Initialize this project
```

## Comparing `spark_rapids_user_tools-23.2.3.dist-info/RECORD` & `spark_rapids_user_tools-23.4.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,64 +1,68 @@
 spark_rapids_dataproc_tools/__init__.py,sha256=9J-l7hjbbI2mJnqqK-jtlv9t9xZ6crJvbHL8u8xjags,648
 spark_rapids_dataproc_tools/cost_estimator.py,sha256=S29-6uYnrTzC82k8ieJfeKGvwbw-WYxGBeRmB43LRAg,10120
 spark_rapids_dataproc_tools/dataproc_utils.py,sha256=Ob4PA7sm9SEvKec142KsUhPdQUD3hjvLmqSncEGGKVM,27693
 spark_rapids_dataproc_tools/dataproc_wrapper.py,sha256=isPLQSE5ornegQ2CNkZJiKfm35ItgJYoQpTTiJ-adJM,15306
 spark_rapids_dataproc_tools/diag.py,sha256=raDYdKSTQzkQz-ZlqO5t4jcPniPu5mprNENkjQICiUM,10514
 spark_rapids_dataproc_tools/diag_dataproc.py,sha256=otMgpwr5xpcHGTspR_pH4dNbY2UuArwnMRnXeW9zxWQ,5344
-spark_rapids_dataproc_tools/rapids_models.py,sha256=NLVD59YsQ8e8Cf46fhuIugcYS_nydHXWykxHRbOu1b8,63036
+spark_rapids_dataproc_tools/rapids_models.py,sha256=EKlmZ9jtUlhIUmYnmBM-AQt_jUG4fLqFg6rPlv30pm8,63046
 spark_rapids_dataproc_tools/utilities.py,sha256=lY23B4lB4MdfznSNuSbX9R_Cfgh4CzwBLGuP9LhP56k,8782
 spark_rapids_dataproc_tools/csp/__init__.py,sha256=ySk_fqBwYU-spTD5AyN8wqyGCl2E-vdIbQhkJ6fVuno,921
 spark_rapids_dataproc_tools/csp/csp.py,sha256=sNDKuJMZ4LwfKuCQWOyzyavb-gdhNa-lz_CuIbf9L7Y,1671
 spark_rapids_dataproc_tools/csp/dataproc.py,sha256=NX1VvXqMTCCSw4whxD_6KoHWVSPQadXazjTtlRj6qGM,5196
 spark_rapids_dataproc_tools/diag_scripts/hello_world.py,sha256=d7OPcIZfQ_as5qJWkwno77kzsryU0_f4t8A6z-kIZP4,1154
 spark_rapids_dataproc_tools/diag_scripts/perf.py,sha256=7vUsfw7hWe2NoD8_37_f6c2AbocBIgAQVP2uSDj3H4I,1276
 spark_rapids_dataproc_tools/resources/bootstrap-conf.yaml,sha256=FfMyJqOt8VwK-FuVAASCenxBCk3EWkZbZ5Hwd9N8WlQ,1273
 spark_rapids_dataproc_tools/resources/gcloud-catalog.json,sha256=gsaT3fho6yLJGdpAMi6oRQjJtombuKa4vZaRPKqxMaM,684199
 spark_rapids_dataproc_tools/resources/profiling-conf.yaml,sha256=xluVs7e1AcbDXWUYt3_2FPZSiwGPCOJlsiwnnZT59nM,1416
 spark_rapids_dataproc_tools/resources/qualification-conf.yaml,sha256=FLC0kHTPNakz6GWtVA5IeolR3golZB1LqRc_rdPY0qc,2554
-spark_rapids_pytools/__init__.py,sha256=wwEDmLWjNN_XHZXGCV21XRJTEOuqHBvKAjY4qOIy8_E,750
+spark_rapids_pytools/__init__.py,sha256=VnkBArGJEnTlfFUuFWuNeF6AMkK2l8uCnwwGfH5UKjM,750
 spark_rapids_pytools/build.py,sha256=Ej4Pc2jPIyeVUUOyC67ZMc6Tj90NKj0DrXyniJc0FnY,992
 spark_rapids_pytools/wrapper.py,sha256=Cl2u6zebY-gMuCJ8jeqeGJP5XSVQToi0spa6563BB_4,1094
 spark_rapids_pytools/cloud_api/__init__.py,sha256=NQSbmxhLzvnZrf4ltpgCLMjCEERPv5QoYo62LEdDBs8,646
-spark_rapids_pytools/cloud_api/databricks_aws.py,sha256=Ac7LRKqCOxFoqGErkA1YE9a4lzJxZnuX1qLCtqFGkgw,12735
+spark_rapids_pytools/cloud_api/databricks_aws.py,sha256=g0XXBVeQwtVAjsm6Po3R3dUA-7CUqaRp9W7iFqkuU4I,12837
 spark_rapids_pytools/cloud_api/databricks_aws_job.py,sha256=ZNYM2pa8fObRhc9c9VcqCT6HxGJNfHeFhdoVYNek51E,1268
-spark_rapids_pytools/cloud_api/dataproc.py,sha256=6hoh63slWrZgK6Cr40S_cjRuilldXZ4Tqu-czxe_2es,23065
+spark_rapids_pytools/cloud_api/dataproc.py,sha256=ibU7ThJvJ6tnEuyG5dDedHApZGYUILGz6gVOOCDl8aI,24216
 spark_rapids_pytools/cloud_api/dataproc_job.py,sha256=gwqDpa_pmwKB5TAFCJqWs8mOFENcMhwfgCJwOh3TMVg,1426
-spark_rapids_pytools/cloud_api/emr.py,sha256=xPD8nBoRrkQmHe55wDLluqYSBDAgpd-QhOP4wgYn1IA,20734
+spark_rapids_pytools/cloud_api/emr.py,sha256=sjnaYwg-n-Fc5Q_rNgWaTd99g7tG6791MyuzlsXo778,21270
 spark_rapids_pytools/cloud_api/emr_job.py,sha256=EoVrjwSL9pYTJk6QDfokQOudla8KLwGAbR0AAZhk2bA,11269
 spark_rapids_pytools/cloud_api/gstorage.py,sha256=88IzLWOzEphCETGQaVN3_U7wUD0M2doDy759WQmNoJI,4939
 spark_rapids_pytools/cloud_api/s3storage.py,sha256=cFqZETxWt_3Yagk3UlQt65pM4A0qb6idpbi8kUOK5pE,4055
-spark_rapids_pytools/cloud_api/sp_types.py,sha256=fgTcWRQCTnjMKfTz0bLg1mapRh_zf5r09GogXiZzQEo,45141
+spark_rapids_pytools/cloud_api/sp_types.py,sha256=XtncEANi3jGbQ-xnEmw6i2jR4ItSBkpHed48mLerL2k,47222
 spark_rapids_pytools/common/__init__.py,sha256=A8h0t211p8t_aATYiwCLWTwTy564kCcZp-HTWKiO4u8,658
 spark_rapids_pytools/common/exceptions.py,sha256=CK9PF75hSrRh6qaz0aKoiNqaU78E8OfiNydZ4i6WgoM,978
 spark_rapids_pytools/common/prop_manager.py,sha256=ijCqsnrLHn8Ntb1dCEDHqITrxjb3LoEP8Eqhqsb0PS8,5432
-spark_rapids_pytools/common/sys_storage.py,sha256=xMQJcZfUeX81ImFvDsgy81kWfRUns--opN-tC3OquHk,13912
-spark_rapids_pytools/common/utilities.py,sha256=xvFSzdIui_uM_v2R696M8ofGXg2ApES4DLWeNSfHNYI,12420
+spark_rapids_pytools/common/sys_storage.py,sha256=-La2Oc5EemnRH2x5B24MHlrtEZh6nL2FjqAyrV953Os,14712
+spark_rapids_pytools/common/utilities.py,sha256=nJ6YAdAhVw0f-d6scwb5mpXQz_CNcySCzN4i4KqRSfM,13037
 spark_rapids_pytools/pricing/__init__.py,sha256=Y_IWTtiKulkkoC84SLS8LkRWTi393zeUGHaNogZfOSg,659
 spark_rapids_pytools/pricing/databricks_pricing.py,sha256=q7pRBs2YGJAJnxItBUDYB00f3hdWsFKgK4sXZocjMC4,3522
 spark_rapids_pytools/pricing/dataproc_pricing.py,sha256=4-RuYxW9V0K0mqj2mqCJsKEUbvxPXHCwF4AsK1z6UXE,4247
 spark_rapids_pytools/pricing/emr_pricing.py,sha256=vat14Df8_1By-McNl2ot_75RAwjJkyTfXA24-iefrM4,4717
-spark_rapids_pytools/pricing/price_provider.py,sha256=yf4uoNI8a6slsD6tY6f4PWOGWOF2AxZb9FpCtG94ARA,6199
+spark_rapids_pytools/pricing/price_provider.py,sha256=zZt2ay-BN1vM0cZXkiEzY5kpnYx1brRCJcyfotU71lo,6400
 spark_rapids_pytools/rapids/__init__.py,sha256=xiYk9b76AV_v3fEELcYzXCUPvXQhCD687eJ5RCYQW7M,666
 spark_rapids_pytools/rapids/bootstrap.py,sha256=sUnTfoutyQ4ml7c7jTstMj2x0YqY7VeQzEIF8wuSdbE,8004
 spark_rapids_pytools/rapids/profiling.py,sha256=KYiAf6lDiDADXBjPLRrCYFbc77l-xqEvWUrxnBzK7Fk,12902
-spark_rapids_pytools/rapids/qualification.py,sha256=clR7-E3L0wzqTQ0p7PHXmMUR_gOoGwDM1vxHASQ7HXY,40902
+spark_rapids_pytools/rapids/qualification.py,sha256=SlBVzXioVqzDhJAGnkvpsPs0TDpk-bFv10kvJHUArhU,41708
 spark_rapids_pytools/rapids/rapids_job.py,sha256=w7D5jZPCsyb7PqupPfQ02bNnOYAwld7RLMHXqoovUTs,6864
-spark_rapids_pytools/rapids/rapids_tool.py,sha256=LaUXq8T5NGKR4BLl8xZuP9JB8_IQH_TuHFZhdOWspi4,32973
-spark_rapids_pytools/rapids/tool_ctxt.py,sha256=aBoZgcqTd8VM65z4o8UbFWlLk5qYvk83ljq2bbqW9aQ,6566
+spark_rapids_pytools/rapids/rapids_tool.py,sha256=TyZxCPaGakzWXzN71S9UyFTIidUhB7SAVSJNXEKMymE,33530
+spark_rapids_pytools/rapids/tool_ctxt.py,sha256=8bWqooTKcIH5ShxTFrXPuSr1KEJeHXU5LdDq8F6ienU,6576
 spark_rapids_pytools/resources/bootstrap-conf.yaml,sha256=IlY7nmhSDlqH0Ahcc_PLP4v6zyU9DQ943LYAv-QbDbU,1295
 spark_rapids_pytools/resources/databricks-premium-catalog.json,sha256=XBptMDeu7Abbrv6xC4C1oDuMuEIqnrGEqwrhJFutLJM,30566
-spark_rapids_pytools/resources/databricks_aws-configs.json,sha256=wuXvwYHpT6tm1-fyC52xnYBNGBU60hDfEXyt-bi9xgI,10358
-spark_rapids_pytools/resources/dataproc-configs.json,sha256=JSzUbmJn2-mlhP09yBLsgQu1sOXoRsnc2Qqx1KyLOP4,7071
-spark_rapids_pytools/resources/emr-configs.json,sha256=D-SrRYzrP_XVLMNmDtiJB1QJU_apv_77HXlvI3QiU4Y,7855
+spark_rapids_pytools/resources/databricks_aws-configs.json,sha256=I_VqLdvavbQGUqCNcUqMpNrH5aF0xBZDdN7kuncNKQk,10448
+spark_rapids_pytools/resources/dataproc-configs.json,sha256=9rEXtuCKsLUMyoRsKAeW2OWckM1AXYsE3JuNjFAYmWA,8047
+spark_rapids_pytools/resources/emr-configs.json,sha256=_F5XxM5kFen_iCAWW5Z_jdD6G626h4hH_Uf2dq2Dn6U,8994
 spark_rapids_pytools/resources/profiling-conf.yaml,sha256=s7TR1agVORA4g28bYYK7jSMpgMT6s-awMySK0jT4kaA,1460
-spark_rapids_pytools/resources/qualification-conf.yaml,sha256=UDDJ09ic8UBgfLXd5B-fD0x_TbGkXfVKZew5BZhD7xY,4197
+spark_rapids_pytools/resources/qualification-conf.yaml,sha256=QOtc8bq0PxPK5LvXFFUd9oeCoy3BEe_gKNx0OGrvrDo,4252
+spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms,sha256=pigL0kAsg1VaP9Jn9-5oNblJRj0IbeZLh1T5Op8CQqA,671
+spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms,sha256=lmnyouNCpbytpRVZ_YbQ6d6hCl3XaYxqzNomxSJwSSk,518
+spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms,sha256=UIlW0GcYn2fovtcDNYPUmg99h0zKy-DxXGO-1Lr_xQs,855
+spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms,sha256=7xqXBMIu5Tg02KCq_YN9oLLiE3c8jgWl8BUvFQZODhs,537
 spark_rapids_pytools/wrappers/__init__.py,sha256=CQ7Mf-YyBFNl6xmQGsPARv1w5GU3jGliByn5IHCcLkE,630
 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py,sha256=FPuw9pnU8huyRfeGYGc9xljYe9xf9cSsuEg0iaHBYtE,8104
 spark_rapids_pytools/wrappers/dataproc_wrapper.py,sha256=0WTdCPfNoB9s3QCJGhv2t28fbrI7Ovv1esiaVSJiQhw,14685
 spark_rapids_pytools/wrappers/emr_wrapper.py,sha256=vWg2hV1MEyjVFXgtsTvTnaSJYNo9eZ_UVM0XHZJhDTo,15570
-spark_rapids_user_tools-23.2.3.dist-info/LICENSE,sha256=Q4cfQjX201ldPCukEkWCu_m9gqQOGraxXCLYHsrBRbU,18745
-spark_rapids_user_tools-23.2.3.dist-info/METADATA,sha256=8OKgK3uZpjKLtSLt9w8F15fVnb0yzlysBBBhK5_Io2s,2966
-spark_rapids_user_tools-23.2.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-spark_rapids_user_tools-23.2.3.dist-info/entry_points.txt,sha256=OQT0O5JpYFfCtexjgM31__xoCugCT1eHZ9PDT-xq054,152
-spark_rapids_user_tools-23.2.3.dist-info/top_level.txt,sha256=u2CXaBsgoZoiL8XafMeC-8ITnOTUn7Bwrmz8fPjwjio,49
-spark_rapids_user_tools-23.2.3.dist-info/RECORD,,
+spark_rapids_user_tools-23.4.0.dist-info/LICENSE,sha256=Q4cfQjX201ldPCukEkWCu_m9gqQOGraxXCLYHsrBRbU,18745
+spark_rapids_user_tools-23.4.0.dist-info/METADATA,sha256=05Vvl4hIzAfj43LQfWAvtWxmzci2EPI6kQSPB0SWQNw,2939
+spark_rapids_user_tools-23.4.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+spark_rapids_user_tools-23.4.0.dist-info/entry_points.txt,sha256=OQT0O5JpYFfCtexjgM31__xoCugCT1eHZ9PDT-xq054,152
+spark_rapids_user_tools-23.4.0.dist-info/top_level.txt,sha256=u2CXaBsgoZoiL8XafMeC-8ITnOTUn7Bwrmz8fPjwjio,49
+spark_rapids_user_tools-23.4.0.dist-info/RECORD,,
```

