# Comparing `tmp/pyFileFixity-3.1.1.tar.gz` & `tmp/pyFileFixity-3.1.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pyFileFixity-3.1.1.tar", last modified: Sun Apr  9 23:53:37 2023, max compression
+gzip compressed data, was "pyFileFixity-3.1.4.tar", last modified: Wed Apr 26 20:48:37 2023, max compression
```

## Comparing `pyFileFixity-3.1.1.tar` & `pyFileFixity-3.1.4.tar`

### file list

```diff
@@ -1,80 +1,80 @@
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/
--rw-rw-rw-   0        0        0      626 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/.coveragerc
--rw-rw-rw-   0        0        0     1106 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/LICENSE
--rw-rw-rw-   0        0        0     1155 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/MANIFEST.in
--rw-rw-rw-   0        0        0    64472 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/PKG-INFO
--rw-rw-rw-   0        0        0    62157 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/README.rst
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:36.798953 pyFileFixity-3.1.1/pyFileFixity/
--rw-rw-rw-   0        0        0      177 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/__init__.py
--rw-rw-rw-   0        0        0     8205 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/easy_profiler.py
--rw-rw-rw-   0        0        0     1032 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/ecc_specification.txt
--rw-rw-rw-   0        0        0    13147 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/ecc_speedtest.py
--rw-rw-rw-   0        0        0    18667 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/filetamper.py
--rw-rw-rw-   0        0        0    60875 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/header_ecc.py
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:36.798953 pyFileFixity-3.1.1/pyFileFixity/lib/
--rw-rw-rw-   0        0        0        3 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/__init__.py
--rw-rw-rw-   0        0        0     1428 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/_compat.py
--rw-rw-rw-   0        0        0    22551 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/aux_funcs.py
--rw-rw-rw-   0        0        0    17370 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/eccman.py
--rw-rw-rw-   0        0        0     3459 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/hasher.py
--rw-rw-rw-   0        0        0    14183 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/md5py.py
--rw-rw-rw-   0        0        0    30966 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/six.py
--rw-rw-rw-   0        0        0     3144 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/lib/tee.py
--rw-rw-rw-   0        0        0     7566 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/pff.py
--rw-rw-rw-   0        0        0    27920 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/repair_ecc.py
--rw-rw-rw-   0        0        0    38261 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/replication_repair.py
--rw-rw-rw-   0        0        0    27706 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/resiliency_tester.py
--rw-rw-rw-   0        0        0     1983 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/resiliency_tester_config.txt
--rw-rw-rw-   0        0        0    37567 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/rfigc.py
--rw-rw-rw-   0        0        0    69990 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/structural_adaptive_ecc.py
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:36.814578 pyFileFixity-3.1.1/pyFileFixity/tests/
--rw-rw-rw-   0        0        0        1 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/__init__.py
--rw-rw-rw-   0        0        0     6753 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/aux_tests.py
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/pyFileFixity/tests/files/
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/pyFileFixity/tests/files/Sub2/
--rw-rw-rw-   0        0        0        6 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/Sub2/testsub2.txt
--rw-rw-rw-   0        0        0    67254 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/alice.pdf
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/pyFileFixity/tests/files/sub/
--rw-rw-rw-   0        0        0     2048 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/sub/Snark.zip
--rw-rw-rw-   0        0        0      255 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/sub/testsub.txt
--rw-rw-rw-   0        0        0        8 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/testaa.txt
--rw-rw-rw-   0        0        0    19719 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/tux.jpg
--rw-rw-rw-   0        0        0     3667 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/files/tuxsmall.jpg
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/pyFileFixity/tests/results/
--rw-rw-rw-   0        0        0     2087 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/resiliency_tester_config_easy.cfg
--rw-rw-rw-   0        0        0     1661 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/resiliency_tester_config_hard.cfg
--rw-rw-rw-   0        0        0     1614 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_algo.db
--rw-rw-rw-   0        0        0     6473 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_dir.db
--rw-rw-rw-   0        0        0     1614 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_one_file.db
--rw-rw-rw-   0        0        0     1614 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_one_file_tamper.db
--rw-rw-rw-   0        0        0     6473 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_repair_ecc_check.db
--rw-rw-rw-   0        0        0    47542 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_repair_ecc_sa_check.db
--rw-rw-rw-   0        0        0      697 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_dir.csv
--rw-rw-rw-   0        0        0      132 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_error_file.log
--rw-rw-rw-   0        0        0      162 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_one_file.csv
--rw-rw-rw-   0        0        0      694 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_update_append.csv
--rw-rw-rw-   0        0        0      166 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_update_remove.csv
--rw-rw-rw-   0        0        0     2883 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_algo.db
--rw-rw-rw-   0        0        0    47542 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_dir.db
--rw-rw-rw-   0        0        0     2883 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file.db
--rw-rw-rw-   0        0        0     2883 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file_tamper.db
--rw-rw-rw-   0        0        0     6753 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_aux_funcs.py
--rw-rw-rw-   0        0        0     7042 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_eccman.py
--rw-rw-rw-   0        0        0     1755 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_hasher.py
--rw-rw-rw-   0        0        0     8255 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_header_ecc.py
--rw-rw-rw-   0        0        0     8394 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_repair_ecc.py
--rw-rw-rw-   0        0        0    13155 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_replication_repair.py
--rw-rw-rw-   0        0        0     8708 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_resiliency_tester.py
--rw-rw-rw-   0        0        0     6577 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_rfigc.py
--rw-rw-rw-   0        0        0    10655 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_structural_adaptive_ecc.py
--rw-rw-rw-   0        0        0     2303 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyFileFixity/tests/test_tee.py
-drwxrwxrwx   0        0        0        0 2023-04-09 23:53:36.798953 pyFileFixity-3.1.1/pyFileFixity.egg-info/
--rw-rw-rw-   0        0        0    64472 2023-04-09 23:53:36.000000 pyFileFixity-3.1.1/pyFileFixity.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     2651 2023-04-09 23:53:36.000000 pyFileFixity-3.1.1/pyFileFixity.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-04-09 23:53:36.000000 pyFileFixity-3.1.1/pyFileFixity.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       46 2023-04-09 23:53:36.000000 pyFileFixity-3.1.1/pyFileFixity.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0      168 2023-04-09 23:53:36.000000 pyFileFixity-3.1.1/pyFileFixity.egg-info/requires.txt
--rw-rw-rw-   0        0        0       13 2023-04-09 23:53:36.000000 pyFileFixity-3.1.1/pyFileFixity.egg-info/top_level.txt
--rw-rw-rw-   0        0        0    10708 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/pyproject.toml
--rw-rw-rw-   0        0        0       42 2023-04-09 23:53:37.361517 pyFileFixity-3.1.1/setup.cfg
--rw-rw-rw-   0        0        0     1762 2023-04-09 23:51:22.000000 pyFileFixity-3.1.1/tox.ini
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.893348 pyFileFixity-3.1.4/
+-rw-r--r--   0 runner     (501) staff       (20)      603 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/.coveragerc
+-rw-r--r--   0 runner     (501) staff       (20)     1084 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/LICENSE
+-rw-r--r--   0 runner     (501) staff       (20)     1132 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/MANIFEST.in
+-rw-r--r--   0 runner     (501) staff       (20)    80481 2023-04-26 20:48:37.892611 pyFileFixity-3.1.4/PKG-INFO
+-rw-r--r--   0 runner     (501) staff       (20)    78211 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/README.rst
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.858253 pyFileFixity-3.1.4/pyFileFixity/
+-rw-r--r--   0 runner     (501) staff       (20)      171 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     8048 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/easy_profiler.py
+-rw-r--r--   0 runner     (501) staff       (20)     1026 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/ecc_specification.txt
+-rw-r--r--   0 runner     (501) staff       (20)    12938 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/ecc_speedtest.py
+-rw-r--r--   0 runner     (501) staff       (20)    18366 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/filetamper.py
+-rw-r--r--   0 runner     (501) staff       (20)    60134 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/header_ecc.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.867892 pyFileFixity-3.1.4/pyFileFixity/lib/
+-rw-r--r--   0 runner     (501) staff       (20)        2 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     1693 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/_compat.py
+-rw-r--r--   0 runner     (501) staff       (20)    22124 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/aux_funcs.py
+-rw-r--r--   0 runner     (501) staff       (20)    17101 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/eccman.py
+-rw-r--r--   0 runner     (501) staff       (20)     3385 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/hasher.py
+-rw-r--r--   0 runner     (501) staff       (20)    14183 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/md5py.py
+-rw-r--r--   0 runner     (501) staff       (20)    30098 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/six.py
+-rw-r--r--   0 runner     (501) staff       (20)     3066 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/lib/tee.py
+-rw-r--r--   0 runner     (501) staff       (20)     7433 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/pff.py
+-rw-r--r--   0 runner     (501) staff       (20)    27551 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/repair_ecc.py
+-rw-r--r--   0 runner     (501) staff       (20)    37637 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/replication_repair.py
+-rw-r--r--   0 runner     (501) staff       (20)    27123 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/resiliency_tester.py
+-rw-r--r--   0 runner     (501) staff       (20)     1946 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/resiliency_tester_config.txt
+-rw-r--r--   0 runner     (501) staff       (20)    36845 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/rfigc.py
+-rw-r--r--   0 runner     (501) staff       (20)    69198 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/structural_adaptive_ecc.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.875700 pyFileFixity-3.1.4/pyFileFixity/tests/
+-rw-r--r--   0 runner     (501) staff       (20)        1 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     6590 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/aux_tests.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.878550 pyFileFixity-3.1.4/pyFileFixity/tests/files/
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.879143 pyFileFixity-3.1.4/pyFileFixity/tests/files/Sub2/
+-rw-r--r--   0 runner     (501) staff       (20)        6 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/Sub2/testsub2.txt
+-rw-r--r--   0 runner     (501) staff       (20)    67254 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/alice.pdf
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.880425 pyFileFixity-3.1.4/pyFileFixity/tests/files/sub/
+-rw-r--r--   0 runner     (501) staff       (20)     2048 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/sub/Snark.zip
+-rw-r--r--   0 runner     (501) staff       (20)      253 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/sub/testsub.txt
+-rw-r--r--   0 runner     (501) staff       (20)        8 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/testaa.txt
+-rw-r--r--   0 runner     (501) staff       (20)    19719 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/tux.jpg
+-rw-r--r--   0 runner     (501) staff       (20)     3667 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/files/tuxsmall.jpg
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.891741 pyFileFixity-3.1.4/pyFileFixity/tests/results/
+-rw-r--r--   0 runner     (501) staff       (20)     2050 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/resiliency_tester_config_easy.cfg
+-rw-r--r--   0 runner     (501) staff       (20)     1627 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/resiliency_tester_config_hard.cfg
+-rw-r--r--   0 runner     (501) staff       (20)     1614 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_algo.db
+-rw-r--r--   0 runner     (501) staff       (20)     6473 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_dir.db
+-rw-r--r--   0 runner     (501) staff       (20)     1614 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_one_file.db
+-rw-r--r--   0 runner     (501) staff       (20)     1614 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_one_file_tamper.db
+-rw-r--r--   0 runner     (501) staff       (20)     6473 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_repair_ecc_check.db
+-rw-r--r--   0 runner     (501) staff       (20)    47542 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_repair_ecc_sa_check.db
+-rw-r--r--   0 runner     (501) staff       (20)      689 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_dir.csv
+-rw-r--r--   0 runner     (501) staff       (20)      131 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_error_file.log
+-rw-r--r--   0 runner     (501) staff       (20)      161 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_one_file.csv
+-rw-r--r--   0 runner     (501) staff       (20)      686 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_update_append.csv
+-rw-r--r--   0 runner     (501) staff       (20)      164 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_update_remove.csv
+-rw-r--r--   0 runner     (501) staff       (20)     2883 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_algo.db
+-rw-r--r--   0 runner     (501) staff       (20)    47542 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_dir.db
+-rw-r--r--   0 runner     (501) staff       (20)     2883 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file.db
+-rw-r--r--   0 runner     (501) staff       (20)     2883 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file_tamper.db
+-rw-r--r--   0 runner     (501) staff       (20)     6626 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_aux_funcs.py
+-rw-r--r--   0 runner     (501) staff       (20)     6916 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_eccman.py
+-rw-r--r--   0 runner     (501) staff       (20)     1713 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_hasher.py
+-rw-r--r--   0 runner     (501) staff       (20)     8111 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_header_ecc.py
+-rw-r--r--   0 runner     (501) staff       (20)     8229 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_repair_ecc.py
+-rw-r--r--   0 runner     (501) staff       (20)    12865 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_replication_repair.py
+-rw-r--r--   0 runner     (501) staff       (20)     8511 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_resiliency_tester.py
+-rw-r--r--   0 runner     (501) staff       (20)     6959 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_rfigc.py
+-rw-r--r--   0 runner     (501) staff       (20)    10510 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_structural_adaptive_ecc.py
+-rw-r--r--   0 runner     (501) staff       (20)     2240 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyFileFixity/tests/test_tee.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-04-26 20:48:37.862494 pyFileFixity-3.1.4/pyFileFixity.egg-info/
+-rw-r--r--   0 runner     (501) staff       (20)    80481 2023-04-26 20:48:37.000000 pyFileFixity-3.1.4/pyFileFixity.egg-info/PKG-INFO
+-rw-r--r--   0 runner     (501) staff       (20)     2651 2023-04-26 20:48:37.000000 pyFileFixity-3.1.4/pyFileFixity.egg-info/SOURCES.txt
+-rw-r--r--   0 runner     (501) staff       (20)        1 2023-04-26 20:48:37.000000 pyFileFixity-3.1.4/pyFileFixity.egg-info/dependency_links.txt
+-rw-r--r--   0 runner     (501) staff       (20)       46 2023-04-26 20:48:37.000000 pyFileFixity-3.1.4/pyFileFixity.egg-info/entry_points.txt
+-rw-r--r--   0 runner     (501) staff       (20)      168 2023-04-26 20:48:37.000000 pyFileFixity-3.1.4/pyFileFixity.egg-info/requires.txt
+-rw-r--r--   0 runner     (501) staff       (20)       13 2023-04-26 20:48:37.000000 pyFileFixity-3.1.4/pyFileFixity.egg-info/top_level.txt
+-rw-r--r--   0 runner     (501) staff       (20)    10452 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/pyproject.toml
+-rw-r--r--   0 runner     (501) staff       (20)       38 2023-04-26 20:48:37.893485 pyFileFixity-3.1.4/setup.cfg
+-rw-r--r--   0 runner     (501) staff       (20)     1712 2023-04-26 20:47:17.000000 pyFileFixity-3.1.4/tox.ini
```

### Comparing `pyFileFixity-3.1.1/.coveragerc` & `pyFileFixity-3.1.4/.coveragerc`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-[run]
-relative_files = True
-branch = True
-omit =
-    pyFileFixity/tests/*
-    pyFileFixity/__init__.py
-    pyFileFixity/easy_profiler.py
-    pyFileFixity/ecc_speedtest.py
-    pyFileFixity/filetamper.py
-    pycleaner.py
-    setup.py
-include = 
-    pyFileFixity/lib/aux_funcs.py
-    pyFileFixity/lib/eccman.py
-    pyFileFixity/lib/hasher.py
-    pyFileFixity/lib/tee.py
-    pyFileFixity/_infos.py
-    pyFileFixity/header_ecc.py
-    pyFileFixity/repair_ecc.py
-    pyFileFixity/replication_repair.py
-    pyFileFixity/resiliency_tester.py
-    pyFileFixity/rfigc.py
-    pyFileFixity/structural_adaptive_ecc.py
+[run]
+relative_files = True
+branch = True
+omit =
+    pyFileFixity/tests/*
+    pyFileFixity/__init__.py
+    pyFileFixity/easy_profiler.py
+    pyFileFixity/ecc_speedtest.py
+    pyFileFixity/filetamper.py
+    pycleaner.py
+    setup.py
+include = 
+    pyFileFixity/lib/aux_funcs.py
+    pyFileFixity/lib/eccman.py
+    pyFileFixity/lib/hasher.py
+    pyFileFixity/lib/tee.py
+    pyFileFixity/_infos.py
+    pyFileFixity/header_ecc.py
+    pyFileFixity/repair_ecc.py
+    pyFileFixity/replication_repair.py
+    pyFileFixity/resiliency_tester.py
+    pyFileFixity/rfigc.py
+    pyFileFixity/structural_adaptive_ecc.py
```

### Comparing `pyFileFixity-3.1.1/LICENSE` & `pyFileFixity-3.1.4/LICENSE`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-The MIT License (MIT)
-
-Copyright (c) 2015 Stephen Larroque
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
-
+The MIT License (MIT)
+
+Copyright (c) 2015 Stephen Larroque
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
```

### Comparing `pyFileFixity-3.1.1/MANIFEST.in` & `pyFileFixity-3.1.4/MANIFEST.in`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-# Cannot yet fully replace MANIFEST.in by pyproject.toml if we use setuptools, see: https://github.com/pypa/setuptools/issues/3341
-# Misc
-include .coveragerc  # for compatibility with Py2, otherwise the coverage is configured in pyproject.toml
-#include LICENSE
-#include Makefile
-#include README.md
-#include README.rst
-#include TODO.md
-
-# Non-python files
-include pyFileFixity/ecc_specification.txt  # done in pyproject.toml, but for Py2 we need to put it here
-include pyFileFixity/resiliency_tester_config.txt  # idem
-
-# Libraries
-recursive-include pyFileFixity/lib *.py
-recursive-include pyFileFixity/lib *.pyx  # Cython files, all were moved to their own modules in distinct repositories, but we may optimize some routines in the future
-recursive-exclude pyFileFixity/lib/profilers *  # no need for profilers
-
-# Test suite
-recursive-include pyFileFixity/tests *.py  # unit test scripts
-recursive-include pyFileFixity/tests/files *  # attach necessary files to run tests
-recursive-include pyFileFixity/tests/results *  # attach necessary py-make config and resulting database files to run and compare tests results
-include tox.ini
+# Cannot yet fully replace MANIFEST.in by pyproject.toml if we use setuptools, see: https://github.com/pypa/setuptools/issues/3341
+# Misc
+include .coveragerc  # for compatibility with Py2, otherwise the coverage is configured in pyproject.toml
+#include LICENSE
+#include Makefile
+#include README.md
+#include README.rst
+#include TODO.md
+
+# Non-python files
+include pyFileFixity/ecc_specification.txt  # done in pyproject.toml, but for Py2 we need to put it here
+include pyFileFixity/resiliency_tester_config.txt  # idem
+
+# Libraries
+recursive-include pyFileFixity/lib *.py
+recursive-include pyFileFixity/lib *.pyx  # Cython files, all were moved to their own modules in distinct repositories, but we may optimize some routines in the future
+recursive-exclude pyFileFixity/lib/profilers *  # no need for profilers
+
+# Test suite
+recursive-include pyFileFixity/tests *.py  # unit test scripts
+recursive-include pyFileFixity/tests/files *  # attach necessary files to run tests
+recursive-include pyFileFixity/tests/results *  # attach necessary py-make config and resulting database files to run and compare tests results
+include tox.ini
```

### Comparing `pyFileFixity-3.1.1/PKG-INFO` & `pyFileFixity-3.1.4/PKG-INFO`

 * *Files 17% similar despite different names*

```diff
@@ -1,1122 +1,1317 @@
-Metadata-Version: 2.1
-Name: pyFileFixity
-Version: 3.1.1
-Summary: Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing.
-Author-email: Stephen Karl Larroque <lrq3000@gmail.com>
-Maintainer-email: Stephen Karl Larroque <lrq3000@gmail.com>
-License: MIT License
-Project-URL: Homepage, https://github.com/lrq3000/pyFileFixity
-Project-URL: Documentation, https://github.com/lrq3000/pyFileFixity/blob/master/README.rst
-Project-URL: Source, https://github.com/lrq3000/pyFileFixity
-Project-URL: Tracker, https://github.com/lrq3000/pyFileFixity/issues
-Project-URL: Download, https://github.com/lrq3000/pyFileFixity/releases
-Keywords: file,repair,monitor,change,reed-solomon,error,correction,error correction,parity,parity files,parity bytes,data protection,data recovery,file protection,qr codes,qr code
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Environment :: Console
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS :: MacOS X
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Programming Language :: Python :: 3.12
-Classifier: Programming Language :: Python :: Implementation :: PyPy
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Classifier: Topic :: System :: Archiving
-Classifier: Topic :: System :: Archiving :: Backup
-Classifier: Topic :: System :: Monitoring
-Classifier: Topic :: System :: Recovery Tools
-Classifier: Topic :: Utilities
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: End Users/Desktop
-Classifier: Intended Audience :: Information Technology
-Classifier: Intended Audience :: System Administrators
-Requires-Python: >=3.7
-Description-Content-Type: text/x-rst
-Provides-Extra: test
-Provides-Extra: testmeta
-License-File: LICENSE
-
-pyFileFixity
-============
-
-|PyPI-Status| |PyPI-Versions| |PyPI-Downloads|
-
-|Build-Status| |Coverage|
-
-pyFileFixity provides a suite of open source, cross-platform, easy
-to use and easy to maintain (readable code) to protect and manage data
-for long term storage/archival, and also test the performance of any data protection algorithm.
-
-The project is done in pure-Python to meet those criteria,
-although cythonized extensions are available for core routines to speed up encoding/decoding,
-but always with a pure python specification available so as to allow long term replication.
-
-Here is an example of what pyFileFixity can do:
-
-|Example|
-
-On the left, this is the original image.
-
-At the center, the same image but
-with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
-which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
-Only a few corrupted bytes are enough to make the image looks like totally
-unrecoverable, and yet we are lucky, because the image could be unreadable at all
-if any of the "magic bytes" were to be corrupted!
-
-At the right, the corrupted image was repaired using `header_ecc.py` of pyFileFixity.
-This repaired only the image header (ie, the first part of the file), so only the first
-3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
-the image looks like it's totally repaired! And the best thing is that it only costed the generation
-of a "ecc repair file", which size is only 3.3KB (17% of the original file)!
-
-This works because most files will store the most important information to read them at
-their beginning, also called "file's header", so repairing this part will almost always ensure
-the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
-you can read it).
-
-Of course, you can also protect the whole file, not only the header, using pyFileFixity's
-`structural_adaptive_ecc.py`. You can also detect any corruption using `rfigc.py`.
-
-------------------------------------------
-
-.. contents:: Table of contents
-   :backlinks: top
-
-Quickstart
-----------
-
-Runs on Python 3 up to Python 3.12-dev. PyPy 3 is also supported.
-
-- To install or update on Python 3:
-
-``pip install --upgrade pyfilefixity``
-
-- For Python 2.7, the latest working version was v3.0.2:
-
-``pip install --upgrade pyfilefixity==3.0.2 reedsolo==1.7.0 unireedsolomon==1.0.5``
-
-- Once installed, the suite of tools can be accessed from a centralized interface script called ``pff`` which provides several subcommands, to list them:
-
-``pff --help``
-
-You should see:
-
-::
-
-    usage: pff [-h]
-               {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
-               ...
-
-    positional arguments:
-      {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
-        hash (rfigc)        Check files integrity fast by hash, size, modification date or by data structure integrity.
-        header (header_ecc, hecc)
-                            Protect/repair files headers with error correction codes
-        whole (structural_adaptive_ecc, saecc, protect, repair)
-                            Protect/repair whole files with error correction codes
-        recover (repair_ecc, recc)
-                            Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery
-                            mode (note: this does NOT recover your files, only the ecc files, which may then be used to
-                            recover your files!)
-        dup (replication_repair)
-                            Repair files from multiple copies of various storage mediums using a majority vote
-        restest (resilience_tester)
-                            Run tests to quantify robustness of a file protection scheme (can be used on any, not just
-                            pyFileFixity)
-        filetamper          Tamper files using various schemes
-        speedtest (ecc_speedtest)
-                            Run error correction encoding and decoding speedtests
-
-    options:
-      -h, --help            show this help message and exit
-
-- Every subcommands provide their own more detailed help instructions, eg for the ``hash`` submodule:
-
-``pff hash --help``
-
-- To generate a monitoring database (to later check very fast which files are corrupted, but cannot repair anything but filesystem metadata):
-
-``pff hash -i "your_folder" -d "dbhash.csv" -g -f -l "log.txt"``
-
-Note: this also works for a single file, just replace "your_folder" by "your_file.ext".
-
-- Later, to check which files were corrupted:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s -e errors.csv``
-
-- To use this monitoring database to recover filesystem metadata such as files names and directory layout by filescraping from files contents:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l "log.txt" -o "output_folder" --filescraping_recovery``
-
-- To protect files headers with a file called ``hecc.txt``:
-
-``pff header -i "your_folder" -d "hecc.txt" -l "log.txt" -g -f --ecc_algo 3``
-
-- To repair files headers and store the repaired files in ``output_folder``:
-
-``pff header -i "your_folder" -d "hecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
-
-- To protect whole files with a file called ``ecc.txt``:
-
-``pff whole -i "your_folder" -d "ecc.txt" -l "log.txt" -g -f -v --ecc_algo 3``
-
-- To repair whole files:
-
-``pff whole -i "your_folder" -d "ecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
-
-Note that ``header`` and ``whole`` can also detect corrupted files and even which blocks inside a file, but they are much slower than ``hash``.
-
-- To try to recover a damaged ecc file ``ecc.txt`` using an index file ``ecc.txt.idx`` (index file is generated automatically with ecc.txt):
-
-``pff recovery -i "ecc.txt" --index "ecc.txt.idx" -o "ecc_repaired.txt" -l "log.txt" -v -f``
-
-- To try to recover a damaged ecc file ``ecc.txt`` without an index file (you can tweak the ``-t`` parameter from 0.0 to 1.0, 1.0 producing many false positives):
-
-``pff recovery -i "ecc.txt" -o "ecc_repaired.txt" -l "log.txt" -v -f -t 0.4``
-
-- To repair your files using multiple duplicated copies that you have stored on different mediums:
-
-``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" --report "rlog.csv" -f -v``
-
-- If you have previously generated a rfigc database, you can use it to enhance the replication repair:
-
-``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" -d "dbhash.csv" --report "rlog.csv" -f -v``
-
-- To run tests on your recovery tools, you can make a Makefile-like configuration file and use the Resiliency Tester submodule:
-
-``pff restest -i "your_folder" -o "test_folder" -c "resiliency_tester_config.txt" -m 3 -l "testlog.txt" -f``
-
-- Internally, ``pff restest`` uses ``pff filetamper`` to tamper files with various schemes, but you can also use ``pff filetamper`` directly.
-
-- To run speedtests of encoding/decoding error correction codes on your machine:
-
-``pff speedtest``
-
-- In case the ``pff`` command does not work, it can be replaced with ``python -m pyFileFixity.pff`` .
-
-The problem of long term storage
---------------------------------
-
-Why are data corrupted with time? One sole reason: entropy.
-Entropy refers to the universal tendency for systems to become
-less ordered over time. Data corruption is exactly that: a disorder
-in bits order. In other words: *the Universe hates your data*.
-
-Long term storage is thus a very difficult topic: it's like fighting with
-death (in this case, the death of data). Indeed, because of entropy,
-data will eventually fade away because of various silent errors such as
-bit rot or cosmic rays. pyFileFixity aims to provide tools to detect any data
-corruption, but also fight data corruption by providing repairing tools.
-
-The only solution is to use a principle of engineering that is long
-known and which makes bridges and planes safe: add some **redundancy**.
-
-There are only 2 ways to add redundancy:
-
--  the simple way is to **duplicate** the object (also called replication),
-   but for data storage, this eats up a lot of storage and is not optimal.
-   However, if storage is cheap, then this is a good solution, as it is
-   much faster than encoding with error correction codes. For replication to work,
-   at least 3 duplicates are necessary at all times, so that if one fails, it must
-   replaced asap. As sailors say: "Either bring 1 compass or 3 compasses, but never
-   two, because then you won't know which one is correct if one fails."
-   Indeed, with 3 duplicates, if you frequently monitor their integrity
-   (eg, with hashes), then if one fails, simply do a majority vote:
-   the bit value given by 2 of the duplicates is probably correct.
--  the second way, the optimal tools ever invented to recover
-   from data corruption, are the **error correction codes** (forward
-   error correction), which are a way to smartly produce redundant codes
-   from your data so that you can later repair your data using these
-   additional pieces of information (ie, an ECC generates n blocks for a
-   file cut in k blocks (with k < n), and then the ecc code can rebuild
-   the whole file with (at least) any k blocks among the total n blocks
-   available). In other words, you can correct up to (n-k) erasures. But
-   error correcting codes can also detect and repair automatically where
-   the errors are (fully automatic data repair for you !), but at the
-   cost that you can then only correct (n-k)/2 errors.
-
-Error correction can seem a bit magical, but for a reasonable intuition,
-it can be seen as a way to average the corruption error rate: on
-average, a bit will still have the same chance to be corrupted, but
-since you have more bits to represent the same data, you lower the
-overall chance to lose this bit.
-
-The problem is that most theoretical and pratical works on error
-correcting codes has been done almost exclusively on channel
-transmission (such as 4G, internet, etc.), but not on data storage,
-which is very different for one reason: whereas in a channel we are in a
-spatial scheme (both the sender and the receiver are different entities
-in space but working at the same timescale), in data storage this is a
-temporal scheme: the sender was you storing the data on your medium at
-time t, and the receiver is again you but now retrieving the data at
-time t+x. Thus, the sender does not exist anymore, thus you cannot ask
-the sender to send again some data if it's too much corrupted: in data
-storage, if a data is corrupted, it's lost for good, whereas in channel theory,
-parts of the data can be submitted again if necessary.
-
-Some attempts were made to translate channel theory and error correcting
-codes theory to data storage, the first being Reed-Solomon which spawned
-the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
-devised for use on optical discs to recover from scratches, which was
-necessary for the technology to be usable for consumers. Since then, new
-less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
-fountain codes such as RaptorQ were invented (or rediscovered), but they
-are still marginally researched for data storage.
-
-This project aims to, first, implement easy tools to evaluate strategies
-(filetamper.py) and file fixity (ie, detect if there are corruptions),
-and then the goal is to provide an open and easy framework to use
-different kinds of error correction codes to protect and repair files.
-
-Also, the ecc file specification is made to be simple and resilient to
-corruption, so that you can process it by your own means if you want to,
-without having to study for hours how the code works (contrary to PAR2
-format).
-
-In practice, both approaches are not exclusive, and the best is to
-combine them: protect the most precious data with error correction codes,
-then duplicate them as well as less sensitive data across multiple storage mediums.
-Hence, this suite of data protection tools, just like any other such suite, is not
-sufficient to guarantee your data is protected, you must have an active data curation
-strategy which includes regularly checking your data and replacing copies that are damaged.
-
-For a primer on storage mediums and data protection strategies, see `this post I wrote <https://web.archive.org/web/20220529125543/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`_.
-
-Why not just use RAID ?
------------------------
-
-RAID is clearly insufficient for long-term data storage, and in fact it
-was primarily meant as a cheap way to get more storage (RAID0) or more
-availability (RAID1) of data, not for archiving data, even on a medium
-timescale:
-
--  RAID 0 is just using multiple disks just like a single one, to extend
-   the available storage. Let's skip this one.
--  RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
-   That's completely useless for long term storage: if either disk
-   fails, or if both disks are partially corrupted, you can't know what
-   are the correct data and which aren't. As an old saying goes: "Never
-   take 2 compasses: either take 3 or 1, because if both compasses show
-   different directions, you will never know which one is correct, nor
-   if both are wrong." That's the principle of Triplication.
--  RAID 5 is based on the triplication idea: you have n disks (but least
-   3), and if one fails you can recover n-1 disks (resilient to only 1
-   disk failure, not more).
--  RAID 6 is an extension of RAID 5 which is closer to error-correction
-   since you can correct n-k disks. However, most (all?) currently
-   commercially available RAID6 devices only implements recovery for at
-   most n-2 (2 disks failures).
--  In any case, RAID cannot detect silent errors automatically, thus you
-   either have to regularly scan, or you risk to lose some of your data
-   permanently, and it's far more common than you can expect (eg, with
-   RAID5, it is enough to have 2 silent errors on two disks on the same
-   bit for the bit to be unrecoverable). That's why a limit of only 1 or
-   2 disks failures is just not enough.
-
-On the opposite, ECC can correct n-k disks (or files). You can configure
-n and k however you want, so that for example you can set k = n/2, which
-means that you can recover all your files from only half of them! (once
-they are encoded with an ecc file of course).
-
-There also are new generation RAID solutions, mainly software based,
-such as SnapRAID or ZFS, which allow you to configure a virtual RAID
-with the value n-k that you want. This is just like an ecc file (but a
-bit less flexible, since it's not a file but a disk mapping, so that you
-can't just copy it around or upload it to a cloud backup hosting). In
-addition to recover (n-k) disks, they can also be configured to recover
-from partial, sectors failures inside the disk and not just the whole
-disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
-and James L. Hafner. "SD codes: erasure codes designed for how storage
-systems really fail." FAST. 2013.).
-
-The other reason RAID is not adapted to long-term storage, is that it
-supposes you store your data on hard-drives exclusively. Hard drives
-aren't a good storage medium for the long term, for two reasons:
-
-| 1- they need a regular plug to keep the internal magnetic disks
-  electrified (else the data will just fade away when there's no
-  residual electricity).
-| 2- the reading instrument is directly included and merged with the
-  data (this is the green electronic board you see from the outside, and
-  the internal head). This is good for quick consumer use (don't need to
-  buy another instrument: the HDD can just be plugged and it works), but
-  it's very bad for long term storage, because the reading instrument is
-  bound to fail, and a lot faster than the data can fade away: this
-  means that even if your magnetic disks inside your HDD still holds
-  your data, if the controller board or the head doesn't work anymore,
-  your data is just lost. And a head (and a controller board) are almost
-  impossible to replace, even by professionals, because the pieces are
-  VERY hard to find (different for each HDD production line) and each
-  HDD has some small physical defects, thus it's impossible to reproduce
-  that too (because the head is so close to the magnetic disk that if
-  you try to do that manually you'll probably fail).
-
-In the end, it's a lot better to just separate the storage medium of
-data, with the reading instrument. The medium I advise is optical disks
-(whether it's BluRay, DVD, CD or whatever), because the reading
-instrument is separate, and the technology (laser reflecting on bumps
-and/or pits) is kind of universal, so that even if the technology is
-lost one day (deprecated by newer technologies, so that you can't find
-the reading instrument anymore because it's not sold anymore), you can
-probably emulate a laser using some software to read your optical disk,
-just like what the CAMiLEON project did to recover data from the
-LaserDiscs of the BBC Domesday Project (see Wikipedia).
-
-Applications included
----------------------
-
-The project currently include the following pure-python applications:
-
--  rfigc.py (subcommand: ``hash``), a hash auditing tool, similar to md5deep/hashdeep, to
-   compute a database of your files along with their metadata, so that
-   later you can check if they were changed/corrupted.
-
--  header\_ecc.py (subcommand: ``header``), an error correction code using Reed-Solomon
-   generator/corrector for files headers. The idea is to supplement
-   other more common redundancy tools such as PAR2 (which is quite
-   reliable), by adding more resiliency only on the critical parts of
-   the files: their headers. Using this script, you can significantly
-   higher the chance of recovering headers, which will allow you to at
-   least open the files.
-
--  structural\_adaptive\_ecc.py (subcommand: ``whole``), a variable error correction rate
-   encoder (kind of a generalization of header\_ecc.py). This script
-   allows to generate an ecc file for the whole content of your files,
-   not just the header part, using a variable resilience rate: the
-   header part will be the most protected, then the rest of each file
-   will be progressively encoded with a smaller and smaller resilience
-   rate. The assumption is that important information is stored first,
-   and then data becomes less and less informative (and thus important,
-   because the end of the file describes less important details). This
-   assumption is very true for all compressed kinds of formats, such as
-   JPG, ZIP, Word, ODT, etc...
-
--  repair\_ecc.py (subcommand: ``recovery``), a script to repair the structure (ie, the entry and
-   fields markers/separators) of an ecc file generated by header\_ecc.py
-   or structural\_adaptive\_ecc.py. The goal is to enhance the
-   resilience of ecc files against corruption by ensuring that their
-   structures can be repaired (up to a certain point which is very high
-   if you use an index backup file, which is a companion file that is
-   generated along an ecc file).
-
--  filetamper.py (subcommand: ``filetamper``) is a quickly made file corrupter, it will erase or
-   change characters in the specified file. This is useful for testing
-   your various protecting strategies and file formats (eg: is PAR2
-   really resilient against corruption? Are zip archives still partially
-   extractable after corruption or are rar archives better? etc.). Do
-   not underestimate the usefulness of this tool, as you should always
-   check the resiliency of your file formats and of your file protection
-   strategies before relying on them.
-
--  replication\_repair.py (subcommand: ``dup``) takes advantage of your multiple copies
-   (replications) of your data over several storage mediums to recover
-   your data in case it gets corrupted. The goal is to take advantage of
-   the storage of your archived files into multiple locations: you will
-   necessarily make replications, so why not use them for repair?
-   Indeed, it's good practice to keep several identical copies of your data
-   on several storage mediums, but in case a corruption happens,
-   usually you will just drop the corrupted copies and keep the intacts ones.
-   However, if all copies are partially corrupted, you're stuck. This script
-   aims to take advantage of these multiple copies to recover your data,
-   without generating a prior ecc file. It works simply by reading through all
-   your different copies of your data, and it casts a majority vote over each
-   byte: the one that is the most often occuring will be kept. In engineering,
-   this is a very common strategy used for very reliable systems such as
-   space rockets, and is called "triple-modular redundancy", because you need
-   at least 3 copies of your data for the majority vote to work (but the more the
-   better).
-
--  resiliency\_tester.py (subcommand: ``restest``) allows you to test the robustness of the
-   corruption correction of the scripts provided here (or any other
-   command-line app). You just have to copy the files you want to test inside a
-   folder, and then the script will copy the files into a test tree, then it
-   will automatically corrupt the files randomly (you can change the parameters
-   like block burst and others), then it will run the file repair command-lines
-   you supply and finally some stats about the repairing power will be
-   generated. This allows you to easily and objectively compare different set
-   of parameters, or even different file repair solutions, on the very data
-   that matters to you, so that you can pick the best option for you.
-
--  ecc\_speedtest.py (subcommand: ``speedtest``) is a simple error correction codes
-   encoder/decoder speedtest. It allows to easily change parameters for the test.
-   This allows to assess how fast your machine can encode/decode with the selected
-   parameters, which can be especially useful to plan ahead for how many files you
-   can reasonably plan to protect with error correction codes (which are time consuming).
-
--  DEPRECATED: easy\_profiler.py is just a quick and simple profiling tool to get
-   you started quickly on what should be optimized to get more speed, if
-   you want to contribute to the project feel free to propose a pull
-   request! (Cython and other optimizations are welcome as long as they
-   are cross-platform and that an alternative pure-python implementation
-   is also available).
-
-Note that all tools are primarily made for command-line usage (type
-script.py --help to get extended info about the accepted arguments), but
-you can also use rfigc.py and header\_ecc.py with a GUI by using the
---gui argument (must be the first and only one argument supplied). The
-GUI is provided as-is and minimal work will be done to maintain it (the
-focus will stay on functionality rather than ergonomy).
-
-IMPORTANT: it is CRITICAL that you use the same parameters for
-correcting mode as when you generated the database/ecc files (this is
-true for all scripts in this bundle). Of course, some options must be
-changed: -g must become -c to correct, and --update is a particular
-case. This works this way on purpose for mainly two reasons: first
-because it is very hard to autodetect the parameters from a database
-file alone and it would produce lots of false positives, and secondly
-(the primary reason) is that storing parameters inside the database file
-is highly unresilient against corruption (if this part of the database
-is tampered, the whole becomes unreadable, while if they are stored
-outside or in your own memory, the database file is always accessible).
-Thus, it is advised to write down the parameters you used to generate
-your database directly on the storage media you will store your database
-file on (eg: if it's an optical disk, write the parameters on the cover
-or directly on the disk using a marker), or better memorize them by
-heart. If you forget them, don't panic, the parameters are always stored
-as comments in the header of the generated ecc files, but you should try
-to store them outside of the ecc files anyway.
-
-For users: what's the advantage of pyFileFixity?
-------------------------------------------------
-
-Pros:
-
--  Open application and open specifications under the MIT license (you
-   can do whatever you want with it and tailor it to your needs if you
-   want to, or add better decoding procedures in the future as science
-   progress so that you can better recover your data from your already
-   generated ecc file).
--  Highly reliable file fixity watcher: rfigc.py will tell you without
-   any ambiguity using several attributes if your files have been
-   corrupted or not, and can even check for images if the header is
-   valid (ie: if the file can still be opened).
--  Readable ecc file format (compared to PAR2 and most other similar
-   specifications).
--  Highly resilient ecc file format against corruption (not only are
-   your data protected by ecc, the ecc file is protected too against
-   critical spots, both because there is no header so that each track is
-   independent and if one track is corrupted beyond repair then other
-   ecc tracks can still be read, and a .idx file will be generated to
-   repair the structure of the ecc file to recover all tracks).
--  Very safe and conservative approach: the recovery process checks that
-   the recovery was successful before committing a repaired block.
--  Partial recovery allowed (even if a file cannot be completely
-   recovered, the parts that can will be repaired and then the rest that
-   can't be repaired will be recopied from the corrupted version).
--  Support directory processing: you can encode an ecc file for a whole
-   directory of files (with any number of sub-directories and depth).
--  No limit on the number of files, and it can recursively protect files
-   in a directory tree.
--  Variable resiliency rate and header-only resilience, ensuring that
-   you can always open your files even if partially corrupted (the
-   structure of your files will be saved, so that you can use other
-   softwares to repair beyond if this set of script is not sufficient to
-   totally repair).
--  Support for erasures (null bytes) and even errors-and-erasures, which
-   literally doubles the repair capabilities. To my knowledge, this is
-   the only freely available parity software that supports erasures.
--  Display the predicted total ecc file size given your parameters,
-   and the total time it will take to encode/decode.
--  Your original files are still accessible as they are, protection files
-   such as ecc files live alongside your original data. Contrary to
-   other data protection schemes such as PAR2 which encode the whole
-   data in par archive files that replace your original files and
-   are not readable without decoding.
--  Opensourced under the very permissive MIT licence, do whatever you
-   want!
-
-Cons:
-
--  Cannot protect meta-data, such as folders paths. The paths are
-   stored, but cannot be recovered (yet? feel free to contribute if you
-   know how). Only files are protected. Thus if your OS or your storage
-   medium crashes and truncate a whole directory tree, the directory
-   tree can't be repaired using the ecc file, and thus you can't access
-   the files neither. However, you can use file scraping to extract the
-   files even if the directory tree is lost, and then use RFIGC.py to
-   reorganize your files correctly. There are alternatives, see the
-   chapters below: you can either package all your files in a single
-   archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
-   DVDisaster as an alternative solution, which is an ecc generator with
-   support for directory trees meta-data (but only on optical disks).
--  Can only repair errors and erasures (characters that are replaced by
-   another character), not deletion nor insertion of characters. However
-   this should not happen with any storage medium (truncation can occur
-   if the file bounds is misdetected, in this case pyFileFixity can
-   partially repair the known parts of the file, but cannot recover the
-   rest past the truncation, except if you used a resiliency rate of at
-   least 0.5, in which case any message block can be recreated with only
-   using the ecc file).
--  Cannot recreate a missing file from other available files (except you
-   have set a resilience\_rate at least 0.5), contrary to Parchives
-   (PAR1/PAR2). Thus, you can only repair a file if you still have it
-   (and its ecc file!) on your filesystem. If it's missing, pyFileFixity
-   cannot do anything (yet, this will be implemented in the future).
-
-Note that the tools were meant for data archival (protect files that you
-won't modify anymore), not for system's files watching nor to protect
-all the files on your computer. To do this, you can use a filesystem
-that directly integrate error correction code capacity, such as ZFS.
-
-Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)
-------------------------------------------------------------------------------
-
-Recursively generate or check the integrity of files by MD5 and SHA1
-hashes, size, modification date or by data structure integrity (only for
-images).
-
-This script is originally meant to be used for data archival, by
-allowing an easy way to check for silent file corruption. Thus, this
-script uses relative paths so that you can easily compute and check the
-same redundant data copied on different mediums (hard drives, optical
-discs, etc.). This script is not meant for system files corruption
-notification, but is more meant to be used from times-to-times to check
-up on your data archives integrity (if you need this kind of application,
-see `avpreserve's fixity <https://github.com/avpreserve/fixity>`_).
-
-Example usage
-~~~~~~~~~~~~~
-
--  To generate the database (only needed once):
-
-``pff hash -i "your_folder" -d "dbhash.csv" -g``
-
--  To check:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s``
-
--  To update your database by appending new files:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -u -a``
-
--  To update your database by appending new files AND removing
-   inexistent files:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -u -a -r``
-
-Note that by default, the script is by default in check mode, to avoid
-wrong manipulations. It will also alert you if you generate over an
-already existing database file.
-
-Arguments
-~~~~~~~~~
-
-::
-
-      -h, --help            show a help message and exit
-      -i /path/to/root/folder, --input /path/to/root/folder
-                            Path to the root folder from where the scanning will occ
-    ur.
-      -d /some/folder/databasefile.csv, --database /some/folder/databasefile.csv
-                            Path to the csv file containing the hash informations.
-      -l /some/folder/filename.log, --log /some/folder/filename.log
-                            Path to the log file. (Output will be piped to both the
-    stdout and the log file)
-      -s, --structure_check
-                            Check images structures for corruption?
-      -e /some/folder/errorsfile.csv, --errors_file /some/folder/errorsfile.csv
-                            Path to the error file, where errors at checking will be
-     stored in CSV for further processing by other softwares (such as file repair so
-    ftwares).
-      -m, --disable_modification_date_checking
-                            Disable modification date checking.
-      --skip_missing        Skip missing files when checking (useful if you split yo
-    ur files into several mediums, for example on optical discs with limited capacit
-    y).
-      -g, --generate        Generate the database? (omit this parameter to check ins
-    tead of generating).
-      -f, --force           Force overwriting the database file even if it already e
-    xists (if --generate).
-      -u, --update          Update database (you must also specify --append or --rem
-    ove).
-      -a, --append          Append new files (if --update).
-      -r, --remove          Remove missing files (if --update).
-      
-      --filescraping_recovery          Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.
-      -o, --output          Path to the output folder where to output the files reorganized after --recover_from_filescraping.
-
-Header Error Correction Code script
------------------------------------
-
-This script was made to be used in combination with other more common
-file redundancy generators (such as PAR2, I advise MultiPar). This is an
-additional layer of protection for your files: by using a higher
-resiliency rate on the headers of your files, you ensure that you will
-be probably able to open them in the future, avoiding the "critical
-spots", also called "fracture-critical" in redundancy engineering (where
-if you modify just one bit, your whole file may become unreadable,
-usually bits residing in the headers - in other words, a single blow
-makes the whole thing collapse, just like non-redundant bridges).
-
-An interesting benefit of this approach is that it has a low storage
-(and computational) overhead that scales linearly to the number of
-files, whatever their size is: for example, if we have a set of 40k
-files for a total size of 60 GB, with a resiliency\_rate of 30% and
-header\_size of 1KB (we limit to the first 1K bytes/characters = our
-file header), then, without counting the hash per block and other
-meta-data, the final ECC file will be about 2 \* resiliency\_rate \*
-number\_of\_files \* header\_size = 24.5 MB. This size can be lower if
-there are many files smaller than 1KB. This is a pretty low storage
-overhead to backup the headers of such a big number of files.
-
-The script is pure-python as are its dependencies: it is thus completely
-cross-platform and open source. The default ecc algo
-(ecc_algo=3 uses `reedsolo <https://github.com/tomerfiliba-org/reedsolomon>`_)
-also provides a speed-optimized C-compiled implementation (``creedsolo``) that will be used
-if available for the user's platform, so pyFileFixity should be fast by default.
-Alternatively, it's possible to use a JIT compiler such as PyPy,
-although this means that ``creedsolo`` will not be useable, so PyPy
-may accelerate other functions but slower ecc encoding/decoding.
-
-Structural Adaptive Error Correction Encoder
---------------------------------------------
-
-This script implements a variable error correction rate encoder: each
-file is ecc encoded using a variable resiliency rate -- using a high
-constant resiliency rate for the header part (resiliency rate stage 1,
-high), then a variable resiliency rate is applied to the rest of the
-file's content, with a higher rate near the beginning of the file
-(resiliency rate stage 2, medium) which progressively decreases until
-the end of file (resiliency rate stage 3, the lowest).
-
-The idea is that the critical parts of files usually are placed at the
-top, and data becomes less and less critical along the file. What is
-meant by critical is both the critical spots (eg: if you tamper only one
-character of a file's header you have good chances of losing your entire
-file, ie, you cannot even open it) and critically encoded information
-(eg: archive formats usually encode compressed symbols as they go along
-the file, which means that the first occurrence is encoded, and then the
-archive simply writes a reference to the symbol. Thus, the first
-occurrence is encoded at the top, and subsequent encoding of this same
-data pattern will just be one symbol, and thus it matters less as long
-as the original symbol is correctly encoded and its information
-preserved, we can always try to restore the reference symbols later).
-Moreover, really redundant data will be placed at the top because they
-can be reused a lot, while data that cannot be too much compressed will
-be placed later, and thus, corruption of this less compressed data is a
-lot less critical because only a few characters will be changed in the
-uncompressed file (since the data is less compressed, a character change
-on the not-so-much compressed data won't have very significant impact on
-the uncompressed data).
-
-This variable error correction rate should allow to protect more the
-critical parts of a file (the header and the beginning of a file, for
-example in compressed file formats such as zip or jpg this is where the
-most importantly strings are encoded) for the same amount of storage as
-a standard constant error correction rate.
-
-Of course, you can set the resiliency rate for each stage to the values
-you want, so that you can even do the opposite: setting a higher
-resiliency rate for stage 3 than stage 2 will produce an ecc that is
-greater towards the end of the contents of your files.
-
-Furthermore, the currently designed format of the ecc file would allow
-two things that are not available in all current file ecc generators
-such as PAR2:
-
-1. it allows to partially repair a file, even if not all
-the blocks can be corrected (in PAR2, a file is repaired only if all
-blocks can be repaired, which is a shame because there are still other
-blocks that could be repaired and thus produce a less corrupted file) ;
-
-2. the ecc file format is quite simple and readable, easy to process by
-any script, which would allow other softwares to also work on it (and it
-was also done in this way to be more resilient against error
-corruptions, so that even if an entry is corrupted, other entries are
-independent and can maybe be used, thus the ecc is very error tolerant.
-This idea was implemented in repair\_ecc.py but it could be extended,
-especially if you know the pattern of the corruption).
-
-The script structural-adaptive-ecc.py implements this idea, which can be
-seen as an extension of header-ecc.py (and in fact the idea was the
-other way around: structural-adaptive-ecc.py was conceived first but was
-too complicated, then header-ecc.py was implemented as a working
-lessened implementation only for headers, and then
-structural-adaptive-ecc.py was finished using header-ecc.py code
-progress). It works, it was a quite well tested for my own needs on
-datasets of hundred of GB, but it's not foolproof so make sure you test
-the script by yourself to see if it's robust enough for your needs (any
-feedback about this would be greatly appreciated!).
-
-ECC Algorithms
---------------
-
-You can specify different ecc algorithms using the ``--ecc_algo`` switch.
-
-For the moment, only Reed-Solomon is implemented, but it's universal
-so you can modify its parameters in lib/eccman.py.
-
-Two Reed-Solomon codecs are available, they are functionally equivalent
-and thoroughly unit tested.
-
--  ``--ecc_algo 1``: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
-   This is the slowest implementation (but also the most easy code to understand).
--  ``--ecc_algo 2``: same as algo 1 but with a faster functions.
--  ``--ecc_algo 3``: use the second codec, which is the fastest.
-   The generated ECC will be compatible with algo 1 and 2.
--  ``--ecc_algo 4``: also use the second, fastest RS codec, but
-   with different parameters (US FAA ADSB UAT RS FEC norm),
-   thus the generated ECC won't be compatible with algo 1 to 3.
-   But do not be scared, the ECC will work just the same.
-
-Note about speed: Also, use a smaller --max\_block\_size to greatly
-speedup the operations! That's the trick used to compute very quickly RS
-ECC on optical discs. You give up a bit of resiliency of course (because
-blocks are smaller, thus you protect a smaller number of characters per
-ECC. In the end, this should not change much about real resiliency, but
-in case you get a big bit error burst on a contiguous block, you may
-lose a whole block at once. That's why using RS255 is better, but it's
-very time consuming. However, the resiliency ratios still hold, so for
-any other case of bit-flipping with average-sized bursts, this should
-not be a problem as long as the size of the bursts is smaller than an
-ecc block.)
-
-In case of a catastrophic event
--------------------------------
-
-TODO: write more here
-
-In case of a catastrophic event of your data due to the failure of your
-storage media (eg: your hard drive crashed), then follow the following
-steps:
-
-1- use dd\_rescue to make a full bit-per-bit verbatim copy of your drive
-before it dies. The nice thing with dd\_rescue is that the copy is
-exact, and also that it can retries or skip in case of bad sectors (it
-won't crash on your suddenly at half the process).
-
-2- Use testdisk to restore partition or to copy files based on partition
-filesystem informations.
-
-3- If you could not recover your files, you can try file scraping using
-`photorec <http://www.cgsecurity.org/wiki/PhotoRec>`_ or
-`plaso  <http://plaso.kiddaland.net/>`_ other similar tools as
-a last resort to extract data based only from files content (no filename,
-often uncorrect filetype, file boundaries may be wrong so some data
-may be cut off, etc.).
-
-4- If you used pyFileFixity before the failure of your storage media,
-you can then use your pre-computed databases to check that files are
-intact (rfigc.py) and if they aren't, you can recover them (using
-header\_ecc.py and structural\_adaptive\_ecc.py). It can also help if
-you recovered your files via data scraping, because your files will be
-totally unorganized, but you can use a previously generated database
-file to recover the full names and directory tree structure using
-rfigc.py --filescraping\_recover.
-
-Also, you can try to fix some of your files using specialized repairing
-tools (but remember that such tool cannot guarantee you the same
-recovering capacity as an error correction code - and in addition, error
-correction code can tell you when it has recovered successfully). For
-example:
-
--  for tar files, you can use `fixtar <https://github.com/BestSolution-at/fixtar>`_.
-   Similar tools (but older): `tarfix <http://www.dmst.aueb.gr/dds/sw/unix/tarfix/>`_
-   and `tar-repair <https://www.datanumen.com/tar-repair/>`_.
--  for RAID mounting and recovery, you can use "Raid faster - recover
-   better" (rfrb) tool by Sabine Seufert and Christian Zoubek:
-   https://github.com/lrq3000/rfrb
--  if your unicode strings were mangled (ie, you see weird symbols),
-   try this script that will automatically demangle them:
-   https://github.com/LuminosoInsight/python-ftfy
--  to repair tabular (2D) data such as .csv, try
-   `Carpenter <https://pypi.python.org/pypi/Carpenter/>`_.
--  tool to identify corrupted files in ddrescue images: 
-   `ddrescue-ffile <https://github.com/Salamek/ddrescue-ffile>`_
-
-Protecting directory tree meta-data
------------------------------------
-
-One main current limitation of pyFileFixity is that it cannot protect
-the directory tree meta-data. This means that in the worst case, if a
-silent error happens on the inode pointing to the root directory that
-you protected with an ecc, the whole directory will vanish, and all the
-files inside too. In less worst cases, sub-directories can vanish, but
-it's still pretty bad, and since the ecc file doesn't store any
-information about inodes, you can't recover the full path.
-
-The inability to store these meta-data is because of two choices in the
-design: 1- portability: we want the ecc file to work even if we move the
-root directory to another place or another storage medium (and of
-course, the inode would change), 2- cross-platform compatibility:
-there's no way to get and store directory meta-data for all platforms,
-but of course we could implement specific instructions for each main
-platform, so this point is not really a problem.
-
-To workaround this issue (directory meta-data are critical spots), other
-softwares use a one-time storage medium (ie, writing your data along
-with generating and writing the ecc). This way, they can access at
-the bit level the inode info, and they are guaranted that the inodes
-won't ever change. This is the approach taken by DVDisaster: by using
-optical mediums, it can compute inodes that will be permanent, and thus
-also encode that info in the ecc file. Another approach is to create a
-virtual filesystem specifically to store just your files, so that you
-manage the inode yourself, and you can then copy the whole filesystem
-around (which is really just a file, just like a zip file - which can
-also be considered as a mini virtual file system in fact) like
-`rsbep <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`_.
-
-Here the portability principle of pyFileFixity prevents this approach.
-But you can mimic this workaround on your hard drive for pyFileFixity to
-work: you just need to package all your files into one file. This way,
-you sort of create a virtual file system: inside the archive, files and
-directories have meta-data just like in a filesystem, but from the
-outside it's just one file, composed of bytes that we can just encode to
-generate an ecc file - in other words, we removed the inodes portability
-problem, since this meta-data is stored relatively inside the archive,
-the archive manage it, and we can just encode this info like any other
-stream of data! The usual way to make an archive from several files is
-to use TAR, but this will generate a solid archive which will prevent
-partial recovery. An alternative is to use DAR, which is a non-solid
-archive version of TAR, with lots of other features too. If you also
-want to compress, you can just use ZIP (with DEFLATE algorithm) your
-files (this also generates a non-solid archive). You can then use
-pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
-will then protect both your files just like before and the directories
-meta-data too now.
-
-Tools like pyFileFixity (or which can be used as complements)
--------------------------------------------------------------
-
-Here are some tools with a similar philosophy to pyFileFixity, which you
-can use if they better fit your needs, either as a replacement of
-pyFileFixity or as a complement (pyFileFixity can always be used to
-generate an ecc file):
-
--  `DAR (Disk ARchive) <http://dar.linux.free.fr/>`__: similar to tar
-   but non-solid thus allows for partial recovery and per-file access,
-   plus it saves the directory tree meta-data -- see catalog isolation
-   -- plus it can handle error correction natively using PAR2 and
-   encryption. Also supports incremental backup, thus it's a very nice
-   versatile tool. Crossplatform and opensource.
--  `DVDisaster <http://dvdisaster.net/>`__: error correction at the bit
-   level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
-   it also protects directory tree meta-data and is resilient to
-   corruption (v2 still has some critical spots but v3 won't have any).
--  rsbep tool that is part of dvbackup package in Debian: allows to
-   generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
-   for your backups, if you're on unix or using cygwin.
--  `rsbep modification by Thanassis
-   Tsiodras <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`__:
-   enhanced rsbep to avoid critical spots and faster speed. Also
-   includes a "freeze" script to encode your files into a virtual
-   filesystem (using Python/FUSE) so that even meta-data such as
-   directory tree are fully protected by the ecc. Great script, but not
-   maintained, it needs some intensive testing by someone knowledgeable
-   to guarantee this script is reliable enough for production.
--  Parchive (PAR1, PAR2, MultiPar): well known error correction file
-   generator. The big advantage of Parchives is that an ecc block
-   depends on multiple files: this allows to completely reconstruct a
-   missing file from scratch using files that are still available. Works
-   good for most people, but most available Parchive generators are not
-   satisfiable for me because 1- they do not allow to generate an ecc
-   for a directory tree recursively (except MultiPar, and even if it is
-   allowed in the PAR2 specs), 2- they can be very slow to generate
-   (even with multiprocessor extensions, because the galois field is
-   over 2^16 instead of 2^8, which is very costly), 3- the spec is not
-   very resilient to errors and tampering over the ecc file, as it
-   assumes the ecc file won't be corrupted (I also tested, it's still a
-   bit resilient, but it could be a lot more with some tweaking of the
-   spec), 4- it doesn't allow for partial recovery (recovering blocks
-   that we can and pass the others that are unrecoverable): with PAR2, a
-   file can be restored fully or it cannot be at all.
--  Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
-   create non-solid archives which are readable by most computers
-   (ubiquitous algorithm). Non-solid archive means that a zip file can
-   still unzip correct files even if it is corrupted, because files are
-   encoded in blocks, and thus even if some blocks are corrupted, the
-   decoding can happen. A `fast implementation with enhanced compression
-   is available in pure Go <https://github.com/klauspost/compress>`__
-   (good for long storage).
--  TestDisk: for file scraping, when nothing else worked.
--  dd\_rescue: for disk scraping (allows to forcefully read a whole disk
-   at the bit level and copy everything it can, passing bad sector with
-   options to retry them later on after a first full pass over the
-   correct sectors).
--  ZFS: a file system which includes ecc correction directly. The whole
-   filesystem, including directory tree meta-data, are protected. If you
-   want ecc protection on your computer for all your files, this is the
-   way to go.
--  Encryption: technically, you can encrypt your files without losing
-   too much redundancy, as long as you use an encryption scheme that is
-   block-based such as DES: if one block gets corrupted, it won't be
-   decryptable, but the rest of the files' encrypted blocks should be
-   decryptable without any problem. So encrypting with such algorithms
-   leads to similar files as non-solid archives such as deflate zip. Of
-   course, for very long term storage, it's better to avoid encryption
-   and compression (because you raise the information contained in a
-   single block of data, thus if you lose one block, you lose more
-   data), but if it's really necessary to you, you can still maintain
-   high chances of recovering your files by using block-based
-   encryption/compression (note: block-based encryption can
-   be seen as the equivalent of non-solid archives for compression,
-   because the data is compressed/encrypted in independent blocks,
-   thus allowing partial uncompression/decryption).
--  `SnapRAID <http://snapraid.sourceforge.net/>`__
--  `par2ools <https://github.com/jmoiron/par2ools>`__: a set of
-   additional tools to manage par2 archives
--  `Checkm <https://pypi.python.org/pypi/Checkm/0.4>`__: a tool similar
-   to rfigc.py
--  `BagIt <https://en.wikipedia.org/wiki/BagIt>`__ with two python
-   implementations `here <https://pypi.python.org/pypi/pybagit/>`__ and
-   `here <https://pypi.python.org/pypi/bagit/>`__: this is a file
-   packaging format for sharing and storing archives for long term
-   preservation, it just formalizes a few common procedures and meta
-   data that are usually added to files for long term archival (such as
-   MD5 digest).
--  `RSArmor <https://github.com/jap/rsarm>`__ a tool based on
-   Reed-Solomon to encode binary data files into hexadecimal, so that
-   you can print the characters on paper. May be interesting for small
-   datasets (below 100 MB).
--  `Ent <https://github.com/lsauer/entropy>`__ a tool to analyze the
-   entropy of your files. Can be very interesting to optimize the error
-   correction algorithm, or your compression tools.
--  `HashFS <https://pypi.python.org/pypi/hashfs/>`_ is a non-redundant,
-   duplication free filesystem, in Python. **Data deduplication** is very
-   important for large scale long term storage: since you want your data
-   to be redundant, this means you will use an additional storage space
-   for your redundant copies that will be proportional to your original data.
-   Having duplicated data will consume more storage and more processing
-   time, for no benefit. That's why it's a good idea to deduplicate your data
-   prior to create redundant copies: this will be faster and save you money.
-   Deduplication can either be done manually (by using duplicates removers)
-   or systematically and automatically using specific filesystems such as
-   zfs (with deduplication enabled) or hashfs.
--  Paper as a storage medium: paper is not a great storage medium,
-   because it has low storage density (ie, you can only store at most 
-   about 100 KB) and it can also degrade just like other storage mediums,
-   but you cannot check that automatically since it's not digital. However,
-   if you are interested, here are a few softwares that do that:
-   `Paper key <http://en.wikipedia.org/wiki/Paper_key>`_,
-   `Paperbak <http://www.ollydbg.de/Paperbak/index.html>`_,
-   `Optar <http://ronja.twibright.com/optar/>`_,
-   `dpaper <https://github.com/penma/dpaper>`_,
-   `QR Backup <http://blog.liw.fi/posts/qr-backup/>`_,
-   `QR Backup (another) <http://blog.shuningbian.net/2009/10/qrbackup.php>`_,
-   `QR Backup (again another) <http://git.pictorii.com/index.php?p=qrbackup.git&a=summary>`_,
-   `QR Backup (again) <http://hansmi.ch/software/qrbackup>`_,
-   `and finally a related paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&rep=rep1&type=pdf>`_.
--  AVPreserve tools, most notably `fixity <https://github.com/avpreserve/fixity>`_ 
-   to monitor for file changes (similarly to rfigc, but actively as a daemon)
-   and `interstitial <https://github.com/avpreserve/interstitial>`_ to detect
-   interstitial errors in audio digitization workflows (great to ensure you
-   correctly digitized a whole audio file into WAV without any error).
-
-FAQ
----
-
--  Can I compress my data files and my ecc file?
-
-As a rule of thumb, you should ALWAYS keep your ecc file in clear
-text, so under no compression nor encryption. This is because in case
-the ecc file gets corrupted, if compressed/encrypted, the
-decompression/decrypting of the corrupted parts may completely flaw
-the whole structure of the ecc file.
-
-Your data files, that you want to protect, *should* remain in clear
-text, but you may choose to compress them if it drastically reduces
-the size of your files, and if you raise the resilience rate of your
-ecc file (so compression may be a good option if you have an
-opportunity to trade the file size reduction for more ecc file
-resilience). Also, make sure to choose a non-solid compression
-algorithm like DEFLATE (zip) so that you can still decode correct
-parts even if some are corrupted (else with a solid archive, if one
-byte is corrupted, the whole archive may become unreadable).
-
-However, in the case that you compress your files, you should generate
-the ecc file only *after* compression, so that the ecc file applies to
-the compressed archive instead of the uncompressed files, else you
-risk being unable to correct your files because the uncompression of
-corrupted parts may output gibberish, and length extended corrupted
-parts (and if the size is different, Reed-Solomon will just freak
-out).
-
--  Can I encrypt my data files and my ecc file ?
-
-NEVER encrypt your ecc file, this is totally useless and
-counterproductive.
-
-You can encrypt your data files, but choose a non-solid algorithm
-(like AES if I'm not mistaken) so that corrupted parts do not prevent
-the decoding of subsequent correct parts. Of course, you're lowering a
-bit your chances of recovering your data files by encrypting them (the
-best chance to keep data for the long term is to keep them in clear
-text), but if it's really necessary, using a non-solid encrypting
-scheme is a good compromise.
-
-You can generate an ecc file on your encrypted data files, thus
-*after* encryption, and keep the ecc file in clear text (never encrypt
-nor compress it). This is not a security risk at all since the ecc
-file does not give any information on the content inside your
-encrypted files, but rather just redundant info to correct corrupted
-bytes (however if you generate the ecc file on the data files before
-encryption, then it's clearly a security risk, and someone could
-recover your data without your permission).
-
-- What medium should I use to store my data?
-
-The details are long and a bit complicated (I may write a complete article
-about it in the future), but the tl;dr answer is that you should use *optical disks*,
-because it decouples the storage medium and the reading hardware
-(eg, at the opposite we have hard drives, which contains both the reading
-hardware and the storage medium, so if one fails, you lose both)
-and because it's most likely future-proof (you only need a laser, which
-is universal, the laser's parameters can always be tweaked).
-
-From scientific studies, it seems that, at the time of writing this (2015),
-BluRay HTL disks are the most resilient against environmental degradation.
-To raise the duration, you can also put optical disks in completely opaque boxes
-(to avoid light degradation) and in addition you can put any storage medium
-(not only optical disks, but also hard drives and anything really) in
-*completely* air-tight and water-tight bags or box and put in a fridge or a freezer.
-This is a law of nature: lower the temperature, lower will be the entropy, in other
-words lower will be the degradation over time. It works the same with digital data.
-
-- What file formats are the most recoverable?
-
-It's difficult to advise a specific format. What we can do is advise the characteristics
-of a good file format:
-
-  * future-proof (should be readable in the future).
-  * non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn't cause a problem to the decoding of other blocks).
-  * open source implementation available.
-  * minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).
-  * No magic bytes or header importance (ie, corrupting the header won't prevent opening the file).
-
-There are a few studies about the most resilient file formats, such as:
-
-  * `"Just one bit in a million: On the effects of data corruption in files" by Volker Heydegger <http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1>`_.
-  * `"Analysing the impact of file formats on data integrity" by Volker Heydegger <http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf>`_.
-  * `"A guide to formats", by The UK national archives <http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf>`_ (you want to look at the Recoverability entry in each table).
-
-- What is Reed-Solomon?
-
-If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon
-
-Also, you may want to read the following resources:
-
-  * "`Reed-Solomon codes for coders <https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders>`_", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.
-  * "Algebraic codes for data transmission", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books <https://books.google.fr/books?id=eQs2i-R9-oYC&lpg=PR11&ots=atCPQJm3OJ&dq=%22Algebraic%20codes%20for%20data%20transmission%22%2C%20Blahut%2C%20Richard%20E.%2C%202003%2C%20Cambridge%20university%20press.&lr&hl=fr&pg=PA193#v=onepage&q=%22Algebraic%20codes%20for%20data%20transmission%22,%20Blahut,%20Richard%20E.,%202003,%20Cambridge%20university%20press.&f=false>`_.
-
-
-.. |Example| image:: https://raw.githubusercontent.com/lrq3000/pyFileFixity/master/tux-example.jpg
-   :scale: 60 %
-   :alt: Image corruption and repair example
-.. |PyPI-Status| image:: https://img.shields.io/pypi/v/pyfilefixity.svg
-   :target: https://pypi.org/project/pyfilefixity
-.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/pyfilefixity.svg?logo=python&logoColor=white
-   :target: https://pypi.org/project/pyfilefixity
-.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/pyfilefixity.svg?label=pypi%20downloads&logo=python&logoColor=white
-   :target: https://pypi.org/project/pyfilefixity
-.. |Build-Status| image:: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml/badge.svg?event=push
-   :target: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml
-.. |Coverage| image:: https://codecov.io/github/lrq3000/pyFileFixity/coverage.svg?branch=master
-   :target: https://codecov.io/github/lrq3000/pyFileFixity?branch=master
+Metadata-Version: 2.1
+Name: pyFileFixity
+Version: 3.1.4
+Summary: Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing.
+Author-email: Stephen Karl Larroque <lrq3000@gmail.com>
+Maintainer-email: Stephen Karl Larroque <lrq3000@gmail.com>
+License: MIT License
+Project-URL: Homepage, https://github.com/lrq3000/pyFileFixity
+Project-URL: Documentation, https://github.com/lrq3000/pyFileFixity/blob/master/README.rst
+Project-URL: Source, https://github.com/lrq3000/pyFileFixity
+Project-URL: Tracker, https://github.com/lrq3000/pyFileFixity/issues
+Project-URL: Download, https://github.com/lrq3000/pyFileFixity/releases
+Keywords: file,repair,monitor,change,reed-solomon,error,correction,error correction,parity,parity files,parity bytes,data protection,data recovery,file protection,qr codes,qr code
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Environment :: Console
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS :: MacOS X
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3.12
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Topic :: System :: Archiving
+Classifier: Topic :: System :: Archiving :: Backup
+Classifier: Topic :: System :: Monitoring
+Classifier: Topic :: System :: Recovery Tools
+Classifier: Topic :: Utilities
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: End Users/Desktop
+Classifier: Intended Audience :: Information Technology
+Classifier: Intended Audience :: System Administrators
+Requires-Python: >=3.7
+Description-Content-Type: text/x-rst
+Provides-Extra: test
+Provides-Extra: testmeta
+License-File: LICENSE
+
+pyFileFixity
+============
+
+|PyPI-Status| |PyPI-Versions| |PyPI-Downloads|
+
+|Build-Status| |Coverage|
+
+pyFileFixity provides a suite of open source, cross-platform, easy
+to use and easy to maintain (readable code) to protect and manage data
+for long term storage/archival, and also test the performance of any data protection algorithm.
+
+The project is done in pure-Python to meet those criteria,
+although cythonized extensions are available for core routines to speed up encoding/decoding,
+but always with a pure python specification available so as to allow long term replication.
+
+Here is an example of what pyFileFixity can do:
+
+|Example|
+
+On the left, this is the original image.
+
+At the center, the same image but
+with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
+which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
+Only a few corrupted bytes are enough to make the image looks like totally
+unrecoverable, and yet we are lucky, because the image could be unreadable at all
+if any of the "magic bytes" were to be corrupted!
+
+At the right, the corrupted image was repaired using ``pff header`` command of pyFileFixity.
+This repaired only the image header (ie, the first part of the file), so only the first
+3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
+the image looks indistinguishable from the untampered original! And the best thing is that
+it only costed the generation of a "ecc repair file" for the header, which size is only a
+constant 3.3KB per file, regardless of the protected file's size!
+
+This works because most files will store the most important information to read them at
+their beginning, also called "file's header", so repairing this part will almost always ensure
+the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
+you can read it). This works especially well for images, compressed files, formatted documents such as
+DOCX and ODT, etc.
+
+Of course, you can also protect the whole file, not only the header, using pyFileFixity's
+``pff whole`` command. You can also detect any corruption using ``pff hash``.
+
+------------------------------------------
+
+.. contents:: Table of contents
+   :backlinks: top
+
+Quickstart
+----------
+
+Runs on Python 3 up to Python 3.12-dev. PyPy 3 is also supported.
+
+- To install or update on Python 3:
+
+``pip install --upgrade pyfilefixity``
+
+- For Python 2.7, the latest working version was v3.0.2:
+
+``pip install --upgrade pyfilefixity==3.0.2 reedsolo==1.7.0 unireedsolomon==1.0.5``
+
+- Once installed, the suite of tools can be accessed from a centralized interface script called ``pff`` which provides several subcommands, to list them:
+
+``pff --help``
+
+You should see:
+
+::
+
+    usage: pff [-h]
+               {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
+               ...
+
+    positional arguments:
+      {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
+        hash (rfigc)        Check files integrity fast by hash, size, modification date or by data structure integrity.
+        header (header_ecc, hecc)
+                            Protect/repair files headers with error correction codes
+        whole (structural_adaptive_ecc, saecc, protect, repair)
+                            Protect/repair whole files with error correction codes
+        recover (repair_ecc, recc)
+                            Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery
+                            mode (note: this does NOT recover your files, only the ecc files, which may then be used to
+                            recover your files!)
+        dup (replication_repair)
+                            Repair files from multiple copies of various storage mediums using a majority vote
+        restest (resilience_tester)
+                            Run tests to quantify robustness of a file protection scheme (can be used on any, not just
+                            pyFileFixity)
+        filetamper          Tamper files using various schemes
+        speedtest (ecc_speedtest)
+                            Run error correction encoding and decoding speedtests
+
+    options:
+      -h, --help            show this help message and exit
+
+- Every subcommands provide their own more detailed help instructions, eg for the ``hash`` submodule:
+
+``pff hash --help``
+
+- To generate a monitoring database (to later check very fast which files are corrupted, but cannot repair anything but filesystem metadata):
+
+``pff hash -i "your_folder" -d "dbhash.csv" -g -f -l "log.txt"``
+
+Note: this also works for a single file, just replace "your_folder" by "your_file.ext".
+
+- To update this monitoring database (check for new files, but does not remove files that do not exist anymore - replace ``--append`` with ``--remove`` for the latter):
+
+``pff hash -i "your_folder -d "dbhash.csv" --update --append``
+
+- Later, to check which files were corrupted:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s -e errors.csv``
+
+- To use this monitoring database to recover filesystem metadata such as files names and directory layout by filescraping from files contents:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l "log.txt" -o "output_folder" --filescraping_recovery``
+
+- To protect files headers with a file called ``hecc.txt``:
+
+``pff header -i "your_folder" -d "hecc.txt" -l "log.txt" -g -f --ecc_algo 3``
+
+- To repair files headers and store the repaired files in ``output_folder``:
+
+``pff header -i "your_folder" -d "hecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
+
+- To protect whole files with a file called ``ecc.txt``:
+
+``pff whole -i "your_folder" -d "ecc.txt" -l "log.txt" -g -f -v --ecc_algo 3``
+
+- To repair whole files:
+
+``pff whole -i "your_folder" -d "ecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
+
+Note that ``header`` and ``whole`` can also detect corrupted files and even which blocks inside a file, but they are much slower than ``hash``.
+
+- To try to recover a damaged ecc file ``ecc.txt`` using an index file ``ecc.txt.idx`` (index file is generated automatically with ecc.txt):
+
+``pff recovery -i "ecc.txt" --index "ecc.txt.idx" -o "ecc_repaired.txt" -l "log.txt" -v -f``
+
+- To try to recover a damaged ecc file ``ecc.txt`` without an index file (you can tweak the ``-t`` parameter from 0.0 to 1.0, 1.0 producing many false positives):
+
+``pff recovery -i "ecc.txt" -o "ecc_repaired.txt" -l "log.txt" -v -f -t 0.4``
+
+- To repair your files using multiple duplicated copies that you have stored on different mediums:
+
+``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" --report "rlog.csv" -f -v``
+
+- If you have previously generated a rfigc database, you can use it to enhance the replication repair:
+
+``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" -d "dbhash.csv" --report "rlog.csv" -f -v``
+
+- To run tests on your recovery tools, you can make a Makefile-like configuration file and use the Resiliency Tester submodule:
+
+``pff restest -i "your_folder" -o "test_folder" -c "resiliency_tester_config.txt" -m 3 -l "testlog.txt" -f``
+
+- Internally, ``pff restest`` uses ``pff filetamper`` to tamper files with various schemes, but you can also use ``pff filetamper`` directly.
+
+- To run speedtests of encoding/decoding error correction codes on your machine:
+
+``pff speedtest``
+
+- In case the ``pff`` command does not work, it can be replaced with ``python -m pyFileFixity.pff`` .
+
+The problem of long term storage
+--------------------------------
+
+Why are data corrupted with time? One sole reason: entropy.
+Entropy refers to the universal tendency for systems to become
+less ordered over time. Data corruption is exactly that: a disorder
+in bits order. In other words: *the Universe hates your data*.
+
+Long term storage is thus a very difficult topic: it's like fighting with
+death (in this case, the death of data). Indeed, because of entropy,
+data will eventually fade away because of various silent errors such as
+bit rot or cosmic rays. pyFileFixity aims to provide tools to detect any data
+corruption, but also fight data corruption by providing repairing tools.
+
+The only solution is to use a principle of engineering that is long
+known and which makes bridges and planes safe: add some **redundancy**.
+
+There are only 2 ways to add redundancy:
+
+-  the simple way is to **duplicate** the object (also called replication),
+   but for data storage, this eats up a lot of storage and is not optimal.
+   However, if storage is cheap, then this is a good solution, as it is
+   much faster than encoding with error correction codes. For replication to work,
+   at least 3 duplicates are necessary at all times, so that if one fails, it must
+   replaced asap. As sailors say: "Either bring 1 compass or 3 compasses, but never
+   two, because then you won't know which one is correct if one fails."
+   Indeed, with 3 duplicates, if you frequently monitor their integrity
+   (eg, with hashes), then if one fails, simply do a majority vote:
+   the bit value given by 2 of the duplicates is probably correct.
+-  the second way, the optimal tools ever invented to recover
+   from data corruption, are the **error correction codes** (forward
+   error correction), which are a way to smartly produce redundant codes
+   from your data so that you can later repair your data using these
+   additional pieces of information (ie, an ECC generates n blocks for a
+   file cut in k blocks (with k < n), and then the ecc code can rebuild
+   the whole file with (at least) any k blocks among the total n blocks
+   available). In other words, you can correct up to (n-k) erasures. But
+   error correcting codes can also detect and repair automatically where
+   the errors are (fully automatic data repair for you !), but at the
+   cost that you can then only correct (n-k)/2 errors.
+
+Error correction can seem a bit magical, but for a reasonable intuition,
+it can be seen as a way to average the corruption error rate: on
+average, a bit will still have the same chance to be corrupted, but
+since you have more bits to represent the same data, you lower the
+overall chance to lose this bit.
+
+The problem is that most theoretical and pratical works on error
+correcting codes has been done almost exclusively on channel
+transmission (such as 4G, internet, etc.), but not on data storage,
+which is very different for one reason: whereas in a channel we are in a
+spatial scheme (both the sender and the receiver are different entities
+in space but working at the same timescale), in data storage this is a
+temporal scheme: the sender was you storing the data on your medium at
+time t, and the receiver is again you but now retrieving the data at
+time t+x. Thus, the sender does not exist anymore, thus you cannot ask
+the sender to send again some data if it's too much corrupted: in data
+storage, if a data is corrupted, it's lost for good, whereas in channel theory,
+parts of the data can be submitted again if necessary.
+
+Some attempts were made to translate channel theory and error correcting
+codes theory to data storage, the first being Reed-Solomon which spawned
+the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
+devised for use on optical discs to recover from scratches, which was
+necessary for the technology to be usable for consumers. Since then, new
+less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
+fountain codes such as RaptorQ were invented (or rediscovered), but they
+are still marginally researched for data storage.
+
+This project aims to, first, implement easy tools to evaluate strategies
+(filetamper.py) and file fixity (ie, detect if there are corruptions),
+and then the goal is to provide an open and easy framework to use
+different kinds of error correction codes to protect and repair files.
+
+Also, the ecc file specification is made to be simple and resilient to
+corruption, so that you can process it by your own means if you want to,
+without having to study for hours how the code works (contrary to PAR2
+format).
+
+In practice, both approaches are not exclusive, and the best is to
+combine them: protect the most precious data with error correction codes,
+then duplicate them as well as less sensitive data across multiple storage mediums.
+Hence, this suite of data protection tools, just like any other such suite, is not
+sufficient to guarantee your data is protected, you must have an active (but infrequent and hence not time consuming)
+data curation strategy that includes regularly checking your data and replacing copies that are damaged every few years.
+
+For a primer on storage mediums and data protection strategies, see `this post I wrote <https://web.archive.org/web/20220529125543/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`_.
+
+Why not just use RAID ?
+-----------------------
+
+RAID is clearly insufficient for long-term data storage, and in fact it
+was primarily meant as a cheap way to get more storage (RAID0) or more
+availability (RAID1) of data, not for archiving data, even on a medium
+timescale:
+
+-  RAID 0 is just using multiple disks just like a single one, to extend
+   the available storage. Let's skip this one.
+-  RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
+   That's completely useless for long term storage: if either disk
+   fails, or if both disks are partially corrupted, you can't know what
+   are the correct data and which aren't. As an old saying goes: "Never
+   take 2 compasses: either take 3 or 1, because if both compasses show
+   different directions, you will never know which one is correct, nor
+   if both are wrong." That's the principle of Triplication.
+-  RAID 5 is based on the triplication idea: you have n disks (but least
+   3), and if one fails you can recover n-1 disks (resilient to only 1
+   disk failure, not more).
+-  RAID 6 is an extension of RAID 5 which is closer to error-correction
+   since you can correct n-k disks. However, most (all?) currently
+   commercially available RAID6 devices only implements recovery for at
+   most n-2 (2 disks failures).
+-  In any case, RAID cannot detect silent errors automatically, thus you
+   either have to regularly scan, or you risk to lose some of your data
+   permanently, and it's far more common than you can expect (eg, with
+   RAID5, it is enough to have 2 silent errors on two disks on the same
+   bit for the bit to be unrecoverable). That's why a limit of only 1 or
+   2 disks failures is just not enough.
+-  Finally, it's worth noting that `hard drives do implement ECC codes <https://superuser.com/a/1554342/157556>`__
+   to be resilient against bad sectors (otherwise we would lose data
+   all the time!), but they only have limited corrective capacity,
+   mainly because the ECC code is short and not configurable.
+
+On the opposite, ECC can correct n-k disks (or files). You can configure
+n and k however you want, so that for example you can set k = n/2, which
+means that you can recover all your files from only half of them! (once
+they are encoded with an ecc file of course).
+
+There also are new generation RAID solutions, mainly software based,
+such as SnapRAID or ZFS, which allow you to configure a virtual RAID
+with the value n-k that you want. This is just like an ecc file (but a
+bit less flexible, since it's not a file but a disk mapping, so that you
+can't just copy it around or upload it to a cloud backup hosting). In
+addition to recover (n-k) disks, they can also be configured to recover
+from partial, sectors failures inside the disk and not just the whole
+disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
+and James L. Hafner. "SD codes: erasure codes designed for how storage
+systems really fail." FAST. 2013.).
+
+The other reason RAID is not adapted to long-term storage, is that it
+supposes you store your data on hard-drives exclusively. Hard drives
+aren't a good storage medium for the long term, for two reasons:
+
+| 1- they need a regular plug to keep the internal magnetic disks
+  electrified (else the data will just fade away when there's no
+  residual electricity).
+| 2- the reading instrument is directly included and merged with the
+  data (this is the green electronic board you see from the outside, and
+  the internal head). This is good for quick consumer use (don't need to
+  buy another instrument: the HDD can just be plugged and it works), but
+  it's very bad for long term storage, because the reading instrument is
+  bound to fail, and a lot faster than the data can fade away: this
+  means that even if your magnetic disks inside your HDD still holds
+  your data, if the controller board or the head doesn't work anymore,
+  your data is just lost. And a head (and a controller board) are almost
+  impossible to replace, even by professionals, because the pieces are
+  VERY hard to find (different for each HDD production line) and each
+  HDD has some small physical defects, thus it's impossible to reproduce
+  that too (because the head is so close to the magnetic disk that if
+  you try to do that manually you'll probably fail).
+
+In the end, it's a lot better to just separate the storage medium of
+data, with the reading instrument.
+
+We will talk later about what storage mediums can be used instead.
+
+Applications included
+---------------------
+
+The pyFileFixity suite currently include the following pure-python applications:
+
+-  rfigc.py (subcommand: ``hash``), a hash auditing tool, similar to md5deep/hashdeep, to
+   compute a database of your files along with their metadata, so that
+   later you can check if they were changed/corrupted.
+
+-  header\_ecc.py (subcommand: ``header``), an error correction code using Reed-Solomon
+   generator/corrector for files headers. The idea is to supplement
+   other more common redundancy tools such as PAR2 (which is quite
+   reliable), by adding more resiliency only on the critical parts of
+   the files: their headers. Using this script, you can significantly
+   higher the chance of recovering headers, which will allow you to at
+   least open the files.
+
+-  structural\_adaptive\_ecc.py (subcommand: ``whole``), a variable error correction rate
+   encoder (kind of a generalization of header\_ecc.py). This script
+   allows to generate an ecc file for the whole content of your files,
+   not just the header part, using a variable resilience rate: the
+   header part will be the most protected, then the rest of each file
+   will be progressively encoded with a smaller and smaller resilience
+   rate. The assumption is that important information is stored first,
+   and then data becomes less and less informative (and thus important,
+   because the end of the file describes less important details). This
+   assumption is very true for all compressed kinds of formats, such as
+   JPG, ZIP, Word, ODT, etc...
+
+-  repair\_ecc.py (subcommand: ``recovery``), a script to repair the structure (ie, the entry and
+   fields markers/separators) of an ecc file generated by header\_ecc.py
+   or structural\_adaptive\_ecc.py. The goal is to enhance the
+   resilience of ecc files against corruption by ensuring that their
+   structures can be repaired (up to a certain point which is very high
+   if you use an index backup file, which is a companion file that is
+   generated along an ecc file).
+
+-  filetamper.py (subcommand: ``filetamper``) is a quickly made file corrupter, it will erase or
+   change characters in the specified file. This is useful for testing
+   your various protecting strategies and file formats (eg: is PAR2
+   really resilient against corruption? Are zip archives still partially
+   extractable after corruption or are rar archives better? etc.). Do
+   not underestimate the usefulness of this tool, as you should always
+   check the resiliency of your file formats and of your file protection
+   strategies before relying on them.
+
+-  replication\_repair.py (subcommand: ``dup``) takes advantage of your multiple copies
+   (replications) of your data over several storage mediums to recover
+   your data in case it gets corrupted. The goal is to take advantage of
+   the storage of your archived files into multiple locations: you will
+   necessarily make replications, so why not use them for repair?
+   Indeed, it's good practice to keep several identical copies of your data
+   on several storage mediums, but in case a corruption happens,
+   usually you will just drop the corrupted copies and keep the intacts ones.
+   However, if all copies are partially corrupted, you're stuck. This script
+   aims to take advantage of these multiple copies to recover your data,
+   without generating a prior ecc file. It works simply by reading through all
+   your different copies of your data, and it casts a majority vote over each
+   byte: the one that is the most often occuring will be kept. In engineering,
+   this is a very common strategy used for very reliable systems such as
+   space rockets, and is called "triple-modular redundancy", because you need
+   at least 3 copies of your data for the majority vote to work (but the more the
+   better).
+
+-  resiliency\_tester.py (subcommand: ``restest``) allows you to test the robustness of the
+   corruption correction of the scripts provided here (or any other
+   command-line app). You just have to copy the files you want to test inside a
+   folder, and then the script will copy the files into a test tree, then it
+   will automatically corrupt the files randomly (you can change the parameters
+   like block burst and others), then it will run the file repair command-lines
+   you supply and finally some stats about the repairing power will be
+   generated. This allows you to easily and objectively compare different set
+   of parameters, or even different file repair solutions, on the very data
+   that matters to you, so that you can pick the best option for you.
+
+-  ecc\_speedtest.py (subcommand: ``speedtest``) is a simple error correction codes
+   encoder/decoder speedtest. It allows to easily change parameters for the test.
+   This allows to assess how fast your machine can encode/decode with the selected
+   parameters, which can be especially useful to plan ahead for how many files you
+   can reasonably plan to protect with error correction codes (which are time consuming).
+
+-  DEPRECATED: easy\_profiler.py is just a quick and simple profiling tool to get
+   you started quickly on what should be optimized to get more speed, if
+   you want to contribute to the project feel free to propose a pull
+   request! (Cython and other optimizations are welcome as long as they
+   are cross-platform and that an alternative pure-python implementation
+   is also available).
+
+Note that all tools are primarily made for command-line usage (type
+pff <subcommand> --help to get extended info about the accepted arguments)
+
+IMPORTANT: it is CRITICAL that you use the same parameters for
+correcting mode as when you generated the database/ecc files (this is
+true for all scripts in this bundle). Of course, some options must be
+changed: -g must become -c to correct, and --update is a particular
+case. This works this way on purpose for mainly two reasons: first
+because it is very hard to autodetect the parameters from a database
+file alone and it would produce lots of false positives, and secondly
+(the primary reason) is that storing parameters inside the database file
+is highly unresilient against corruption (if this part of the database
+is tampered, the whole becomes unreadable, while if they are stored
+outside or in your own memory, the database file is always accessible).
+Thus, it is advised to write down the parameters you used to generate
+your database directly on the storage media you will store your database
+file on (eg: if it's an optical disk, write the parameters on the cover
+or directly on the disk using a marker), or better memorize them by
+heart. If you forget them, don't panic, the parameters are always stored
+as comments in the header of the generated ecc files, but you should try
+to store them outside of the ecc files anyway.
+
+For users: what are the advantages of pyFileFixity?
+---------------------------------------------------
+
+Pros:
+
+-  Open application and open specifications under the MIT license (you
+   can do whatever you want with it and tailor it to your needs if you
+   want to, or add better decoding procedures in the future as science
+   progress so that you can better recover your data from your already
+   generated ecc file).
+-  Highly reliable file fixity watcher: rfigc.py will tell you without
+   any ambiguity using several attributes if your files have been
+   corrupted or not, and can even check for images if the header is
+   valid (ie: if the file can still be opened).
+-  Readable ecc file format (compared to PAR2 and most other similar
+   specifications).
+-  Highly resilient ecc file format against corruption (not only are
+   your data protected by ecc, the ecc file is protected too against
+   critical spots, both because there is no header so that each track is
+   independent and if one track is corrupted beyond repair then other
+   ecc tracks can still be read, and a .idx file will be generated to
+   repair the structure of the ecc file to recover all tracks).
+-  Very safe and conservative approach: the recovery process checks that
+   the recovery was successful before committing a repaired block.
+-  Partial recovery allowed (even if a file cannot be completely
+   recovered, the parts that can will be repaired and then the rest that
+   can't be repaired will be recopied from the corrupted version).
+-  Support directory processing: you can encode an ecc file for a whole
+   directory of files (with any number of sub-directories and depth).
+-  No limit on the number of files, and it can recursively protect files
+   in a directory tree.
+-  Variable resiliency rate and header-only resilience, ensuring that
+   you can always open your files even if partially corrupted (the
+   structure of your files will be saved, so that you can use other
+   softwares to repair beyond if this set of script is not sufficient to
+   totally repair).
+-  Support for erasures (null bytes) and even errors-and-erasures, which
+   literally doubles the repair capabilities. To my knowledge, this is
+   the only freely available parity software that supports erasures.
+-  Display the predicted total ecc file size given your parameters,
+   and the total time it will take to encode/decode.
+-  Your original files are still accessible as they are, protection files
+   such as ecc files live alongside your original data. Contrary to
+   other data protection schemes such as PAR2 which encode the whole
+   data in par archive files that replace your original files and
+   are not readable without decoding.
+-  Opensourced under the very permissive MIT licence, do whatever you
+   want!
+
+Cons:
+
+-  Cannot protect meta-data, such as folders paths. The paths are
+   stored, but cannot be recovered (yet? feel free to contribute if you
+   know how). Only files are protected. Thus if your OS or your storage
+   medium crashes and truncate a whole directory tree, the directory
+   tree can't be repaired using the ecc file, and thus you can't access
+   the files neither. However, you can use file scraping to extract the
+   files even if the directory tree is lost, and then use RFIGC.py to
+   reorganize your files correctly. There are alternatives, see the
+   chapters below: you can either package all your files in a single
+   archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
+   DVDisaster as an alternative solution, which is an ecc generator with
+   support for directory trees meta-data (but only on optical disks).
+-  Can only repair errors and erasures (characters that are replaced by
+   another character), not deletion nor insertion of characters. However
+   this should not happen with any storage medium (truncation can occur
+   if the file bounds is misdetected, in this case pyFileFixity can
+   partially repair the known parts of the file, but cannot recover the
+   rest past the truncation, except if you used a resiliency rate of at
+   least 0.5, in which case any message block can be recreated with only
+   using the ecc file).
+-  Cannot recreate a missing file from other available files (except you
+   have set a resilience\_rate at least 0.5), contrary to Parchives
+   (PAR1/PAR2). Thus, you can only repair a file if you still have it
+   (and its ecc file!) on your filesystem. If it's missing, pyFileFixity
+   cannot do anything (yet, this will be implemented in the future).
+
+Note that the tools were meant for data archival (protect files that you
+won't modify anymore), not for system's files watching nor to protect
+all the files on your computer. To do this, you can use a filesystem
+that directly integrate error correction code capacity, such as ZFS.
+
+Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)
+------------------------------------------------------------------------------
+
+Recursively generate or check the integrity of files by MD5 and SHA1
+hashes, size, modification date or by data structure integrity (only for
+images).
+
+This script is originally meant to be used for data archival, by
+allowing an easy way to check for silent file corruption. Thus, this
+script uses relative paths so that you can easily compute and check the
+same redundant data copied on different mediums (hard drives, optical
+discs, etc.). This script is not meant for system files corruption
+notification, but is more meant to be used from times-to-times to check
+up on your data archives integrity (if you need this kind of application,
+see `avpreserve's fixity <https://github.com/avpreserve/fixity>`_).
+
+Example usage
+~~~~~~~~~~~~~
+
+-  To generate the database (only needed once):
+
+``pff hash -i "your_folder" -d "dbhash.csv" -g``
+
+-  To check:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s``
+
+-  To update your database by appending new files:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -u -a``
+
+-  To update your database by appending new files AND removing
+   inexistent files:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -u -a -r``
+
+Note that by default, the script is by default in check mode, to avoid
+wrong manipulations. It will also alert you if you generate over an
+already existing database file.
+
+Arguments
+~~~~~~~~~
+
+::
+
+      -h, --help            show a help message and exit
+      -i /path/to/root/folder, --input /path/to/root/folder
+                            Path to the root folder from where the scanning will occ
+    ur.
+      -d /some/folder/databasefile.csv, --database /some/folder/databasefile.csv
+                            Path to the csv file containing the hash informations.
+      -l /some/folder/filename.log, --log /some/folder/filename.log
+                            Path to the log file. (Output will be piped to both the
+    stdout and the log file)
+      -s, --structure_check
+                            Check images structures for corruption?
+      -e /some/folder/errorsfile.csv, --errors_file /some/folder/errorsfile.csv
+                            Path to the error file, where errors at checking will be
+     stored in CSV for further processing by other softwares (such as file repair so
+    ftwares).
+      -m, --disable_modification_date_checking
+                            Disable modification date checking.
+      --skip_missing        Skip missing files when checking (useful if you split yo
+    ur files into several mediums, for example on optical discs with limited capacit
+    y).
+      -g, --generate        Generate the database? (omit this parameter to check ins
+    tead of generating).
+      -f, --force           Force overwriting the database file even if it already e
+    xists (if --generate).
+      -u, --update          Update database (you must also specify --append or --rem
+    ove).
+      -a, --append          Append new files (if --update).
+      -r, --remove          Remove missing files (if --update).
+      
+      --filescraping_recovery          Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.
+      -o, --output          Path to the output folder where to output the files reorganized after --recover_from_filescraping.
+
+Header Error Correction Code script
+-----------------------------------
+
+This script was made to be used in combination with other more common
+file redundancy generators (such as PAR2, I advise MultiPar). This is an
+additional layer of protection for your files: by using a higher
+resiliency rate on the headers of your files, you ensure that you will
+be probably able to open them in the future, avoiding the "critical
+spots", also called "fracture-critical" in redundancy engineering (where
+if you modify just one bit, your whole file may become unreadable,
+usually bits residing in the headers - in other words, a single blow
+makes the whole thing collapse, just like non-redundant bridges).
+
+An interesting benefit of this approach is that it has a low storage
+(and computational) overhead that scales linearly to the number of
+files, whatever their size is: for example, if we have a set of 40k
+files for a total size of 60 GB, with a resiliency\_rate of 30% and
+header\_size of 1KB (we limit to the first 1K bytes/characters = our
+file header), then, without counting the hash per block and other
+meta-data, the final ECC file will be about 2 \* resiliency\_rate \*
+number\_of\_files \* header\_size = 24.5 MB. This size can be lower if
+there are many files smaller than 1KB. This is a pretty low storage
+overhead to backup the headers of such a big number of files.
+
+The script is pure-python as are its dependencies: it is thus completely
+cross-platform and open source. The default ecc algo
+(ecc_algo=3 uses `reedsolo <https://github.com/tomerfiliba-org/reedsolomon>`_)
+also provides a speed-optimized C-compiled implementation (``creedsolo``) that will be used
+if available for the user's platform, so pyFileFixity should be fast by default.
+Alternatively, it's possible to use a JIT compiler such as PyPy,
+although this means that ``creedsolo`` will not be useable, so PyPy
+may accelerate other functions but slower ecc encoding/decoding.
+
+Structural Adaptive Error Correction Encoder
+--------------------------------------------
+
+This script implements a variable error correction rate encoder: each
+file is ecc encoded using a variable resiliency rate -- using a high
+constant resiliency rate for the header part (resiliency rate stage 1,
+high), then a variable resiliency rate is applied to the rest of the
+file's content, with a higher rate near the beginning of the file
+(resiliency rate stage 2, medium) which progressively decreases until
+the end of file (resiliency rate stage 3, the lowest).
+
+The idea is that the critical parts of files usually are placed at the
+top, and data becomes less and less critical along the file. What is
+meant by critical is both the critical spots (eg: if you tamper only one
+character of a file's header you have good chances of losing your entire
+file, ie, you cannot even open it) and critically encoded information
+(eg: archive formats usually encode compressed symbols as they go along
+the file, which means that the first occurrence is encoded, and then the
+archive simply writes a reference to the symbol. Thus, the first
+occurrence is encoded at the top, and subsequent encoding of this same
+data pattern will just be one symbol, and thus it matters less as long
+as the original symbol is correctly encoded and its information
+preserved, we can always try to restore the reference symbols later).
+Moreover, really redundant data will be placed at the top because they
+can be reused a lot, while data that cannot be too much compressed will
+be placed later, and thus, corruption of this less compressed data is a
+lot less critical because only a few characters will be changed in the
+uncompressed file (since the data is less compressed, a character change
+on the not-so-much compressed data won't have very significant impact on
+the uncompressed data).
+
+This variable error correction rate should allow to protect more the
+critical parts of a file (the header and the beginning of a file, for
+example in compressed file formats such as zip or jpg this is where the
+most importantly strings are encoded) for the same amount of storage as
+a standard constant error correction rate.
+
+Of course, you can set the resiliency rate for each stage to the values
+you want, so that you can even do the opposite: setting a higher
+resiliency rate for stage 3 than stage 2 will produce an ecc that is
+greater towards the end of the contents of your files.
+
+Furthermore, the currently designed format of the ecc file would allow
+two things that are not available in all current file ecc generators
+such as PAR2:
+
+1. it allows to partially repair a file, even if not all
+the blocks can be corrected (in PAR2, a file is repaired only if all
+blocks can be repaired, which is a shame because there are still other
+blocks that could be repaired and thus produce a less corrupted file) ;
+
+2. the ecc file format is quite simple and readable, easy to process by
+any script, which would allow other softwares to also work on it (and it
+was also done in this way to be more resilient against error
+corruptions, so that even if an entry is corrupted, other entries are
+independent and can maybe be used, thus the ecc is very error tolerant.
+This idea was implemented in repair\_ecc.py but it could be extended,
+especially if you know the pattern of the corruption).
+
+The script structural-adaptive-ecc.py implements this idea, which can be
+seen as an extension of header-ecc.py (and in fact the idea was the
+other way around: structural-adaptive-ecc.py was conceived first but was
+too complicated, then header-ecc.py was implemented as a working
+lessened implementation only for headers, and then
+structural-adaptive-ecc.py was finished using header-ecc.py code
+progress). It works, it was a quite well tested for my own needs on
+datasets of hundred of GB, but it's not foolproof so make sure you test
+the script by yourself to see if it's robust enough for your needs (any
+feedback about this would be greatly appreciated!).
+
+ECC Algorithms
+--------------
+
+You can specify different ecc algorithms using the ``--ecc_algo`` switch.
+
+For the moment, only Reed-Solomon is implemented, but it's universal
+so you can modify its parameters in lib/eccman.py.
+
+Two Reed-Solomon codecs are available, they are functionally equivalent
+and thoroughly unit tested.
+
+-  ``--ecc_algo 1``: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
+   This is the slowest implementation (but also the most easy code to understand).
+-  ``--ecc_algo 2``: same as algo 1 but with a faster functions.
+-  ``--ecc_algo 3``: use the second codec, which is the fastest.
+   The generated ECC will be compatible with algo 1 and 2.
+-  ``--ecc_algo 4``: also use the second, fastest RS codec, but
+   with different parameters (US FAA ADSB UAT RS FEC norm),
+   thus the generated ECC won't be compatible with algo 1 to 3.
+   But do not be scared, the ECC will work just the same.
+
+Note about speed: Also, use a smaller --max\_block\_size to greatly
+speedup the operations! That's the trick used to compute very quickly RS
+ECC on optical discs. You give up a bit of resiliency of course (because
+blocks are smaller, thus you protect a smaller number of characters per
+ECC. In the end, this should not change much about real resiliency, but
+in case you get a big bit error burst on a contiguous block, you may
+lose a whole block at once. That's why using RS255 is better, but it's
+very time consuming. However, the resiliency ratios still hold, so for
+any other case of bit-flipping with average-sized bursts, this should
+not be a problem as long as the size of the bursts is smaller than an
+ecc block.)
+
+In case of a catastrophic event
+-------------------------------
+
+TODO: write more here
+
+In case of a catastrophic event of your data due to the failure of your
+storage media (eg: your hard drive crashed), then follow the following
+steps:
+
+1- use dd\_rescue to make a full bit-per-bit verbatim copy of your drive
+before it dies. The nice thing with dd\_rescue is that the copy is
+exact, and also that it can retries or skip in case of bad sectors (it
+won't crash on your suddenly at half the process).
+
+2- Use testdisk to restore partition or to copy files based on partition
+filesystem informations.
+
+3- If you could not recover your files, you can try file scraping using
+`photorec <http://www.cgsecurity.org/wiki/PhotoRec>`_ or
+`plaso  <http://plaso.kiddaland.net/>`_ other similar tools as
+a last resort to extract data based only from files content (no filename,
+often uncorrect filetype, file boundaries may be wrong so some data
+may be cut off, etc.).
+
+4- If you used pyFileFixity before the failure of your storage media,
+you can then use your pre-computed databases to check that files are
+intact (rfigc.py) and if they aren't, you can recover them (using
+header\_ecc.py and structural\_adaptive\_ecc.py). It can also help if
+you recovered your files via data scraping, because your files will be
+totally unorganized, but you can use a previously generated database
+file to recover the full names and directory tree structure using
+rfigc.py --filescraping\_recover.
+
+Also, you can try to fix some of your files using specialized repairing
+tools (but remember that such tool cannot guarantee you the same
+recovering capacity as an error correction code - and in addition, error
+correction code can tell you when it has recovered successfully). For
+example:
+
+-  for tar files, you can use `fixtar <https://github.com/BestSolution-at/fixtar>`_.
+   Similar tools (but older): `tarfix <http://www.dmst.aueb.gr/dds/sw/unix/tarfix/>`_
+   and `tar-repair <https://www.datanumen.com/tar-repair/>`_.
+-  for RAID mounting and recovery, you can use "Raid faster - recover
+   better" (rfrb) tool by Sabine Seufert and Christian Zoubek:
+   https://github.com/lrq3000/rfrb
+-  if your unicode strings were mangled (ie, you see weird symbols),
+   try this script that will automatically demangle them:
+   https://github.com/LuminosoInsight/python-ftfy
+-  to repair tabular (2D) data such as .csv, try
+   `Carpenter <https://pypi.python.org/pypi/Carpenter/>`_.
+-  tool to identify corrupted files in ddrescue images: 
+   `ddrescue-ffile <https://github.com/Salamek/ddrescue-ffile>`_
+
+Protecting directory tree meta-data
+-----------------------------------
+
+One main current limitation of pyFileFixity is that it cannot protect
+the directory tree meta-data. This means that in the worst case, if a
+silent error happens on the inode pointing to the root directory that
+you protected with an ecc, the whole directory will vanish, and all the
+files inside too. In less worst cases, sub-directories can vanish, but
+it's still pretty bad, and since the ecc file doesn't store any
+information about inodes, you can't recover the full path.
+
+The inability to store these meta-data is because of two choices in the
+design:
+
+1.  portability: we want the ecc file to work even if we move the
+    root directory to another place or another storage medium (and of
+    course, the inode would change),
+
+2.  cross-platform compatibility: there's no way to get and store
+    directory meta-data for all platforms, but of course we could implement specific instructions for each main
+    platform, so this point is not really a problem.
+
+To workaround this issue (directory meta-data are critical spots), other
+softwares use a one-time storage medium (ie, writing your data along
+with generating and writing the ecc). This way, they can access at
+the bit level the inode info, and they are guaranted that the inodes
+won't ever change. This is the approach taken by DVDisaster: by using
+optical mediums, it can compute inodes that will be permanent, and thus
+also encode that info in the ecc file. Another approach is to create a
+virtual filesystem specifically to store just your files, so that you
+manage the inode yourself, and you can then copy the whole filesystem
+around (which is really just a file, just like a zip file - which can
+also be considered as a mini virtual file system in fact) like
+`rsbep <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`_.
+
+Here the portability principle of pyFileFixity prevents this approach.
+But you can mimic this workaround on your hard drive for pyFileFixity to
+work: you just need to package all your files into one file. This way,
+you sort of create a virtual file system: inside the archive, files and
+directories have meta-data just like in a filesystem, but from the
+outside it's just one file, composed of bytes that we can just encode to
+generate an ecc file - in other words, we removed the inodes portability
+problem, since this meta-data is stored relatively inside the archive,
+the archive manage it, and we can just encode this info like any other
+stream of data! The usual way to make an archive from several files is
+to use TAR, but this will generate a solid archive which will prevent
+partial recovery. An alternative is to use DAR, which is a non-solid
+archive version of TAR, with lots of other features too. If you also
+want to compress, you can just use ZIP (with DEFLATE algorithm) your
+files (this also generates a non-solid archive). You can then use
+pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
+will then protect both your files just like before and the directories
+meta-data too now.
+
+Which storage medium to use
+---------------------------
+Since hard drives have a relatively short timespan (5-10 years, often less)
+and require regular plugging to an electrical outlet to keep the magnetic
+plates from decaying, other solutions are more advisable.
+
+The medium I used to advise was optical disks (whether it's BluRay, DVD - not CDs!),
+because the reading instrument is distinct from the storage medium, and
+the technology (laser reflecting on bumps and/or pits) is kind of universal,
+so that even if the technology is lost one day (deprecated by newer technologies,
+so that you can't find the reading instrument anymore because it's not sold anymore),
+you can probably emulate a laser using some software to read your optical disk,
+just like what the CAMiLEON project did to recover data from the
+LaserDiscs of the BBC Domesday Project (see Wikipedia). BluRays have an estimated
+lifespan of 20-50 years depending on if they are "gold archival grade", whereas
+DVD should live up from 10-30 years. CDs are only required to live a minimum of 1 year
+up to 10 years max, hence are not fit for archival. Archival optimized optical discs
+such as M-Discs boast about being able to live up to 100 years, but there is no
+independent scientific backing of these claims currently. For more details, you can read
+a longer explanation I wrote with references on
+`StackOverflow <https://web.archive.org/web/20230424112000/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`__.
+
+However, limitations of optical discs include their limited storage space, low
+transfer speed, and limited rewriteability.
+
+A more convenient solution is to use magnetic tape, especially with an open standard
+such as `Linear Tape Open (LTO) <https://en.wikipedia.org/wiki/Linear_Tape-Open>`__,
+which ensures interoperability between manufacturers
+and hence also reduces cost because of competition. LTO works as a two components
+system: the tape drive, and the cartridges (with the magnetic bands). There
+are lots of versions of LTO, each generation improving on the previous one.
+LTO cartridges have a shorter lifespan than optical discs, being 15-30 years on average,
+but they are much more convenient to use:
+
+-  they provide extremely big storage space (one cartridge being several TB as of LTO-4,
+   and the storage capacity approximately doubles every few years with every new version!),
+-  are fast to write (about 5h to write the full cartridge, speed increases with new versions
+   so the total time to fill a cartridge stays about the same),
+-  the storage medium (cartridges) is also distinct from the reading/writing instrument (LTO tape drive), 
+-  are easily rewriteable, although it is necessary to reformat to free up space, but the idea is
+   that "full mirror backups" can be made regularly by overwriting an old tape.
+-  being an open standard, drives to read older versions 25 years old (LTO-1 is from 2000)
+   are still available.
+-  15-30 years of lifespan is still great for archival! But requires active curation (ie, checking
+   cartridges every 5 years and making a full new copy on a new cartridge each decade should be largely sufficient).
+-  Cartridges are cheap: LTO7 cartridges allowing storage of up to 15 TB cost only 60 bucks brand new, often
+   much less in refurbished (already used, but can be overwritten and reused). This is MUCH less expensive
+   than hard drives.
+-  Fit for cold storage: unlike hard drives (using magnetic platters) and like optical discs,
+   the cartridges do not need to be plugged to an electrical outlet regularly, the magnetic band does not
+   decay without electrical current, so the cartridges can be cold stored in air-tight, temperature-proofed
+   and humidity-proof containers, which can be stored off-site (fire-proof data recovery plan).
+-  Recovery of failed LTO cartridges is
+   `inexpensive and readily available <https://www.quora.com/I-have-an-old-LTO-tape-Can-I-recover-its-data-and-save-it-into-a-hard-drive>`__,
+   whereas recovering the magnetic signal from failed hard drives costs
+   `thousands of euros/dollars <https://www.quora.com/Is-there-any-way-of-recovering-data-from-dead-hard-disk>`__.
+   LTO tapes are also fully compatible with DAR archives, improving chances of recovery with error correction codes
+   and non-solid archives that can be partially recovered.
+
+Sounds perfect, right? Well, nothing is, LTO also has several disadvantages:
+
+-  Initial cost of starting is very expensive: a brand new LTO drive of latest generations
+   cost several thousand euros/dollars. Second-hand or refurbished drives of older generations
+   are much less expensive, but they are difficult to setup, as it is unlikely you will find them
+   in an all-in-one package, you will have to get the tape drive separately from the computer system
+   to plug it to (more on that in the next section).
+-  Limited retrocompatibility: the LTO standard specifies that each generation of drives
+   only need to support the current gen and one past gen. However, this is counterbalanced by the fact that
+   the LTO standard is open, so anybody can make LTO drives, including in the future, and it is possible someday
+   a manufacturer will make a LTO drive that supports multiple past generations (just like there are old tapes
+   digitizers that can be connected in USB, for archival purposes). Until then, in practice,
+   it means that ideally when upgrading your LTO system, you need to upgrade by one generation at a time,
+   or if you get a drive of 2+ later gens, you need to keep or buy a drive of the older gen you had to
+   read your tapes to then transfer to the latest gen you have. As of 2023, there are still LTO1 tape drives
+   available for cheap in second-hand, a technology that was published in 2000 and already deprecated
+   in 2001 by LTO2, so this shows that LTO tape drives of older generations should still be plentily available.
+-  LTO is a sequential technology: it is very fast to write and read sequentially, but if you want to
+   download a specific file, the tape has to be fully read up to where the file is stored, contrary to
+   hard drives with random access that can access in linear or sublinear time.
+-  (Old fixed issue) Before LTO-5, which introduced the LTFS standardized filesystem that allows mounting on
+   any operating file system such as Windows, Linux and MacOS, the various LTO drives
+   manufacturers used their own closed-source filesystems that were often incompatible with each others.
+   Hence, make sure to get an LTO-5 drive or above to ensure future access to your long term archives.
+
+Given all the above characteristics, LTO>=5 appears to be the best practical solution
+for long term archival, if coupled with an active (but infrequent) curation process.
+
+There is however one exception: if you need to cold store the medium in a non temperate
+environment (outside of 10-40°C), then using optical discs may be more resilient,
+although LTO cartridges should also be able to sustain a wider range of temperature
+but you need to wait while they "warm up" in the environment where the reader is
+before reading, so that the magnetic elements have time to stabilize at normal temperature.
+
+How to get a LTO tape drive and system running
+----------------------------------------------
+
+To get started with LTO tape drives and which one to choose and how to make your own
+rig, `Matthew Millman made an excellent tutorial <https://www.mattmillman.com/attaching-lto-tape-drives-via-usb-or-thunderbolt/>`__
+on which we build upon below, so you should read this tutorial and then read the instructions below.
+
+The process is as follows: first find a second-hand/refurbished LTO drive with the highest revision you can for your budget,
+then find a server of a similar generation, or make an eGPU + SAS card of the highest speed the tape drive can support.
+Generally, you can aim for a LTO drive 3-4 generations older than the latest one (eg, if current is LTO9, you can expect
+cheap - 150-300 dollars per drive) for a LTO5 or LTO6). Aim only for LTO5+, because only LTFS did not exist before LTO5,
+but keep in mind some LTO5 drives need a firmware update to support LTFS, whereas all LTO6 drives support out of the box.
+
+Once you find a second-hand LTO drive, consult its user manual beforehand to see
+what SAS or fibre cable (FC) you need (if SAS, any version should work, even greater versions, but older
+versions will just limit the read/write speed performance). For example, here is the manual for the
+`HP LTO6 drive <https://docs.oracle.com/cd/E38452_01/en/LTO6_Vol1_E1_D7/LTO6_Vol1_E1_D7.pdf>`__.
+All LTO drives are compatible with all computers provided you have the adequate connectivity (a SAS or FC adapter).
+
+Once you have a LTO drive, then you can look for a computer to plug your LTO to. Essentially, you just need a computer that supports SAS. If not, then at least a free PCIe or mini-PCIe slot to be able to connect a SAS adapter.
+
+The general outline is that you just need to have a computer with a PCIe slot, and get a SAS or FC adapter (depending
+on whether your LTO drive is SAS or FC) so that you can plug your LTO drive. There is
+currently no SAS to USB adapter, and only one manufacturer makes LTO drives with USB ports but
+they are super expensive, so just stick with internal SAS or FC drives (usually you want SAS,
+FC are better for long range connections, whereas SAS is compatible with SATA and SCSI drives,
+so you can also plug all your other hard drives plus the LTO tape drive on the same SAS adapter with this protocol).
+
+In practice, there are 2 different available cost-effective approaches:
+
+-  If you have an external tape drive, then the best is to get a (second-hand) eGPU casing, and a PCIe SAS adapter, that you will plug in the eGPU casing instead of a GPU card. The eGPU casing should support Thunderbolt so this is how you will connect to the SAS and hence to your tape drive: you connect your laptop to the eGPU casing, and the eGPU casing to the external tape drive via the SAS adapter in the eGPU casing. This usually costs about 150-200 euros/dollars as of 2023.
+
+  *  An alternative is to buy a low footprint PCIe dock such as `EXP GDC <https://wiki.geekworm.com/GDC>`__ produces, which essentially replaces the eGPU casing. The disadvantage is that your PCIe SAS adapter will be exposed, but this can be more cost effective (especially in second hand, you can get them at 20-40 euros/dollars instead of 120-150 euros/dollars brand new). But remember you also need to buy a power supply unit!
+
+-  If you got an internal tape drive, which are usually cheaper than external ones, then the approach is different: instead of configuring a sort of SAS-to-Thunderbolt bridge, here you get a standalone computer with either a motherboard that natively supports SAS (which is usually the case of computers meant to be servers), or at least a motherboard with a PCIe slot to buy separately a PCIe SAS adapter, and you plug your internal drive inside. So you will not be able to connect your laptop directly to the tape drive, you will have to pilot the server (which is just a standard desktop computer). Given these requirements, you can either make such a server yourself, but then keep in mind you have to build the whole computer, with a motherboard, a power supply, RAM, CPU, network, etc. Or, the easiest and usually cheapest route, is to just buy an old server with SAS hard drives second-hand (and every other components already in it), of a similar or later generation than your tape drive. Indeed, if the server has SAS hard drives, then it means you can connect your SAS tape drive too, no need for an adapter! Usually you can get them for cheap, for example if you get a 3-4 previous gen tape drive (eg, LTO-6 when current is LTO-9), then you can easily get a server computer of a similar generation for 100-250 euros/dollars, and everything is ready for you. Just make sure not to get a rack/blade computer, get one in tower form, easier to manipulate. Search on second hand websites: "server sas", then check that the SAS speed is on par with what your tape drive can accept, but if lower or higher, no biggie, it will just be slower, but it should work nevertheless. May also have to buy the right connectors but not an issue, just check the manual of your tape drive. Note: avoid HP Enterprise (HPE) servers, as there is a suspicion of programmed obsolescence in the `Smart Array's Smart Storage Battery <https://www.youtube.com/watch?v=6jxdGXA0RYk>`__.
+
+The consumables, the tapes, can also be easily found second-hand and usually are very cheap, eg, LTO6 tapes are sold at 10-20 euros/dollars one, for a storage space of 3TB to 6.25TB per tape.
+
+With both approaches, expect at the cheapest a total cost of about 500 euros/dollars for the tape drive and attachment system (eGPU casing or dedicated server) as of 2023, which is very good and amortizable very fast with just a few tapes, even compared to the cheapest hard drives!
+
+A modern data curation strategy for individuals
+-----------------------------------------------
+
+Here is an example curation strategy, which is accessible to individuals and not just
+big data centers:
+
+-  Get a LTO>=5 drive. Essentially, the idea with LTO is that you can just dump a copy
+   of your whole hard drives, since the cartridges are big and inexpensive. And you can
+   regularly reformat and overwrite the previous copy with a newer one. Store some LTO cartridges
+   out of side to be robust against fires.
+-  If you want additional protection, especially by adding error-correction codes,
+   DAR can be used to compress the data with PAR2 and is
+   `compatible <https://superuser.com/questions/963246/how-to-read-an-dar-archive-via-lto-6-tape>`__
+   with LTO. Alternatively, pyFileFixity can also be used to generate ECC codes, that can
+   either be stored on the same cartridge alongside the files or on a separate cartridge depending
+   on your threat model.
+-  Two kinds of archival plans are possible:
+
+  1.  either only use LTO cartridges, then try to use cartridges of different brands
+      (to avoid them failing at the same time - cartridges produced by the same industrial
+      line will tend to include the same defects and similar lifespan)
+      and store your data on at least 3 different copies/cartridges, per the redundancy principle
+      (ie, "either bring one compass or three, but never two, because you will never know which one is correct").
+
+  2.  either use LTO cartridges as ONE archival medium, and use other kinds of storage
+      for the additional 2 copies you need: one can be an external hard drive, and the last one
+      a cloud backup solution such as SpiderOak. The advantage of this solution is that
+      it is more convenient: use your external hard drive to frequently backup,
+      then also use your cloud backup to auto backup your most critical data online (off-site),
+      and finally from time to time update your last copy on a LTO cartridge by mirroring your
+      external hard drive.
+
+-  Curation strategy is then the same for all plans:
+
+  1.  Every 5 years, the "small checkup": check your 3 copies, either by scanning sectors or by your own
+      precomputed hashes (pyFileFixity's ``hash`` command).
+
+  2.  If there is an error, assume the whole medium is dead and needs to be replaced
+      and your data needs to be recovered: first using your error correction codes if you have,
+      and then using pyFileFixity ``dup`` command to use a majority vote to reconstruct one valid copy out of the 3 copies.
+
+  3.  Every 10 years, the "big checkup": even if the mediums did not fail, replace them by newer ones: mirror the old hard drive to
+      a new one, the old LTO cartridge to a new one (it can be on a newer LTO version, so that you keep pace with the technology), etc.
+
+With the above strategy, you should be able to preserve your data for as long as you can actively curate it. In case you want
+more robustness against accidents or the risk that 2 copies get corrupted under 5 years, then you can make more copies, preferably
+as LTO cartridges, but it can be other hard drives.
+
+For more information on how to cold store LTO drives, read pp32-33 "Caring for Cartridges" instruction of this
+`user manual <https://docs.oracle.com/cd/E38452_01/en/LTO6_Vol1_E1_D7/LTO6_Vol1_E1_D7.pdf>`__. For HP LTO6 drives,
+Matthew Millman made an open-source commandline tool to do advanced LTO manipulations on Windows:
+`ltfscmd <https://github.com/inaxeon/ltfscmd>`__.
+
+In case you cannot afford a LTO drive, you can replace these by external hard drives, as they are less expensive to start with,
+but then your curation strategy should be done more frequently (ie, every 2-3 years a small checkup, and every 5 years, a big checkup).
+
+Tools like pyFileFixity (or which can be used as complements)
+-------------------------------------------------------------
+
+Here are some tools with a similar philosophy to pyFileFixity, which you
+can use if they better fit your needs, either as a replacement of
+pyFileFixity or as a complement (pyFileFixity can always be used to
+generate an ecc file):
+
+-  `DAR (Disk ARchive) <http://dar.linux.free.fr/>`__: similar to tar
+   but non-solid thus allows for partial recovery and per-file access,
+   plus it saves the directory tree meta-data -- see catalog isolation
+   -- plus it can handle error correction natively using PAR2 and
+   encryption. Also supports incremental backup, thus it's a very nice
+   versatile tool. Crossplatform and opensource. Compatible with
+   `Linear Tape Open (LTO) <https://en.wikipedia.org/wiki/Linear_Tape-Open>`__
+   magnetic bands storage (see instructions
+   `here <https://superuser.com/questions/963246/how-to-read-an-dar-archive-via-lto-6-tape>`__)
+-  `DVDisaster <http://dvdisaster.net/>`__: error correction at the bit
+   level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
+   it also protects directory tree meta-data and is resilient to
+   corruption (v2 still has some critical spots but v3 won't have any).
+-  rsbep tool that is part of dvbackup package in Debian: allows to
+   generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
+   for your backups, if you're on unix or using cygwin.
+-  `rsbep modification by Thanassis
+   Tsiodras <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`__:
+   enhanced rsbep to avoid critical spots and faster speed. Also
+   includes a "freeze" script to encode your files into a virtual
+   filesystem (using Python/FUSE) so that even meta-data such as
+   directory tree are fully protected by the ecc. Great script, but not
+   maintained, it needs some intensive testing by someone knowledgeable
+   to guarantee this script is reliable enough for production.
+-  Parchive (PAR1, PAR2, MultiPar): well known error correction file
+   generator. The big advantage of Parchives is that an ecc block
+   depends on multiple files: this allows to completely reconstruct a
+   missing file from scratch using files that are still available. Works
+   good for most people, but most available Parchive generators are not
+   satisfiable for me because 1- they do not allow to generate an ecc
+   for a directory tree recursively (except MultiPar, and even if it is
+   allowed in the PAR2 specs), 2- they can be very slow to generate
+   (even with multiprocessor extensions, because the galois field is
+   over 2^16 instead of 2^8, which is very costly), 3- the spec is not
+   very resilient to errors and tampering over the ecc file, as it
+   assumes the ecc file won't be corrupted (I also tested, it's still a
+   bit resilient, but it could be a lot more with some tweaking of the
+   spec), 4- it doesn't allow for partial recovery (recovering blocks
+   that we can and pass the others that are unrecoverable): with PAR2, a
+   file can be restored fully or it cannot be at all.
+-  Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
+   create non-solid archives which are readable by most computers
+   (ubiquitous algorithm). Non-solid archive means that a zip file can
+   still unzip correct files even if it is corrupted, because files are
+   encoded in blocks, and thus even if some blocks are corrupted, the
+   decoding can happen. A `fast implementation with enhanced compression
+   is available in pure Go <https://github.com/klauspost/compress>`__
+   (good for long storage).
+-  TestDisk: for file scraping, when nothing else worked.
+-  dd\_rescue: for disk scraping (allows to forcefully read a whole disk
+   at the bit level and copy everything it can, passing bad sector with
+   options to retry them later on after a first full pass over the
+   correct sectors).
+-  ZFS: a file system which includes ecc correction directly. The whole
+   filesystem, including directory tree meta-data, are protected. If you
+   want ecc protection on your computer for all your files, this is the
+   way to go.
+-  Encryption: technically, you can encrypt your files without losing
+   too much redundancy, as long as you use an encryption scheme that is
+   block-based such as DES: if one block gets corrupted, it won't be
+   decryptable, but the rest of the files' encrypted blocks should be
+   decryptable without any problem. So encrypting with such algorithms
+   leads to similar files as non-solid archives such as deflate zip. Of
+   course, for very long term storage, it's better to avoid encryption
+   and compression (because you raise the information contained in a
+   single block of data, thus if you lose one block, you lose more
+   data), but if it's really necessary to you, you can still maintain
+   high chances of recovering your files by using block-based
+   encryption/compression (note: block-based encryption can
+   be seen as the equivalent of non-solid archives for compression,
+   because the data is compressed/encrypted in independent blocks,
+   thus allowing partial uncompression/decryption).
+-  `SnapRAID <http://snapraid.sourceforge.net/>`__
+-  `par2ools <https://github.com/jmoiron/par2ools>`__: a set of
+   additional tools to manage par2 archives
+-  `Checkm <https://pypi.python.org/pypi/Checkm/0.4>`__: a tool similar
+   to rfigc.py
+-  `BagIt <https://en.wikipedia.org/wiki/BagIt>`__ with two python
+   implementations `here <https://pypi.python.org/pypi/pybagit/>`__ and
+   `here <https://pypi.python.org/pypi/bagit/>`__: this is a file
+   packaging format for sharing and storing archives for long term
+   preservation, it just formalizes a few common procedures and meta
+   data that are usually added to files for long term archival (such as
+   MD5 digest).
+-  `RSArmor <https://github.com/jap/rsarm>`__ a tool based on
+   Reed-Solomon to encode binary data files into hexadecimal, so that
+   you can print the characters on paper. May be interesting for small
+   datasets (below 100 MB).
+-  `Ent <https://github.com/lsauer/entropy>`__ a tool to analyze the
+   entropy of your files. Can be very interesting to optimize the error
+   correction algorithm, or your compression tools.
+-  `HashFS <https://pypi.python.org/pypi/hashfs/>`_ is a non-redundant,
+   duplication free filesystem, in Python. **Data deduplication** is very
+   important for large scale long term storage: since you want your data
+   to be redundant, this means you will use an additional storage space
+   for your redundant copies that will be proportional to your original data.
+   Having duplicated data will consume more storage and more processing
+   time, for no benefit. That's why it's a good idea to deduplicate your data
+   prior to create redundant copies: this will be faster and save you money.
+   Deduplication can either be done manually (by using duplicates removers)
+   or systematically and automatically using specific filesystems such as
+   zfs (with deduplication enabled) or hashfs.
+-  Paper as a storage medium: paper is not a great storage medium,
+   because it has low storage density (ie, you can only store at most 
+   about 100 KB) and it can also degrade just like other storage mediums,
+   but you cannot check that automatically since it's not digital. However,
+   if you are interested, here are a few softwares that do that:
+   `Paper key <http://en.wikipedia.org/wiki/Paper_key>`_,
+   `Paperbak <http://www.ollydbg.de/Paperbak/index.html>`_,
+   `Optar <http://ronja.twibright.com/optar/>`_,
+   `dpaper <https://github.com/penma/dpaper>`_,
+   `QR Backup <http://blog.liw.fi/posts/qr-backup/>`_,
+   `QR Backup (another) <http://blog.shuningbian.net/2009/10/qrbackup.php>`_,
+   `QR Backup (again another) <http://git.pictorii.com/index.php?p=qrbackup.git&a=summary>`_,
+   `QR Backup (again) <http://hansmi.ch/software/qrbackup>`_,
+   `and finally a related paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&rep=rep1&type=pdf>`_.
+-  AVPreserve tools, most notably `fixity <https://github.com/avpreserve/fixity>`_ 
+   to monitor for file changes (similarly to rfigc, but actively as a daemon)
+   and `interstitial <https://github.com/avpreserve/interstitial>`_ to detect
+   interstitial errors in audio digitization workflows (great to ensure you
+   correctly digitized a whole audio file into WAV without any error).
+
+FAQ
+---
+
+-  Can I compress my data files and my ecc file?
+
+As a rule of thumb, you should ALWAYS keep your ecc file in clear
+text, so under no compression nor encryption. This is because in case
+the ecc file gets corrupted, if compressed/encrypted, the
+decompression/decrypting of the corrupted parts may completely flaw
+the whole structure of the ecc file.
+
+Your data files, that you want to protect, *should* remain in clear
+text, but you may choose to compress them if it drastically reduces
+the size of your files, and if you raise the resilience rate of your
+ecc file (so compression may be a good option if you have an
+opportunity to trade the file size reduction for more ecc file
+resilience). Also, make sure to choose a non-solid compression
+algorithm like DEFLATE (zip) so that you can still decode correct
+parts even if some are corrupted (else with a solid archive, if one
+byte is corrupted, the whole archive may become unreadable).
+
+However, in the case that you compress your files, you should generate
+the ecc file only *after* compression, so that the ecc file applies to
+the compressed archive instead of the uncompressed files, else you
+risk being unable to correct your files because the uncompression of
+corrupted parts may output gibberish, and length extended corrupted
+parts (and if the size is different, Reed-Solomon will just freak
+out).
+
+-  Can I encrypt my data files and my ecc file ?
+
+NEVER encrypt your ecc file, this is totally useless and
+counterproductive.
+
+You can encrypt your data files, but choose a non-solid algorithm
+(like AES if I'm not mistaken) so that corrupted parts do not prevent
+the decoding of subsequent correct parts. Of course, you're lowering a
+bit your chances of recovering your data files by encrypting them (the
+best chance to keep data for the long term is to keep them in clear
+text), but if it's really necessary, using a non-solid encrypting
+scheme is a good compromise.
+
+You can generate an ecc file on your encrypted data files, thus
+*after* encryption, and keep the ecc file in clear text (never encrypt
+nor compress it). This is not a security risk at all since the ecc
+file does not give any information on the content inside your
+encrypted files, but rather just redundant info to correct corrupted
+bytes (however if you generate the ecc file on the data files before
+encryption, then it's clearly a security risk, and someone could
+recover your data without your permission).
+
+- What medium should I use to store my data?
+
+The details are long and a bit complicated (I may write a complete article
+about it in the future), but the tl;dr answer is that you should use *optical disks*,
+because it decouples the storage medium and the reading hardware
+(eg, at the opposite we have hard drives, which contains both the reading
+hardware and the storage medium, so if one fails, you lose both)
+and because it's most likely future-proof (you only need a laser, which
+is universal, the laser's parameters can always be tweaked).
+
+From scientific studies, it seems that, at the time of writing this (2015),
+BluRay HTL disks are the most resilient against environmental degradation.
+To raise the duration, you can also put optical disks in completely opaque boxes
+(to avoid light degradation) and in addition you can put any storage medium
+(not only optical disks, but also hard drives and anything really) in
+*completely* air-tight and water-tight bags or box and put in a fridge or a freezer.
+This is a law of nature: lower the temperature, lower will be the entropy, in other
+words lower will be the degradation over time. It works the same with digital data.
+
+- What file formats are the most recoverable?
+
+It's difficult to advise a specific format. What we can do is advise the characteristics
+of a good file format:
+
+  * future-proof (should be readable in the future).
+  * non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn't cause a problem to the decoding of other blocks).
+  * open source implementation available.
+  * minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).
+  * No magic bytes or header importance (ie, corrupting the header won't prevent opening the file).
+
+There are a few studies about the most resilient file formats, such as:
+
+  * `"Just one bit in a million: On the effects of data corruption in files" by Volker Heydegger <http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1>`_.
+  * `"Analysing the impact of file formats on data integrity" by Volker Heydegger <http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf>`_.
+  * `"A guide to formats", by The UK national archives <http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf>`_ (you want to look at the Recoverability entry in each table).
+
+- What is Reed-Solomon?
+
+If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon
+
+Also, you may want to read the following resources:
+
+  * "`Reed-Solomon codes for coders <https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders>`_", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.
+  * "Algebraic codes for data transmission", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books <https://books.google.fr/books?id=eQs2i-R9-oYC&lpg=PR11&ots=atCPQJm3OJ&dq=%22Algebraic%20codes%20for%20data%20transmission%22%2C%20Blahut%2C%20Richard%20E.%2C%202003%2C%20Cambridge%20university%20press.&lr&hl=fr&pg=PA193#v=onepage&q=%22Algebraic%20codes%20for%20data%20transmission%22,%20Blahut,%20Richard%20E.,%202003,%20Cambridge%20university%20press.&f=false>`_.
+
+
+.. |Example| image:: https://raw.githubusercontent.com/lrq3000/pyFileFixity/master/tux-example.jpg
+   :scale: 60 %
+   :alt: Image corruption and repair example
+.. |PyPI-Status| image:: https://img.shields.io/pypi/v/pyfilefixity.svg
+   :target: https://pypi.org/project/pyfilefixity
+.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/pyfilefixity.svg?logo=python&logoColor=white
+   :target: https://pypi.org/project/pyfilefixity
+.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/pyfilefixity.svg?label=pypi%20downloads&logo=python&logoColor=white
+   :target: https://pypi.org/project/pyfilefixity
+.. |Build-Status| image:: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml/badge.svg?event=push
+   :target: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml
+.. |Coverage| image:: https://codecov.io/github/lrq3000/pyFileFixity/coverage.svg?branch=master
+   :target: https://codecov.io/github/lrq3000/pyFileFixity?branch=master
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pyFileFixity-3.1.1/README.rst` & `pyFileFixity-3.1.4/pyFileFixity.egg-info/PKG-INFO`

 * *Files 18% similar despite different names*

```diff
@@ -1,1077 +1,1317 @@
-pyFileFixity
-============
-
-|PyPI-Status| |PyPI-Versions| |PyPI-Downloads|
-
-|Build-Status| |Coverage|
-
-pyFileFixity provides a suite of open source, cross-platform, easy
-to use and easy to maintain (readable code) to protect and manage data
-for long term storage/archival, and also test the performance of any data protection algorithm.
-
-The project is done in pure-Python to meet those criteria,
-although cythonized extensions are available for core routines to speed up encoding/decoding,
-but always with a pure python specification available so as to allow long term replication.
-
-Here is an example of what pyFileFixity can do:
-
-|Example|
-
-On the left, this is the original image.
-
-At the center, the same image but
-with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
-which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
-Only a few corrupted bytes are enough to make the image looks like totally
-unrecoverable, and yet we are lucky, because the image could be unreadable at all
-if any of the "magic bytes" were to be corrupted!
-
-At the right, the corrupted image was repaired using `header_ecc.py` of pyFileFixity.
-This repaired only the image header (ie, the first part of the file), so only the first
-3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
-the image looks like it's totally repaired! And the best thing is that it only costed the generation
-of a "ecc repair file", which size is only 3.3KB (17% of the original file)!
-
-This works because most files will store the most important information to read them at
-their beginning, also called "file's header", so repairing this part will almost always ensure
-the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
-you can read it).
-
-Of course, you can also protect the whole file, not only the header, using pyFileFixity's
-`structural_adaptive_ecc.py`. You can also detect any corruption using `rfigc.py`.
-
-------------------------------------------
-
-.. contents:: Table of contents
-   :backlinks: top
-
-Quickstart
-----------
-
-Runs on Python 3 up to Python 3.12-dev. PyPy 3 is also supported.
-
-- To install or update on Python 3:
-
-``pip install --upgrade pyfilefixity``
-
-- For Python 2.7, the latest working version was v3.0.2:
-
-``pip install --upgrade pyfilefixity==3.0.2 reedsolo==1.7.0 unireedsolomon==1.0.5``
-
-- Once installed, the suite of tools can be accessed from a centralized interface script called ``pff`` which provides several subcommands, to list them:
-
-``pff --help``
-
-You should see:
-
-::
-
-    usage: pff [-h]
-               {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
-               ...
-
-    positional arguments:
-      {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
-        hash (rfigc)        Check files integrity fast by hash, size, modification date or by data structure integrity.
-        header (header_ecc, hecc)
-                            Protect/repair files headers with error correction codes
-        whole (structural_adaptive_ecc, saecc, protect, repair)
-                            Protect/repair whole files with error correction codes
-        recover (repair_ecc, recc)
-                            Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery
-                            mode (note: this does NOT recover your files, only the ecc files, which may then be used to
-                            recover your files!)
-        dup (replication_repair)
-                            Repair files from multiple copies of various storage mediums using a majority vote
-        restest (resilience_tester)
-                            Run tests to quantify robustness of a file protection scheme (can be used on any, not just
-                            pyFileFixity)
-        filetamper          Tamper files using various schemes
-        speedtest (ecc_speedtest)
-                            Run error correction encoding and decoding speedtests
-
-    options:
-      -h, --help            show this help message and exit
-
-- Every subcommands provide their own more detailed help instructions, eg for the ``hash`` submodule:
-
-``pff hash --help``
-
-- To generate a monitoring database (to later check very fast which files are corrupted, but cannot repair anything but filesystem metadata):
-
-``pff hash -i "your_folder" -d "dbhash.csv" -g -f -l "log.txt"``
-
-Note: this also works for a single file, just replace "your_folder" by "your_file.ext".
-
-- Later, to check which files were corrupted:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s -e errors.csv``
-
-- To use this monitoring database to recover filesystem metadata such as files names and directory layout by filescraping from files contents:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l "log.txt" -o "output_folder" --filescraping_recovery``
-
-- To protect files headers with a file called ``hecc.txt``:
-
-``pff header -i "your_folder" -d "hecc.txt" -l "log.txt" -g -f --ecc_algo 3``
-
-- To repair files headers and store the repaired files in ``output_folder``:
-
-``pff header -i "your_folder" -d "hecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
-
-- To protect whole files with a file called ``ecc.txt``:
-
-``pff whole -i "your_folder" -d "ecc.txt" -l "log.txt" -g -f -v --ecc_algo 3``
-
-- To repair whole files:
-
-``pff whole -i "your_folder" -d "ecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
-
-Note that ``header`` and ``whole`` can also detect corrupted files and even which blocks inside a file, but they are much slower than ``hash``.
-
-- To try to recover a damaged ecc file ``ecc.txt`` using an index file ``ecc.txt.idx`` (index file is generated automatically with ecc.txt):
-
-``pff recovery -i "ecc.txt" --index "ecc.txt.idx" -o "ecc_repaired.txt" -l "log.txt" -v -f``
-
-- To try to recover a damaged ecc file ``ecc.txt`` without an index file (you can tweak the ``-t`` parameter from 0.0 to 1.0, 1.0 producing many false positives):
-
-``pff recovery -i "ecc.txt" -o "ecc_repaired.txt" -l "log.txt" -v -f -t 0.4``
-
-- To repair your files using multiple duplicated copies that you have stored on different mediums:
-
-``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" --report "rlog.csv" -f -v``
-
-- If you have previously generated a rfigc database, you can use it to enhance the replication repair:
-
-``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" -d "dbhash.csv" --report "rlog.csv" -f -v``
-
-- To run tests on your recovery tools, you can make a Makefile-like configuration file and use the Resiliency Tester submodule:
-
-``pff restest -i "your_folder" -o "test_folder" -c "resiliency_tester_config.txt" -m 3 -l "testlog.txt" -f``
-
-- Internally, ``pff restest`` uses ``pff filetamper`` to tamper files with various schemes, but you can also use ``pff filetamper`` directly.
-
-- To run speedtests of encoding/decoding error correction codes on your machine:
-
-``pff speedtest``
-
-- In case the ``pff`` command does not work, it can be replaced with ``python -m pyFileFixity.pff`` .
-
-The problem of long term storage
---------------------------------
-
-Why are data corrupted with time? One sole reason: entropy.
-Entropy refers to the universal tendency for systems to become
-less ordered over time. Data corruption is exactly that: a disorder
-in bits order. In other words: *the Universe hates your data*.
-
-Long term storage is thus a very difficult topic: it's like fighting with
-death (in this case, the death of data). Indeed, because of entropy,
-data will eventually fade away because of various silent errors such as
-bit rot or cosmic rays. pyFileFixity aims to provide tools to detect any data
-corruption, but also fight data corruption by providing repairing tools.
-
-The only solution is to use a principle of engineering that is long
-known and which makes bridges and planes safe: add some **redundancy**.
-
-There are only 2 ways to add redundancy:
-
--  the simple way is to **duplicate** the object (also called replication),
-   but for data storage, this eats up a lot of storage and is not optimal.
-   However, if storage is cheap, then this is a good solution, as it is
-   much faster than encoding with error correction codes. For replication to work,
-   at least 3 duplicates are necessary at all times, so that if one fails, it must
-   replaced asap. As sailors say: "Either bring 1 compass or 3 compasses, but never
-   two, because then you won't know which one is correct if one fails."
-   Indeed, with 3 duplicates, if you frequently monitor their integrity
-   (eg, with hashes), then if one fails, simply do a majority vote:
-   the bit value given by 2 of the duplicates is probably correct.
--  the second way, the optimal tools ever invented to recover
-   from data corruption, are the **error correction codes** (forward
-   error correction), which are a way to smartly produce redundant codes
-   from your data so that you can later repair your data using these
-   additional pieces of information (ie, an ECC generates n blocks for a
-   file cut in k blocks (with k < n), and then the ecc code can rebuild
-   the whole file with (at least) any k blocks among the total n blocks
-   available). In other words, you can correct up to (n-k) erasures. But
-   error correcting codes can also detect and repair automatically where
-   the errors are (fully automatic data repair for you !), but at the
-   cost that you can then only correct (n-k)/2 errors.
-
-Error correction can seem a bit magical, but for a reasonable intuition,
-it can be seen as a way to average the corruption error rate: on
-average, a bit will still have the same chance to be corrupted, but
-since you have more bits to represent the same data, you lower the
-overall chance to lose this bit.
-
-The problem is that most theoretical and pratical works on error
-correcting codes has been done almost exclusively on channel
-transmission (such as 4G, internet, etc.), but not on data storage,
-which is very different for one reason: whereas in a channel we are in a
-spatial scheme (both the sender and the receiver are different entities
-in space but working at the same timescale), in data storage this is a
-temporal scheme: the sender was you storing the data on your medium at
-time t, and the receiver is again you but now retrieving the data at
-time t+x. Thus, the sender does not exist anymore, thus you cannot ask
-the sender to send again some data if it's too much corrupted: in data
-storage, if a data is corrupted, it's lost for good, whereas in channel theory,
-parts of the data can be submitted again if necessary.
-
-Some attempts were made to translate channel theory and error correcting
-codes theory to data storage, the first being Reed-Solomon which spawned
-the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
-devised for use on optical discs to recover from scratches, which was
-necessary for the technology to be usable for consumers. Since then, new
-less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
-fountain codes such as RaptorQ were invented (or rediscovered), but they
-are still marginally researched for data storage.
-
-This project aims to, first, implement easy tools to evaluate strategies
-(filetamper.py) and file fixity (ie, detect if there are corruptions),
-and then the goal is to provide an open and easy framework to use
-different kinds of error correction codes to protect and repair files.
-
-Also, the ecc file specification is made to be simple and resilient to
-corruption, so that you can process it by your own means if you want to,
-without having to study for hours how the code works (contrary to PAR2
-format).
-
-In practice, both approaches are not exclusive, and the best is to
-combine them: protect the most precious data with error correction codes,
-then duplicate them as well as less sensitive data across multiple storage mediums.
-Hence, this suite of data protection tools, just like any other such suite, is not
-sufficient to guarantee your data is protected, you must have an active data curation
-strategy which includes regularly checking your data and replacing copies that are damaged.
-
-For a primer on storage mediums and data protection strategies, see `this post I wrote <https://web.archive.org/web/20220529125543/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`_.
-
-Why not just use RAID ?
------------------------
-
-RAID is clearly insufficient for long-term data storage, and in fact it
-was primarily meant as a cheap way to get more storage (RAID0) or more
-availability (RAID1) of data, not for archiving data, even on a medium
-timescale:
-
--  RAID 0 is just using multiple disks just like a single one, to extend
-   the available storage. Let's skip this one.
--  RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
-   That's completely useless for long term storage: if either disk
-   fails, or if both disks are partially corrupted, you can't know what
-   are the correct data and which aren't. As an old saying goes: "Never
-   take 2 compasses: either take 3 or 1, because if both compasses show
-   different directions, you will never know which one is correct, nor
-   if both are wrong." That's the principle of Triplication.
--  RAID 5 is based on the triplication idea: you have n disks (but least
-   3), and if one fails you can recover n-1 disks (resilient to only 1
-   disk failure, not more).
--  RAID 6 is an extension of RAID 5 which is closer to error-correction
-   since you can correct n-k disks. However, most (all?) currently
-   commercially available RAID6 devices only implements recovery for at
-   most n-2 (2 disks failures).
--  In any case, RAID cannot detect silent errors automatically, thus you
-   either have to regularly scan, or you risk to lose some of your data
-   permanently, and it's far more common than you can expect (eg, with
-   RAID5, it is enough to have 2 silent errors on two disks on the same
-   bit for the bit to be unrecoverable). That's why a limit of only 1 or
-   2 disks failures is just not enough.
-
-On the opposite, ECC can correct n-k disks (or files). You can configure
-n and k however you want, so that for example you can set k = n/2, which
-means that you can recover all your files from only half of them! (once
-they are encoded with an ecc file of course).
-
-There also are new generation RAID solutions, mainly software based,
-such as SnapRAID or ZFS, which allow you to configure a virtual RAID
-with the value n-k that you want. This is just like an ecc file (but a
-bit less flexible, since it's not a file but a disk mapping, so that you
-can't just copy it around or upload it to a cloud backup hosting). In
-addition to recover (n-k) disks, they can also be configured to recover
-from partial, sectors failures inside the disk and not just the whole
-disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
-and James L. Hafner. "SD codes: erasure codes designed for how storage
-systems really fail." FAST. 2013.).
-
-The other reason RAID is not adapted to long-term storage, is that it
-supposes you store your data on hard-drives exclusively. Hard drives
-aren't a good storage medium for the long term, for two reasons:
-
-| 1- they need a regular plug to keep the internal magnetic disks
-  electrified (else the data will just fade away when there's no
-  residual electricity).
-| 2- the reading instrument is directly included and merged with the
-  data (this is the green electronic board you see from the outside, and
-  the internal head). This is good for quick consumer use (don't need to
-  buy another instrument: the HDD can just be plugged and it works), but
-  it's very bad for long term storage, because the reading instrument is
-  bound to fail, and a lot faster than the data can fade away: this
-  means that even if your magnetic disks inside your HDD still holds
-  your data, if the controller board or the head doesn't work anymore,
-  your data is just lost. And a head (and a controller board) are almost
-  impossible to replace, even by professionals, because the pieces are
-  VERY hard to find (different for each HDD production line) and each
-  HDD has some small physical defects, thus it's impossible to reproduce
-  that too (because the head is so close to the magnetic disk that if
-  you try to do that manually you'll probably fail).
-
-In the end, it's a lot better to just separate the storage medium of
-data, with the reading instrument. The medium I advise is optical disks
-(whether it's BluRay, DVD, CD or whatever), because the reading
-instrument is separate, and the technology (laser reflecting on bumps
-and/or pits) is kind of universal, so that even if the technology is
-lost one day (deprecated by newer technologies, so that you can't find
-the reading instrument anymore because it's not sold anymore), you can
-probably emulate a laser using some software to read your optical disk,
-just like what the CAMiLEON project did to recover data from the
-LaserDiscs of the BBC Domesday Project (see Wikipedia).
-
-Applications included
----------------------
-
-The project currently include the following pure-python applications:
-
--  rfigc.py (subcommand: ``hash``), a hash auditing tool, similar to md5deep/hashdeep, to
-   compute a database of your files along with their metadata, so that
-   later you can check if they were changed/corrupted.
-
--  header\_ecc.py (subcommand: ``header``), an error correction code using Reed-Solomon
-   generator/corrector for files headers. The idea is to supplement
-   other more common redundancy tools such as PAR2 (which is quite
-   reliable), by adding more resiliency only on the critical parts of
-   the files: their headers. Using this script, you can significantly
-   higher the chance of recovering headers, which will allow you to at
-   least open the files.
-
--  structural\_adaptive\_ecc.py (subcommand: ``whole``), a variable error correction rate
-   encoder (kind of a generalization of header\_ecc.py). This script
-   allows to generate an ecc file for the whole content of your files,
-   not just the header part, using a variable resilience rate: the
-   header part will be the most protected, then the rest of each file
-   will be progressively encoded with a smaller and smaller resilience
-   rate. The assumption is that important information is stored first,
-   and then data becomes less and less informative (and thus important,
-   because the end of the file describes less important details). This
-   assumption is very true for all compressed kinds of formats, such as
-   JPG, ZIP, Word, ODT, etc...
-
--  repair\_ecc.py (subcommand: ``recovery``), a script to repair the structure (ie, the entry and
-   fields markers/separators) of an ecc file generated by header\_ecc.py
-   or structural\_adaptive\_ecc.py. The goal is to enhance the
-   resilience of ecc files against corruption by ensuring that their
-   structures can be repaired (up to a certain point which is very high
-   if you use an index backup file, which is a companion file that is
-   generated along an ecc file).
-
--  filetamper.py (subcommand: ``filetamper``) is a quickly made file corrupter, it will erase or
-   change characters in the specified file. This is useful for testing
-   your various protecting strategies and file formats (eg: is PAR2
-   really resilient against corruption? Are zip archives still partially
-   extractable after corruption or are rar archives better? etc.). Do
-   not underestimate the usefulness of this tool, as you should always
-   check the resiliency of your file formats and of your file protection
-   strategies before relying on them.
-
--  replication\_repair.py (subcommand: ``dup``) takes advantage of your multiple copies
-   (replications) of your data over several storage mediums to recover
-   your data in case it gets corrupted. The goal is to take advantage of
-   the storage of your archived files into multiple locations: you will
-   necessarily make replications, so why not use them for repair?
-   Indeed, it's good practice to keep several identical copies of your data
-   on several storage mediums, but in case a corruption happens,
-   usually you will just drop the corrupted copies and keep the intacts ones.
-   However, if all copies are partially corrupted, you're stuck. This script
-   aims to take advantage of these multiple copies to recover your data,
-   without generating a prior ecc file. It works simply by reading through all
-   your different copies of your data, and it casts a majority vote over each
-   byte: the one that is the most often occuring will be kept. In engineering,
-   this is a very common strategy used for very reliable systems such as
-   space rockets, and is called "triple-modular redundancy", because you need
-   at least 3 copies of your data for the majority vote to work (but the more the
-   better).
-
--  resiliency\_tester.py (subcommand: ``restest``) allows you to test the robustness of the
-   corruption correction of the scripts provided here (or any other
-   command-line app). You just have to copy the files you want to test inside a
-   folder, and then the script will copy the files into a test tree, then it
-   will automatically corrupt the files randomly (you can change the parameters
-   like block burst and others), then it will run the file repair command-lines
-   you supply and finally some stats about the repairing power will be
-   generated. This allows you to easily and objectively compare different set
-   of parameters, or even different file repair solutions, on the very data
-   that matters to you, so that you can pick the best option for you.
-
--  ecc\_speedtest.py (subcommand: ``speedtest``) is a simple error correction codes
-   encoder/decoder speedtest. It allows to easily change parameters for the test.
-   This allows to assess how fast your machine can encode/decode with the selected
-   parameters, which can be especially useful to plan ahead for how many files you
-   can reasonably plan to protect with error correction codes (which are time consuming).
-
--  DEPRECATED: easy\_profiler.py is just a quick and simple profiling tool to get
-   you started quickly on what should be optimized to get more speed, if
-   you want to contribute to the project feel free to propose a pull
-   request! (Cython and other optimizations are welcome as long as they
-   are cross-platform and that an alternative pure-python implementation
-   is also available).
-
-Note that all tools are primarily made for command-line usage (type
-script.py --help to get extended info about the accepted arguments), but
-you can also use rfigc.py and header\_ecc.py with a GUI by using the
---gui argument (must be the first and only one argument supplied). The
-GUI is provided as-is and minimal work will be done to maintain it (the
-focus will stay on functionality rather than ergonomy).
-
-IMPORTANT: it is CRITICAL that you use the same parameters for
-correcting mode as when you generated the database/ecc files (this is
-true for all scripts in this bundle). Of course, some options must be
-changed: -g must become -c to correct, and --update is a particular
-case. This works this way on purpose for mainly two reasons: first
-because it is very hard to autodetect the parameters from a database
-file alone and it would produce lots of false positives, and secondly
-(the primary reason) is that storing parameters inside the database file
-is highly unresilient against corruption (if this part of the database
-is tampered, the whole becomes unreadable, while if they are stored
-outside or in your own memory, the database file is always accessible).
-Thus, it is advised to write down the parameters you used to generate
-your database directly on the storage media you will store your database
-file on (eg: if it's an optical disk, write the parameters on the cover
-or directly on the disk using a marker), or better memorize them by
-heart. If you forget them, don't panic, the parameters are always stored
-as comments in the header of the generated ecc files, but you should try
-to store them outside of the ecc files anyway.
-
-For users: what's the advantage of pyFileFixity?
-------------------------------------------------
-
-Pros:
-
--  Open application and open specifications under the MIT license (you
-   can do whatever you want with it and tailor it to your needs if you
-   want to, or add better decoding procedures in the future as science
-   progress so that you can better recover your data from your already
-   generated ecc file).
--  Highly reliable file fixity watcher: rfigc.py will tell you without
-   any ambiguity using several attributes if your files have been
-   corrupted or not, and can even check for images if the header is
-   valid (ie: if the file can still be opened).
--  Readable ecc file format (compared to PAR2 and most other similar
-   specifications).
--  Highly resilient ecc file format against corruption (not only are
-   your data protected by ecc, the ecc file is protected too against
-   critical spots, both because there is no header so that each track is
-   independent and if one track is corrupted beyond repair then other
-   ecc tracks can still be read, and a .idx file will be generated to
-   repair the structure of the ecc file to recover all tracks).
--  Very safe and conservative approach: the recovery process checks that
-   the recovery was successful before committing a repaired block.
--  Partial recovery allowed (even if a file cannot be completely
-   recovered, the parts that can will be repaired and then the rest that
-   can't be repaired will be recopied from the corrupted version).
--  Support directory processing: you can encode an ecc file for a whole
-   directory of files (with any number of sub-directories and depth).
--  No limit on the number of files, and it can recursively protect files
-   in a directory tree.
--  Variable resiliency rate and header-only resilience, ensuring that
-   you can always open your files even if partially corrupted (the
-   structure of your files will be saved, so that you can use other
-   softwares to repair beyond if this set of script is not sufficient to
-   totally repair).
--  Support for erasures (null bytes) and even errors-and-erasures, which
-   literally doubles the repair capabilities. To my knowledge, this is
-   the only freely available parity software that supports erasures.
--  Display the predicted total ecc file size given your parameters,
-   and the total time it will take to encode/decode.
--  Your original files are still accessible as they are, protection files
-   such as ecc files live alongside your original data. Contrary to
-   other data protection schemes such as PAR2 which encode the whole
-   data in par archive files that replace your original files and
-   are not readable without decoding.
--  Opensourced under the very permissive MIT licence, do whatever you
-   want!
-
-Cons:
-
--  Cannot protect meta-data, such as folders paths. The paths are
-   stored, but cannot be recovered (yet? feel free to contribute if you
-   know how). Only files are protected. Thus if your OS or your storage
-   medium crashes and truncate a whole directory tree, the directory
-   tree can't be repaired using the ecc file, and thus you can't access
-   the files neither. However, you can use file scraping to extract the
-   files even if the directory tree is lost, and then use RFIGC.py to
-   reorganize your files correctly. There are alternatives, see the
-   chapters below: you can either package all your files in a single
-   archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
-   DVDisaster as an alternative solution, which is an ecc generator with
-   support for directory trees meta-data (but only on optical disks).
--  Can only repair errors and erasures (characters that are replaced by
-   another character), not deletion nor insertion of characters. However
-   this should not happen with any storage medium (truncation can occur
-   if the file bounds is misdetected, in this case pyFileFixity can
-   partially repair the known parts of the file, but cannot recover the
-   rest past the truncation, except if you used a resiliency rate of at
-   least 0.5, in which case any message block can be recreated with only
-   using the ecc file).
--  Cannot recreate a missing file from other available files (except you
-   have set a resilience\_rate at least 0.5), contrary to Parchives
-   (PAR1/PAR2). Thus, you can only repair a file if you still have it
-   (and its ecc file!) on your filesystem. If it's missing, pyFileFixity
-   cannot do anything (yet, this will be implemented in the future).
-
-Note that the tools were meant for data archival (protect files that you
-won't modify anymore), not for system's files watching nor to protect
-all the files on your computer. To do this, you can use a filesystem
-that directly integrate error correction code capacity, such as ZFS.
-
-Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)
-------------------------------------------------------------------------------
-
-Recursively generate or check the integrity of files by MD5 and SHA1
-hashes, size, modification date or by data structure integrity (only for
-images).
-
-This script is originally meant to be used for data archival, by
-allowing an easy way to check for silent file corruption. Thus, this
-script uses relative paths so that you can easily compute and check the
-same redundant data copied on different mediums (hard drives, optical
-discs, etc.). This script is not meant for system files corruption
-notification, but is more meant to be used from times-to-times to check
-up on your data archives integrity (if you need this kind of application,
-see `avpreserve's fixity <https://github.com/avpreserve/fixity>`_).
-
-Example usage
-~~~~~~~~~~~~~
-
--  To generate the database (only needed once):
-
-``pff hash -i "your_folder" -d "dbhash.csv" -g``
-
--  To check:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s``
-
--  To update your database by appending new files:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -u -a``
-
--  To update your database by appending new files AND removing
-   inexistent files:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -u -a -r``
-
-Note that by default, the script is by default in check mode, to avoid
-wrong manipulations. It will also alert you if you generate over an
-already existing database file.
-
-Arguments
-~~~~~~~~~
-
-::
-
-      -h, --help            show a help message and exit
-      -i /path/to/root/folder, --input /path/to/root/folder
-                            Path to the root folder from where the scanning will occ
-    ur.
-      -d /some/folder/databasefile.csv, --database /some/folder/databasefile.csv
-                            Path to the csv file containing the hash informations.
-      -l /some/folder/filename.log, --log /some/folder/filename.log
-                            Path to the log file. (Output will be piped to both the
-    stdout and the log file)
-      -s, --structure_check
-                            Check images structures for corruption?
-      -e /some/folder/errorsfile.csv, --errors_file /some/folder/errorsfile.csv
-                            Path to the error file, where errors at checking will be
-     stored in CSV for further processing by other softwares (such as file repair so
-    ftwares).
-      -m, --disable_modification_date_checking
-                            Disable modification date checking.
-      --skip_missing        Skip missing files when checking (useful if you split yo
-    ur files into several mediums, for example on optical discs with limited capacit
-    y).
-      -g, --generate        Generate the database? (omit this parameter to check ins
-    tead of generating).
-      -f, --force           Force overwriting the database file even if it already e
-    xists (if --generate).
-      -u, --update          Update database (you must also specify --append or --rem
-    ove).
-      -a, --append          Append new files (if --update).
-      -r, --remove          Remove missing files (if --update).
-      
-      --filescraping_recovery          Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.
-      -o, --output          Path to the output folder where to output the files reorganized after --recover_from_filescraping.
-
-Header Error Correction Code script
------------------------------------
-
-This script was made to be used in combination with other more common
-file redundancy generators (such as PAR2, I advise MultiPar). This is an
-additional layer of protection for your files: by using a higher
-resiliency rate on the headers of your files, you ensure that you will
-be probably able to open them in the future, avoiding the "critical
-spots", also called "fracture-critical" in redundancy engineering (where
-if you modify just one bit, your whole file may become unreadable,
-usually bits residing in the headers - in other words, a single blow
-makes the whole thing collapse, just like non-redundant bridges).
-
-An interesting benefit of this approach is that it has a low storage
-(and computational) overhead that scales linearly to the number of
-files, whatever their size is: for example, if we have a set of 40k
-files for a total size of 60 GB, with a resiliency\_rate of 30% and
-header\_size of 1KB (we limit to the first 1K bytes/characters = our
-file header), then, without counting the hash per block and other
-meta-data, the final ECC file will be about 2 \* resiliency\_rate \*
-number\_of\_files \* header\_size = 24.5 MB. This size can be lower if
-there are many files smaller than 1KB. This is a pretty low storage
-overhead to backup the headers of such a big number of files.
-
-The script is pure-python as are its dependencies: it is thus completely
-cross-platform and open source. The default ecc algo
-(ecc_algo=3 uses `reedsolo <https://github.com/tomerfiliba-org/reedsolomon>`_)
-also provides a speed-optimized C-compiled implementation (``creedsolo``) that will be used
-if available for the user's platform, so pyFileFixity should be fast by default.
-Alternatively, it's possible to use a JIT compiler such as PyPy,
-although this means that ``creedsolo`` will not be useable, so PyPy
-may accelerate other functions but slower ecc encoding/decoding.
-
-Structural Adaptive Error Correction Encoder
---------------------------------------------
-
-This script implements a variable error correction rate encoder: each
-file is ecc encoded using a variable resiliency rate -- using a high
-constant resiliency rate for the header part (resiliency rate stage 1,
-high), then a variable resiliency rate is applied to the rest of the
-file's content, with a higher rate near the beginning of the file
-(resiliency rate stage 2, medium) which progressively decreases until
-the end of file (resiliency rate stage 3, the lowest).
-
-The idea is that the critical parts of files usually are placed at the
-top, and data becomes less and less critical along the file. What is
-meant by critical is both the critical spots (eg: if you tamper only one
-character of a file's header you have good chances of losing your entire
-file, ie, you cannot even open it) and critically encoded information
-(eg: archive formats usually encode compressed symbols as they go along
-the file, which means that the first occurrence is encoded, and then the
-archive simply writes a reference to the symbol. Thus, the first
-occurrence is encoded at the top, and subsequent encoding of this same
-data pattern will just be one symbol, and thus it matters less as long
-as the original symbol is correctly encoded and its information
-preserved, we can always try to restore the reference symbols later).
-Moreover, really redundant data will be placed at the top because they
-can be reused a lot, while data that cannot be too much compressed will
-be placed later, and thus, corruption of this less compressed data is a
-lot less critical because only a few characters will be changed in the
-uncompressed file (since the data is less compressed, a character change
-on the not-so-much compressed data won't have very significant impact on
-the uncompressed data).
-
-This variable error correction rate should allow to protect more the
-critical parts of a file (the header and the beginning of a file, for
-example in compressed file formats such as zip or jpg this is where the
-most importantly strings are encoded) for the same amount of storage as
-a standard constant error correction rate.
-
-Of course, you can set the resiliency rate for each stage to the values
-you want, so that you can even do the opposite: setting a higher
-resiliency rate for stage 3 than stage 2 will produce an ecc that is
-greater towards the end of the contents of your files.
-
-Furthermore, the currently designed format of the ecc file would allow
-two things that are not available in all current file ecc generators
-such as PAR2:
-
-1. it allows to partially repair a file, even if not all
-the blocks can be corrected (in PAR2, a file is repaired only if all
-blocks can be repaired, which is a shame because there are still other
-blocks that could be repaired and thus produce a less corrupted file) ;
-
-2. the ecc file format is quite simple and readable, easy to process by
-any script, which would allow other softwares to also work on it (and it
-was also done in this way to be more resilient against error
-corruptions, so that even if an entry is corrupted, other entries are
-independent and can maybe be used, thus the ecc is very error tolerant.
-This idea was implemented in repair\_ecc.py but it could be extended,
-especially if you know the pattern of the corruption).
-
-The script structural-adaptive-ecc.py implements this idea, which can be
-seen as an extension of header-ecc.py (and in fact the idea was the
-other way around: structural-adaptive-ecc.py was conceived first but was
-too complicated, then header-ecc.py was implemented as a working
-lessened implementation only for headers, and then
-structural-adaptive-ecc.py was finished using header-ecc.py code
-progress). It works, it was a quite well tested for my own needs on
-datasets of hundred of GB, but it's not foolproof so make sure you test
-the script by yourself to see if it's robust enough for your needs (any
-feedback about this would be greatly appreciated!).
-
-ECC Algorithms
---------------
-
-You can specify different ecc algorithms using the ``--ecc_algo`` switch.
-
-For the moment, only Reed-Solomon is implemented, but it's universal
-so you can modify its parameters in lib/eccman.py.
-
-Two Reed-Solomon codecs are available, they are functionally equivalent
-and thoroughly unit tested.
-
--  ``--ecc_algo 1``: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
-   This is the slowest implementation (but also the most easy code to understand).
--  ``--ecc_algo 2``: same as algo 1 but with a faster functions.
--  ``--ecc_algo 3``: use the second codec, which is the fastest.
-   The generated ECC will be compatible with algo 1 and 2.
--  ``--ecc_algo 4``: also use the second, fastest RS codec, but
-   with different parameters (US FAA ADSB UAT RS FEC norm),
-   thus the generated ECC won't be compatible with algo 1 to 3.
-   But do not be scared, the ECC will work just the same.
-
-Note about speed: Also, use a smaller --max\_block\_size to greatly
-speedup the operations! That's the trick used to compute very quickly RS
-ECC on optical discs. You give up a bit of resiliency of course (because
-blocks are smaller, thus you protect a smaller number of characters per
-ECC. In the end, this should not change much about real resiliency, but
-in case you get a big bit error burst on a contiguous block, you may
-lose a whole block at once. That's why using RS255 is better, but it's
-very time consuming. However, the resiliency ratios still hold, so for
-any other case of bit-flipping with average-sized bursts, this should
-not be a problem as long as the size of the bursts is smaller than an
-ecc block.)
-
-In case of a catastrophic event
--------------------------------
-
-TODO: write more here
-
-In case of a catastrophic event of your data due to the failure of your
-storage media (eg: your hard drive crashed), then follow the following
-steps:
-
-1- use dd\_rescue to make a full bit-per-bit verbatim copy of your drive
-before it dies. The nice thing with dd\_rescue is that the copy is
-exact, and also that it can retries or skip in case of bad sectors (it
-won't crash on your suddenly at half the process).
-
-2- Use testdisk to restore partition or to copy files based on partition
-filesystem informations.
-
-3- If you could not recover your files, you can try file scraping using
-`photorec <http://www.cgsecurity.org/wiki/PhotoRec>`_ or
-`plaso  <http://plaso.kiddaland.net/>`_ other similar tools as
-a last resort to extract data based only from files content (no filename,
-often uncorrect filetype, file boundaries may be wrong so some data
-may be cut off, etc.).
-
-4- If you used pyFileFixity before the failure of your storage media,
-you can then use your pre-computed databases to check that files are
-intact (rfigc.py) and if they aren't, you can recover them (using
-header\_ecc.py and structural\_adaptive\_ecc.py). It can also help if
-you recovered your files via data scraping, because your files will be
-totally unorganized, but you can use a previously generated database
-file to recover the full names and directory tree structure using
-rfigc.py --filescraping\_recover.
-
-Also, you can try to fix some of your files using specialized repairing
-tools (but remember that such tool cannot guarantee you the same
-recovering capacity as an error correction code - and in addition, error
-correction code can tell you when it has recovered successfully). For
-example:
-
--  for tar files, you can use `fixtar <https://github.com/BestSolution-at/fixtar>`_.
-   Similar tools (but older): `tarfix <http://www.dmst.aueb.gr/dds/sw/unix/tarfix/>`_
-   and `tar-repair <https://www.datanumen.com/tar-repair/>`_.
--  for RAID mounting and recovery, you can use "Raid faster - recover
-   better" (rfrb) tool by Sabine Seufert and Christian Zoubek:
-   https://github.com/lrq3000/rfrb
--  if your unicode strings were mangled (ie, you see weird symbols),
-   try this script that will automatically demangle them:
-   https://github.com/LuminosoInsight/python-ftfy
--  to repair tabular (2D) data such as .csv, try
-   `Carpenter <https://pypi.python.org/pypi/Carpenter/>`_.
--  tool to identify corrupted files in ddrescue images: 
-   `ddrescue-ffile <https://github.com/Salamek/ddrescue-ffile>`_
-
-Protecting directory tree meta-data
------------------------------------
-
-One main current limitation of pyFileFixity is that it cannot protect
-the directory tree meta-data. This means that in the worst case, if a
-silent error happens on the inode pointing to the root directory that
-you protected with an ecc, the whole directory will vanish, and all the
-files inside too. In less worst cases, sub-directories can vanish, but
-it's still pretty bad, and since the ecc file doesn't store any
-information about inodes, you can't recover the full path.
-
-The inability to store these meta-data is because of two choices in the
-design: 1- portability: we want the ecc file to work even if we move the
-root directory to another place or another storage medium (and of
-course, the inode would change), 2- cross-platform compatibility:
-there's no way to get and store directory meta-data for all platforms,
-but of course we could implement specific instructions for each main
-platform, so this point is not really a problem.
-
-To workaround this issue (directory meta-data are critical spots), other
-softwares use a one-time storage medium (ie, writing your data along
-with generating and writing the ecc). This way, they can access at
-the bit level the inode info, and they are guaranted that the inodes
-won't ever change. This is the approach taken by DVDisaster: by using
-optical mediums, it can compute inodes that will be permanent, and thus
-also encode that info in the ecc file. Another approach is to create a
-virtual filesystem specifically to store just your files, so that you
-manage the inode yourself, and you can then copy the whole filesystem
-around (which is really just a file, just like a zip file - which can
-also be considered as a mini virtual file system in fact) like
-`rsbep <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`_.
-
-Here the portability principle of pyFileFixity prevents this approach.
-But you can mimic this workaround on your hard drive for pyFileFixity to
-work: you just need to package all your files into one file. This way,
-you sort of create a virtual file system: inside the archive, files and
-directories have meta-data just like in a filesystem, but from the
-outside it's just one file, composed of bytes that we can just encode to
-generate an ecc file - in other words, we removed the inodes portability
-problem, since this meta-data is stored relatively inside the archive,
-the archive manage it, and we can just encode this info like any other
-stream of data! The usual way to make an archive from several files is
-to use TAR, but this will generate a solid archive which will prevent
-partial recovery. An alternative is to use DAR, which is a non-solid
-archive version of TAR, with lots of other features too. If you also
-want to compress, you can just use ZIP (with DEFLATE algorithm) your
-files (this also generates a non-solid archive). You can then use
-pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
-will then protect both your files just like before and the directories
-meta-data too now.
-
-Tools like pyFileFixity (or which can be used as complements)
--------------------------------------------------------------
-
-Here are some tools with a similar philosophy to pyFileFixity, which you
-can use if they better fit your needs, either as a replacement of
-pyFileFixity or as a complement (pyFileFixity can always be used to
-generate an ecc file):
-
--  `DAR (Disk ARchive) <http://dar.linux.free.fr/>`__: similar to tar
-   but non-solid thus allows for partial recovery and per-file access,
-   plus it saves the directory tree meta-data -- see catalog isolation
-   -- plus it can handle error correction natively using PAR2 and
-   encryption. Also supports incremental backup, thus it's a very nice
-   versatile tool. Crossplatform and opensource.
--  `DVDisaster <http://dvdisaster.net/>`__: error correction at the bit
-   level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
-   it also protects directory tree meta-data and is resilient to
-   corruption (v2 still has some critical spots but v3 won't have any).
--  rsbep tool that is part of dvbackup package in Debian: allows to
-   generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
-   for your backups, if you're on unix or using cygwin.
--  `rsbep modification by Thanassis
-   Tsiodras <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`__:
-   enhanced rsbep to avoid critical spots and faster speed. Also
-   includes a "freeze" script to encode your files into a virtual
-   filesystem (using Python/FUSE) so that even meta-data such as
-   directory tree are fully protected by the ecc. Great script, but not
-   maintained, it needs some intensive testing by someone knowledgeable
-   to guarantee this script is reliable enough for production.
--  Parchive (PAR1, PAR2, MultiPar): well known error correction file
-   generator. The big advantage of Parchives is that an ecc block
-   depends on multiple files: this allows to completely reconstruct a
-   missing file from scratch using files that are still available. Works
-   good for most people, but most available Parchive generators are not
-   satisfiable for me because 1- they do not allow to generate an ecc
-   for a directory tree recursively (except MultiPar, and even if it is
-   allowed in the PAR2 specs), 2- they can be very slow to generate
-   (even with multiprocessor extensions, because the galois field is
-   over 2^16 instead of 2^8, which is very costly), 3- the spec is not
-   very resilient to errors and tampering over the ecc file, as it
-   assumes the ecc file won't be corrupted (I also tested, it's still a
-   bit resilient, but it could be a lot more with some tweaking of the
-   spec), 4- it doesn't allow for partial recovery (recovering blocks
-   that we can and pass the others that are unrecoverable): with PAR2, a
-   file can be restored fully or it cannot be at all.
--  Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
-   create non-solid archives which are readable by most computers
-   (ubiquitous algorithm). Non-solid archive means that a zip file can
-   still unzip correct files even if it is corrupted, because files are
-   encoded in blocks, and thus even if some blocks are corrupted, the
-   decoding can happen. A `fast implementation with enhanced compression
-   is available in pure Go <https://github.com/klauspost/compress>`__
-   (good for long storage).
--  TestDisk: for file scraping, when nothing else worked.
--  dd\_rescue: for disk scraping (allows to forcefully read a whole disk
-   at the bit level and copy everything it can, passing bad sector with
-   options to retry them later on after a first full pass over the
-   correct sectors).
--  ZFS: a file system which includes ecc correction directly. The whole
-   filesystem, including directory tree meta-data, are protected. If you
-   want ecc protection on your computer for all your files, this is the
-   way to go.
--  Encryption: technically, you can encrypt your files without losing
-   too much redundancy, as long as you use an encryption scheme that is
-   block-based such as DES: if one block gets corrupted, it won't be
-   decryptable, but the rest of the files' encrypted blocks should be
-   decryptable without any problem. So encrypting with such algorithms
-   leads to similar files as non-solid archives such as deflate zip. Of
-   course, for very long term storage, it's better to avoid encryption
-   and compression (because you raise the information contained in a
-   single block of data, thus if you lose one block, you lose more
-   data), but if it's really necessary to you, you can still maintain
-   high chances of recovering your files by using block-based
-   encryption/compression (note: block-based encryption can
-   be seen as the equivalent of non-solid archives for compression,
-   because the data is compressed/encrypted in independent blocks,
-   thus allowing partial uncompression/decryption).
--  `SnapRAID <http://snapraid.sourceforge.net/>`__
--  `par2ools <https://github.com/jmoiron/par2ools>`__: a set of
-   additional tools to manage par2 archives
--  `Checkm <https://pypi.python.org/pypi/Checkm/0.4>`__: a tool similar
-   to rfigc.py
--  `BagIt <https://en.wikipedia.org/wiki/BagIt>`__ with two python
-   implementations `here <https://pypi.python.org/pypi/pybagit/>`__ and
-   `here <https://pypi.python.org/pypi/bagit/>`__: this is a file
-   packaging format for sharing and storing archives for long term
-   preservation, it just formalizes a few common procedures and meta
-   data that are usually added to files for long term archival (such as
-   MD5 digest).
--  `RSArmor <https://github.com/jap/rsarm>`__ a tool based on
-   Reed-Solomon to encode binary data files into hexadecimal, so that
-   you can print the characters on paper. May be interesting for small
-   datasets (below 100 MB).
--  `Ent <https://github.com/lsauer/entropy>`__ a tool to analyze the
-   entropy of your files. Can be very interesting to optimize the error
-   correction algorithm, or your compression tools.
--  `HashFS <https://pypi.python.org/pypi/hashfs/>`_ is a non-redundant,
-   duplication free filesystem, in Python. **Data deduplication** is very
-   important for large scale long term storage: since you want your data
-   to be redundant, this means you will use an additional storage space
-   for your redundant copies that will be proportional to your original data.
-   Having duplicated data will consume more storage and more processing
-   time, for no benefit. That's why it's a good idea to deduplicate your data
-   prior to create redundant copies: this will be faster and save you money.
-   Deduplication can either be done manually (by using duplicates removers)
-   or systematically and automatically using specific filesystems such as
-   zfs (with deduplication enabled) or hashfs.
--  Paper as a storage medium: paper is not a great storage medium,
-   because it has low storage density (ie, you can only store at most 
-   about 100 KB) and it can also degrade just like other storage mediums,
-   but you cannot check that automatically since it's not digital. However,
-   if you are interested, here are a few softwares that do that:
-   `Paper key <http://en.wikipedia.org/wiki/Paper_key>`_,
-   `Paperbak <http://www.ollydbg.de/Paperbak/index.html>`_,
-   `Optar <http://ronja.twibright.com/optar/>`_,
-   `dpaper <https://github.com/penma/dpaper>`_,
-   `QR Backup <http://blog.liw.fi/posts/qr-backup/>`_,
-   `QR Backup (another) <http://blog.shuningbian.net/2009/10/qrbackup.php>`_,
-   `QR Backup (again another) <http://git.pictorii.com/index.php?p=qrbackup.git&a=summary>`_,
-   `QR Backup (again) <http://hansmi.ch/software/qrbackup>`_,
-   `and finally a related paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&rep=rep1&type=pdf>`_.
--  AVPreserve tools, most notably `fixity <https://github.com/avpreserve/fixity>`_ 
-   to monitor for file changes (similarly to rfigc, but actively as a daemon)
-   and `interstitial <https://github.com/avpreserve/interstitial>`_ to detect
-   interstitial errors in audio digitization workflows (great to ensure you
-   correctly digitized a whole audio file into WAV without any error).
-
-FAQ
----
-
--  Can I compress my data files and my ecc file?
-
-As a rule of thumb, you should ALWAYS keep your ecc file in clear
-text, so under no compression nor encryption. This is because in case
-the ecc file gets corrupted, if compressed/encrypted, the
-decompression/decrypting of the corrupted parts may completely flaw
-the whole structure of the ecc file.
-
-Your data files, that you want to protect, *should* remain in clear
-text, but you may choose to compress them if it drastically reduces
-the size of your files, and if you raise the resilience rate of your
-ecc file (so compression may be a good option if you have an
-opportunity to trade the file size reduction for more ecc file
-resilience). Also, make sure to choose a non-solid compression
-algorithm like DEFLATE (zip) so that you can still decode correct
-parts even if some are corrupted (else with a solid archive, if one
-byte is corrupted, the whole archive may become unreadable).
-
-However, in the case that you compress your files, you should generate
-the ecc file only *after* compression, so that the ecc file applies to
-the compressed archive instead of the uncompressed files, else you
-risk being unable to correct your files because the uncompression of
-corrupted parts may output gibberish, and length extended corrupted
-parts (and if the size is different, Reed-Solomon will just freak
-out).
-
--  Can I encrypt my data files and my ecc file ?
-
-NEVER encrypt your ecc file, this is totally useless and
-counterproductive.
-
-You can encrypt your data files, but choose a non-solid algorithm
-(like AES if I'm not mistaken) so that corrupted parts do not prevent
-the decoding of subsequent correct parts. Of course, you're lowering a
-bit your chances of recovering your data files by encrypting them (the
-best chance to keep data for the long term is to keep them in clear
-text), but if it's really necessary, using a non-solid encrypting
-scheme is a good compromise.
-
-You can generate an ecc file on your encrypted data files, thus
-*after* encryption, and keep the ecc file in clear text (never encrypt
-nor compress it). This is not a security risk at all since the ecc
-file does not give any information on the content inside your
-encrypted files, but rather just redundant info to correct corrupted
-bytes (however if you generate the ecc file on the data files before
-encryption, then it's clearly a security risk, and someone could
-recover your data without your permission).
-
-- What medium should I use to store my data?
-
-The details are long and a bit complicated (I may write a complete article
-about it in the future), but the tl;dr answer is that you should use *optical disks*,
-because it decouples the storage medium and the reading hardware
-(eg, at the opposite we have hard drives, which contains both the reading
-hardware and the storage medium, so if one fails, you lose both)
-and because it's most likely future-proof (you only need a laser, which
-is universal, the laser's parameters can always be tweaked).
-
-From scientific studies, it seems that, at the time of writing this (2015),
-BluRay HTL disks are the most resilient against environmental degradation.
-To raise the duration, you can also put optical disks in completely opaque boxes
-(to avoid light degradation) and in addition you can put any storage medium
-(not only optical disks, but also hard drives and anything really) in
-*completely* air-tight and water-tight bags or box and put in a fridge or a freezer.
-This is a law of nature: lower the temperature, lower will be the entropy, in other
-words lower will be the degradation over time. It works the same with digital data.
-
-- What file formats are the most recoverable?
-
-It's difficult to advise a specific format. What we can do is advise the characteristics
-of a good file format:
-
-  * future-proof (should be readable in the future).
-  * non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn't cause a problem to the decoding of other blocks).
-  * open source implementation available.
-  * minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).
-  * No magic bytes or header importance (ie, corrupting the header won't prevent opening the file).
-
-There are a few studies about the most resilient file formats, such as:
-
-  * `"Just one bit in a million: On the effects of data corruption in files" by Volker Heydegger <http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1>`_.
-  * `"Analysing the impact of file formats on data integrity" by Volker Heydegger <http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf>`_.
-  * `"A guide to formats", by The UK national archives <http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf>`_ (you want to look at the Recoverability entry in each table).
-
-- What is Reed-Solomon?
-
-If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon
-
-Also, you may want to read the following resources:
-
-  * "`Reed-Solomon codes for coders <https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders>`_", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.
-  * "Algebraic codes for data transmission", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books <https://books.google.fr/books?id=eQs2i-R9-oYC&lpg=PR11&ots=atCPQJm3OJ&dq=%22Algebraic%20codes%20for%20data%20transmission%22%2C%20Blahut%2C%20Richard%20E.%2C%202003%2C%20Cambridge%20university%20press.&lr&hl=fr&pg=PA193#v=onepage&q=%22Algebraic%20codes%20for%20data%20transmission%22,%20Blahut,%20Richard%20E.,%202003,%20Cambridge%20university%20press.&f=false>`_.
-
-
-.. |Example| image:: https://raw.githubusercontent.com/lrq3000/pyFileFixity/master/tux-example.jpg
-   :scale: 60 %
-   :alt: Image corruption and repair example
-.. |PyPI-Status| image:: https://img.shields.io/pypi/v/pyfilefixity.svg
-   :target: https://pypi.org/project/pyfilefixity
-.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/pyfilefixity.svg?logo=python&logoColor=white
-   :target: https://pypi.org/project/pyfilefixity
-.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/pyfilefixity.svg?label=pypi%20downloads&logo=python&logoColor=white
-   :target: https://pypi.org/project/pyfilefixity
-.. |Build-Status| image:: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml/badge.svg?event=push
-   :target: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml
-.. |Coverage| image:: https://codecov.io/github/lrq3000/pyFileFixity/coverage.svg?branch=master
-   :target: https://codecov.io/github/lrq3000/pyFileFixity?branch=master
+Metadata-Version: 2.1
+Name: pyFileFixity
+Version: 3.1.4
+Summary: Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing.
+Author-email: Stephen Karl Larroque <lrq3000@gmail.com>
+Maintainer-email: Stephen Karl Larroque <lrq3000@gmail.com>
+License: MIT License
+Project-URL: Homepage, https://github.com/lrq3000/pyFileFixity
+Project-URL: Documentation, https://github.com/lrq3000/pyFileFixity/blob/master/README.rst
+Project-URL: Source, https://github.com/lrq3000/pyFileFixity
+Project-URL: Tracker, https://github.com/lrq3000/pyFileFixity/issues
+Project-URL: Download, https://github.com/lrq3000/pyFileFixity/releases
+Keywords: file,repair,monitor,change,reed-solomon,error,correction,error correction,parity,parity files,parity bytes,data protection,data recovery,file protection,qr codes,qr code
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Environment :: Console
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: MacOS :: MacOS X
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3.12
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Topic :: System :: Archiving
+Classifier: Topic :: System :: Archiving :: Backup
+Classifier: Topic :: System :: Monitoring
+Classifier: Topic :: System :: Recovery Tools
+Classifier: Topic :: Utilities
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: End Users/Desktop
+Classifier: Intended Audience :: Information Technology
+Classifier: Intended Audience :: System Administrators
+Requires-Python: >=3.7
+Description-Content-Type: text/x-rst
+Provides-Extra: test
+Provides-Extra: testmeta
+License-File: LICENSE
+
+pyFileFixity
+============
+
+|PyPI-Status| |PyPI-Versions| |PyPI-Downloads|
+
+|Build-Status| |Coverage|
+
+pyFileFixity provides a suite of open source, cross-platform, easy
+to use and easy to maintain (readable code) to protect and manage data
+for long term storage/archival, and also test the performance of any data protection algorithm.
+
+The project is done in pure-Python to meet those criteria,
+although cythonized extensions are available for core routines to speed up encoding/decoding,
+but always with a pure python specification available so as to allow long term replication.
+
+Here is an example of what pyFileFixity can do:
+
+|Example|
+
+On the left, this is the original image.
+
+At the center, the same image but
+with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
+which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
+Only a few corrupted bytes are enough to make the image looks like totally
+unrecoverable, and yet we are lucky, because the image could be unreadable at all
+if any of the "magic bytes" were to be corrupted!
+
+At the right, the corrupted image was repaired using ``pff header`` command of pyFileFixity.
+This repaired only the image header (ie, the first part of the file), so only the first
+3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
+the image looks indistinguishable from the untampered original! And the best thing is that
+it only costed the generation of a "ecc repair file" for the header, which size is only a
+constant 3.3KB per file, regardless of the protected file's size!
+
+This works because most files will store the most important information to read them at
+their beginning, also called "file's header", so repairing this part will almost always ensure
+the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
+you can read it). This works especially well for images, compressed files, formatted documents such as
+DOCX and ODT, etc.
+
+Of course, you can also protect the whole file, not only the header, using pyFileFixity's
+``pff whole`` command. You can also detect any corruption using ``pff hash``.
+
+------------------------------------------
+
+.. contents:: Table of contents
+   :backlinks: top
+
+Quickstart
+----------
+
+Runs on Python 3 up to Python 3.12-dev. PyPy 3 is also supported.
+
+- To install or update on Python 3:
+
+``pip install --upgrade pyfilefixity``
+
+- For Python 2.7, the latest working version was v3.0.2:
+
+``pip install --upgrade pyfilefixity==3.0.2 reedsolo==1.7.0 unireedsolomon==1.0.5``
+
+- Once installed, the suite of tools can be accessed from a centralized interface script called ``pff`` which provides several subcommands, to list them:
+
+``pff --help``
+
+You should see:
+
+::
+
+    usage: pff [-h]
+               {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
+               ...
+
+    positional arguments:
+      {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
+        hash (rfigc)        Check files integrity fast by hash, size, modification date or by data structure integrity.
+        header (header_ecc, hecc)
+                            Protect/repair files headers with error correction codes
+        whole (structural_adaptive_ecc, saecc, protect, repair)
+                            Protect/repair whole files with error correction codes
+        recover (repair_ecc, recc)
+                            Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery
+                            mode (note: this does NOT recover your files, only the ecc files, which may then be used to
+                            recover your files!)
+        dup (replication_repair)
+                            Repair files from multiple copies of various storage mediums using a majority vote
+        restest (resilience_tester)
+                            Run tests to quantify robustness of a file protection scheme (can be used on any, not just
+                            pyFileFixity)
+        filetamper          Tamper files using various schemes
+        speedtest (ecc_speedtest)
+                            Run error correction encoding and decoding speedtests
+
+    options:
+      -h, --help            show this help message and exit
+
+- Every subcommands provide their own more detailed help instructions, eg for the ``hash`` submodule:
+
+``pff hash --help``
+
+- To generate a monitoring database (to later check very fast which files are corrupted, but cannot repair anything but filesystem metadata):
+
+``pff hash -i "your_folder" -d "dbhash.csv" -g -f -l "log.txt"``
+
+Note: this also works for a single file, just replace "your_folder" by "your_file.ext".
+
+- To update this monitoring database (check for new files, but does not remove files that do not exist anymore - replace ``--append`` with ``--remove`` for the latter):
+
+``pff hash -i "your_folder -d "dbhash.csv" --update --append``
+
+- Later, to check which files were corrupted:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s -e errors.csv``
+
+- To use this monitoring database to recover filesystem metadata such as files names and directory layout by filescraping from files contents:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l "log.txt" -o "output_folder" --filescraping_recovery``
+
+- To protect files headers with a file called ``hecc.txt``:
+
+``pff header -i "your_folder" -d "hecc.txt" -l "log.txt" -g -f --ecc_algo 3``
+
+- To repair files headers and store the repaired files in ``output_folder``:
+
+``pff header -i "your_folder" -d "hecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
+
+- To protect whole files with a file called ``ecc.txt``:
+
+``pff whole -i "your_folder" -d "ecc.txt" -l "log.txt" -g -f -v --ecc_algo 3``
+
+- To repair whole files:
+
+``pff whole -i "your_folder" -d "ecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
+
+Note that ``header`` and ``whole`` can also detect corrupted files and even which blocks inside a file, but they are much slower than ``hash``.
+
+- To try to recover a damaged ecc file ``ecc.txt`` using an index file ``ecc.txt.idx`` (index file is generated automatically with ecc.txt):
+
+``pff recovery -i "ecc.txt" --index "ecc.txt.idx" -o "ecc_repaired.txt" -l "log.txt" -v -f``
+
+- To try to recover a damaged ecc file ``ecc.txt`` without an index file (you can tweak the ``-t`` parameter from 0.0 to 1.0, 1.0 producing many false positives):
+
+``pff recovery -i "ecc.txt" -o "ecc_repaired.txt" -l "log.txt" -v -f -t 0.4``
+
+- To repair your files using multiple duplicated copies that you have stored on different mediums:
+
+``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" --report "rlog.csv" -f -v``
+
+- If you have previously generated a rfigc database, you can use it to enhance the replication repair:
+
+``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" -d "dbhash.csv" --report "rlog.csv" -f -v``
+
+- To run tests on your recovery tools, you can make a Makefile-like configuration file and use the Resiliency Tester submodule:
+
+``pff restest -i "your_folder" -o "test_folder" -c "resiliency_tester_config.txt" -m 3 -l "testlog.txt" -f``
+
+- Internally, ``pff restest`` uses ``pff filetamper`` to tamper files with various schemes, but you can also use ``pff filetamper`` directly.
+
+- To run speedtests of encoding/decoding error correction codes on your machine:
+
+``pff speedtest``
+
+- In case the ``pff`` command does not work, it can be replaced with ``python -m pyFileFixity.pff`` .
+
+The problem of long term storage
+--------------------------------
+
+Why are data corrupted with time? One sole reason: entropy.
+Entropy refers to the universal tendency for systems to become
+less ordered over time. Data corruption is exactly that: a disorder
+in bits order. In other words: *the Universe hates your data*.
+
+Long term storage is thus a very difficult topic: it's like fighting with
+death (in this case, the death of data). Indeed, because of entropy,
+data will eventually fade away because of various silent errors such as
+bit rot or cosmic rays. pyFileFixity aims to provide tools to detect any data
+corruption, but also fight data corruption by providing repairing tools.
+
+The only solution is to use a principle of engineering that is long
+known and which makes bridges and planes safe: add some **redundancy**.
+
+There are only 2 ways to add redundancy:
+
+-  the simple way is to **duplicate** the object (also called replication),
+   but for data storage, this eats up a lot of storage and is not optimal.
+   However, if storage is cheap, then this is a good solution, as it is
+   much faster than encoding with error correction codes. For replication to work,
+   at least 3 duplicates are necessary at all times, so that if one fails, it must
+   replaced asap. As sailors say: "Either bring 1 compass or 3 compasses, but never
+   two, because then you won't know which one is correct if one fails."
+   Indeed, with 3 duplicates, if you frequently monitor their integrity
+   (eg, with hashes), then if one fails, simply do a majority vote:
+   the bit value given by 2 of the duplicates is probably correct.
+-  the second way, the optimal tools ever invented to recover
+   from data corruption, are the **error correction codes** (forward
+   error correction), which are a way to smartly produce redundant codes
+   from your data so that you can later repair your data using these
+   additional pieces of information (ie, an ECC generates n blocks for a
+   file cut in k blocks (with k < n), and then the ecc code can rebuild
+   the whole file with (at least) any k blocks among the total n blocks
+   available). In other words, you can correct up to (n-k) erasures. But
+   error correcting codes can also detect and repair automatically where
+   the errors are (fully automatic data repair for you !), but at the
+   cost that you can then only correct (n-k)/2 errors.
+
+Error correction can seem a bit magical, but for a reasonable intuition,
+it can be seen as a way to average the corruption error rate: on
+average, a bit will still have the same chance to be corrupted, but
+since you have more bits to represent the same data, you lower the
+overall chance to lose this bit.
+
+The problem is that most theoretical and pratical works on error
+correcting codes has been done almost exclusively on channel
+transmission (such as 4G, internet, etc.), but not on data storage,
+which is very different for one reason: whereas in a channel we are in a
+spatial scheme (both the sender and the receiver are different entities
+in space but working at the same timescale), in data storage this is a
+temporal scheme: the sender was you storing the data on your medium at
+time t, and the receiver is again you but now retrieving the data at
+time t+x. Thus, the sender does not exist anymore, thus you cannot ask
+the sender to send again some data if it's too much corrupted: in data
+storage, if a data is corrupted, it's lost for good, whereas in channel theory,
+parts of the data can be submitted again if necessary.
+
+Some attempts were made to translate channel theory and error correcting
+codes theory to data storage, the first being Reed-Solomon which spawned
+the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
+devised for use on optical discs to recover from scratches, which was
+necessary for the technology to be usable for consumers. Since then, new
+less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
+fountain codes such as RaptorQ were invented (or rediscovered), but they
+are still marginally researched for data storage.
+
+This project aims to, first, implement easy tools to evaluate strategies
+(filetamper.py) and file fixity (ie, detect if there are corruptions),
+and then the goal is to provide an open and easy framework to use
+different kinds of error correction codes to protect and repair files.
+
+Also, the ecc file specification is made to be simple and resilient to
+corruption, so that you can process it by your own means if you want to,
+without having to study for hours how the code works (contrary to PAR2
+format).
+
+In practice, both approaches are not exclusive, and the best is to
+combine them: protect the most precious data with error correction codes,
+then duplicate them as well as less sensitive data across multiple storage mediums.
+Hence, this suite of data protection tools, just like any other such suite, is not
+sufficient to guarantee your data is protected, you must have an active (but infrequent and hence not time consuming)
+data curation strategy that includes regularly checking your data and replacing copies that are damaged every few years.
+
+For a primer on storage mediums and data protection strategies, see `this post I wrote <https://web.archive.org/web/20220529125543/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`_.
+
+Why not just use RAID ?
+-----------------------
+
+RAID is clearly insufficient for long-term data storage, and in fact it
+was primarily meant as a cheap way to get more storage (RAID0) or more
+availability (RAID1) of data, not for archiving data, even on a medium
+timescale:
+
+-  RAID 0 is just using multiple disks just like a single one, to extend
+   the available storage. Let's skip this one.
+-  RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
+   That's completely useless for long term storage: if either disk
+   fails, or if both disks are partially corrupted, you can't know what
+   are the correct data and which aren't. As an old saying goes: "Never
+   take 2 compasses: either take 3 or 1, because if both compasses show
+   different directions, you will never know which one is correct, nor
+   if both are wrong." That's the principle of Triplication.
+-  RAID 5 is based on the triplication idea: you have n disks (but least
+   3), and if one fails you can recover n-1 disks (resilient to only 1
+   disk failure, not more).
+-  RAID 6 is an extension of RAID 5 which is closer to error-correction
+   since you can correct n-k disks. However, most (all?) currently
+   commercially available RAID6 devices only implements recovery for at
+   most n-2 (2 disks failures).
+-  In any case, RAID cannot detect silent errors automatically, thus you
+   either have to regularly scan, or you risk to lose some of your data
+   permanently, and it's far more common than you can expect (eg, with
+   RAID5, it is enough to have 2 silent errors on two disks on the same
+   bit for the bit to be unrecoverable). That's why a limit of only 1 or
+   2 disks failures is just not enough.
+-  Finally, it's worth noting that `hard drives do implement ECC codes <https://superuser.com/a/1554342/157556>`__
+   to be resilient against bad sectors (otherwise we would lose data
+   all the time!), but they only have limited corrective capacity,
+   mainly because the ECC code is short and not configurable.
+
+On the opposite, ECC can correct n-k disks (or files). You can configure
+n and k however you want, so that for example you can set k = n/2, which
+means that you can recover all your files from only half of them! (once
+they are encoded with an ecc file of course).
+
+There also are new generation RAID solutions, mainly software based,
+such as SnapRAID or ZFS, which allow you to configure a virtual RAID
+with the value n-k that you want. This is just like an ecc file (but a
+bit less flexible, since it's not a file but a disk mapping, so that you
+can't just copy it around or upload it to a cloud backup hosting). In
+addition to recover (n-k) disks, they can also be configured to recover
+from partial, sectors failures inside the disk and not just the whole
+disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
+and James L. Hafner. "SD codes: erasure codes designed for how storage
+systems really fail." FAST. 2013.).
+
+The other reason RAID is not adapted to long-term storage, is that it
+supposes you store your data on hard-drives exclusively. Hard drives
+aren't a good storage medium for the long term, for two reasons:
+
+| 1- they need a regular plug to keep the internal magnetic disks
+  electrified (else the data will just fade away when there's no
+  residual electricity).
+| 2- the reading instrument is directly included and merged with the
+  data (this is the green electronic board you see from the outside, and
+  the internal head). This is good for quick consumer use (don't need to
+  buy another instrument: the HDD can just be plugged and it works), but
+  it's very bad for long term storage, because the reading instrument is
+  bound to fail, and a lot faster than the data can fade away: this
+  means that even if your magnetic disks inside your HDD still holds
+  your data, if the controller board or the head doesn't work anymore,
+  your data is just lost. And a head (and a controller board) are almost
+  impossible to replace, even by professionals, because the pieces are
+  VERY hard to find (different for each HDD production line) and each
+  HDD has some small physical defects, thus it's impossible to reproduce
+  that too (because the head is so close to the magnetic disk that if
+  you try to do that manually you'll probably fail).
+
+In the end, it's a lot better to just separate the storage medium of
+data, with the reading instrument.
+
+We will talk later about what storage mediums can be used instead.
+
+Applications included
+---------------------
+
+The pyFileFixity suite currently include the following pure-python applications:
+
+-  rfigc.py (subcommand: ``hash``), a hash auditing tool, similar to md5deep/hashdeep, to
+   compute a database of your files along with their metadata, so that
+   later you can check if they were changed/corrupted.
+
+-  header\_ecc.py (subcommand: ``header``), an error correction code using Reed-Solomon
+   generator/corrector for files headers. The idea is to supplement
+   other more common redundancy tools such as PAR2 (which is quite
+   reliable), by adding more resiliency only on the critical parts of
+   the files: their headers. Using this script, you can significantly
+   higher the chance of recovering headers, which will allow you to at
+   least open the files.
+
+-  structural\_adaptive\_ecc.py (subcommand: ``whole``), a variable error correction rate
+   encoder (kind of a generalization of header\_ecc.py). This script
+   allows to generate an ecc file for the whole content of your files,
+   not just the header part, using a variable resilience rate: the
+   header part will be the most protected, then the rest of each file
+   will be progressively encoded with a smaller and smaller resilience
+   rate. The assumption is that important information is stored first,
+   and then data becomes less and less informative (and thus important,
+   because the end of the file describes less important details). This
+   assumption is very true for all compressed kinds of formats, such as
+   JPG, ZIP, Word, ODT, etc...
+
+-  repair\_ecc.py (subcommand: ``recovery``), a script to repair the structure (ie, the entry and
+   fields markers/separators) of an ecc file generated by header\_ecc.py
+   or structural\_adaptive\_ecc.py. The goal is to enhance the
+   resilience of ecc files against corruption by ensuring that their
+   structures can be repaired (up to a certain point which is very high
+   if you use an index backup file, which is a companion file that is
+   generated along an ecc file).
+
+-  filetamper.py (subcommand: ``filetamper``) is a quickly made file corrupter, it will erase or
+   change characters in the specified file. This is useful for testing
+   your various protecting strategies and file formats (eg: is PAR2
+   really resilient against corruption? Are zip archives still partially
+   extractable after corruption or are rar archives better? etc.). Do
+   not underestimate the usefulness of this tool, as you should always
+   check the resiliency of your file formats and of your file protection
+   strategies before relying on them.
+
+-  replication\_repair.py (subcommand: ``dup``) takes advantage of your multiple copies
+   (replications) of your data over several storage mediums to recover
+   your data in case it gets corrupted. The goal is to take advantage of
+   the storage of your archived files into multiple locations: you will
+   necessarily make replications, so why not use them for repair?
+   Indeed, it's good practice to keep several identical copies of your data
+   on several storage mediums, but in case a corruption happens,
+   usually you will just drop the corrupted copies and keep the intacts ones.
+   However, if all copies are partially corrupted, you're stuck. This script
+   aims to take advantage of these multiple copies to recover your data,
+   without generating a prior ecc file. It works simply by reading through all
+   your different copies of your data, and it casts a majority vote over each
+   byte: the one that is the most often occuring will be kept. In engineering,
+   this is a very common strategy used for very reliable systems such as
+   space rockets, and is called "triple-modular redundancy", because you need
+   at least 3 copies of your data for the majority vote to work (but the more the
+   better).
+
+-  resiliency\_tester.py (subcommand: ``restest``) allows you to test the robustness of the
+   corruption correction of the scripts provided here (or any other
+   command-line app). You just have to copy the files you want to test inside a
+   folder, and then the script will copy the files into a test tree, then it
+   will automatically corrupt the files randomly (you can change the parameters
+   like block burst and others), then it will run the file repair command-lines
+   you supply and finally some stats about the repairing power will be
+   generated. This allows you to easily and objectively compare different set
+   of parameters, or even different file repair solutions, on the very data
+   that matters to you, so that you can pick the best option for you.
+
+-  ecc\_speedtest.py (subcommand: ``speedtest``) is a simple error correction codes
+   encoder/decoder speedtest. It allows to easily change parameters for the test.
+   This allows to assess how fast your machine can encode/decode with the selected
+   parameters, which can be especially useful to plan ahead for how many files you
+   can reasonably plan to protect with error correction codes (which are time consuming).
+
+-  DEPRECATED: easy\_profiler.py is just a quick and simple profiling tool to get
+   you started quickly on what should be optimized to get more speed, if
+   you want to contribute to the project feel free to propose a pull
+   request! (Cython and other optimizations are welcome as long as they
+   are cross-platform and that an alternative pure-python implementation
+   is also available).
+
+Note that all tools are primarily made for command-line usage (type
+pff <subcommand> --help to get extended info about the accepted arguments)
+
+IMPORTANT: it is CRITICAL that you use the same parameters for
+correcting mode as when you generated the database/ecc files (this is
+true for all scripts in this bundle). Of course, some options must be
+changed: -g must become -c to correct, and --update is a particular
+case. This works this way on purpose for mainly two reasons: first
+because it is very hard to autodetect the parameters from a database
+file alone and it would produce lots of false positives, and secondly
+(the primary reason) is that storing parameters inside the database file
+is highly unresilient against corruption (if this part of the database
+is tampered, the whole becomes unreadable, while if they are stored
+outside or in your own memory, the database file is always accessible).
+Thus, it is advised to write down the parameters you used to generate
+your database directly on the storage media you will store your database
+file on (eg: if it's an optical disk, write the parameters on the cover
+or directly on the disk using a marker), or better memorize them by
+heart. If you forget them, don't panic, the parameters are always stored
+as comments in the header of the generated ecc files, but you should try
+to store them outside of the ecc files anyway.
+
+For users: what are the advantages of pyFileFixity?
+---------------------------------------------------
+
+Pros:
+
+-  Open application and open specifications under the MIT license (you
+   can do whatever you want with it and tailor it to your needs if you
+   want to, or add better decoding procedures in the future as science
+   progress so that you can better recover your data from your already
+   generated ecc file).
+-  Highly reliable file fixity watcher: rfigc.py will tell you without
+   any ambiguity using several attributes if your files have been
+   corrupted or not, and can even check for images if the header is
+   valid (ie: if the file can still be opened).
+-  Readable ecc file format (compared to PAR2 and most other similar
+   specifications).
+-  Highly resilient ecc file format against corruption (not only are
+   your data protected by ecc, the ecc file is protected too against
+   critical spots, both because there is no header so that each track is
+   independent and if one track is corrupted beyond repair then other
+   ecc tracks can still be read, and a .idx file will be generated to
+   repair the structure of the ecc file to recover all tracks).
+-  Very safe and conservative approach: the recovery process checks that
+   the recovery was successful before committing a repaired block.
+-  Partial recovery allowed (even if a file cannot be completely
+   recovered, the parts that can will be repaired and then the rest that
+   can't be repaired will be recopied from the corrupted version).
+-  Support directory processing: you can encode an ecc file for a whole
+   directory of files (with any number of sub-directories and depth).
+-  No limit on the number of files, and it can recursively protect files
+   in a directory tree.
+-  Variable resiliency rate and header-only resilience, ensuring that
+   you can always open your files even if partially corrupted (the
+   structure of your files will be saved, so that you can use other
+   softwares to repair beyond if this set of script is not sufficient to
+   totally repair).
+-  Support for erasures (null bytes) and even errors-and-erasures, which
+   literally doubles the repair capabilities. To my knowledge, this is
+   the only freely available parity software that supports erasures.
+-  Display the predicted total ecc file size given your parameters,
+   and the total time it will take to encode/decode.
+-  Your original files are still accessible as they are, protection files
+   such as ecc files live alongside your original data. Contrary to
+   other data protection schemes such as PAR2 which encode the whole
+   data in par archive files that replace your original files and
+   are not readable without decoding.
+-  Opensourced under the very permissive MIT licence, do whatever you
+   want!
+
+Cons:
+
+-  Cannot protect meta-data, such as folders paths. The paths are
+   stored, but cannot be recovered (yet? feel free to contribute if you
+   know how). Only files are protected. Thus if your OS or your storage
+   medium crashes and truncate a whole directory tree, the directory
+   tree can't be repaired using the ecc file, and thus you can't access
+   the files neither. However, you can use file scraping to extract the
+   files even if the directory tree is lost, and then use RFIGC.py to
+   reorganize your files correctly. There are alternatives, see the
+   chapters below: you can either package all your files in a single
+   archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
+   DVDisaster as an alternative solution, which is an ecc generator with
+   support for directory trees meta-data (but only on optical disks).
+-  Can only repair errors and erasures (characters that are replaced by
+   another character), not deletion nor insertion of characters. However
+   this should not happen with any storage medium (truncation can occur
+   if the file bounds is misdetected, in this case pyFileFixity can
+   partially repair the known parts of the file, but cannot recover the
+   rest past the truncation, except if you used a resiliency rate of at
+   least 0.5, in which case any message block can be recreated with only
+   using the ecc file).
+-  Cannot recreate a missing file from other available files (except you
+   have set a resilience\_rate at least 0.5), contrary to Parchives
+   (PAR1/PAR2). Thus, you can only repair a file if you still have it
+   (and its ecc file!) on your filesystem. If it's missing, pyFileFixity
+   cannot do anything (yet, this will be implemented in the future).
+
+Note that the tools were meant for data archival (protect files that you
+won't modify anymore), not for system's files watching nor to protect
+all the files on your computer. To do this, you can use a filesystem
+that directly integrate error correction code capacity, such as ZFS.
+
+Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)
+------------------------------------------------------------------------------
+
+Recursively generate or check the integrity of files by MD5 and SHA1
+hashes, size, modification date or by data structure integrity (only for
+images).
+
+This script is originally meant to be used for data archival, by
+allowing an easy way to check for silent file corruption. Thus, this
+script uses relative paths so that you can easily compute and check the
+same redundant data copied on different mediums (hard drives, optical
+discs, etc.). This script is not meant for system files corruption
+notification, but is more meant to be used from times-to-times to check
+up on your data archives integrity (if you need this kind of application,
+see `avpreserve's fixity <https://github.com/avpreserve/fixity>`_).
+
+Example usage
+~~~~~~~~~~~~~
+
+-  To generate the database (only needed once):
+
+``pff hash -i "your_folder" -d "dbhash.csv" -g``
+
+-  To check:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s``
+
+-  To update your database by appending new files:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -u -a``
+
+-  To update your database by appending new files AND removing
+   inexistent files:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -u -a -r``
+
+Note that by default, the script is by default in check mode, to avoid
+wrong manipulations. It will also alert you if you generate over an
+already existing database file.
+
+Arguments
+~~~~~~~~~
+
+::
+
+      -h, --help            show a help message and exit
+      -i /path/to/root/folder, --input /path/to/root/folder
+                            Path to the root folder from where the scanning will occ
+    ur.
+      -d /some/folder/databasefile.csv, --database /some/folder/databasefile.csv
+                            Path to the csv file containing the hash informations.
+      -l /some/folder/filename.log, --log /some/folder/filename.log
+                            Path to the log file. (Output will be piped to both the
+    stdout and the log file)
+      -s, --structure_check
+                            Check images structures for corruption?
+      -e /some/folder/errorsfile.csv, --errors_file /some/folder/errorsfile.csv
+                            Path to the error file, where errors at checking will be
+     stored in CSV for further processing by other softwares (such as file repair so
+    ftwares).
+      -m, --disable_modification_date_checking
+                            Disable modification date checking.
+      --skip_missing        Skip missing files when checking (useful if you split yo
+    ur files into several mediums, for example on optical discs with limited capacit
+    y).
+      -g, --generate        Generate the database? (omit this parameter to check ins
+    tead of generating).
+      -f, --force           Force overwriting the database file even if it already e
+    xists (if --generate).
+      -u, --update          Update database (you must also specify --append or --rem
+    ove).
+      -a, --append          Append new files (if --update).
+      -r, --remove          Remove missing files (if --update).
+      
+      --filescraping_recovery          Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.
+      -o, --output          Path to the output folder where to output the files reorganized after --recover_from_filescraping.
+
+Header Error Correction Code script
+-----------------------------------
+
+This script was made to be used in combination with other more common
+file redundancy generators (such as PAR2, I advise MultiPar). This is an
+additional layer of protection for your files: by using a higher
+resiliency rate on the headers of your files, you ensure that you will
+be probably able to open them in the future, avoiding the "critical
+spots", also called "fracture-critical" in redundancy engineering (where
+if you modify just one bit, your whole file may become unreadable,
+usually bits residing in the headers - in other words, a single blow
+makes the whole thing collapse, just like non-redundant bridges).
+
+An interesting benefit of this approach is that it has a low storage
+(and computational) overhead that scales linearly to the number of
+files, whatever their size is: for example, if we have a set of 40k
+files for a total size of 60 GB, with a resiliency\_rate of 30% and
+header\_size of 1KB (we limit to the first 1K bytes/characters = our
+file header), then, without counting the hash per block and other
+meta-data, the final ECC file will be about 2 \* resiliency\_rate \*
+number\_of\_files \* header\_size = 24.5 MB. This size can be lower if
+there are many files smaller than 1KB. This is a pretty low storage
+overhead to backup the headers of such a big number of files.
+
+The script is pure-python as are its dependencies: it is thus completely
+cross-platform and open source. The default ecc algo
+(ecc_algo=3 uses `reedsolo <https://github.com/tomerfiliba-org/reedsolomon>`_)
+also provides a speed-optimized C-compiled implementation (``creedsolo``) that will be used
+if available for the user's platform, so pyFileFixity should be fast by default.
+Alternatively, it's possible to use a JIT compiler such as PyPy,
+although this means that ``creedsolo`` will not be useable, so PyPy
+may accelerate other functions but slower ecc encoding/decoding.
+
+Structural Adaptive Error Correction Encoder
+--------------------------------------------
+
+This script implements a variable error correction rate encoder: each
+file is ecc encoded using a variable resiliency rate -- using a high
+constant resiliency rate for the header part (resiliency rate stage 1,
+high), then a variable resiliency rate is applied to the rest of the
+file's content, with a higher rate near the beginning of the file
+(resiliency rate stage 2, medium) which progressively decreases until
+the end of file (resiliency rate stage 3, the lowest).
+
+The idea is that the critical parts of files usually are placed at the
+top, and data becomes less and less critical along the file. What is
+meant by critical is both the critical spots (eg: if you tamper only one
+character of a file's header you have good chances of losing your entire
+file, ie, you cannot even open it) and critically encoded information
+(eg: archive formats usually encode compressed symbols as they go along
+the file, which means that the first occurrence is encoded, and then the
+archive simply writes a reference to the symbol. Thus, the first
+occurrence is encoded at the top, and subsequent encoding of this same
+data pattern will just be one symbol, and thus it matters less as long
+as the original symbol is correctly encoded and its information
+preserved, we can always try to restore the reference symbols later).
+Moreover, really redundant data will be placed at the top because they
+can be reused a lot, while data that cannot be too much compressed will
+be placed later, and thus, corruption of this less compressed data is a
+lot less critical because only a few characters will be changed in the
+uncompressed file (since the data is less compressed, a character change
+on the not-so-much compressed data won't have very significant impact on
+the uncompressed data).
+
+This variable error correction rate should allow to protect more the
+critical parts of a file (the header and the beginning of a file, for
+example in compressed file formats such as zip or jpg this is where the
+most importantly strings are encoded) for the same amount of storage as
+a standard constant error correction rate.
+
+Of course, you can set the resiliency rate for each stage to the values
+you want, so that you can even do the opposite: setting a higher
+resiliency rate for stage 3 than stage 2 will produce an ecc that is
+greater towards the end of the contents of your files.
+
+Furthermore, the currently designed format of the ecc file would allow
+two things that are not available in all current file ecc generators
+such as PAR2:
+
+1. it allows to partially repair a file, even if not all
+the blocks can be corrected (in PAR2, a file is repaired only if all
+blocks can be repaired, which is a shame because there are still other
+blocks that could be repaired and thus produce a less corrupted file) ;
+
+2. the ecc file format is quite simple and readable, easy to process by
+any script, which would allow other softwares to also work on it (and it
+was also done in this way to be more resilient against error
+corruptions, so that even if an entry is corrupted, other entries are
+independent and can maybe be used, thus the ecc is very error tolerant.
+This idea was implemented in repair\_ecc.py but it could be extended,
+especially if you know the pattern of the corruption).
+
+The script structural-adaptive-ecc.py implements this idea, which can be
+seen as an extension of header-ecc.py (and in fact the idea was the
+other way around: structural-adaptive-ecc.py was conceived first but was
+too complicated, then header-ecc.py was implemented as a working
+lessened implementation only for headers, and then
+structural-adaptive-ecc.py was finished using header-ecc.py code
+progress). It works, it was a quite well tested for my own needs on
+datasets of hundred of GB, but it's not foolproof so make sure you test
+the script by yourself to see if it's robust enough for your needs (any
+feedback about this would be greatly appreciated!).
+
+ECC Algorithms
+--------------
+
+You can specify different ecc algorithms using the ``--ecc_algo`` switch.
+
+For the moment, only Reed-Solomon is implemented, but it's universal
+so you can modify its parameters in lib/eccman.py.
+
+Two Reed-Solomon codecs are available, they are functionally equivalent
+and thoroughly unit tested.
+
+-  ``--ecc_algo 1``: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
+   This is the slowest implementation (but also the most easy code to understand).
+-  ``--ecc_algo 2``: same as algo 1 but with a faster functions.
+-  ``--ecc_algo 3``: use the second codec, which is the fastest.
+   The generated ECC will be compatible with algo 1 and 2.
+-  ``--ecc_algo 4``: also use the second, fastest RS codec, but
+   with different parameters (US FAA ADSB UAT RS FEC norm),
+   thus the generated ECC won't be compatible with algo 1 to 3.
+   But do not be scared, the ECC will work just the same.
+
+Note about speed: Also, use a smaller --max\_block\_size to greatly
+speedup the operations! That's the trick used to compute very quickly RS
+ECC on optical discs. You give up a bit of resiliency of course (because
+blocks are smaller, thus you protect a smaller number of characters per
+ECC. In the end, this should not change much about real resiliency, but
+in case you get a big bit error burst on a contiguous block, you may
+lose a whole block at once. That's why using RS255 is better, but it's
+very time consuming. However, the resiliency ratios still hold, so for
+any other case of bit-flipping with average-sized bursts, this should
+not be a problem as long as the size of the bursts is smaller than an
+ecc block.)
+
+In case of a catastrophic event
+-------------------------------
+
+TODO: write more here
+
+In case of a catastrophic event of your data due to the failure of your
+storage media (eg: your hard drive crashed), then follow the following
+steps:
+
+1- use dd\_rescue to make a full bit-per-bit verbatim copy of your drive
+before it dies. The nice thing with dd\_rescue is that the copy is
+exact, and also that it can retries or skip in case of bad sectors (it
+won't crash on your suddenly at half the process).
+
+2- Use testdisk to restore partition or to copy files based on partition
+filesystem informations.
+
+3- If you could not recover your files, you can try file scraping using
+`photorec <http://www.cgsecurity.org/wiki/PhotoRec>`_ or
+`plaso  <http://plaso.kiddaland.net/>`_ other similar tools as
+a last resort to extract data based only from files content (no filename,
+often uncorrect filetype, file boundaries may be wrong so some data
+may be cut off, etc.).
+
+4- If you used pyFileFixity before the failure of your storage media,
+you can then use your pre-computed databases to check that files are
+intact (rfigc.py) and if they aren't, you can recover them (using
+header\_ecc.py and structural\_adaptive\_ecc.py). It can also help if
+you recovered your files via data scraping, because your files will be
+totally unorganized, but you can use a previously generated database
+file to recover the full names and directory tree structure using
+rfigc.py --filescraping\_recover.
+
+Also, you can try to fix some of your files using specialized repairing
+tools (but remember that such tool cannot guarantee you the same
+recovering capacity as an error correction code - and in addition, error
+correction code can tell you when it has recovered successfully). For
+example:
+
+-  for tar files, you can use `fixtar <https://github.com/BestSolution-at/fixtar>`_.
+   Similar tools (but older): `tarfix <http://www.dmst.aueb.gr/dds/sw/unix/tarfix/>`_
+   and `tar-repair <https://www.datanumen.com/tar-repair/>`_.
+-  for RAID mounting and recovery, you can use "Raid faster - recover
+   better" (rfrb) tool by Sabine Seufert and Christian Zoubek:
+   https://github.com/lrq3000/rfrb
+-  if your unicode strings were mangled (ie, you see weird symbols),
+   try this script that will automatically demangle them:
+   https://github.com/LuminosoInsight/python-ftfy
+-  to repair tabular (2D) data such as .csv, try
+   `Carpenter <https://pypi.python.org/pypi/Carpenter/>`_.
+-  tool to identify corrupted files in ddrescue images: 
+   `ddrescue-ffile <https://github.com/Salamek/ddrescue-ffile>`_
+
+Protecting directory tree meta-data
+-----------------------------------
+
+One main current limitation of pyFileFixity is that it cannot protect
+the directory tree meta-data. This means that in the worst case, if a
+silent error happens on the inode pointing to the root directory that
+you protected with an ecc, the whole directory will vanish, and all the
+files inside too. In less worst cases, sub-directories can vanish, but
+it's still pretty bad, and since the ecc file doesn't store any
+information about inodes, you can't recover the full path.
+
+The inability to store these meta-data is because of two choices in the
+design:
+
+1.  portability: we want the ecc file to work even if we move the
+    root directory to another place or another storage medium (and of
+    course, the inode would change),
+
+2.  cross-platform compatibility: there's no way to get and store
+    directory meta-data for all platforms, but of course we could implement specific instructions for each main
+    platform, so this point is not really a problem.
+
+To workaround this issue (directory meta-data are critical spots), other
+softwares use a one-time storage medium (ie, writing your data along
+with generating and writing the ecc). This way, they can access at
+the bit level the inode info, and they are guaranted that the inodes
+won't ever change. This is the approach taken by DVDisaster: by using
+optical mediums, it can compute inodes that will be permanent, and thus
+also encode that info in the ecc file. Another approach is to create a
+virtual filesystem specifically to store just your files, so that you
+manage the inode yourself, and you can then copy the whole filesystem
+around (which is really just a file, just like a zip file - which can
+also be considered as a mini virtual file system in fact) like
+`rsbep <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`_.
+
+Here the portability principle of pyFileFixity prevents this approach.
+But you can mimic this workaround on your hard drive for pyFileFixity to
+work: you just need to package all your files into one file. This way,
+you sort of create a virtual file system: inside the archive, files and
+directories have meta-data just like in a filesystem, but from the
+outside it's just one file, composed of bytes that we can just encode to
+generate an ecc file - in other words, we removed the inodes portability
+problem, since this meta-data is stored relatively inside the archive,
+the archive manage it, and we can just encode this info like any other
+stream of data! The usual way to make an archive from several files is
+to use TAR, but this will generate a solid archive which will prevent
+partial recovery. An alternative is to use DAR, which is a non-solid
+archive version of TAR, with lots of other features too. If you also
+want to compress, you can just use ZIP (with DEFLATE algorithm) your
+files (this also generates a non-solid archive). You can then use
+pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
+will then protect both your files just like before and the directories
+meta-data too now.
+
+Which storage medium to use
+---------------------------
+Since hard drives have a relatively short timespan (5-10 years, often less)
+and require regular plugging to an electrical outlet to keep the magnetic
+plates from decaying, other solutions are more advisable.
+
+The medium I used to advise was optical disks (whether it's BluRay, DVD - not CDs!),
+because the reading instrument is distinct from the storage medium, and
+the technology (laser reflecting on bumps and/or pits) is kind of universal,
+so that even if the technology is lost one day (deprecated by newer technologies,
+so that you can't find the reading instrument anymore because it's not sold anymore),
+you can probably emulate a laser using some software to read your optical disk,
+just like what the CAMiLEON project did to recover data from the
+LaserDiscs of the BBC Domesday Project (see Wikipedia). BluRays have an estimated
+lifespan of 20-50 years depending on if they are "gold archival grade", whereas
+DVD should live up from 10-30 years. CDs are only required to live a minimum of 1 year
+up to 10 years max, hence are not fit for archival. Archival optimized optical discs
+such as M-Discs boast about being able to live up to 100 years, but there is no
+independent scientific backing of these claims currently. For more details, you can read
+a longer explanation I wrote with references on
+`StackOverflow <https://web.archive.org/web/20230424112000/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`__.
+
+However, limitations of optical discs include their limited storage space, low
+transfer speed, and limited rewriteability.
+
+A more convenient solution is to use magnetic tape, especially with an open standard
+such as `Linear Tape Open (LTO) <https://en.wikipedia.org/wiki/Linear_Tape-Open>`__,
+which ensures interoperability between manufacturers
+and hence also reduces cost because of competition. LTO works as a two components
+system: the tape drive, and the cartridges (with the magnetic bands). There
+are lots of versions of LTO, each generation improving on the previous one.
+LTO cartridges have a shorter lifespan than optical discs, being 15-30 years on average,
+but they are much more convenient to use:
+
+-  they provide extremely big storage space (one cartridge being several TB as of LTO-4,
+   and the storage capacity approximately doubles every few years with every new version!),
+-  are fast to write (about 5h to write the full cartridge, speed increases with new versions
+   so the total time to fill a cartridge stays about the same),
+-  the storage medium (cartridges) is also distinct from the reading/writing instrument (LTO tape drive), 
+-  are easily rewriteable, although it is necessary to reformat to free up space, but the idea is
+   that "full mirror backups" can be made regularly by overwriting an old tape.
+-  being an open standard, drives to read older versions 25 years old (LTO-1 is from 2000)
+   are still available.
+-  15-30 years of lifespan is still great for archival! But requires active curation (ie, checking
+   cartridges every 5 years and making a full new copy on a new cartridge each decade should be largely sufficient).
+-  Cartridges are cheap: LTO7 cartridges allowing storage of up to 15 TB cost only 60 bucks brand new, often
+   much less in refurbished (already used, but can be overwritten and reused). This is MUCH less expensive
+   than hard drives.
+-  Fit for cold storage: unlike hard drives (using magnetic platters) and like optical discs,
+   the cartridges do not need to be plugged to an electrical outlet regularly, the magnetic band does not
+   decay without electrical current, so the cartridges can be cold stored in air-tight, temperature-proofed
+   and humidity-proof containers, which can be stored off-site (fire-proof data recovery plan).
+-  Recovery of failed LTO cartridges is
+   `inexpensive and readily available <https://www.quora.com/I-have-an-old-LTO-tape-Can-I-recover-its-data-and-save-it-into-a-hard-drive>`__,
+   whereas recovering the magnetic signal from failed hard drives costs
+   `thousands of euros/dollars <https://www.quora.com/Is-there-any-way-of-recovering-data-from-dead-hard-disk>`__.
+   LTO tapes are also fully compatible with DAR archives, improving chances of recovery with error correction codes
+   and non-solid archives that can be partially recovered.
+
+Sounds perfect, right? Well, nothing is, LTO also has several disadvantages:
+
+-  Initial cost of starting is very expensive: a brand new LTO drive of latest generations
+   cost several thousand euros/dollars. Second-hand or refurbished drives of older generations
+   are much less expensive, but they are difficult to setup, as it is unlikely you will find them
+   in an all-in-one package, you will have to get the tape drive separately from the computer system
+   to plug it to (more on that in the next section).
+-  Limited retrocompatibility: the LTO standard specifies that each generation of drives
+   only need to support the current gen and one past gen. However, this is counterbalanced by the fact that
+   the LTO standard is open, so anybody can make LTO drives, including in the future, and it is possible someday
+   a manufacturer will make a LTO drive that supports multiple past generations (just like there are old tapes
+   digitizers that can be connected in USB, for archival purposes). Until then, in practice,
+   it means that ideally when upgrading your LTO system, you need to upgrade by one generation at a time,
+   or if you get a drive of 2+ later gens, you need to keep or buy a drive of the older gen you had to
+   read your tapes to then transfer to the latest gen you have. As of 2023, there are still LTO1 tape drives
+   available for cheap in second-hand, a technology that was published in 2000 and already deprecated
+   in 2001 by LTO2, so this shows that LTO tape drives of older generations should still be plentily available.
+-  LTO is a sequential technology: it is very fast to write and read sequentially, but if you want to
+   download a specific file, the tape has to be fully read up to where the file is stored, contrary to
+   hard drives with random access that can access in linear or sublinear time.
+-  (Old fixed issue) Before LTO-5, which introduced the LTFS standardized filesystem that allows mounting on
+   any operating file system such as Windows, Linux and MacOS, the various LTO drives
+   manufacturers used their own closed-source filesystems that were often incompatible with each others.
+   Hence, make sure to get an LTO-5 drive or above to ensure future access to your long term archives.
+
+Given all the above characteristics, LTO>=5 appears to be the best practical solution
+for long term archival, if coupled with an active (but infrequent) curation process.
+
+There is however one exception: if you need to cold store the medium in a non temperate
+environment (outside of 10-40°C), then using optical discs may be more resilient,
+although LTO cartridges should also be able to sustain a wider range of temperature
+but you need to wait while they "warm up" in the environment where the reader is
+before reading, so that the magnetic elements have time to stabilize at normal temperature.
+
+How to get a LTO tape drive and system running
+----------------------------------------------
+
+To get started with LTO tape drives and which one to choose and how to make your own
+rig, `Matthew Millman made an excellent tutorial <https://www.mattmillman.com/attaching-lto-tape-drives-via-usb-or-thunderbolt/>`__
+on which we build upon below, so you should read this tutorial and then read the instructions below.
+
+The process is as follows: first find a second-hand/refurbished LTO drive with the highest revision you can for your budget,
+then find a server of a similar generation, or make an eGPU + SAS card of the highest speed the tape drive can support.
+Generally, you can aim for a LTO drive 3-4 generations older than the latest one (eg, if current is LTO9, you can expect
+cheap - 150-300 dollars per drive) for a LTO5 or LTO6). Aim only for LTO5+, because only LTFS did not exist before LTO5,
+but keep in mind some LTO5 drives need a firmware update to support LTFS, whereas all LTO6 drives support out of the box.
+
+Once you find a second-hand LTO drive, consult its user manual beforehand to see
+what SAS or fibre cable (FC) you need (if SAS, any version should work, even greater versions, but older
+versions will just limit the read/write speed performance). For example, here is the manual for the
+`HP LTO6 drive <https://docs.oracle.com/cd/E38452_01/en/LTO6_Vol1_E1_D7/LTO6_Vol1_E1_D7.pdf>`__.
+All LTO drives are compatible with all computers provided you have the adequate connectivity (a SAS or FC adapter).
+
+Once you have a LTO drive, then you can look for a computer to plug your LTO to. Essentially, you just need a computer that supports SAS. If not, then at least a free PCIe or mini-PCIe slot to be able to connect a SAS adapter.
+
+The general outline is that you just need to have a computer with a PCIe slot, and get a SAS or FC adapter (depending
+on whether your LTO drive is SAS or FC) so that you can plug your LTO drive. There is
+currently no SAS to USB adapter, and only one manufacturer makes LTO drives with USB ports but
+they are super expensive, so just stick with internal SAS or FC drives (usually you want SAS,
+FC are better for long range connections, whereas SAS is compatible with SATA and SCSI drives,
+so you can also plug all your other hard drives plus the LTO tape drive on the same SAS adapter with this protocol).
+
+In practice, there are 2 different available cost-effective approaches:
+
+-  If you have an external tape drive, then the best is to get a (second-hand) eGPU casing, and a PCIe SAS adapter, that you will plug in the eGPU casing instead of a GPU card. The eGPU casing should support Thunderbolt so this is how you will connect to the SAS and hence to your tape drive: you connect your laptop to the eGPU casing, and the eGPU casing to the external tape drive via the SAS adapter in the eGPU casing. This usually costs about 150-200 euros/dollars as of 2023.
+
+  *  An alternative is to buy a low footprint PCIe dock such as `EXP GDC <https://wiki.geekworm.com/GDC>`__ produces, which essentially replaces the eGPU casing. The disadvantage is that your PCIe SAS adapter will be exposed, but this can be more cost effective (especially in second hand, you can get them at 20-40 euros/dollars instead of 120-150 euros/dollars brand new). But remember you also need to buy a power supply unit!
+
+-  If you got an internal tape drive, which are usually cheaper than external ones, then the approach is different: instead of configuring a sort of SAS-to-Thunderbolt bridge, here you get a standalone computer with either a motherboard that natively supports SAS (which is usually the case of computers meant to be servers), or at least a motherboard with a PCIe slot to buy separately a PCIe SAS adapter, and you plug your internal drive inside. So you will not be able to connect your laptop directly to the tape drive, you will have to pilot the server (which is just a standard desktop computer). Given these requirements, you can either make such a server yourself, but then keep in mind you have to build the whole computer, with a motherboard, a power supply, RAM, CPU, network, etc. Or, the easiest and usually cheapest route, is to just buy an old server with SAS hard drives second-hand (and every other components already in it), of a similar or later generation than your tape drive. Indeed, if the server has SAS hard drives, then it means you can connect your SAS tape drive too, no need for an adapter! Usually you can get them for cheap, for example if you get a 3-4 previous gen tape drive (eg, LTO-6 when current is LTO-9), then you can easily get a server computer of a similar generation for 100-250 euros/dollars, and everything is ready for you. Just make sure not to get a rack/blade computer, get one in tower form, easier to manipulate. Search on second hand websites: "server sas", then check that the SAS speed is on par with what your tape drive can accept, but if lower or higher, no biggie, it will just be slower, but it should work nevertheless. May also have to buy the right connectors but not an issue, just check the manual of your tape drive. Note: avoid HP Enterprise (HPE) servers, as there is a suspicion of programmed obsolescence in the `Smart Array's Smart Storage Battery <https://www.youtube.com/watch?v=6jxdGXA0RYk>`__.
+
+The consumables, the tapes, can also be easily found second-hand and usually are very cheap, eg, LTO6 tapes are sold at 10-20 euros/dollars one, for a storage space of 3TB to 6.25TB per tape.
+
+With both approaches, expect at the cheapest a total cost of about 500 euros/dollars for the tape drive and attachment system (eGPU casing or dedicated server) as of 2023, which is very good and amortizable very fast with just a few tapes, even compared to the cheapest hard drives!
+
+A modern data curation strategy for individuals
+-----------------------------------------------
+
+Here is an example curation strategy, which is accessible to individuals and not just
+big data centers:
+
+-  Get a LTO>=5 drive. Essentially, the idea with LTO is that you can just dump a copy
+   of your whole hard drives, since the cartridges are big and inexpensive. And you can
+   regularly reformat and overwrite the previous copy with a newer one. Store some LTO cartridges
+   out of side to be robust against fires.
+-  If you want additional protection, especially by adding error-correction codes,
+   DAR can be used to compress the data with PAR2 and is
+   `compatible <https://superuser.com/questions/963246/how-to-read-an-dar-archive-via-lto-6-tape>`__
+   with LTO. Alternatively, pyFileFixity can also be used to generate ECC codes, that can
+   either be stored on the same cartridge alongside the files or on a separate cartridge depending
+   on your threat model.
+-  Two kinds of archival plans are possible:
+
+  1.  either only use LTO cartridges, then try to use cartridges of different brands
+      (to avoid them failing at the same time - cartridges produced by the same industrial
+      line will tend to include the same defects and similar lifespan)
+      and store your data on at least 3 different copies/cartridges, per the redundancy principle
+      (ie, "either bring one compass or three, but never two, because you will never know which one is correct").
+
+  2.  either use LTO cartridges as ONE archival medium, and use other kinds of storage
+      for the additional 2 copies you need: one can be an external hard drive, and the last one
+      a cloud backup solution such as SpiderOak. The advantage of this solution is that
+      it is more convenient: use your external hard drive to frequently backup,
+      then also use your cloud backup to auto backup your most critical data online (off-site),
+      and finally from time to time update your last copy on a LTO cartridge by mirroring your
+      external hard drive.
+
+-  Curation strategy is then the same for all plans:
+
+  1.  Every 5 years, the "small checkup": check your 3 copies, either by scanning sectors or by your own
+      precomputed hashes (pyFileFixity's ``hash`` command).
+
+  2.  If there is an error, assume the whole medium is dead and needs to be replaced
+      and your data needs to be recovered: first using your error correction codes if you have,
+      and then using pyFileFixity ``dup`` command to use a majority vote to reconstruct one valid copy out of the 3 copies.
+
+  3.  Every 10 years, the "big checkup": even if the mediums did not fail, replace them by newer ones: mirror the old hard drive to
+      a new one, the old LTO cartridge to a new one (it can be on a newer LTO version, so that you keep pace with the technology), etc.
+
+With the above strategy, you should be able to preserve your data for as long as you can actively curate it. In case you want
+more robustness against accidents or the risk that 2 copies get corrupted under 5 years, then you can make more copies, preferably
+as LTO cartridges, but it can be other hard drives.
+
+For more information on how to cold store LTO drives, read pp32-33 "Caring for Cartridges" instruction of this
+`user manual <https://docs.oracle.com/cd/E38452_01/en/LTO6_Vol1_E1_D7/LTO6_Vol1_E1_D7.pdf>`__. For HP LTO6 drives,
+Matthew Millman made an open-source commandline tool to do advanced LTO manipulations on Windows:
+`ltfscmd <https://github.com/inaxeon/ltfscmd>`__.
+
+In case you cannot afford a LTO drive, you can replace these by external hard drives, as they are less expensive to start with,
+but then your curation strategy should be done more frequently (ie, every 2-3 years a small checkup, and every 5 years, a big checkup).
+
+Tools like pyFileFixity (or which can be used as complements)
+-------------------------------------------------------------
+
+Here are some tools with a similar philosophy to pyFileFixity, which you
+can use if they better fit your needs, either as a replacement of
+pyFileFixity or as a complement (pyFileFixity can always be used to
+generate an ecc file):
+
+-  `DAR (Disk ARchive) <http://dar.linux.free.fr/>`__: similar to tar
+   but non-solid thus allows for partial recovery and per-file access,
+   plus it saves the directory tree meta-data -- see catalog isolation
+   -- plus it can handle error correction natively using PAR2 and
+   encryption. Also supports incremental backup, thus it's a very nice
+   versatile tool. Crossplatform and opensource. Compatible with
+   `Linear Tape Open (LTO) <https://en.wikipedia.org/wiki/Linear_Tape-Open>`__
+   magnetic bands storage (see instructions
+   `here <https://superuser.com/questions/963246/how-to-read-an-dar-archive-via-lto-6-tape>`__)
+-  `DVDisaster <http://dvdisaster.net/>`__: error correction at the bit
+   level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
+   it also protects directory tree meta-data and is resilient to
+   corruption (v2 still has some critical spots but v3 won't have any).
+-  rsbep tool that is part of dvbackup package in Debian: allows to
+   generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
+   for your backups, if you're on unix or using cygwin.
+-  `rsbep modification by Thanassis
+   Tsiodras <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`__:
+   enhanced rsbep to avoid critical spots and faster speed. Also
+   includes a "freeze" script to encode your files into a virtual
+   filesystem (using Python/FUSE) so that even meta-data such as
+   directory tree are fully protected by the ecc. Great script, but not
+   maintained, it needs some intensive testing by someone knowledgeable
+   to guarantee this script is reliable enough for production.
+-  Parchive (PAR1, PAR2, MultiPar): well known error correction file
+   generator. The big advantage of Parchives is that an ecc block
+   depends on multiple files: this allows to completely reconstruct a
+   missing file from scratch using files that are still available. Works
+   good for most people, but most available Parchive generators are not
+   satisfiable for me because 1- they do not allow to generate an ecc
+   for a directory tree recursively (except MultiPar, and even if it is
+   allowed in the PAR2 specs), 2- they can be very slow to generate
+   (even with multiprocessor extensions, because the galois field is
+   over 2^16 instead of 2^8, which is very costly), 3- the spec is not
+   very resilient to errors and tampering over the ecc file, as it
+   assumes the ecc file won't be corrupted (I also tested, it's still a
+   bit resilient, but it could be a lot more with some tweaking of the
+   spec), 4- it doesn't allow for partial recovery (recovering blocks
+   that we can and pass the others that are unrecoverable): with PAR2, a
+   file can be restored fully or it cannot be at all.
+-  Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
+   create non-solid archives which are readable by most computers
+   (ubiquitous algorithm). Non-solid archive means that a zip file can
+   still unzip correct files even if it is corrupted, because files are
+   encoded in blocks, and thus even if some blocks are corrupted, the
+   decoding can happen. A `fast implementation with enhanced compression
+   is available in pure Go <https://github.com/klauspost/compress>`__
+   (good for long storage).
+-  TestDisk: for file scraping, when nothing else worked.
+-  dd\_rescue: for disk scraping (allows to forcefully read a whole disk
+   at the bit level and copy everything it can, passing bad sector with
+   options to retry them later on after a first full pass over the
+   correct sectors).
+-  ZFS: a file system which includes ecc correction directly. The whole
+   filesystem, including directory tree meta-data, are protected. If you
+   want ecc protection on your computer for all your files, this is the
+   way to go.
+-  Encryption: technically, you can encrypt your files without losing
+   too much redundancy, as long as you use an encryption scheme that is
+   block-based such as DES: if one block gets corrupted, it won't be
+   decryptable, but the rest of the files' encrypted blocks should be
+   decryptable without any problem. So encrypting with such algorithms
+   leads to similar files as non-solid archives such as deflate zip. Of
+   course, for very long term storage, it's better to avoid encryption
+   and compression (because you raise the information contained in a
+   single block of data, thus if you lose one block, you lose more
+   data), but if it's really necessary to you, you can still maintain
+   high chances of recovering your files by using block-based
+   encryption/compression (note: block-based encryption can
+   be seen as the equivalent of non-solid archives for compression,
+   because the data is compressed/encrypted in independent blocks,
+   thus allowing partial uncompression/decryption).
+-  `SnapRAID <http://snapraid.sourceforge.net/>`__
+-  `par2ools <https://github.com/jmoiron/par2ools>`__: a set of
+   additional tools to manage par2 archives
+-  `Checkm <https://pypi.python.org/pypi/Checkm/0.4>`__: a tool similar
+   to rfigc.py
+-  `BagIt <https://en.wikipedia.org/wiki/BagIt>`__ with two python
+   implementations `here <https://pypi.python.org/pypi/pybagit/>`__ and
+   `here <https://pypi.python.org/pypi/bagit/>`__: this is a file
+   packaging format for sharing and storing archives for long term
+   preservation, it just formalizes a few common procedures and meta
+   data that are usually added to files for long term archival (such as
+   MD5 digest).
+-  `RSArmor <https://github.com/jap/rsarm>`__ a tool based on
+   Reed-Solomon to encode binary data files into hexadecimal, so that
+   you can print the characters on paper. May be interesting for small
+   datasets (below 100 MB).
+-  `Ent <https://github.com/lsauer/entropy>`__ a tool to analyze the
+   entropy of your files. Can be very interesting to optimize the error
+   correction algorithm, or your compression tools.
+-  `HashFS <https://pypi.python.org/pypi/hashfs/>`_ is a non-redundant,
+   duplication free filesystem, in Python. **Data deduplication** is very
+   important for large scale long term storage: since you want your data
+   to be redundant, this means you will use an additional storage space
+   for your redundant copies that will be proportional to your original data.
+   Having duplicated data will consume more storage and more processing
+   time, for no benefit. That's why it's a good idea to deduplicate your data
+   prior to create redundant copies: this will be faster and save you money.
+   Deduplication can either be done manually (by using duplicates removers)
+   or systematically and automatically using specific filesystems such as
+   zfs (with deduplication enabled) or hashfs.
+-  Paper as a storage medium: paper is not a great storage medium,
+   because it has low storage density (ie, you can only store at most 
+   about 100 KB) and it can also degrade just like other storage mediums,
+   but you cannot check that automatically since it's not digital. However,
+   if you are interested, here are a few softwares that do that:
+   `Paper key <http://en.wikipedia.org/wiki/Paper_key>`_,
+   `Paperbak <http://www.ollydbg.de/Paperbak/index.html>`_,
+   `Optar <http://ronja.twibright.com/optar/>`_,
+   `dpaper <https://github.com/penma/dpaper>`_,
+   `QR Backup <http://blog.liw.fi/posts/qr-backup/>`_,
+   `QR Backup (another) <http://blog.shuningbian.net/2009/10/qrbackup.php>`_,
+   `QR Backup (again another) <http://git.pictorii.com/index.php?p=qrbackup.git&a=summary>`_,
+   `QR Backup (again) <http://hansmi.ch/software/qrbackup>`_,
+   `and finally a related paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&rep=rep1&type=pdf>`_.
+-  AVPreserve tools, most notably `fixity <https://github.com/avpreserve/fixity>`_ 
+   to monitor for file changes (similarly to rfigc, but actively as a daemon)
+   and `interstitial <https://github.com/avpreserve/interstitial>`_ to detect
+   interstitial errors in audio digitization workflows (great to ensure you
+   correctly digitized a whole audio file into WAV without any error).
+
+FAQ
+---
+
+-  Can I compress my data files and my ecc file?
+
+As a rule of thumb, you should ALWAYS keep your ecc file in clear
+text, so under no compression nor encryption. This is because in case
+the ecc file gets corrupted, if compressed/encrypted, the
+decompression/decrypting of the corrupted parts may completely flaw
+the whole structure of the ecc file.
+
+Your data files, that you want to protect, *should* remain in clear
+text, but you may choose to compress them if it drastically reduces
+the size of your files, and if you raise the resilience rate of your
+ecc file (so compression may be a good option if you have an
+opportunity to trade the file size reduction for more ecc file
+resilience). Also, make sure to choose a non-solid compression
+algorithm like DEFLATE (zip) so that you can still decode correct
+parts even if some are corrupted (else with a solid archive, if one
+byte is corrupted, the whole archive may become unreadable).
+
+However, in the case that you compress your files, you should generate
+the ecc file only *after* compression, so that the ecc file applies to
+the compressed archive instead of the uncompressed files, else you
+risk being unable to correct your files because the uncompression of
+corrupted parts may output gibberish, and length extended corrupted
+parts (and if the size is different, Reed-Solomon will just freak
+out).
+
+-  Can I encrypt my data files and my ecc file ?
+
+NEVER encrypt your ecc file, this is totally useless and
+counterproductive.
+
+You can encrypt your data files, but choose a non-solid algorithm
+(like AES if I'm not mistaken) so that corrupted parts do not prevent
+the decoding of subsequent correct parts. Of course, you're lowering a
+bit your chances of recovering your data files by encrypting them (the
+best chance to keep data for the long term is to keep them in clear
+text), but if it's really necessary, using a non-solid encrypting
+scheme is a good compromise.
+
+You can generate an ecc file on your encrypted data files, thus
+*after* encryption, and keep the ecc file in clear text (never encrypt
+nor compress it). This is not a security risk at all since the ecc
+file does not give any information on the content inside your
+encrypted files, but rather just redundant info to correct corrupted
+bytes (however if you generate the ecc file on the data files before
+encryption, then it's clearly a security risk, and someone could
+recover your data without your permission).
+
+- What medium should I use to store my data?
+
+The details are long and a bit complicated (I may write a complete article
+about it in the future), but the tl;dr answer is that you should use *optical disks*,
+because it decouples the storage medium and the reading hardware
+(eg, at the opposite we have hard drives, which contains both the reading
+hardware and the storage medium, so if one fails, you lose both)
+and because it's most likely future-proof (you only need a laser, which
+is universal, the laser's parameters can always be tweaked).
+
+From scientific studies, it seems that, at the time of writing this (2015),
+BluRay HTL disks are the most resilient against environmental degradation.
+To raise the duration, you can also put optical disks in completely opaque boxes
+(to avoid light degradation) and in addition you can put any storage medium
+(not only optical disks, but also hard drives and anything really) in
+*completely* air-tight and water-tight bags or box and put in a fridge or a freezer.
+This is a law of nature: lower the temperature, lower will be the entropy, in other
+words lower will be the degradation over time. It works the same with digital data.
+
+- What file formats are the most recoverable?
+
+It's difficult to advise a specific format. What we can do is advise the characteristics
+of a good file format:
+
+  * future-proof (should be readable in the future).
+  * non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn't cause a problem to the decoding of other blocks).
+  * open source implementation available.
+  * minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).
+  * No magic bytes or header importance (ie, corrupting the header won't prevent opening the file).
+
+There are a few studies about the most resilient file formats, such as:
+
+  * `"Just one bit in a million: On the effects of data corruption in files" by Volker Heydegger <http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1>`_.
+  * `"Analysing the impact of file formats on data integrity" by Volker Heydegger <http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf>`_.
+  * `"A guide to formats", by The UK national archives <http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf>`_ (you want to look at the Recoverability entry in each table).
+
+- What is Reed-Solomon?
+
+If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon
+
+Also, you may want to read the following resources:
+
+  * "`Reed-Solomon codes for coders <https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders>`_", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.
+  * "Algebraic codes for data transmission", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books <https://books.google.fr/books?id=eQs2i-R9-oYC&lpg=PR11&ots=atCPQJm3OJ&dq=%22Algebraic%20codes%20for%20data%20transmission%22%2C%20Blahut%2C%20Richard%20E.%2C%202003%2C%20Cambridge%20university%20press.&lr&hl=fr&pg=PA193#v=onepage&q=%22Algebraic%20codes%20for%20data%20transmission%22,%20Blahut,%20Richard%20E.,%202003,%20Cambridge%20university%20press.&f=false>`_.
+
+
+.. |Example| image:: https://raw.githubusercontent.com/lrq3000/pyFileFixity/master/tux-example.jpg
+   :scale: 60 %
+   :alt: Image corruption and repair example
+.. |PyPI-Status| image:: https://img.shields.io/pypi/v/pyfilefixity.svg
+   :target: https://pypi.org/project/pyfilefixity
+.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/pyfilefixity.svg?logo=python&logoColor=white
+   :target: https://pypi.org/project/pyfilefixity
+.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/pyfilefixity.svg?label=pypi%20downloads&logo=python&logoColor=white
+   :target: https://pypi.org/project/pyfilefixity
+.. |Build-Status| image:: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml/badge.svg?event=push
+   :target: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml
+.. |Coverage| image:: https://codecov.io/github/lrq3000/pyFileFixity/coverage.svg?branch=master
+   :target: https://codecov.io/github/lrq3000/pyFileFixity?branch=master
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/easy_profiler.py` & `pyFileFixity-3.1.4/pyFileFixity/easy_profiler.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,157 +1,157 @@
-#!/usr/bin/env python
-#
-# Easy Profiler
-# Copyright (C) 2015 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#------------------------------
-#
-# This script is similar to what has been done in a paper, albeit in a dumbed down version (since they could automatically check for corruption measures based on file format): Heydegger, Volker. "Analysing the impact of file formats on data integrity." Archiving Conference. Vol. 2008. No. 1. Society for Imaging Science and Technology, 2008.
-# And another interesting paper by the same author: Heydegger, Volker. "Just one bit in a million: On the effects of data corruption in files." Research and Advanced Technology for Digital Libraries. Springer Berlin Heidelberg, 2009. 315-326.
-# Errors are not evenly spread but rather block level (thus concentrated). Addis, Matthew, et al. "Reliable Audiovisual Archiving Using Unreliable Storage Technology and Services." (2009).
-#
-
-# Import necessary libraries
-from lib.aux_funcs import fullpath
-import argparse
-import os, sys
-
-
-def main(argv=None):
-    if argv is None:
-        argv = sys.argv[1:]
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Easy Profiler for Python scripts
-Description: Provide an easy way to launch CPU/Memory profile (with GUI support) of python scripts. You can supply arguments of the target script by appending them at the end of the arguments for this script, without any special formatting (unrecognized arguments will be passed along to the target script).
-    '''
-    ep = ''' '''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter)
-    # Required arguments
-    main_parser.add_argument('--script', metavar='script.py', type=str, nargs=1, required=True,
-                        help='Path to the script to import and execute (the script must implement a main() function).')
-    main_parser.add_argument('--profile_log', metavar='profile.log', type=str, nargs=1, required=False,
-                        help='Path where to store the profile log.')
-    main_parser.add_argument('--cpu', action='store_true', required=False, default=False,
-                        help='CPU line-by-line profiler (pprofile.py).')
-    main_parser.add_argument('--cpu_stack', action='store_true', required=False, default=False,
-                        help='CPU stack (tree-like) profiler (pyinstrument.py).')
-    main_parser.add_argument('--memory', action='store_true', required=False, default=False,
-                        help='Memory line-by-line profiler (memory_profiler.py).')
-    main_parser.add_argument('--gui', action='store_true', required=False, default=False,
-                        help='GUI interface for the CPU line-by-line profiler (not ready for the memory profiler) using RunSnakeRun.')
-    # Optional arguments
-
-    #== Parsing the arguments
-    args, args_rest = main_parser.parse_known_args(argv) # Storing all arguments to args
-
-    #-- Set variables from arguments
-    script = args.script[0]
-    cpu = args.cpu
-    memory = args.memory
-    gui = args.gui
-    cpu_stack = args.cpu_stack
-
-    profile_log = None
-    if args.profile_log:
-        profile_log = fullpath(args.profile_log[0])
-    
-    if script.find('.') == -1:
-        script = script + '.py'
-
-    if not os.path.isfile(script):
-        print("File does not exist: %s" % script)
-    else:
-        print("==== LAUNCHING PROFILING ====")
-    
-        scriptname = os.path.splitext(script)[0] # remove any extension to be able to import
-        scriptmod = __import__(scriptname) # dynamic import
-
-        if cpu:
-            # Line-by-line CPU runtime profiling (pure python using pprofile)
-            from lib.profilers.pprofile import pprofile
-            # Load the profiler
-            pprof = pprofile.Profile()
-            # Launch experiment under the profiler
-            args_rest = ' '.join(args_rest)
-            with pprof:
-                scriptmod.main(args_rest)
-            # Print the result
-            print("==> Profiling done.")
-            if profile_log:
-                pprof.dump_stats(profile_log)
-            else:
-                pprof.print_stats()
-        elif memory:
-            # Line-by-line memory profiler (pure python using memory_profiler)
-            from lib.profilers.memory_profiler import memory_profiler
-            # Load the memory profiler
-            mprof = memory_profiler.LineProfiler()
-            # Launch experiment under the memory profiler
-            args_rest = ' '.join(args_rest)
-            mpr = mprof(scriptmod.main)(args_rest)
-            # Print results
-            print("==> Profiling done.")
-            if not mprof.code_map: # just to check that everything's alright
-                print 'Error: the memory_profiler did not work! Please check that your are correctly calling mprof(func)(arguments)'
-            else:
-                if profile_log:
-                    with open(profile_log, 'w') as pf:
-                        memory_profiler.show_results(mprof, stream=pf)
-                else:
-                    print(memory_profiler.show_results(mprof, stream=None))
-        elif gui:
-            # Visual profiler with GUI (runsnakerun)
-            # NOTE: you need wxPython to launch it
-            from lib.profilers.visual.debug import runprofilerandshow
-            if not profile_log: profile_log = 'profile.log' # a profile log is necessary to use the GUI because the profile will be generated separately, and then the GUI will read the file. File based communication is currently the only way to communicate with RunSnakeRun.
-            args_rest = ' '.join(args_rest)
-            runprofilerandshow('import '+scriptname+"\n"+scriptname+'.main', profile_log, argv=args_rest, calibrate=True)
-            #runscriptprofilerandshow(script, profile_log, argv=args_rest, calibrate=True)
-        elif cpu_stack:
-            # Tree like cpu profiling
-            from lib.profilers.pyinstrument import Profiler
-            from lib.profilers.pyinstrument.profiler import SignalUnavailableError
-            try:
-                profiler = Profiler() # or if signal is not available on your system, use Profiler(use_signal=False), see below
-            except SignalUnavailableError as e:
-                profiler = Profiler(use_signal=False)
-            profiler.start()
-            scriptmod.main(args_rest)
-            profiler.stop()
-            print("==> Profiling done.")
-            if profile_log:
-                import codecs
-                with codecs.open(profile_log, 'wb', encoding='utf8') as pf:
-                    pf.write( profiler.output_text(unicode=True, color=True) )
-            else:
-                print(profiler.output_text(unicode=True, color=True))
-
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Easy Profiler
+# Copyright (C) 2015 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#------------------------------
+#
+# This script is similar to what has been done in a paper, albeit in a dumbed down version (since they could automatically check for corruption measures based on file format): Heydegger, Volker. "Analysing the impact of file formats on data integrity." Archiving Conference. Vol. 2008. No. 1. Society for Imaging Science and Technology, 2008.
+# And another interesting paper by the same author: Heydegger, Volker. "Just one bit in a million: On the effects of data corruption in files." Research and Advanced Technology for Digital Libraries. Springer Berlin Heidelberg, 2009. 315-326.
+# Errors are not evenly spread but rather block level (thus concentrated). Addis, Matthew, et al. "Reliable Audiovisual Archiving Using Unreliable Storage Technology and Services." (2009).
+#
+
+# Import necessary libraries
+from lib.aux_funcs import fullpath
+import argparse
+import os, sys
+
+
+def main(argv=None):
+    if argv is None:
+        argv = sys.argv[1:]
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Easy Profiler for Python scripts
+Description: Provide an easy way to launch CPU/Memory profile (with GUI support) of python scripts. You can supply arguments of the target script by appending them at the end of the arguments for this script, without any special formatting (unrecognized arguments will be passed along to the target script).
+    '''
+    ep = ''' '''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter)
+    # Required arguments
+    main_parser.add_argument('--script', metavar='script.py', type=str, nargs=1, required=True,
+                        help='Path to the script to import and execute (the script must implement a main() function).')
+    main_parser.add_argument('--profile_log', metavar='profile.log', type=str, nargs=1, required=False,
+                        help='Path where to store the profile log.')
+    main_parser.add_argument('--cpu', action='store_true', required=False, default=False,
+                        help='CPU line-by-line profiler (pprofile.py).')
+    main_parser.add_argument('--cpu_stack', action='store_true', required=False, default=False,
+                        help='CPU stack (tree-like) profiler (pyinstrument.py).')
+    main_parser.add_argument('--memory', action='store_true', required=False, default=False,
+                        help='Memory line-by-line profiler (memory_profiler.py).')
+    main_parser.add_argument('--gui', action='store_true', required=False, default=False,
+                        help='GUI interface for the CPU line-by-line profiler (not ready for the memory profiler) using RunSnakeRun.')
+    # Optional arguments
+
+    #== Parsing the arguments
+    args, args_rest = main_parser.parse_known_args(argv) # Storing all arguments to args
+
+    #-- Set variables from arguments
+    script = args.script[0]
+    cpu = args.cpu
+    memory = args.memory
+    gui = args.gui
+    cpu_stack = args.cpu_stack
+
+    profile_log = None
+    if args.profile_log:
+        profile_log = fullpath(args.profile_log[0])
+    
+    if script.find('.') == -1:
+        script = script + '.py'
+
+    if not os.path.isfile(script):
+        print("File does not exist: %s" % script)
+    else:
+        print("==== LAUNCHING PROFILING ====")
+    
+        scriptname = os.path.splitext(script)[0] # remove any extension to be able to import
+        scriptmod = __import__(scriptname) # dynamic import
+
+        if cpu:
+            # Line-by-line CPU runtime profiling (pure python using pprofile)
+            from lib.profilers.pprofile import pprofile
+            # Load the profiler
+            pprof = pprofile.Profile()
+            # Launch experiment under the profiler
+            args_rest = ' '.join(args_rest)
+            with pprof:
+                scriptmod.main(args_rest)
+            # Print the result
+            print("==> Profiling done.")
+            if profile_log:
+                pprof.dump_stats(profile_log)
+            else:
+                pprof.print_stats()
+        elif memory:
+            # Line-by-line memory profiler (pure python using memory_profiler)
+            from lib.profilers.memory_profiler import memory_profiler
+            # Load the memory profiler
+            mprof = memory_profiler.LineProfiler()
+            # Launch experiment under the memory profiler
+            args_rest = ' '.join(args_rest)
+            mpr = mprof(scriptmod.main)(args_rest)
+            # Print results
+            print("==> Profiling done.")
+            if not mprof.code_map: # just to check that everything's alright
+                print 'Error: the memory_profiler did not work! Please check that your are correctly calling mprof(func)(arguments)'
+            else:
+                if profile_log:
+                    with open(profile_log, 'w') as pf:
+                        memory_profiler.show_results(mprof, stream=pf)
+                else:
+                    print(memory_profiler.show_results(mprof, stream=None))
+        elif gui:
+            # Visual profiler with GUI (runsnakerun)
+            # NOTE: you need wxPython to launch it
+            from lib.profilers.visual.debug import runprofilerandshow
+            if not profile_log: profile_log = 'profile.log' # a profile log is necessary to use the GUI because the profile will be generated separately, and then the GUI will read the file. File based communication is currently the only way to communicate with RunSnakeRun.
+            args_rest = ' '.join(args_rest)
+            runprofilerandshow('import '+scriptname+"\n"+scriptname+'.main', profile_log, argv=args_rest, calibrate=True)
+            #runscriptprofilerandshow(script, profile_log, argv=args_rest, calibrate=True)
+        elif cpu_stack:
+            # Tree like cpu profiling
+            from lib.profilers.pyinstrument import Profiler
+            from lib.profilers.pyinstrument.profiler import SignalUnavailableError
+            try:
+                profiler = Profiler() # or if signal is not available on your system, use Profiler(use_signal=False), see below
+            except SignalUnavailableError as e:
+                profiler = Profiler(use_signal=False)
+            profiler.start()
+            scriptmod.main(args_rest)
+            profiler.stop()
+            print("==> Profiling done.")
+            if profile_log:
+                import codecs
+                with codecs.open(profile_log, 'wb', encoding='utf8') as pf:
+                    pf.write( profiler.output_text(unicode=True, color=True) )
+            else:
+                print(profiler.output_text(unicode=True, color=True))
+
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/ecc_specification.txt` & `pyFileFixity-3.1.4/pyFileFixity/ecc_specification.txt`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,7 @@
-**This is an example ECC file with clear specification of each field. Lines beginning with ** and finishing with a line return \n are considered comments. In fact, all lines before the first entrymarker are considered comments and are just skipped. Note that after the headers comments, the file is binary, and thus there's no line returns.
-**SOFTWAREIDENTv111...000**
-** Parameters: <recap-of-parameters-here>
-** Parameters: <recap-of-parameters-here>
-** Parameters: <recap-of-parameters-here>
-** Generated under <recap-of-ecc-algorithm-and-parameters-here>.
+**This is an example ECC file with clear specification of each field. Lines beginning with ** and finishing with a line return \n are considered comments. In fact, all lines before the first entrymarker are considered comments and are just skipped. Note that after the headers comments, the file is binary, and thus there's no line returns.
+**SOFTWAREIDENTv111...000**
+** Parameters: <recap-of-parameters-here>
+** Parameters: <recap-of-parameters-here>
+** Parameters: <recap-of-parameters-here>
+** Generated under <recap-of-ecc-algorithm-and-parameters-here>.
 <entry-marker>[relative-file1-path.file-extension]<field_delim>[file1-size]<field_delim>[relative-file1-path-ecc]<field_delim>[file1-size-ecc]<field_delim>[block0-hash][block0-ecc][block1-hash][block1-ecc][block2-hash][block2-ecc]...<entry-marker>[relative-file2-path.file-extension]<field_delim>[file2-size]<field_delim>[relative-file2-path-ecc]<field_delim>[file2-size-ecc]<field_delim>[block0-hash][block0-ecc][block1-hash][block1-ecc][block2-hash][block2-ecc]...
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/ecc_speedtest.py` & `pyFileFixity-3.1.4/pyFileFixity/ecc_speedtest.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,209 +1,209 @@
-#!/usr/bin/env python
-#
-# ECC Speed Tester
-# Copyright (C) 2015-2023 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# ECC and hashing facade libraries
-from lib._compat import _range, _bytes, _str
-from lib.aux_funcs import sizeof_fmt
-from lib.eccman import ECCMan, compute_ecc_params
-from lib.hasher import Hasher
-from reedsolo import ReedSolomonError
-
-# Import necessary libraries
-import argparse
-import math, random
-import datetime, time
-
-# Import progress bar
-import tqdm
-
-
-def gen_random_string(n, size):
-    '''Generate very fastly a random hexadecimal string. Kudos to jcdryer http://stackoverflow.com/users/131084/jcdyer'''
-    # The main insight is that we can just compute a big int of the total message size and then convert it to a string
-
-    # Init length of string (this will be used to convert the bigint to a string)
-    hexstr = '%0'+str(size)+'x'
-
-    for _ in _range(n):
-        # Generate a random string
-        yield hexstr % random.randrange(16**size) # Generate a random bigint of the size we require, and convert to a string
-
-def format_sizeof(num, suffix='bytes'):
-    return sizeof_fmt(num, suffix, 1000)
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''ECC Speedtest
-Description: Tests encoding and/or decoding speed of error correction codes using the specified configuration and ecc algorithms.
-    '''
-    ep = ''' '''
-
-    #-- Constructing the parser
-    # Only command-line usage, use the standard argparse
-    # Delete the special argument to avoid unrecognized argument error in argparse
-    # Initialize the normal argparse parser
-    # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-    main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-    # Define dummy dict to keep compatibile with command-line usage
-    widget_dir = {}
-    widget_filesave = {}
-    widget_file = {}
-    widget_text = {}
-
-    # Required arguments
-
-    # Optional arguments
-    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
-                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
-    main_parser.add_argument('--max_block_size', type=int, default=255, required=False,
-                        help='Max block size, cannot exceed 255 (because limited to Galois Field 2^8 in this speedtest). Use a smaller max_block_size to greatly speedup the operations! This is the trick used to compute very quickly RS ECC on optical discs. You give up a bit of resiliency of course (because blocks are smaller, thus you protect a smaller number of characters per ECC. In the end, this should not change much about real resiliency, but in case you get a big bit error burst on a contiguous block, you may lose a whole block at once. That is why using RS255 is better, but it is very time consuming. However, the resiliency ratios still hold, so for any other case of bit-flipping with average-sized bursts, this should not be a problem as long as the size of the bursts is smaller than an ecc block.)', **widget_text)
-    main_parser.add_argument('--resilience_rate', type=float, default=0.2, required=False,
-                        help='Number of ecc symbols to compute per block, as a percent of the whole message size.', **widget_text)
-    main_parser.add_argument('--tamper_rate', type=float, default=0.4, required=False,
-                        help='How much we tamper each message before decoding. Note that tamper rate is relative to the number of ecc bytes, not the whole message (unlike --resilience_rate).', **widget_text)
-    main_parser.add_argument('--tamper_mode', type=str, default='noise', required=False,
-                        help='How do we tamper each message before decoding, can be "noise" or "erasure" (tamper_rate can be 2x as much with erasure than with noise). Noise means that characters that get tampered are considered errors, their positions will not be given to the decoder. Hence, with erasure mode, only erasures are done as the positions of tampered characters are provided to the decoder, so that the Reed-Solomon algorithm works slightly differently and may be much faster since it does not have to search for where errors are located.', **widget_text)
-    main_parser.add_argument('--msg_nb', type=int, default=1000000, required=False,
-                        help='Total number of messages/blocks to test.', **widget_text)
-    main_parser.add_argument('--decoding_mode', type=int, required=False, default=1,
-                        help='0 for only encoding, 1 for both encoding then decoding, 2 for only decoding (which includes an encoding step).')
-    main_parser.add_argument('--subchunking', action='store_true', required=False, default=False,
-                        help='Subchunk directly in the speedtest, instead of letting the Reed-Solomon library do it.')
-    main_parser.add_argument('--subchunking_size', type=int, required=False, default=50,
-                        help='Size of subchunks, if enabled.')
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    # Setup configuration variables. Change here if you want to.
-    max_block_size = args.max_block_size
-    resilience_rate = args.resilience_rate
-    ecc_algo = args.ecc_algo
-    msg_nb = args.msg_nb
-    tamper_rate = args.tamper_rate
-    tamper_mode = args.tamper_mode
-    decoding_mode = args.decoding_mode
-    subchunking = args.subchunking
-    subchunk_size = args.subchunking_size
-
-    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
-    hasher_none = Hasher('none') # for index ecc we don't use any hash
-    ecc_params = compute_ecc_params(max_block_size, resilience_rate, hasher_none)
-    ecc_params_subchunk = compute_ecc_params(subchunk_size, resilience_rate, hasher_none)
-    ecc_manager = ECCMan(max_block_size, ecc_params["message_size"], algo=ecc_algo)
-    ecc_manager_subchunk = ECCMan(subchunk_size, ecc_params_subchunk["message_size"], algo=ecc_algo)
-
-    # == Main loop
-    print("====================================")
-    print("ECC Speed Test, started on %s" % datetime.datetime.now().isoformat())
-    print("====================================")
-    print("ECC algorithm: %i." % ecc_algo)
-    print("%s." % ("Only encoding test" if decoding_mode == 0 else ("Encoding and decoding test" if decoding_mode == 1 else "Decoding test only (including an encoding step)")))
-
-    # -- Encoding test
-    # IMPORTANT: we do NOT check the correctness of encoding, only the speed! It's up to you to verify that you are computing the ecc correctly.
-    if not decoding_mode == 2:
-        total_time = 0
-        total_size = msg_nb*max_block_size
-        bardisp = tqdm.tqdm(total=total_size, leave=True, desc='ENC', unit='B', unit_scale=True, ncols=79, mininterval=0.5) # display progress bar based on the number of bytes encoded
-        k = ecc_params["message_size"]
-        # Generate a random string and encode it
-        for msg in gen_random_string(msg_nb, k):
-            start = time.process_time()  # time.clock() was dropped in Py3.8, use time.perf_counter() instead to time performance including sleep, or time.process_time() without sleep periods.
-            if subchunking:
-                for i in xrange(0, len(msg), subchunk_size):
-                    ecc_manager_subchunk.encode(msg[i:i+subchunk_size])
-            else:
-                ecc_manager.encode(msg)
-            total_time += time.process_time() - start
-            bardisp.update(max_block_size)
-        bardisp.close()
-        print("Encoding: total time elapsed: %f sec for %s of data. Real Speed (only encoding, no other computation): %s." % (total_time, format_sizeof(total_size, 'B'), format_sizeof(total_size/total_time, 'B/sec') ))
-    
-    # -- Decoding test
-    if decoding_mode in [1, 2]:
-        total_time = 0
-        total_size = msg_nb*max_block_size
-        bardisp = tqdm.tqdm(total=total_size*2, leave=True, desc='DEC', unit='B', unit_scale=True) # display progress bar based on the number of bytes encoded
-        # Generate a random string and encode it
-        for msg in gen_random_string(msg_nb, ecc_params["message_size"]):
-            # Make it into a bytearray first
-            msg = bytearray(msg, 'utf-8')
-            # Computing the ecc first
-            ecc = ecc_manager.encode(msg)
-
-            # Then tamper it randomly
-            # First generate a list of random indices where we will tamper
-            tamper_idx = random.sample(_range(ecc_params["message_size"]), int(math.floor(ecc_params["ecc_size"] * tamper_rate)))
-            # Convert to bytearray to easily modify characters in the message
-            msg_tampered = bytearray(msg)
-            # Tamper the characters
-            for pos in tamper_idx:
-                if tamper_mode == 'n' or tamper_mode == 'noise': # Noising the character (set a random ASCII character)
-                    msg_tampered[pos] = random.randint(0,255)
-                elif tamper_mode == 'e' or tamper_mode == 'erasure': # Erase the character (set a null byte)
-                    msg_tampered[pos] = 0
-            # Convert back to a string
-            msg_tampered = _bytes(msg_tampered)
-            ecc = _bytes(ecc)
-
-            # Decode the tampered message with ecc
-            start = time.process_time()
-            try:
-                msg_repaired, ecc_repaired = ecc_manager.decode(msg_tampered, ecc)
-                # Check if the decoding was successful, else there's a problem, the decoding may be buggy
-                if not ecc_manager.check(msg_repaired, ecc_repaired): raise ReedSolomonError
-            except ReedSolomonError:
-                print("Warning, there was an error while decoding. Please check your parameters (tamper_rate not too high) or the decoding procedure.")
-                pass
-            total_time += time.process_time() - start
-            bardisp.update(max_block_size*2)  # update x2 because we encode AND decode
-        bardisp.close()
-        print("Decoding: total time elapsed: %f sec for %s of data. Real Speed (only decoding, no other computation): %s." % (total_time, format_sizeof(total_size, 'B'), format_sizeof(total_size/total_time, 'B/sec') ))
-
-    return 0
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# ECC Speed Tester
+# Copyright (C) 2015-2023 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# ECC and hashing facade libraries
+from lib._compat import _range, _bytes, _str
+from lib.aux_funcs import sizeof_fmt
+from lib.eccman import ECCMan, compute_ecc_params
+from lib.hasher import Hasher
+from reedsolo import ReedSolomonError
+
+# Import necessary libraries
+import argparse
+import math, random
+import datetime, time
+
+# Import progress bar
+import tqdm
+
+
+def gen_random_string(n, size):
+    '''Generate very fastly a random hexadecimal string. Kudos to jcdryer http://stackoverflow.com/users/131084/jcdyer'''
+    # The main insight is that we can just compute a big int of the total message size and then convert it to a string
+
+    # Init length of string (this will be used to convert the bigint to a string)
+    hexstr = '%0'+str(size)+'x'
+
+    for _ in _range(n):
+        # Generate a random string
+        yield hexstr % random.randrange(16**size) # Generate a random bigint of the size we require, and convert to a string
+
+def format_sizeof(num, suffix='bytes'):
+    return sizeof_fmt(num, suffix, 1000)
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''ECC Speedtest
+Description: Tests encoding and/or decoding speed of error correction codes using the specified configuration and ecc algorithms.
+    '''
+    ep = ''' '''
+
+    #-- Constructing the parser
+    # Only command-line usage, use the standard argparse
+    # Delete the special argument to avoid unrecognized argument error in argparse
+    # Initialize the normal argparse parser
+    # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+    main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+    # Define dummy dict to keep compatibile with command-line usage
+    widget_dir = {}
+    widget_filesave = {}
+    widget_file = {}
+    widget_text = {}
+
+    # Required arguments
+
+    # Optional arguments
+    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
+                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
+    main_parser.add_argument('--max_block_size', type=int, default=255, required=False,
+                        help='Max block size, cannot exceed 255 (because limited to Galois Field 2^8 in this speedtest). Use a smaller max_block_size to greatly speedup the operations! This is the trick used to compute very quickly RS ECC on optical discs. You give up a bit of resiliency of course (because blocks are smaller, thus you protect a smaller number of characters per ECC. In the end, this should not change much about real resiliency, but in case you get a big bit error burst on a contiguous block, you may lose a whole block at once. That is why using RS255 is better, but it is very time consuming. However, the resiliency ratios still hold, so for any other case of bit-flipping with average-sized bursts, this should not be a problem as long as the size of the bursts is smaller than an ecc block.)', **widget_text)
+    main_parser.add_argument('--resilience_rate', type=float, default=0.2, required=False,
+                        help='Number of ecc symbols to compute per block, as a percent of the whole message size.', **widget_text)
+    main_parser.add_argument('--tamper_rate', type=float, default=0.4, required=False,
+                        help='How much we tamper each message before decoding. Note that tamper rate is relative to the number of ecc bytes, not the whole message (unlike --resilience_rate).', **widget_text)
+    main_parser.add_argument('--tamper_mode', type=str, default='noise', required=False,
+                        help='How do we tamper each message before decoding, can be "noise" or "erasure" (tamper_rate can be 2x as much with erasure than with noise). Noise means that characters that get tampered are considered errors, their positions will not be given to the decoder. Hence, with erasure mode, only erasures are done as the positions of tampered characters are provided to the decoder, so that the Reed-Solomon algorithm works slightly differently and may be much faster since it does not have to search for where errors are located.', **widget_text)
+    main_parser.add_argument('--msg_nb', type=int, default=1000000, required=False,
+                        help='Total number of messages/blocks to test.', **widget_text)
+    main_parser.add_argument('--decoding_mode', type=int, required=False, default=1,
+                        help='0 for only encoding, 1 for both encoding then decoding, 2 for only decoding (which includes an encoding step).')
+    main_parser.add_argument('--subchunking', action='store_true', required=False, default=False,
+                        help='Subchunk directly in the speedtest, instead of letting the Reed-Solomon library do it.')
+    main_parser.add_argument('--subchunking_size', type=int, required=False, default=50,
+                        help='Size of subchunks, if enabled.')
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    # Setup configuration variables. Change here if you want to.
+    max_block_size = args.max_block_size
+    resilience_rate = args.resilience_rate
+    ecc_algo = args.ecc_algo
+    msg_nb = args.msg_nb
+    tamper_rate = args.tamper_rate
+    tamper_mode = args.tamper_mode
+    decoding_mode = args.decoding_mode
+    subchunking = args.subchunking
+    subchunk_size = args.subchunking_size
+
+    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
+    hasher_none = Hasher('none') # for index ecc we don't use any hash
+    ecc_params = compute_ecc_params(max_block_size, resilience_rate, hasher_none)
+    ecc_params_subchunk = compute_ecc_params(subchunk_size, resilience_rate, hasher_none)
+    ecc_manager = ECCMan(max_block_size, ecc_params["message_size"], algo=ecc_algo)
+    ecc_manager_subchunk = ECCMan(subchunk_size, ecc_params_subchunk["message_size"], algo=ecc_algo)
+
+    # == Main loop
+    print("====================================")
+    print("ECC Speed Test, started on %s" % datetime.datetime.now().isoformat())
+    print("====================================")
+    print("ECC algorithm: %i." % ecc_algo)
+    print("%s." % ("Only encoding test" if decoding_mode == 0 else ("Encoding and decoding test" if decoding_mode == 1 else "Decoding test only (including an encoding step)")))
+
+    # -- Encoding test
+    # IMPORTANT: we do NOT check the correctness of encoding, only the speed! It's up to you to verify that you are computing the ecc correctly.
+    if not decoding_mode == 2:
+        total_time = 0
+        total_size = msg_nb*max_block_size
+        bardisp = tqdm.tqdm(total=total_size, leave=True, desc='ENC', unit='B', unit_scale=True, ncols=79, mininterval=0.5) # display progress bar based on the number of bytes encoded
+        k = ecc_params["message_size"]
+        # Generate a random string and encode it
+        for msg in gen_random_string(msg_nb, k):
+            start = time.process_time()  # time.clock() was dropped in Py3.8, use time.perf_counter() instead to time performance including sleep, or time.process_time() without sleep periods.
+            if subchunking:
+                for i in xrange(0, len(msg), subchunk_size):
+                    ecc_manager_subchunk.encode(msg[i:i+subchunk_size])
+            else:
+                ecc_manager.encode(msg)
+            total_time += time.process_time() - start
+            bardisp.update(max_block_size)
+        bardisp.close()
+        print("Encoding: total time elapsed: %f sec for %s of data. Real Speed (only encoding, no other computation): %s." % (total_time, format_sizeof(total_size, 'B'), format_sizeof(total_size/total_time, 'B/sec') ))
+    
+    # -- Decoding test
+    if decoding_mode in [1, 2]:
+        total_time = 0
+        total_size = msg_nb*max_block_size
+        bardisp = tqdm.tqdm(total=total_size*2, leave=True, desc='DEC', unit='B', unit_scale=True) # display progress bar based on the number of bytes encoded
+        # Generate a random string and encode it
+        for msg in gen_random_string(msg_nb, ecc_params["message_size"]):
+            # Make it into a bytearray first
+            msg = bytearray(msg, 'utf-8')
+            # Computing the ecc first
+            ecc = ecc_manager.encode(msg)
+
+            # Then tamper it randomly
+            # First generate a list of random indices where we will tamper
+            tamper_idx = random.sample(_range(ecc_params["message_size"]), int(math.floor(ecc_params["ecc_size"] * tamper_rate)))
+            # Convert to bytearray to easily modify characters in the message
+            msg_tampered = bytearray(msg)
+            # Tamper the characters
+            for pos in tamper_idx:
+                if tamper_mode == 'n' or tamper_mode == 'noise': # Noising the character (set a random ASCII character)
+                    msg_tampered[pos] = random.randint(0,255)
+                elif tamper_mode == 'e' or tamper_mode == 'erasure': # Erase the character (set a null byte)
+                    msg_tampered[pos] = 0
+            # Convert back to a string
+            msg_tampered = _bytes(msg_tampered)
+            ecc = _bytes(ecc)
+
+            # Decode the tampered message with ecc
+            start = time.process_time()
+            try:
+                msg_repaired, ecc_repaired = ecc_manager.decode(msg_tampered, ecc)
+                # Check if the decoding was successful, else there's a problem, the decoding may be buggy
+                if not ecc_manager.check(msg_repaired, ecc_repaired): raise ReedSolomonError
+            except ReedSolomonError:
+                print("Warning, there was an error while decoding. Please check your parameters (tamper_rate not too high) or the decoding procedure.")
+                pass
+            total_time += time.process_time() - start
+            bardisp.update(max_block_size*2)  # update x2 because we encode AND decode
+        bardisp.close()
+        print("Decoding: total time elapsed: %f sec for %s of data. Real Speed (only decoding, no other computation): %s." % (total_time, format_sizeof(total_size, 'B'), format_sizeof(total_size/total_time, 'B/sec') ))
+
+    return 0
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/filetamper.py` & `pyFileFixity-3.1.4/pyFileFixity/filetamper.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,301 +1,301 @@
-#!/usr/bin/env python
-#
-# Random file characters tamperer
-# Randomly tampers characters in a file in order to test for integrity after.
-# Copyright (C) 2015 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#------------------------------
-#
-# This script is similar to what has been done in a paper, albeit in a dumbed down version (since they could automatically check for corruption measures based on file format): Heydegger, Volker. "Analysing the impact of file formats on data integrity." Archiving Conference. Vol. 2008. No. 1. Society for Imaging Science and Technology, 2008.
-# And another interesting paper by the same author: Heydegger, Volker. "Just one bit in a million: On the effects of data corruption in files." Research and Advanced Technology for Digital Libraries. Springer Berlin Heidelberg, 2009. 315-326.
-# Errors are not evenly spread but rather block level (thus concentrated). Addis, Matthew, et al. "Reliable Audiovisual Archiving Using Unreliable Storage Technology and Services." (2009).
-#
-
-# future division is important to divide integers and get as
-# a result precise floating numbers (instead of truncated int)
-#from __future__ import division, absolute_import, with_statement  # unnecessary since we dropped Python 2 support, but still can be interesting if we reimplement in Cython, Cython v3 will be needed for the maths
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, _range
-from lib.aux_funcs import fullpath, recwalk, is_dir, is_file, is_dir_or_file
-import argparse
-import os, sys, random
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-from tqdm import tqdm
-
-
-#***********************************
-#     AUXILIARY FUNCTIONS
-#***********************************
-
-def tamper_file_at(path, pos=0, replace_str=None):
-    """ Tamper a file at the given position and using the given string """
-    if not replace_str:
-        replace_str = "\x00"
-    try:
-        with open(path, "r+b") as fh:
-            if pos < 0: # if negative, we calculate the position backward from the end of file
-                fsize = os.fstat(fh.fileno()).st_size
-                pos = fsize + pos
-            fh.seek(pos)
-            fh.write(replace_str)
-    except IOError:
-        return False
-    finally:
-        try:
-            fh.close()
-        except Exception:
-            pass
-    return True
-
-def tamper_file(filepath, mode='e', proba=0.03, block_proba=None, blocksize=65535, burst_length=None, header=None):
-    """ Randomly tamper a file's content """
-    if header and header > 0:
-        blocksize = header
-
-    tamper_count = 0 # total number of characters tampered in the file
-    total_size = 0 # total buffer size, NOT necessarily the total file size (depends if you set header or not)
-    with open(filepath, "r+b") as fh: # 'r+' allows to read AND overwrite characters. Else any other option won't allow both ('a+' read and append, 'w+' erases the file first then allow to read and write), and 'b' is just for binary because we can open any filetype.
-        if proba >= 1: proba = 1.0/os.fstat(fh.fileno()).st_size * proba # normalizing probability if it's an integer (ie: the number of characters to flip on average)
-        buf = fh.read(blocksize) # We process blocks by blocks because it's a lot faster (IO is still the slowest operation in any computing system)
-        while len(buf) > 0:
-            total_size += len(buf)
-            if not block_proba or (random.random() < block_proba): # If block tampering is enabled, process only if this block is selected by probability
-                pos2tamper = []
-                burst_remain = 0 # if burst is enabled and corruption probability is triggered, then we will here store the remaining number of characters to corrupt (the length is uniformly sampled over the range specified in arguments)
-                # Create the list of bits to tamper (it's a lot more efficient to precompute the list of characters to corrupt, and then modify in the file the characters all at once)
-                for i in _range(len(buf)):
-                    if burst_remain > 0 or (random.random() < proba): # Corruption probability: corrupt only if below the bit-flip proba
-                        pos2tamper.append(i) # keep this character's position in the to-be-corrupted list
-                        if burst_remain > 0: # if we're already in a burst, we minus one and continue onto the next character
-                            burst_remain -= 1
-                        elif burst_length: # else we're not in a burst, we create one (triggered by corruption probability: as soon as one character triggers the corruption probability, then we do a burst)
-                            burst_remain = random.randint(burst_length[0], burst_length[1]) - 1 # if burst is enabled, then we randomly (uniformly) pick a random length for the burst between the range specified, and since we already tampered one character, we minus 1
-                # If there's any character to tamper in the list, we tamper the string
-                if pos2tamper:
-                    tamper_count = tamper_count + len(pos2tamper)
-                    #print("Before: %s" % buf)
-                    buf = bytearray(buf) # Strings in Python are immutable, thus we need to convert to a bytearray
-                    for pos in pos2tamper:
-                        if mode == 'e' or mode == 'erasure': # Erase the character (set a null byte)
-                            buf[pos] = 0
-                        elif mode == 'n' or mode == 'noise': # Noising the character (set a random ASCII character)
-                            buf[pos] = random.randint(0,255)
-                    #print("After: %s" % buf)
-                    # Overwriting the string into the file
-                    prevpos = fh.tell() # need to store and place back the seek cursor because after the write, if it's the end of the file, the next read may be buggy (getting characters that are not part of the file)
-                    fh.seek(fh.tell()-len(buf)) # Move the cursor at the beginning of the string we just read
-                    fh.write(buf) # Overwrite it
-                    fh.seek(prevpos) # Restore the previous position after the string
-            # If we only tamper the header, we stop here by setting the buffer to an empty string
-            if header and header > 0:
-                buf = ''
-            # Else we continue to the next data block
-            else:
-                # Load the next characters from file
-                buf = fh.read(blocksize)
-    return [tamper_count, total_size]
-
-def tamper_dir(inputpath, *args, **kwargs):
-    """ Randomly tamper the files content in a directory tree, recursively """
-    silent = kwargs.get('silent', False)
-    if 'silent' in kwargs: del kwargs['silent']
-
-    filescount = 0
-    for _ in tqdm(recwalk(inputpath), desc='Precomputing', disable=silent):
-        filescount += 1
-
-    files_tampered = 0
-    tamper_count = 0
-    total_size = 0
-    for dirname, filepath in tqdm(recwalk(inputpath), total=filescount, leave=True, desc='Tamper file n.', disable=silent):
-        tcount, tsize = tamper_file(os.path.join(dirname, filepath), *args, **kwargs)
-        if tcount > 0:
-            tamper_count += tcount
-            files_tampered += 1
-        total_size += tsize
-    return [files_tampered, filescount, tamper_count, total_size]
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Random file/directory characters tamperer in Python
-Description: Randomly tampers characters in a file or in a directory tree recursively (useful to test for integrity/repair after).
-WARNING: this will tamper the file you specify. Please ensure you keep a copy of the original!
-    '''
-    ep = '''NOTE: this script tampers at the character (byte) level, not the bits! Thus the measures you will get here may be different from those you will find in papers (you must divide your probability by 8).'''
-
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='filetotamper.ext', type=is_dir_or_file, nargs=1, required=True,
-                        help='Path to the file (or directory tree) to tamper.', **widget_dir)
-    main_parser.add_argument('-m', '--mode', metavar='e, erasure, n, noise', type=str, nargs=1, required=True,
-                        help='Tampering mode: erasure or noise?', **widget_text)
-    main_parser.add_argument('-p', '--probability', type=float, nargs=1, required=True,
-                        help='Probability of corruption (float between 0.0 and 1.0)', **widget_text)
-    # Optional arguments
-    main_parser.add_argument('--block_probability', type=float, nargs=1, required=False,
-                        help='Probability of block tampering (between 0.0 and 1.0, do not set it if you want to spread errors evenly, but researchs have shown that errors are rather at block level and not evenly distributed)', **widget_text)
-    main_parser.add_argument('-b', '--burst_length', metavar="startint|endint", type=str, required=False,
-                        help='If specified, this will define the number of consecutive characters that will be corrupted when the corruption probability (--probability) is triggered. Specify a range startint|endint, the burst length will be uniformly sampled over this range.')
-    main_parser.add_argument('--header', type=int, required=False,
-                        help='Only tamper the header of the file')
-
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set variables from arguments
-    filepath = fullpath(args.input[0])
-    mode = args.mode[0]
-    proba = float(args.probability[0])
-    verbose = args.verbose
-    silent = args.silent
-
-    burst_length = args.burst_length
-    if burst_length: burst_length = [int(r) for r in burst_length.split('|')] # split range and convert to int
-
-    block_proba = None
-    if args.block_probability:
-        block_proba = float(args.block_probability[0])
-
-    blocksize = 65536
-    header = args.header
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log[0], 'a', nostdout=silent)
-        #sys.stdout = Tee(args.log[0], 'a')
-        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-    # == PROCESSING BRANCHING == #
-    # Sanity check
-    if not os.path.exists(filepath):
-        raise RuntimeError("Path does not exist: %s" % filepath)
-    else:
-        # -- Tampering a file
-        if os.path.isfile(filepath):
-            ptee.write('Tampering the file %s, please wait...' % os.path.basename(filepath))
-            tcount, tsize = tamper_file(filepath, mode=mode, proba=proba, block_proba=block_proba, blocksize=blocksize, burst_length=burst_length, header=header, silent=silent)
-            ptee.write("Tampering done: %i/%i (%.2f%%) characters tampered." % (tcount, tsize, tcount / max(1, tsize) * 100))
-        # -- Tampering a directory tree recursively
-        elif os.path.isdir(filepath):
-            ptee.write('Tampering all files in directory %s, please wait...' % filepath)
-            files_tampered, filescount, tcount, tsize = tamper_dir(filepath, mode=mode, proba=proba, block_proba=block_proba, blocksize=blocksize, burst_length=burst_length, header=header, silent=silent)
-            ptee.write("Tampering done: %i/%i files tampered and overall %i/%i (%.2f%%) characters were tampered." % (files_tampered, filescount, tcount, tsize, tcount / max(1, tsize) * 100))
-
-    ptee.close()
-    return 0
-
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Random file characters tamperer
+# Randomly tampers characters in a file in order to test for integrity after.
+# Copyright (C) 2015 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#------------------------------
+#
+# This script is similar to what has been done in a paper, albeit in a dumbed down version (since they could automatically check for corruption measures based on file format): Heydegger, Volker. "Analysing the impact of file formats on data integrity." Archiving Conference. Vol. 2008. No. 1. Society for Imaging Science and Technology, 2008.
+# And another interesting paper by the same author: Heydegger, Volker. "Just one bit in a million: On the effects of data corruption in files." Research and Advanced Technology for Digital Libraries. Springer Berlin Heidelberg, 2009. 315-326.
+# Errors are not evenly spread but rather block level (thus concentrated). Addis, Matthew, et al. "Reliable Audiovisual Archiving Using Unreliable Storage Technology and Services." (2009).
+#
+
+# future division is important to divide integers and get as
+# a result precise floating numbers (instead of truncated int)
+#from __future__ import division, absolute_import, with_statement  # unnecessary since we dropped Python 2 support, but still can be interesting if we reimplement in Cython, Cython v3 will be needed for the maths
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, _range
+from lib.aux_funcs import fullpath, recwalk, is_dir, is_file, is_dir_or_file
+import argparse
+import os, sys, random
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+from tqdm import tqdm
+
+
+#***********************************
+#     AUXILIARY FUNCTIONS
+#***********************************
+
+def tamper_file_at(path, pos=0, replace_str=None):
+    """ Tamper a file at the given position and using the given string """
+    if not replace_str:
+        replace_str = "\x00"
+    try:
+        with open(path, "r+b") as fh:
+            if pos < 0: # if negative, we calculate the position backward from the end of file
+                fsize = os.fstat(fh.fileno()).st_size
+                pos = fsize + pos
+            fh.seek(pos)
+            fh.write(replace_str)
+    except IOError:
+        return False
+    finally:
+        try:
+            fh.close()
+        except Exception:
+            pass
+    return True
+
+def tamper_file(filepath, mode='e', proba=0.03, block_proba=None, blocksize=65535, burst_length=None, header=None):
+    """ Randomly tamper a file's content """
+    if header and header > 0:
+        blocksize = header
+
+    tamper_count = 0 # total number of characters tampered in the file
+    total_size = 0 # total buffer size, NOT necessarily the total file size (depends if you set header or not)
+    with open(filepath, "r+b") as fh: # 'r+' allows to read AND overwrite characters. Else any other option won't allow both ('a+' read and append, 'w+' erases the file first then allow to read and write), and 'b' is just for binary because we can open any filetype.
+        if proba >= 1: proba = 1.0/os.fstat(fh.fileno()).st_size * proba # normalizing probability if it's an integer (ie: the number of characters to flip on average)
+        buf = fh.read(blocksize) # We process blocks by blocks because it's a lot faster (IO is still the slowest operation in any computing system)
+        while len(buf) > 0:
+            total_size += len(buf)
+            if not block_proba or (random.random() < block_proba): # If block tampering is enabled, process only if this block is selected by probability
+                pos2tamper = []
+                burst_remain = 0 # if burst is enabled and corruption probability is triggered, then we will here store the remaining number of characters to corrupt (the length is uniformly sampled over the range specified in arguments)
+                # Create the list of bits to tamper (it's a lot more efficient to precompute the list of characters to corrupt, and then modify in the file the characters all at once)
+                for i in _range(len(buf)):
+                    if burst_remain > 0 or (random.random() < proba): # Corruption probability: corrupt only if below the bit-flip proba
+                        pos2tamper.append(i) # keep this character's position in the to-be-corrupted list
+                        if burst_remain > 0: # if we're already in a burst, we minus one and continue onto the next character
+                            burst_remain -= 1
+                        elif burst_length: # else we're not in a burst, we create one (triggered by corruption probability: as soon as one character triggers the corruption probability, then we do a burst)
+                            burst_remain = random.randint(burst_length[0], burst_length[1]) - 1 # if burst is enabled, then we randomly (uniformly) pick a random length for the burst between the range specified, and since we already tampered one character, we minus 1
+                # If there's any character to tamper in the list, we tamper the string
+                if pos2tamper:
+                    tamper_count = tamper_count + len(pos2tamper)
+                    #print("Before: %s" % buf)
+                    buf = bytearray(buf) # Strings in Python are immutable, thus we need to convert to a bytearray
+                    for pos in pos2tamper:
+                        if mode == 'e' or mode == 'erasure': # Erase the character (set a null byte)
+                            buf[pos] = 0
+                        elif mode == 'n' or mode == 'noise': # Noising the character (set a random ASCII character)
+                            buf[pos] = random.randint(0,255)
+                    #print("After: %s" % buf)
+                    # Overwriting the string into the file
+                    prevpos = fh.tell() # need to store and place back the seek cursor because after the write, if it's the end of the file, the next read may be buggy (getting characters that are not part of the file)
+                    fh.seek(fh.tell()-len(buf)) # Move the cursor at the beginning of the string we just read
+                    fh.write(buf) # Overwrite it
+                    fh.seek(prevpos) # Restore the previous position after the string
+            # If we only tamper the header, we stop here by setting the buffer to an empty string
+            if header and header > 0:
+                buf = ''
+            # Else we continue to the next data block
+            else:
+                # Load the next characters from file
+                buf = fh.read(blocksize)
+    return [tamper_count, total_size]
+
+def tamper_dir(inputpath, *args, **kwargs):
+    """ Randomly tamper the files content in a directory tree, recursively """
+    silent = kwargs.get('silent', False)
+    if 'silent' in kwargs: del kwargs['silent']
+
+    filescount = 0
+    for _ in tqdm(recwalk(inputpath), desc='Precomputing', disable=silent):
+        filescount += 1
+
+    files_tampered = 0
+    tamper_count = 0
+    total_size = 0
+    for dirname, filepath in tqdm(recwalk(inputpath), total=filescount, leave=True, desc='Tamper file n.', disable=silent):
+        tcount, tsize = tamper_file(os.path.join(dirname, filepath), *args, **kwargs)
+        if tcount > 0:
+            tamper_count += tcount
+            files_tampered += 1
+        total_size += tsize
+    return [files_tampered, filescount, tamper_count, total_size]
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Random file/directory characters tamperer in Python
+Description: Randomly tampers characters in a file or in a directory tree recursively (useful to test for integrity/repair after).
+WARNING: this will tamper the file you specify. Please ensure you keep a copy of the original!
+    '''
+    ep = '''NOTE: this script tampers at the character (byte) level, not the bits! Thus the measures you will get here may be different from those you will find in papers (you must divide your probability by 8).'''
+
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='filetotamper.ext', type=is_dir_or_file, nargs=1, required=True,
+                        help='Path to the file (or directory tree) to tamper.', **widget_dir)
+    main_parser.add_argument('-m', '--mode', metavar='e, erasure, n, noise', type=str, nargs=1, required=True,
+                        help='Tampering mode: erasure or noise?', **widget_text)
+    main_parser.add_argument('-p', '--probability', type=float, nargs=1, required=True,
+                        help='Probability of corruption (float between 0.0 and 1.0)', **widget_text)
+    # Optional arguments
+    main_parser.add_argument('--block_probability', type=float, nargs=1, required=False,
+                        help='Probability of block tampering (between 0.0 and 1.0, do not set it if you want to spread errors evenly, but researchs have shown that errors are rather at block level and not evenly distributed)', **widget_text)
+    main_parser.add_argument('-b', '--burst_length', metavar="startint|endint", type=str, required=False,
+                        help='If specified, this will define the number of consecutive characters that will be corrupted when the corruption probability (--probability) is triggered. Specify a range startint|endint, the burst length will be uniformly sampled over this range.')
+    main_parser.add_argument('--header', type=int, required=False,
+                        help='Only tamper the header of the file')
+
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set variables from arguments
+    filepath = fullpath(args.input[0])
+    mode = args.mode[0]
+    proba = float(args.probability[0])
+    verbose = args.verbose
+    silent = args.silent
+
+    burst_length = args.burst_length
+    if burst_length: burst_length = [int(r) for r in burst_length.split('|')] # split range and convert to int
+
+    block_proba = None
+    if args.block_probability:
+        block_proba = float(args.block_probability[0])
+
+    blocksize = 65536
+    header = args.header
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log[0], 'a', nostdout=silent)
+        #sys.stdout = Tee(args.log[0], 'a')
+        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+    # == PROCESSING BRANCHING == #
+    # Sanity check
+    if not os.path.exists(filepath):
+        raise RuntimeError("Path does not exist: %s" % filepath)
+    else:
+        # -- Tampering a file
+        if os.path.isfile(filepath):
+            ptee.write('Tampering the file %s, please wait...' % os.path.basename(filepath))
+            tcount, tsize = tamper_file(filepath, mode=mode, proba=proba, block_proba=block_proba, blocksize=blocksize, burst_length=burst_length, header=header, silent=silent)
+            ptee.write("Tampering done: %i/%i (%.2f%%) characters tampered." % (tcount, tsize, tcount / max(1, tsize) * 100))
+        # -- Tampering a directory tree recursively
+        elif os.path.isdir(filepath):
+            ptee.write('Tampering all files in directory %s, please wait...' % filepath)
+            files_tampered, filescount, tcount, tsize = tamper_dir(filepath, mode=mode, proba=proba, block_proba=block_proba, blocksize=blocksize, burst_length=burst_length, header=header, silent=silent)
+            ptee.write("Tampering done: %i/%i files tampered and overall %i/%i (%.2f%%) characters were tampered." % (files_tampered, filescount, tcount, tsize, tcount / max(1, tsize) * 100))
+
+    ptee.close()
+    return 0
+
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/header_ecc.py` & `pyFileFixity-3.1.4/pyFileFixity/header_ecc.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,741 +1,741 @@
-#!/usr/bin/env python
-#
-# Error Correction Code for Files Headers
-# Copyright (C) 2015 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#=================================
-#  Error Correction Code for Files Headers
-#                by Stephen Larroque
-#                       License: MIT
-#              Creation date: 2015-03-12
-#=================================
-#
-# From : http://simple.wikipedia.org/wiki/Reed-Solomon_error_correction
-# The key idea behind a Reed-Solomon code is that the data encoded is first visualized as a polynomial. The code relies on a theorem from algebra that states that any k distinct points uniquely determine a polynomial of degree at most k-1.
-# The sender determines a degree k-1 polynomial, over a finite field, that represents the k data points. The polynomial is then "encoded" by its evaluation at various points, and these values are what is actually sent. During transmission, some of these values may become corrupted. Therefore, more than k points are actually sent. As long as sufficient values are received correctly, the receiver can deduce what the original polynomial was, and decode the original data.
-# In the same sense that one can correct a curve by interpolating past a gap, a Reed-Solomon code can bridge a series of errors in a block of data to recover the coefficients of the polynomial that drew the original curve.
-# This is done automatically using a simple trick: the matrix inversion. For more infos, see this well-explained blog post by Richard Kiss: How to be Minimally Redundant (or "A Splitting Headache") http://blog.richardkiss.com/?p=264
-#
-# codeword_rate: rs = @(n,k) k / n
-# @(n,k) (n - k) / k
-#
-# To get higher resilience against corruption: make the codeword bigger (decrease k and increase n = higher resilience ratio), and use smaller packets (n should be smaller). Source: Kumar, Sanjeev, and Ragini Gupta. "Bit error rate analysis of Reed-Solomon code for efficient communication system." International Journal of Computer Applications 30.12 (2011): 11-15.
-# Note: in the paper, they told that their results indicate to use bigger blocks, but if you look at the Figure 4, this shows the opposite: the best performing parameters is: RS(246, 164) with code rate 0.66667 = resilience ratio 0.25
-# Note2: however, logically, it's more sensible to use bigger blocks to be more resilient. Indeed, since data stream is hashed and ecc'ed per block, this means that if an error burst corrupt a block twice the size of max_block_size (or just the size of max_block_size but exactly overlapping on one block, no shift), then the block will be unrecoverable. In other words, if we have bigger blocks, we raise the required size for an error burst to permanently corrupt our data (thus we diminish the risk). Thus, it would make sense to use error correcting codes with very big blocks. However, this would probably incur a huge performance drop, at least for Reed-Solomon which is basically grounded on matrix operations (thus the complexity would be about O(max_block_size^2), a quadratic complexity).
-#
-#  If 2s + r < 2t (s errors, r erasures) t
-#
-# TODO:
-# - Performance boost in Reed-Solomon libraries. A big work was done, and it's quite fast when using PyPy 2.5.0, but 10x more speedup (to attain 10MB/s encoding rate) would just be perfect! Decoding rate is sufficiently speedy as it is, no need to optimize that part.
-# Use Cauchy Reed-Solomon, which significantly outperforms simple Reed-Solomon?
-# or https://bitbucket.org/kmgreen2/pyeclib with jerasure
-# or http://www.bth.se/fou/cuppsats.nsf/all/bcb2fd16e55a96c2c1257c5e00666323/$file/BTH2013KARLSSON.pdf
-# or https://www.usenix.org/legacy/events/fast09/tech/full_papers/plank/plank_html/
-# - Also backup folders meta-data? (to reconstruct the tree in case a folder is truncated by bit rot)
-#
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import os, sys
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, _range, b, _izip
-from lib.aux_funcs import get_next_entry, is_dir, is_dir_or_file, fullpath, recwalk, sizeof_fmt, path2unix, get_version
-import argparse
-import datetime, time
-import tqdm
-import itertools
-import math
-#import operator # to get the max out of a dict
-import csv # to process the errors_file from rfigc.py
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-import struct # to support indexes backup file
-#import pprint # Unnecessary, used only for debugging purposes
-
-# ECC and hashing facade libraries
-from lib.eccman import ECCMan, compute_ecc_params
-from lib.hasher import Hasher
-from reedsolo import ReedSolomonError
-from unireedsolomon import RSCodecError
-
-# Get package version, necessary to input in the ECC the version of pyFileFixity we used, it will be stored in the ecc files,, this can be helpful to disambiguate in the future which version of the software to use for optimal recovery
-__version__ = get_version("__init__.py", thispathname)  # aux function will return root_path as path to lib/ subfolder, so we need to provide our own path here
-
-
-
-#***********************************
-#     AUXILIARY FUNCTIONS
-#***********************************
-
-def entry_fields(entry, field_delim="\xFF"):
-    '''From a raw ecc entry (a string), extract the metadata fields (filename, filesize, ecc for both), and the rest being blocks of hash and ecc per blocks of the original file's header'''
-    entry = entry.lstrip(field_delim) # if there was some slight adjustment error (example: the last ecc block of the last file was the field_delim, then we will start with a field_delim, and thus we need to remove the trailing field_delim which is useless and will make the field detection buggy). This is not really a big problem for the previous file's ecc block: the missing ecc characters (which were mistaken for a field_delim), will just be missing (so we will lose a bit of resiliency for the last block of the previous file, but that's not a huge issue, the correction can still rely on the other characters).
-
-    # Find metadata fields delimiters positions
-    # TODO: automate this part, just give in argument the number of field_delim to find, and the func will find the x field_delims (the number needs to be specified in argument because the field_delim can maybe be found wrongly inside the ecc stream, which we don't want)
-    first = entry.find(field_delim)
-    second = entry.find(field_delim, first+len(field_delim))
-    third = entry.find(field_delim, second+len(field_delim))
-    fourth = entry.find(field_delim, third+len(field_delim))
-    # Note: we do not try to find all the field delimiters because we optimize here: we just walk the string to find the exact number of field_delim we are looking for, and after we stop, no need to walk through the whole string.
-
-    # Extract the content of the fields
-    # Metadata fields
-    relfilepath = entry[:first]
-    filesize = entry[first+len(field_delim):second]
-    relfilepath_ecc = entry[second+len(field_delim):third]
-    filesize_ecc = entry[third+len(field_delim):fourth]
-    # Ecc stream field (aka ecc blocks)
-    ecc_field = entry[fourth+len(field_delim):]
-
-    # Try to convert to an int, an error may happen
-    try:
-        filesize = int(filesize)
-    except Exception as e:
-        print("Exception when trying to detect the filesize in ecc field (it may be corrupted), skipping: ")
-        print(e)
-        #filesize = 0 # avoid setting to 0, we keep as an int so that we can try to fix using intra-ecc
-
-    # entries = [ {"message":, "ecc":, "hash":}, etc.]
-    #print(entry)
-    #print(len(entry))
-    return {"relfilepath": relfilepath, "relfilepath_ecc": relfilepath_ecc, "filesize": filesize, "filesize_ecc": filesize_ecc, "ecc_field": ecc_field}
-
-def entry_assemble(entry_fields, ecc_params, header_size, filepath, fileheader=None):
-    '''From an entry with its parameters (filename, filesize), assemble a list of each block from the original file along with the relative hash and ecc for easy processing later.'''
-    # Extract the header from the file
-    if fileheader is None:
-        with open(filepath, 'rb') as file: # filepath is the absolute path to the original file (the one with maybe corruptions, NOT the output repaired file!)
-            # Compute the size of the buffer to read: either header_size if possible, but if the file is smaller than that then we will read the whole file.
-            if isinstance(entry_fields["filesize"], int) and entry_fields["filesize"] > 0 and entry_fields["filesize"] < header_size:
-                fileheader = file.read(entry_fields["filesize"])
-            else:
-                fileheader = file.read(header_size)
-
-    # Cut the header and the ecc entry into blocks, and then assemble them so that we can easily process block by block
-    entry_asm = []
-    for i, j in _izip(_range(0, len(fileheader), ecc_params["message_size"]), _range(0, len(entry_fields["ecc_field"]), ecc_params["hash_size"] + ecc_params["ecc_size"])):
-        # Extract each fields from each block
-        mes = fileheader[i:i+ecc_params["message_size"]]
-        hash = entry_fields["ecc_field"][j:j+ecc_params["hash_size"]]
-        ecc = entry_fields["ecc_field"][j+ecc_params["hash_size"]:j+ecc_params["hash_size"]+ecc_params["ecc_size"]]
-        entry_asm.append({"message": mes, "hash": hash, "ecc": ecc})
-
-    # Return a list of fields for each block
-    return entry_asm
-
-def compute_ecc_hash(ecc_manager, hasher, buf, max_block_size, rate, message_size=None, as_string=False):
-    '''Split a string in blocks given max_block_size and compute the hash and ecc for each block, and then return a nice list with both for easy processing.'''
-    result = []
-    # If required parameters were not provided, we compute them
-    if not message_size:
-        ecc_params = compute_ecc_params(max_block_size, rate, hasher)
-        message_size = ecc_params["message_size"]
-    # Split the buffer string in blocks (necessary for Reed-Solomon encoding because it's limited to 255 characters max)
-    for i in _range(0, len(buf), message_size):
-        # Compute the message block
-        mes = buf[i:i+message_size]
-        # Compute the ecc
-        ecc = ecc_manager.encode(mes)
-        # Compute the hash
-        hash = hasher.hash(mes)
-        #crc = zlib.crc32(mes) # DEPRECATED: CRC is not resilient enough
-        #print("mes %i (%i) - ecc %i (%i) - hash %i (%i)" % (len(mes), message_size, len(ecc), ecc_params["ecc_size"], len(hash), ecc_params["hash_size"])) # DEBUGLINE
-
-        # Return the result (either in string for easy writing into a file, or in a list for easy post-processing)
-        if as_string:
-            result.append(b(hash)+b(ecc))
-        else:
-            result.append([b(hash), b(ecc)])
-    return result
-
-def ecc_correct_intra(ecc_manager_intra, ecc_params_intra, field, ecc, entry_pos, enable_erasures=False, erasures_char="\x00", only_erasures=False):
-    """ Correct an intra-field with its corresponding intra-ecc if necessary """
-    fentry_fields = {"ecc_field": ecc}
-    field_correct = [] # will store each block of the corrected (or already correct) filepath
-    fcorrupted = False # check if field was corrupted
-    fcorrected = True # check if field was corrected (if it was corrupted)
-    errmsg = ''
-    # Decode each block of the filepath
-    for e in entry_assemble(fentry_fields, ecc_params_intra, len(field), '', field):
-        # Check if this block of the filepath is OK, if yes then we just copy it over
-        if ecc_manager_intra.check(e["message"], e["ecc"]):
-            field_correct.append(e["message"])
-        else: # Else this block is corrupted, we will try to fix it using the ecc
-            fcorrupted = True
-            # Repair the message block and the ecc
-            try:
-                repaired_block, repaired_ecc = ecc_manager_intra.decode(e["message"], e["ecc"], enable_erasures=enable_erasures, erasures_char=erasures_char, only_erasures=only_erasures)
-            except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
-                repaired_block = None
-                repaired_ecc = None
-                errmsg += "- Error: unrecoverable corrupted metadata field at offset %i: %s\n" % (entry_pos[0], exc)
-            # Check if the block was successfully repaired: if yes then we copy the repaired block...
-            if repaired_block is not None and ecc_manager_intra.check(repaired_block, repaired_ecc):
-                field_correct.append(repaired_block)
-            else: # ... else it failed, then we copy the original corrupted block and report an error later
-                field_correct.append(e["message"])
-                fcorrected = False
-    # Join all the blocks into one string to build the final filepath
-    field_correct = [b(x) for x in field_correct] # workaround when using --ecc_algo 3 or 4, because we get a list of bytearrays instead of str
-    field = b''.join(field_correct)
-    # Report errors
-    return (field, fcorrupted, fcorrected, errmsg)
-
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-#@conditional_decorator(check_gui_arg(), gooey.Gooey) # alternative to AutoGooey which also correctly works
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Error Correction Code for Files Headers
-Description: Given a directory, generate or check/correct the headers (defined by a constant number of bits in arguments) for every files recursively. Using Reed-Solomon for the ECC management.
-Headers are the most sensible part of any file: this is where the format definition and parameters are specified, and in addition for compression formats, the beginning of the file (just after the header) is usually where the most important strings of data are stored and compressed. Thus, having a high redundancy specifically for the headers means that you ensure that you will be able to at least open the file (file format will be recognized), and for compressed files that the most important symbols will be restituted.
-The concept is to use this script in addition to more common parity files like PAR2 so that you get an additional protection at low cost (because headers are just in the first KB of the file, thus it won't cost much in storage and processing time to add more redundancy to such a small stream of data).
-Note: Folders meta-data is NOT accounted, only the files! Use DVDisaster or a similar tool to also cover folders meta-data.
-    '''
-    ep = '''Use --gui as the first argument to use with a GUI (via Gooey).
-
-Note1: this is a pure-python implementation (except for MD5 hash but a pure-python alternative is provided in lib/md5py.py), thus it may be VERY slow to generate an ecc file. To speed-up things considerably, you can use PyPy v2.5.0 or above, there will be a speed-up of at least 100x from our experiments (you can expect an encoding rate of more than 1MB/s). Feel free to profile using easy_profiler.py and try to optimize the encoding parts of the reed-solomon libraries.
-
-Note2: that Reed-Solomon can correct up to 2*resilience_rate erasures (eg, null bytes, you know where they are) or resilience_rate errors (an error is a corrupted character but you don't know its position) and amount to an additional storage of 2*resilience_rate storage compared to the original total files size.
-'''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='/path/to/root/folder', type=is_dir_or_file, nargs=1, required=True,
-                        help='Path to the root folder (or a single file) from where the scanning will occur.', **widget_dir)
-    main_parser.add_argument('-d', '--database', metavar='/some/folder/ecc.txt', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
-                        help='Path to the file containing the ECC informations.', **widget_filesave)
-                        
-
-    # Optional general arguments
-    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
-                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
-    main_parser.add_argument('--max_block_size', type=int, default=255, required=False,
-                        help='Reed-Solomon max block size (maximum = 255). It is advised to keep it at the maximum for more resilience (see comments at the top of the script for more info).', **widget_text)
-    main_parser.add_argument('-s', '--size', type=int, default=1024, required=False,
-                        help='Headers block size to protect with ecc (eg: 1024 meants that the first 1k of each file will be protected).', **widget_text)
-    main_parser.add_argument('-r', '--resilience_rate', type=float, default=0.3, required=False,
-                        help='Resilience rate for files headers (eg: 0.3 = 30%% of errors can be recovered but size of codeword will be 60%% of the data block, thus the ecc file will be about 60%% the size of your data).', **widget_text)
-    main_parser.add_argument('-ri', '--resilience_rate_intra', type=float, default=0.5, required=False,
-                        help='Resilience rate for intra-ecc (ecc on meta-data, such as filepath, thus this defines the ecc for the critical spots!).', **widget_text)
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('--stats_only', action='store_true', required=False, default=False,
-                        help='Only show the predicted total size of the ECC file given the parameters.')
-    main_parser.add_argument('--hash', metavar='md5;shortmd5;shortsha256...', type=str, required=False,
-                        help='Hash algorithm to use. Choose between: md5, shortmd5, shortsha256, minimd5, minisha256.', **widget_text)
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    # Correction mode arguments
-    main_parser.add_argument('-c', '--correct', action='store_true', required=False, default=False,
-                        help='Check/Correct the files?')
-    main_parser.add_argument('-o', '--output', metavar='/path/to/output/folder', type=is_dir, nargs=1, required=False,
-                        help='Path of the folder where the repaired files will be copied (only repaired corrupted files will be copied there, files that weren\'t corrupted at all won\'t be copied so you have to copy them by yourself afterwards).', **widget_dir)
-    main_parser.add_argument('-e', '--errors_file', metavar='/some/folder/errorsfile.csv', type=str, nargs=1, required=False, #type=argparse.FileType('rt')
-                        help='Path to the error file generated by RFIGC.py (this specify in csv format the list of files to check, and only those files will be checked and repaired). Do not specify this argument if you want to check and repair all files.', **widget_file)
-    main_parser.add_argument('--ignore_size', action='store_true', required=False, default=False,
-                        help='On correction, if the file size differs from when the ecc file was generated, ignore and try to correct anyway (this may work with file where data was appended without changing the rest. For compressed formats like zip, this will probably fail).')
-    main_parser.add_argument('--no_fast_check', action='store_true', required=False, default=False,
-                        help='On correction, block corruption is only checked with the hash (the ecc will still be checked after correction, but not before). If no_fast_check is enabled, then ecc will also be checked before. This allows to find blocks corrupted by malicious intent (the block is corrupted but the hash has been corrupted as well to match the corrupted block, because it\'s almost impossible that following a hardware or logical fault, the hash match the corrupted block).')
-    main_parser.add_argument('--skip_missing', action='store_true', required=False, default=False,
-                        help='Skip missing files (no warning).')
-    main_parser.add_argument('--enable_erasures', action='store_true', required=False, default=False,
-                        help='Enable errors-and-erasures correction. Reed-Solomon can correct twice more erasures than errors (eg, if resilience rate is 0.3, then you can correct 30%% errors and 60%% erasures and any combination of errors and erasures between 30%%-60%% corruption). An erasure is a corrupted symbol where we know the position, while errors are not known at all. To find erasures, we will find any symbol that is equal to --erasure_symbol and flag it as an erasure. This is particularly useful if the software you use (eg, a disk scraper) can mark bad sectors with a constant character (eg, null byte). Misdetected erasures will just eat one ecc symbol, and won\'t change the decoded message.')
-    main_parser.add_argument('--only_erasures', action='store_true', required=False, default=False,
-                        help='Enable only erasures correction (no errors). Use this only if you are sure that all corrupted symbols have the same value (eg, if your disk scraper replace bad sectors by null bytes). This will ensure that you can correct up to 2*resilience_rate corrupted symbols.')
-    main_parser.add_argument('--erasure_symbol', type=int, default=0, required=False,
-                        help='Symbol that will be flagged as an erasure. Default: null byte 0. (value must be an integer)', **widget_text)
-
-    # Generate mode arguments
-    main_parser.add_argument('-g', '--generate', action='store_true', required=False, default=False,
-                        help='Generate the ecc file?')
-    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
-                        help='Force overwriting the ecc file even if it already exists (if --generate).')
-    main_parser.add_argument('--skip_size_below', type=int, default=None, required=False,
-                        help='Skip files below the specified size (in bytes).', **widget_text)
-    main_parser.add_argument('--always_include_ext', metavar='txt|jpg|png', type=str, default=None, required=False,
-                        help='Always include files with the specified extensions, useful in combination with --skip_size_below to keep files of certain types even if they are below the size. Format: extensions separated by |.', **widget_text)
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set hard-coded variables
-    entrymarker = "\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF" # marker that will signal the beginning of an ecc entry - use an alternating pattern of several characters, this avoids confusion (eg: if you use "AAA" as a pattern, if the ecc block of the previous file ends with "EGA" for example, then the full string for example will be "EGAAAAC:\yourfolder\filea.jpg" and then the entry reader will detect the first "AAA" occurrence as the entry start - this should not make the next entry bug because there is an automatic trim - but the previous ecc block will miss one character that could be used to repair the block because it will be "EG" instead of "EGA"!)
-    field_delim = "\xFA\xFF\xFA\xFF\xFA" # delimiter between fields (filepath, filesize, hash+ecc blocks) inside an ecc entry
-
-    #-- Set variables from arguments
-    inputpath = fullpath(args.input[0]) # path to the files to protect (either a folder or a single file)
-    rootfolderpath = inputpath # path to the root folder (to compute relative paths)
-    database = fullpath(args.database[0])
-    generate = args.generate
-    correct = args.correct
-    force = args.force
-    stats_only = args.stats_only
-    max_block_size = args.max_block_size
-    header_size = args.size
-    resilience_rate = args.resilience_rate
-    resilience_rate_intra = args.resilience_rate_intra
-    enable_erasures = args.enable_erasures
-    only_erasures = args.only_erasures
-    erasure_symbol = args.erasure_symbol
-    ignore_size = args.ignore_size
-    skip_missing = args.skip_missing
-    skip_size_below = args.skip_size_below
-    always_include_ext = args.always_include_ext
-    if always_include_ext: always_include_ext = tuple(['.'+ext for ext in always_include_ext.split('|')]) # prepare a tuple of extensions (prepending with a dot) so that str.endswith() works (it doesn't with a list, only a tuple)
-    hash_algo = args.hash
-    if not hash_algo: hash_algo = "md5"
-    ecc_algo = args.ecc_algo
-    fast_check = not args.no_fast_check
-    verbose = args.verbose
-    silent = args.silent
-
-    if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
-        rootfolderpath = os.path.dirname(inputpath)
-
-    if correct:
-        if not args.output:
-            raise NameError('Output path is necessary when in correction mode!')
-        outputpath = fullpath(args.output[0])
-
-    errors_file = None
-    if args.errors_file: errors_file = os.path.basename(fullpath(args.errors_file[0]))
-
-    # -- Checking arguments
-    if not stats_only and not generate and not os.path.isfile(database):
-        raise NameError('Specified database ecc file %s does not exist!' % database)
-    elif generate and os.path.isfile(database) and not force:
-        raise NameError('Specified database ecc file %s already exists! Use --force if you want to overwrite.' % database)
-
-    if resilience_rate <= 0 or resilience_rate_intra <= 0:
-        raise ValueError('Resilience rate cannot be negative nor zero and it must be a float number.');
-
-    if max_block_size < 2 or max_block_size > 255:
-        raise ValueError('RS max block size must be between 2 and 255.')
-
-    if header_size < 1:
-        raise ValueError('Header size cannot be negative.')
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log[0], 'a', nostdout=silent)
-        #sys.stdout = Tee(args.log[0], 'a')
-        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-
-    # == PROCESSING BRANCHING == #
-
-    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
-    ptee.write("Initializing the ECC codecs, please wait...")
-    hasher = Hasher(hash_algo)
-    hasher_intra = Hasher('none') # for intra_ecc we don't use any hash
-    ecc_params = compute_ecc_params(max_block_size, resilience_rate, hasher)
-    ecc_manager = ECCMan(max_block_size, ecc_params["message_size"], algo=ecc_algo)
-    ecc_params_intra = compute_ecc_params(max_block_size, resilience_rate_intra, hasher_intra)
-    ecc_manager_intra = ECCMan(max_block_size, ecc_params_intra["message_size"], algo=ecc_algo)
-    ecc_params_idx = compute_ecc_params(27, 1, hasher_intra)
-    ecc_manager_idx = ECCMan(27, ecc_params_idx["message_size"], algo=ecc_algo)
-
-    # == Precomputation of ecc file size
-    # Precomputing is important so that the user can know what size to expect before starting (and how much time it will take...).
-    filescount = 0
-    sizetotal = 0
-    sizeheaders = 0
-    ptee.write("Precomputing list of files and predicted statistics...")
-    for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee):
-        filescount = filescount + 1 # counting the total number of files we will process (so that we can show a progress bar with ETA)
-        # Get full absolute filepath
-        filepath = os.path.join(dirpath, filename)
-        relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
-        # Get the current file's size
-        size = os.stat(filepath).st_size
-        # Check if we must skip this file because size is too small, and then if we still keep it because it's extension is always to be included
-        if skip_size_below and size < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)): continue
-
-        # Compute total size of all files
-        sizetotal = sizetotal + size
-        # Compute predicted size of their headers
-        if size >= header_size: # for big files, we limit the size to the header size
-            header_size_add = header_size
-        else: # else for size smaller than the defined header size, it will just be the size of the file
-            header_size_add = size
-        # Size of the ecc entry for this file will be: entrymarker-bytes + field_delim-bytes*occurrence + length-filepath-string + length-size-string + length-filepath-ecc + size of the ecc per block for all blocks in file header + size of the hash per block for all blocks in file header.
-        sizeheaders = sizeheaders + (len(entrymarker) + len(field_delim)*3 + len(relfilepath) + len(str(size)) + int(float(len(relfilepath))*resilience_rate_intra) + (int(math.ceil(float(header_size_add) / ecc_params["message_size"])) * (ecc_params["ecc_size"]+ecc_params["hash_size"])) ) # Compute the total number of bytes we will add with ecc + hash (accounting for the padding of the remaining characters at the end of the sequence in case it doesn't fit with the message_size, by using ceil() )
-    ptee.write("Precomputing done.")
-    if generate: # show statistics only if generating an ecc file
-        # TODO: add the size of the ecc format header? (arguments string + PYHEADERECC identifier)
-        total_pred_percentage = sizeheaders * 100 / sizetotal
-        ptee.write("Total ECC size estimation: %s = %g%% of total files size %s." % (sizeof_fmt(sizeheaders), total_pred_percentage, sizeof_fmt(sizetotal)))
-        ptee.write("Details: resiliency of %i%%: For the header (first %i characters) of each file: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate*100, header_size, ecc_params["message_size"], ecc_params["ecc_size"], int(ecc_params["ecc_size"] / 2), ecc_params["ecc_size"]))
-
-    if stats_only:
-        ptee.close()
-        return 0
-
-    # == Generation mode
-    # Generate an ecc file, containing ecc entries for every files recursively in the specified root folder.
-    # The file header will be split by blocks depending on max_block_size and resilience_rate, and each of those blocks will be hashed and a Reed-Solomon code will be produced.
-    if generate:
-        ptee.write("====================================")
-        ptee.write("Header ECC generation, started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        with open(database, 'wb') as db, open(database+".idx", 'wb') as dbidx:
-            # Write ECC file header identifier (unique string + version)
-            db.write( b("**PYHEADERECCv%s**\n" % (''.join([x * 3 for x in __version__]))) ) # each character in the version will be repeated 3 times, so that in case of tampering, a majority vote can try to disambiguate
-            # Write the parameters (they are NOT reloaded automatically, you have to specify them at commandline! It's the user role to memorize those parameters (using any means: own brain memory, keep a copy on paper, on email, etc.), so that the parameters are NEVER tampered. The parameters MUST be ultra reliable so that errors in the ECC file can be more efficiently recovered.
-            for i in _range(3): db.write( ("** Parameters: "+" ".join(argv) + "\n").encode() ) # copy them 3 times just to be redundant in case of ecc file corruption
-            db.write( b("** Generated under %s\n" % ecc_manager.description()) )
-            # NOTE: there's NO HEADER for the ecc file! Ecc entries are all independent of each others, you just need to supply the decoding arguments at commandline, and the ecc entries can be decoded. This is done on purpose to be remove the risk of critical spots in ecc file.
-
-            # Processing ecc on files
-            files_done = 0
-            files_skipped = 0
-            for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee, total=filescount, leave=True, unit="files"):
-                # Get full absolute filepath
-                filepath = os.path.join(dirpath, filename)
-                # Get database relative path (from scanning root folder)
-                relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
-                # Get file size
-                filesize = os.stat(filepath).st_size
-                # If skip size is enabled and size is below the skip size, we skip UNLESS the file extension is in the always include list
-                if skip_size_below and filesize < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)):
-                    files_skipped += 1
-                    continue
-
-                # Opening the input file's to read its header and compute the ecc/hash blocks
-                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
-                with open(os.path.join(rootfolderpath, filepath), 'rb') as file:
-                    # -- Intra-ecc generation: Compute an ecc for the filepath and filesize, to avoid a critical spot here (so that we don't care that the filepath gets corrupted, we have an ecc to fix it!)
-                    relfilepath_ecc = b''.join(compute_ecc_hash(ecc_manager_intra, hasher_intra, relfilepath, max_block_size, resilience_rate_intra, ecc_params_intra["message_size"], True))
-                    filesize_ecc = b''.join(compute_ecc_hash(ecc_manager_intra, hasher_intra, str(filesize), max_block_size, resilience_rate_intra, ecc_params_intra["message_size"], True))
-                    # -- Hash/Ecc encoding of file's content (everything is managed inside compute_ecc_hash)
-                    buf = file.read(header_size) # read the file's header
-                    ecc_stream = compute_ecc_hash(ecc_manager, hasher, buf, max_block_size, resilience_rate, ecc_params["message_size"], True) # then compute the ecc/hash entry for this file's header (this will be a chain of multiple ecc/hash fields per block of data, because Reed-Solomon is limited to a maximum of 255 bytes, including the original_message+ecc!)
-                    # -- Build the ecc entry
-                    # First put the ecc metadata
-                    ecc_entry = b''.join([b(entrymarker), b(relfilepath), b(field_delim), b(str(filesize)), b(field_delim), b(relfilepath_ecc), b(field_delim), b(filesize_ecc), b(field_delim)]) # first save the file's metadata (filename, filesize, filepath ecc, ...)
-                    # Then put the ecc stream (the ecc blocks for the file's data)
-                    for es in ecc_stream:
-                        ecc_entry += es
-                    # -- Commit the ecc entry into the database
-                    entrymarker_pos = db.tell() # backup the position of the start of this ecc entry
-                    # -- Committing the hash/ecc encoding of the file's content
-                    db.write(b(ecc_entry)) # commit to the ecc file, and replicate the number of times required
-                    # -- External indexes backup: calculate the position of the entrymarker and of each field delimiter, and compute their ecc, and save into the index backup file. This will allow later to retrieve the position of each marker in the ecc file, and repair them if necessary, while just incurring a very cheap storage cost.
-                    # Also, the index backup file is fixed delimited fields sizes, which means that each field has a very specifically delimited size, so that we don't need any marker: we can just compute the total size for each entry, and thus find all entries independently even if one or several are corrupted beyond repair, so that this won't affect other index entries.
-                    markers_pos = [entrymarker_pos,
-                                                entrymarker_pos+len(entrymarker)+len(relfilepath),
-                                                entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize)),
-                                                entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize))+len(field_delim)+len(relfilepath_ecc),
-                                                entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize))+len(field_delim)+len(relfilepath_ecc)+len(field_delim)+len(filesize_ecc),
-                                                ] # Make the list of all markers positions for this ecc entry. The first and last indexes are the most important (first is the entrymarker, the last is the field_delim just before the ecc track start)
-                    markers_pos = [struct.pack('>Q', x) for x in markers_pos] # Convert to a binary representation in 8 bytes using unsigned long long (up to 16 EB, this should be more than sufficient)
-                    markers_types = [b'1', b'2', b'2', b'2', b'2']
-                    markers_pos_ecc = [ecc_manager_idx.encode(x+y) for x,y in _izip(markers_types,markers_pos)] # compute the ecc for each number
-                    # Couple each marker's position with its type and with its ecc, and write them all consecutively into the index backup file
-                    for items in _izip(markers_types,markers_pos,markers_pos_ecc):
-                        for item in items:
-                            dbidx.write(b(item))
-                files_done += 1
-        ptee.write("All done! Total number of files processed: %i, skipped: %i" % (files_done, files_skipped))
-        ptee.close()
-        return 0
-
-    # == Error Correction (and checking by hash) mode
-    # For each file, check their headers by block by checking each block against a hash, and if the hash does not match, try to correct with Reed-Solomon and then check the hash again to see if we correctly repaired the block (else the ecc entry might have been corrupted, whether it's the hash or the ecc field, in both cases it's highly unlikely that a wrong repair will match the hash after this wrong repair)
-    elif correct:
-        ptee.write("====================================")
-        ptee.write("Header ECC verification and correction, started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        # Prepare the list of files with errors to reduce the scan (only if provided)
-        errors_filelist = []
-        if errors_file:
-            with open(errors_file, 'rb') as efile:
-                for row in csv.DictReader(efile, lineterminator='\n', delimiter='|', quotechar='"', fieldnames=['filepath', 'error']): # need to specify the fieldnames, else the first row in the csv file will be skipped (it will be used as the columns names)
-                    errors_filelist.append(row['filepath'])
-
-        # Read the ecc file
-        dbsize = os.stat(database).st_size # must get db file size before opening it in order not to move the cursor
-        with open(database, 'rb') as db:
-            # Counters
-            files_count = 0
-            files_corrupted = 0
-            files_repaired_partially = 0
-            files_repaired_completely = 0
-            files_skipped = 0
-
-            # Main loop: process each ecc entry
-            entry = 1 # to start the while loop
-            bardisp = tqdm.tqdm(total=dbsize, file=ptee, leave=True, desc='DBREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
-            while entry:
-
-                # -- Read the next ecc entry (extract the raw string from the ecc file)
-                #if replication_rate == 1:
-                entry = get_next_entry(db, entrymarker, False)
-                if entry: bardisp.update(len(entry)) # update progress bar
-
-                # No entry? Then we finished because this is the end of file (stop condition)
-                if not entry: break
-
-                # -- Get position of current entry (for debugging purposes)
-                entry_pos = [db.tell(), db.tell()-len(entry)]
-
-                # -- Extract the fields from the ecc entry
-                entry_p = entry_fields(entry, b(field_delim))
-
-                # -- Get file path, check its correctness and correct it by using intra-ecc if necessary
-                relfilepath = entry_p["relfilepath"] # Relative file path
-                relfilepath, fpcorrupted, fpcorrected, fperrmsg = ecc_correct_intra(ecc_manager_intra, ecc_params_intra, relfilepath, entry_p["relfilepath_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
-
-                # Report errors
-                if fpcorrupted:
-                    if fpcorrected: ptee.write("\n- Fixed error in metadata field at offset %i filepath %s." % (db.tell()-len(entry), relfilepath))
-                    else: ptee.write("\n- Error in filepath, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], relfilepath))
-                ptee.write(fperrmsg)
-
-                # Convert to str (so that we can use os.path funcs)
-                relfilepath = relfilepath.decode('latin-1')
-                # Update entry_p
-                entry_p["relfilepath"] = relfilepath
-                # -- End of intra-ecc on filepath
-
-                # -- Get file size, check its correctness and correct it by using intra-ecc if necessary
-                filesize = str(entry_p["filesize"])
-                filesize, fscorrupted, fscorrected, fserrmsg = ecc_correct_intra(ecc_manager_intra, ecc_params_intra, filesize, entry_p["filesize_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
-
-                # Report errors
-                if fscorrupted:
-                    if fscorrected: ptee.write("\n- Fixed error in metadata field at offset %i filesize %s." % (db.tell()-len(entry), filesize))
-                    else: ptee.write("\n- Error in filesize, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], filesize))
-                ptee.write(fserrmsg)
-
-                # Convert filesize intra-field into an int
-                filesize = int(filesize)
-
-                # Update entry_p
-                entry_p["filesize"] = filesize # need to update entry_p because various funcs will directly access filesize this way...
-                # -- End of intra-ecc on filesize
-
-                # Build the absolute file path
-                filepath = os.path.join(rootfolderpath, relfilepath) # Get full absolute filepath from given input folder (because the files may be specified in any folder, in the ecc file the paths are relative, so that the files can be moved around or burnt on optical discs)
-                if errors_filelist and relfilepath not in errors_filelist: continue # if a list of files with errors was supplied (for example by rfigc.py), then we will check only those files and skip the others
-
-                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
-
-                # -- Check filepath
-                # Check that the filepath isn't corrupted (if a silent error erase a character (not only flip a bit), then it will also be detected this way)
-                if relfilepath.find("\x00") >= 0:
-                    ptee.write("Error: ecc entry corrupted on filepath field, please try to manually repair the filepath (filepath: %s - missing/corrupted character at %i)." % (filepath, relfilepath.find("\x00")))
-                    files_skipped += 1
-                    continue
-                # Check that file still exists before checking it
-                if not os.path.isfile(filepath):
-                    if not skip_missing: ptee.write("Error: file %s could not be found: either file was moved or the ecc entry was corrupted. You may try to fix manually the entry." % filepath)
-                    files_skipped += 1
-                    continue
-
-                # -- Checking file size: if the size has changed, the blocks may not match anymore!
-                real_filesize = os.stat(filepath).st_size
-                if filesize != real_filesize:
-                    if ignore_size:
-                        ptee.write("Warning: file %s has a different size: %s (before: %s). Will still try to correct it (but the blocks may not match!)." % (relfilepath, real_filesize, filesize))
-                    else:
-                        ptee.write("Error: file %s has a different size: %s (before: %s). Skipping the file correction because blocks may not match (you can set --ignore_size to still correct even if size is different, maybe just the entry was corrupted)." % (relfilepath, real_filesize, filesize))
-                        files_skipped += 1
-                        continue
-
-                files_count += 1
-                # -- Check blocks and repair if necessary
-                entry_asm = entry_assemble(entry_p, ecc_params, header_size, filepath) # Extract and assemble each message block from the original file with its corresponding ecc and hash
-                corrupted = False # flag to signal that the file was corrupted and we need to reconstruct it afterwards
-                repaired_partially = False # flag to signal if a file was repaired only partially
-                err_consecutive = True # flag to check if the ecc track is misaligned/misdetected (we only encounter corrupted blocks that we can't fix)
-                # For each message block, check the message with hash and repair with ecc if necessary
-                for i, e in enumerate(entry_asm):
-                    # If the message block has a different hash, it was corrupted (or the hash is corrupted, or both)
-                    if hasher.hash(e["message"]) != e["hash"] or (not fast_check and not ecc_manager.check(e["message"], e["ecc"])):
-                        corrupted = True
-                        # Try to repair the block using ECC
-                        ptee.write("File %s: corruption in block %i. Trying to fix it." % (relfilepath, i))
-                        try:
-                            repaired_block, repaired_ecc = ecc_manager.decode(e["message"], e["ecc"], enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
-                        except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
-                            repaired_block = None
-                            repaired_ecc = None
-                            print("Error: file %s: block %i: %s" % (relfilepath, i, exc))
-                        # Check if the repair was successful.
-                        hash_ok = None
-                        ecc_ok = None
-                        if repaired_block is not None:
-                            hash_ok = (hasher.hash(repaired_block) == e["hash"])
-                            ecc_ok = ecc_manager.check(repaired_block, repaired_ecc)
-                        if repaired_block is not None and (hash_ok or ecc_ok): # If either the hash or the ecc check now match the repaired message block, we commit the new block
-                            entry_asm[i]["message_repaired"] = repaired_block # save the repaired block
-                            # Show a precise report about the repair
-                            if hash_ok and ecc_ok: ptee.write("File %s: block %i repaired!" % (relfilepath, i))
-                            elif not hash_ok: ptee.write("File %s: block %i probably repaired with matching ecc check but with a hash error (assume the hash was corrupted)." % (relfilepath, i))
-                            elif not ecc_ok: ptee.write("File %s: block %i probably repaired with matching hash but with ecc check error (assume the ecc was partially corrupted)." % (relfilepath, i))
-                            err_consecutive = False
-                        else: # Else the hash and the ecc check do not match: the repair failed (either because the ecc is too much tampered, or because the hash is corrupted. Either way, we don't commit).
-                            ptee.write("Error: file %s could not repair block %i (both hash and ecc check mismatch)." % (relfilepath, i)) # you need to code yourself to use bit-recover, it's in perl but it should work given the hash computed by this script and the corresponding message block.
-                            repaired_partially = True
-                            # Detect if the ecc track is misaligned/misdetected (we encounter only errors that we can't fix)
-                            if err_consecutive and i >= 10: # threshold is ten consecutive uncorrectable errors
-                                ptee.write("Failure: Too many consecutive uncorrectable errors for %s. Most likely, the ecc track was misdetected (try to repair the entrymarkers and field delimiters). Skipping this track/file." % relfilepath)
-                                break
-                    else:
-                        err_consecutive = False
-                # -- Reconstruct/Copying the repaired file
-                # If this file had a corruption in one of its header blocks, then we will reconstruct the file header and then append the rest of the file (which can then be further repaired by other tools such as PAR2).
-                if corrupted:
-                    # Counters...
-                    files_corrupted += 1
-                    if repaired_partially:
-                        files_repaired_partially += 1
-                    else:
-                        files_repaired_completely += 1
-                    # Reconstructing the file
-                    outfilepath = os.path.join(outputpath, relfilepath) # get the full path to the output file
-                    outfiledir = os.path.dirname(outfilepath)
-                    if not os.path.isdir(outfiledir): os.makedirs(outfiledir) # if the target directory does not exist, create it (and create recursively all parent directories too)
-                    with open(outfilepath, 'wb') as out:
-                        # Reconstruct the header using repaired blocks (and the other non corrupted blocks)
-                        for e in entry_asm:
-                            if "message_repaired" in e:
-                                out.write(e["message_repaired"])
-                            else:
-                                out.write(e["message"])
-                        # Append the rest of the file by copying from the original
-                        with open(filepath, 'rb') as originalfile:
-                            blocksize = 65535
-                            originalfile.seek(header_size)
-                            buf = originalfile.read(blocksize)
-                            while buf:
-                                out.write(buf)
-                                buf = originalfile.read(blocksize)
-                    # Copying the last access time and last modification time from the original file TODO: a more reliable way would be to use the db computed by rfigc.py, because if a software maliciously tampered the data, then the modification date may also have changed (but not if it's a silent error, in that case we're ok).
-                    filestats = os.stat(filepath)
-                    os.utime(outfilepath, (filestats.st_atime, filestats.st_mtime))
-        # All ecc entries processed for checking and potentally repairing, we're done correcting!
-        bardisp.close() # at the end, the bar may not be 100% because of the headers that are skipped by get_next_entry() and are not accounted in bardisp.
-        ptee.write("All done! Stats:\n- Total files processed: %i\n- Total files corrupted: %i\n- Total files repaired completely: %i\n- Total files repaired partially: %i\n- Total files corrupted but not repaired at all: %i\n- Total files skipped: %i" % (files_count, files_corrupted, files_repaired_completely, files_repaired_partially, files_corrupted - (files_repaired_partially + files_repaired_completely), files_skipped) )
-        ptee.close()
-        if files_corrupted == 0 or files_repaired_completely == files_corrupted:
-            return 0
-        else:
-            return 1
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":  # pragma: no cover
-    global __package__
-    if __package__ is None:
-        #sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
-        print('HOHO')
-        __package__ = 'pyFileFixity.header_ecc'
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Error Correction Code for Files Headers
+# Copyright (C) 2015 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#=================================
+#  Error Correction Code for Files Headers
+#                by Stephen Larroque
+#                       License: MIT
+#              Creation date: 2015-03-12
+#=================================
+#
+# From : http://simple.wikipedia.org/wiki/Reed-Solomon_error_correction
+# The key idea behind a Reed-Solomon code is that the data encoded is first visualized as a polynomial. The code relies on a theorem from algebra that states that any k distinct points uniquely determine a polynomial of degree at most k-1.
+# The sender determines a degree k-1 polynomial, over a finite field, that represents the k data points. The polynomial is then "encoded" by its evaluation at various points, and these values are what is actually sent. During transmission, some of these values may become corrupted. Therefore, more than k points are actually sent. As long as sufficient values are received correctly, the receiver can deduce what the original polynomial was, and decode the original data.
+# In the same sense that one can correct a curve by interpolating past a gap, a Reed-Solomon code can bridge a series of errors in a block of data to recover the coefficients of the polynomial that drew the original curve.
+# This is done automatically using a simple trick: the matrix inversion. For more infos, see this well-explained blog post by Richard Kiss: How to be Minimally Redundant (or "A Splitting Headache") http://blog.richardkiss.com/?p=264
+#
+# codeword_rate: rs = @(n,k) k / n
+# @(n,k) (n - k) / k
+#
+# To get higher resilience against corruption: make the codeword bigger (decrease k and increase n = higher resilience ratio), and use smaller packets (n should be smaller). Source: Kumar, Sanjeev, and Ragini Gupta. "Bit error rate analysis of Reed-Solomon code for efficient communication system." International Journal of Computer Applications 30.12 (2011): 11-15.
+# Note: in the paper, they told that their results indicate to use bigger blocks, but if you look at the Figure 4, this shows the opposite: the best performing parameters is: RS(246, 164) with code rate 0.66667 = resilience ratio 0.25
+# Note2: however, logically, it's more sensible to use bigger blocks to be more resilient. Indeed, since data stream is hashed and ecc'ed per block, this means that if an error burst corrupt a block twice the size of max_block_size (or just the size of max_block_size but exactly overlapping on one block, no shift), then the block will be unrecoverable. In other words, if we have bigger blocks, we raise the required size for an error burst to permanently corrupt our data (thus we diminish the risk). Thus, it would make sense to use error correcting codes with very big blocks. However, this would probably incur a huge performance drop, at least for Reed-Solomon which is basically grounded on matrix operations (thus the complexity would be about O(max_block_size^2), a quadratic complexity).
+#
+#  If 2s + r < 2t (s errors, r erasures) t
+#
+# TODO:
+# - Performance boost in Reed-Solomon libraries. A big work was done, and it's quite fast when using PyPy 2.5.0, but 10x more speedup (to attain 10MB/s encoding rate) would just be perfect! Decoding rate is sufficiently speedy as it is, no need to optimize that part.
+# Use Cauchy Reed-Solomon, which significantly outperforms simple Reed-Solomon?
+# or https://bitbucket.org/kmgreen2/pyeclib with jerasure
+# or http://www.bth.se/fou/cuppsats.nsf/all/bcb2fd16e55a96c2c1257c5e00666323/$file/BTH2013KARLSSON.pdf
+# or https://www.usenix.org/legacy/events/fast09/tech/full_papers/plank/plank_html/
+# - Also backup folders meta-data? (to reconstruct the tree in case a folder is truncated by bit rot)
+#
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import os, sys
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, _range, b, _izip
+from lib.aux_funcs import get_next_entry, is_dir, is_dir_or_file, fullpath, recwalk, sizeof_fmt, path2unix, get_version
+import argparse
+import datetime, time
+import tqdm
+import itertools
+import math
+#import operator # to get the max out of a dict
+import csv # to process the errors_file from rfigc.py
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+import struct # to support indexes backup file
+#import pprint # Unnecessary, used only for debugging purposes
+
+# ECC and hashing facade libraries
+from lib.eccman import ECCMan, compute_ecc_params
+from lib.hasher import Hasher
+from reedsolo import ReedSolomonError
+from unireedsolomon import RSCodecError
+
+# Get package version, necessary to input in the ECC the version of pyFileFixity we used, it will be stored in the ecc files,, this can be helpful to disambiguate in the future which version of the software to use for optimal recovery
+__version__ = get_version("__init__.py", thispathname)  # aux function will return root_path as path to lib/ subfolder, so we need to provide our own path here
+
+
+
+#***********************************
+#     AUXILIARY FUNCTIONS
+#***********************************
+
+def entry_fields(entry, field_delim="\xFF"):
+    '''From a raw ecc entry (a string), extract the metadata fields (filename, filesize, ecc for both), and the rest being blocks of hash and ecc per blocks of the original file's header'''
+    entry = entry.lstrip(field_delim) # if there was some slight adjustment error (example: the last ecc block of the last file was the field_delim, then we will start with a field_delim, and thus we need to remove the trailing field_delim which is useless and will make the field detection buggy). This is not really a big problem for the previous file's ecc block: the missing ecc characters (which were mistaken for a field_delim), will just be missing (so we will lose a bit of resiliency for the last block of the previous file, but that's not a huge issue, the correction can still rely on the other characters).
+
+    # Find metadata fields delimiters positions
+    # TODO: automate this part, just give in argument the number of field_delim to find, and the func will find the x field_delims (the number needs to be specified in argument because the field_delim can maybe be found wrongly inside the ecc stream, which we don't want)
+    first = entry.find(field_delim)
+    second = entry.find(field_delim, first+len(field_delim))
+    third = entry.find(field_delim, second+len(field_delim))
+    fourth = entry.find(field_delim, third+len(field_delim))
+    # Note: we do not try to find all the field delimiters because we optimize here: we just walk the string to find the exact number of field_delim we are looking for, and after we stop, no need to walk through the whole string.
+
+    # Extract the content of the fields
+    # Metadata fields
+    relfilepath = entry[:first]
+    filesize = entry[first+len(field_delim):second]
+    relfilepath_ecc = entry[second+len(field_delim):third]
+    filesize_ecc = entry[third+len(field_delim):fourth]
+    # Ecc stream field (aka ecc blocks)
+    ecc_field = entry[fourth+len(field_delim):]
+
+    # Try to convert to an int, an error may happen
+    try:
+        filesize = int(filesize)
+    except Exception as e:
+        print("Exception when trying to detect the filesize in ecc field (it may be corrupted), skipping: ")
+        print(e)
+        #filesize = 0 # avoid setting to 0, we keep as an int so that we can try to fix using intra-ecc
+
+    # entries = [ {"message":, "ecc":, "hash":}, etc.]
+    #print(entry)
+    #print(len(entry))
+    return {"relfilepath": relfilepath, "relfilepath_ecc": relfilepath_ecc, "filesize": filesize, "filesize_ecc": filesize_ecc, "ecc_field": ecc_field}
+
+def entry_assemble(entry_fields, ecc_params, header_size, filepath, fileheader=None):
+    '''From an entry with its parameters (filename, filesize), assemble a list of each block from the original file along with the relative hash and ecc for easy processing later.'''
+    # Extract the header from the file
+    if fileheader is None:
+        with open(filepath, 'rb') as file: # filepath is the absolute path to the original file (the one with maybe corruptions, NOT the output repaired file!)
+            # Compute the size of the buffer to read: either header_size if possible, but if the file is smaller than that then we will read the whole file.
+            if isinstance(entry_fields["filesize"], int) and entry_fields["filesize"] > 0 and entry_fields["filesize"] < header_size:
+                fileheader = file.read(entry_fields["filesize"])
+            else:
+                fileheader = file.read(header_size)
+
+    # Cut the header and the ecc entry into blocks, and then assemble them so that we can easily process block by block
+    entry_asm = []
+    for i, j in _izip(_range(0, len(fileheader), ecc_params["message_size"]), _range(0, len(entry_fields["ecc_field"]), ecc_params["hash_size"] + ecc_params["ecc_size"])):
+        # Extract each fields from each block
+        mes = fileheader[i:i+ecc_params["message_size"]]
+        hash = entry_fields["ecc_field"][j:j+ecc_params["hash_size"]]
+        ecc = entry_fields["ecc_field"][j+ecc_params["hash_size"]:j+ecc_params["hash_size"]+ecc_params["ecc_size"]]
+        entry_asm.append({"message": mes, "hash": hash, "ecc": ecc})
+
+    # Return a list of fields for each block
+    return entry_asm
+
+def compute_ecc_hash(ecc_manager, hasher, buf, max_block_size, rate, message_size=None, as_string=False):
+    '''Split a string in blocks given max_block_size and compute the hash and ecc for each block, and then return a nice list with both for easy processing.'''
+    result = []
+    # If required parameters were not provided, we compute them
+    if not message_size:
+        ecc_params = compute_ecc_params(max_block_size, rate, hasher)
+        message_size = ecc_params["message_size"]
+    # Split the buffer string in blocks (necessary for Reed-Solomon encoding because it's limited to 255 characters max)
+    for i in _range(0, len(buf), message_size):
+        # Compute the message block
+        mes = buf[i:i+message_size]
+        # Compute the ecc
+        ecc = ecc_manager.encode(mes)
+        # Compute the hash
+        hash = hasher.hash(mes)
+        #crc = zlib.crc32(mes) # DEPRECATED: CRC is not resilient enough
+        #print("mes %i (%i) - ecc %i (%i) - hash %i (%i)" % (len(mes), message_size, len(ecc), ecc_params["ecc_size"], len(hash), ecc_params["hash_size"])) # DEBUGLINE
+
+        # Return the result (either in string for easy writing into a file, or in a list for easy post-processing)
+        if as_string:
+            result.append(b(hash)+b(ecc))
+        else:
+            result.append([b(hash), b(ecc)])
+    return result
+
+def ecc_correct_intra(ecc_manager_intra, ecc_params_intra, field, ecc, entry_pos, enable_erasures=False, erasures_char="\x00", only_erasures=False):
+    """ Correct an intra-field with its corresponding intra-ecc if necessary """
+    fentry_fields = {"ecc_field": ecc}
+    field_correct = [] # will store each block of the corrected (or already correct) filepath
+    fcorrupted = False # check if field was corrupted
+    fcorrected = True # check if field was corrected (if it was corrupted)
+    errmsg = ''
+    # Decode each block of the filepath
+    for e in entry_assemble(fentry_fields, ecc_params_intra, len(field), '', field):
+        # Check if this block of the filepath is OK, if yes then we just copy it over
+        if ecc_manager_intra.check(e["message"], e["ecc"]):
+            field_correct.append(e["message"])
+        else: # Else this block is corrupted, we will try to fix it using the ecc
+            fcorrupted = True
+            # Repair the message block and the ecc
+            try:
+                repaired_block, repaired_ecc = ecc_manager_intra.decode(e["message"], e["ecc"], enable_erasures=enable_erasures, erasures_char=erasures_char, only_erasures=only_erasures)
+            except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
+                repaired_block = None
+                repaired_ecc = None
+                errmsg += "- Error: unrecoverable corrupted metadata field at offset %i: %s\n" % (entry_pos[0], exc)
+            # Check if the block was successfully repaired: if yes then we copy the repaired block...
+            if repaired_block is not None and ecc_manager_intra.check(repaired_block, repaired_ecc):
+                field_correct.append(repaired_block)
+            else: # ... else it failed, then we copy the original corrupted block and report an error later
+                field_correct.append(e["message"])
+                fcorrected = False
+    # Join all the blocks into one string to build the final filepath
+    field_correct = [b(x) for x in field_correct] # workaround when using --ecc_algo 3 or 4, because we get a list of bytearrays instead of str
+    field = b''.join(field_correct)
+    # Report errors
+    return (field, fcorrupted, fcorrected, errmsg)
+
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+#@conditional_decorator(check_gui_arg(), gooey.Gooey) # alternative to AutoGooey which also correctly works
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Error Correction Code for Files Headers
+Description: Given a directory, generate or check/correct the headers (defined by a constant number of bits in arguments) for every files recursively. Using Reed-Solomon for the ECC management.
+Headers are the most sensible part of any file: this is where the format definition and parameters are specified, and in addition for compression formats, the beginning of the file (just after the header) is usually where the most important strings of data are stored and compressed. Thus, having a high redundancy specifically for the headers means that you ensure that you will be able to at least open the file (file format will be recognized), and for compressed files that the most important symbols will be restituted.
+The concept is to use this script in addition to more common parity files like PAR2 so that you get an additional protection at low cost (because headers are just in the first KB of the file, thus it won't cost much in storage and processing time to add more redundancy to such a small stream of data).
+Note: Folders meta-data is NOT accounted, only the files! Use DVDisaster or a similar tool to also cover folders meta-data.
+    '''
+    ep = '''Use --gui as the first argument to use with a GUI (via Gooey).
+
+Note1: this is a pure-python implementation (except for MD5 hash but a pure-python alternative is provided in lib/md5py.py), thus it may be VERY slow to generate an ecc file. To speed-up things considerably, you can use PyPy v2.5.0 or above, there will be a speed-up of at least 100x from our experiments (you can expect an encoding rate of more than 1MB/s). Feel free to profile using easy_profiler.py and try to optimize the encoding parts of the reed-solomon libraries.
+
+Note2: that Reed-Solomon can correct up to 2*resilience_rate erasures (eg, null bytes, you know where they are) or resilience_rate errors (an error is a corrupted character but you don't know its position) and amount to an additional storage of 2*resilience_rate storage compared to the original total files size.
+'''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='/path/to/root/folder', type=is_dir_or_file, nargs=1, required=True,
+                        help='Path to the root folder (or a single file) from where the scanning will occur.', **widget_dir)
+    main_parser.add_argument('-d', '--database', metavar='/some/folder/ecc.txt', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
+                        help='Path to the file containing the ECC informations.', **widget_filesave)
+                        
+
+    # Optional general arguments
+    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
+                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
+    main_parser.add_argument('--max_block_size', type=int, default=255, required=False,
+                        help='Reed-Solomon max block size (maximum = 255). It is advised to keep it at the maximum for more resilience (see comments at the top of the script for more info).', **widget_text)
+    main_parser.add_argument('-s', '--size', type=int, default=1024, required=False,
+                        help='Headers block size to protect with ecc (eg: 1024 meants that the first 1k of each file will be protected).', **widget_text)
+    main_parser.add_argument('-r', '--resilience_rate', type=float, default=0.3, required=False,
+                        help='Resilience rate for files headers (eg: 0.3 = 30%% of errors can be recovered but size of codeword will be 60%% of the data block, thus the ecc file will be about 60%% the size of your data).', **widget_text)
+    main_parser.add_argument('-ri', '--resilience_rate_intra', type=float, default=0.5, required=False,
+                        help='Resilience rate for intra-ecc (ecc on meta-data, such as filepath, thus this defines the ecc for the critical spots!).', **widget_text)
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('--stats_only', action='store_true', required=False, default=False,
+                        help='Only show the predicted total size of the ECC file given the parameters.')
+    main_parser.add_argument('--hash', metavar='md5;shortmd5;shortsha256...', type=str, required=False,
+                        help='Hash algorithm to use. Choose between: md5, shortmd5, shortsha256, minimd5, minisha256.', **widget_text)
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    # Correction mode arguments
+    main_parser.add_argument('-c', '--correct', action='store_true', required=False, default=False,
+                        help='Check/Correct the files?')
+    main_parser.add_argument('-o', '--output', metavar='/path/to/output/folder', type=is_dir, nargs=1, required=False,
+                        help='Path of the folder where the repaired files will be copied (only repaired corrupted files will be copied there, files that weren\'t corrupted at all won\'t be copied so you have to copy them by yourself afterwards).', **widget_dir)
+    main_parser.add_argument('-e', '--errors_file', metavar='/some/folder/errorsfile.csv', type=str, nargs=1, required=False, #type=argparse.FileType('rt')
+                        help='Path to the error file generated by RFIGC.py (this specify in csv format the list of files to check, and only those files will be checked and repaired). Do not specify this argument if you want to check and repair all files.', **widget_file)
+    main_parser.add_argument('--ignore_size', action='store_true', required=False, default=False,
+                        help='On correction, if the file size differs from when the ecc file was generated, ignore and try to correct anyway (this may work with file where data was appended without changing the rest. For compressed formats like zip, this will probably fail).')
+    main_parser.add_argument('--no_fast_check', action='store_true', required=False, default=False,
+                        help='On correction, block corruption is only checked with the hash (the ecc will still be checked after correction, but not before). If no_fast_check is enabled, then ecc will also be checked before. This allows to find blocks corrupted by malicious intent (the block is corrupted but the hash has been corrupted as well to match the corrupted block, because it\'s almost impossible that following a hardware or logical fault, the hash match the corrupted block).')
+    main_parser.add_argument('--skip_missing', action='store_true', required=False, default=False,
+                        help='Skip missing files (no warning).')
+    main_parser.add_argument('--enable_erasures', action='store_true', required=False, default=False,
+                        help='Enable errors-and-erasures correction. Reed-Solomon can correct twice more erasures than errors (eg, if resilience rate is 0.3, then you can correct 30%% errors and 60%% erasures and any combination of errors and erasures between 30%%-60%% corruption). An erasure is a corrupted symbol where we know the position, while errors are not known at all. To find erasures, we will find any symbol that is equal to --erasure_symbol and flag it as an erasure. This is particularly useful if the software you use (eg, a disk scraper) can mark bad sectors with a constant character (eg, null byte). Misdetected erasures will just eat one ecc symbol, and won\'t change the decoded message.')
+    main_parser.add_argument('--only_erasures', action='store_true', required=False, default=False,
+                        help='Enable only erasures correction (no errors). Use this only if you are sure that all corrupted symbols have the same value (eg, if your disk scraper replace bad sectors by null bytes). This will ensure that you can correct up to 2*resilience_rate corrupted symbols.')
+    main_parser.add_argument('--erasure_symbol', type=int, default=0, required=False,
+                        help='Symbol that will be flagged as an erasure. Default: null byte 0. (value must be an integer)', **widget_text)
+
+    # Generate mode arguments
+    main_parser.add_argument('-g', '--generate', action='store_true', required=False, default=False,
+                        help='Generate the ecc file?')
+    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
+                        help='Force overwriting the ecc file even if it already exists (if --generate).')
+    main_parser.add_argument('--skip_size_below', type=int, default=None, required=False,
+                        help='Skip files below the specified size (in bytes).', **widget_text)
+    main_parser.add_argument('--always_include_ext', metavar='txt|jpg|png', type=str, default=None, required=False,
+                        help='Always include files with the specified extensions, useful in combination with --skip_size_below to keep files of certain types even if they are below the size. Format: extensions separated by |.', **widget_text)
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set hard-coded variables
+    entrymarker = "\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF" # marker that will signal the beginning of an ecc entry - use an alternating pattern of several characters, this avoids confusion (eg: if you use "AAA" as a pattern, if the ecc block of the previous file ends with "EGA" for example, then the full string for example will be "EGAAAAC:\yourfolder\filea.jpg" and then the entry reader will detect the first "AAA" occurrence as the entry start - this should not make the next entry bug because there is an automatic trim - but the previous ecc block will miss one character that could be used to repair the block because it will be "EG" instead of "EGA"!)
+    field_delim = "\xFA\xFF\xFA\xFF\xFA" # delimiter between fields (filepath, filesize, hash+ecc blocks) inside an ecc entry
+
+    #-- Set variables from arguments
+    inputpath = fullpath(args.input[0]) # path to the files to protect (either a folder or a single file)
+    rootfolderpath = inputpath # path to the root folder (to compute relative paths)
+    database = fullpath(args.database[0])
+    generate = args.generate
+    correct = args.correct
+    force = args.force
+    stats_only = args.stats_only
+    max_block_size = args.max_block_size
+    header_size = args.size
+    resilience_rate = args.resilience_rate
+    resilience_rate_intra = args.resilience_rate_intra
+    enable_erasures = args.enable_erasures
+    only_erasures = args.only_erasures
+    erasure_symbol = args.erasure_symbol
+    ignore_size = args.ignore_size
+    skip_missing = args.skip_missing
+    skip_size_below = args.skip_size_below
+    always_include_ext = args.always_include_ext
+    if always_include_ext: always_include_ext = tuple(['.'+ext for ext in always_include_ext.split('|')]) # prepare a tuple of extensions (prepending with a dot) so that str.endswith() works (it doesn't with a list, only a tuple)
+    hash_algo = args.hash
+    if not hash_algo: hash_algo = "md5"
+    ecc_algo = args.ecc_algo
+    fast_check = not args.no_fast_check
+    verbose = args.verbose
+    silent = args.silent
+
+    if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
+        rootfolderpath = os.path.dirname(inputpath)
+
+    if correct:
+        if not args.output:
+            raise NameError('Output path is necessary when in correction mode!')
+        outputpath = fullpath(args.output[0])
+
+    errors_file = None
+    if args.errors_file: errors_file = os.path.basename(fullpath(args.errors_file[0]))
+
+    # -- Checking arguments
+    if not stats_only and not generate and not os.path.isfile(database):
+        raise NameError('Specified database ecc file %s does not exist!' % database)
+    elif generate and os.path.isfile(database) and not force:
+        raise NameError('Specified database ecc file %s already exists! Use --force if you want to overwrite.' % database)
+
+    if resilience_rate <= 0 or resilience_rate_intra <= 0:
+        raise ValueError('Resilience rate cannot be negative nor zero and it must be a float number.');
+
+    if max_block_size < 2 or max_block_size > 255:
+        raise ValueError('RS max block size must be between 2 and 255.')
+
+    if header_size < 1:
+        raise ValueError('Header size cannot be negative.')
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log[0], 'a', nostdout=silent)
+        #sys.stdout = Tee(args.log[0], 'a')
+        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+
+    # == PROCESSING BRANCHING == #
+
+    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
+    ptee.write("Initializing the ECC codecs, please wait...")
+    hasher = Hasher(hash_algo)
+    hasher_intra = Hasher('none') # for intra_ecc we don't use any hash
+    ecc_params = compute_ecc_params(max_block_size, resilience_rate, hasher)
+    ecc_manager = ECCMan(max_block_size, ecc_params["message_size"], algo=ecc_algo)
+    ecc_params_intra = compute_ecc_params(max_block_size, resilience_rate_intra, hasher_intra)
+    ecc_manager_intra = ECCMan(max_block_size, ecc_params_intra["message_size"], algo=ecc_algo)
+    ecc_params_idx = compute_ecc_params(27, 1, hasher_intra)
+    ecc_manager_idx = ECCMan(27, ecc_params_idx["message_size"], algo=ecc_algo)
+
+    # == Precomputation of ecc file size
+    # Precomputing is important so that the user can know what size to expect before starting (and how much time it will take...).
+    filescount = 0
+    sizetotal = 0
+    sizeheaders = 0
+    ptee.write("Precomputing list of files and predicted statistics...")
+    for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee):
+        filescount = filescount + 1 # counting the total number of files we will process (so that we can show a progress bar with ETA)
+        # Get full absolute filepath
+        filepath = os.path.join(dirpath, filename)
+        relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
+        # Get the current file's size
+        size = os.stat(filepath).st_size
+        # Check if we must skip this file because size is too small, and then if we still keep it because it's extension is always to be included
+        if skip_size_below and size < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)): continue
+
+        # Compute total size of all files
+        sizetotal = sizetotal + size
+        # Compute predicted size of their headers
+        if size >= header_size: # for big files, we limit the size to the header size
+            header_size_add = header_size
+        else: # else for size smaller than the defined header size, it will just be the size of the file
+            header_size_add = size
+        # Size of the ecc entry for this file will be: entrymarker-bytes + field_delim-bytes*occurrence + length-filepath-string + length-size-string + length-filepath-ecc + size of the ecc per block for all blocks in file header + size of the hash per block for all blocks in file header.
+        sizeheaders = sizeheaders + (len(entrymarker) + len(field_delim)*3 + len(relfilepath) + len(str(size)) + int(float(len(relfilepath))*resilience_rate_intra) + (int(math.ceil(float(header_size_add) / ecc_params["message_size"])) * (ecc_params["ecc_size"]+ecc_params["hash_size"])) ) # Compute the total number of bytes we will add with ecc + hash (accounting for the padding of the remaining characters at the end of the sequence in case it doesn't fit with the message_size, by using ceil() )
+    ptee.write("Precomputing done.")
+    if generate: # show statistics only if generating an ecc file
+        # TODO: add the size of the ecc format header? (arguments string + PYHEADERECC identifier)
+        total_pred_percentage = sizeheaders * 100 / sizetotal
+        ptee.write("Total ECC size estimation: %s = %g%% of total files size %s." % (sizeof_fmt(sizeheaders), total_pred_percentage, sizeof_fmt(sizetotal)))
+        ptee.write("Details: resiliency of %i%%: For the header (first %i characters) of each file: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate*100, header_size, ecc_params["message_size"], ecc_params["ecc_size"], int(ecc_params["ecc_size"] / 2), ecc_params["ecc_size"]))
+
+    if stats_only:
+        ptee.close()
+        return 0
+
+    # == Generation mode
+    # Generate an ecc file, containing ecc entries for every files recursively in the specified root folder.
+    # The file header will be split by blocks depending on max_block_size and resilience_rate, and each of those blocks will be hashed and a Reed-Solomon code will be produced.
+    if generate:
+        ptee.write("====================================")
+        ptee.write("Header ECC generation, started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        with open(database, 'wb') as db, open(database+".idx", 'wb') as dbidx:
+            # Write ECC file header identifier (unique string + version)
+            db.write( b("**PYHEADERECCv%s**\n" % (''.join([x * 3 for x in __version__]))) ) # each character in the version will be repeated 3 times, so that in case of tampering, a majority vote can try to disambiguate
+            # Write the parameters (they are NOT reloaded automatically, you have to specify them at commandline! It's the user role to memorize those parameters (using any means: own brain memory, keep a copy on paper, on email, etc.), so that the parameters are NEVER tampered. The parameters MUST be ultra reliable so that errors in the ECC file can be more efficiently recovered.
+            for i in _range(3): db.write( ("** Parameters: "+" ".join(argv) + "\n").encode() ) # copy them 3 times just to be redundant in case of ecc file corruption
+            db.write( b("** Generated under %s\n" % ecc_manager.description()) )
+            # NOTE: there's NO HEADER for the ecc file! Ecc entries are all independent of each others, you just need to supply the decoding arguments at commandline, and the ecc entries can be decoded. This is done on purpose to be remove the risk of critical spots in ecc file.
+
+            # Processing ecc on files
+            files_done = 0
+            files_skipped = 0
+            for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee, total=filescount, leave=True, unit="files"):
+                # Get full absolute filepath
+                filepath = os.path.join(dirpath, filename)
+                # Get database relative path (from scanning root folder)
+                relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
+                # Get file size
+                filesize = os.stat(filepath).st_size
+                # If skip size is enabled and size is below the skip size, we skip UNLESS the file extension is in the always include list
+                if skip_size_below and filesize < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)):
+                    files_skipped += 1
+                    continue
+
+                # Opening the input file's to read its header and compute the ecc/hash blocks
+                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
+                with open(os.path.join(rootfolderpath, filepath), 'rb') as file:
+                    # -- Intra-ecc generation: Compute an ecc for the filepath and filesize, to avoid a critical spot here (so that we don't care that the filepath gets corrupted, we have an ecc to fix it!)
+                    relfilepath_ecc = b''.join(compute_ecc_hash(ecc_manager_intra, hasher_intra, relfilepath, max_block_size, resilience_rate_intra, ecc_params_intra["message_size"], True))
+                    filesize_ecc = b''.join(compute_ecc_hash(ecc_manager_intra, hasher_intra, str(filesize), max_block_size, resilience_rate_intra, ecc_params_intra["message_size"], True))
+                    # -- Hash/Ecc encoding of file's content (everything is managed inside compute_ecc_hash)
+                    buf = file.read(header_size) # read the file's header
+                    ecc_stream = compute_ecc_hash(ecc_manager, hasher, buf, max_block_size, resilience_rate, ecc_params["message_size"], True) # then compute the ecc/hash entry for this file's header (this will be a chain of multiple ecc/hash fields per block of data, because Reed-Solomon is limited to a maximum of 255 bytes, including the original_message+ecc!)
+                    # -- Build the ecc entry
+                    # First put the ecc metadata
+                    ecc_entry = b''.join([b(entrymarker), b(relfilepath), b(field_delim), b(str(filesize)), b(field_delim), b(relfilepath_ecc), b(field_delim), b(filesize_ecc), b(field_delim)]) # first save the file's metadata (filename, filesize, filepath ecc, ...)
+                    # Then put the ecc stream (the ecc blocks for the file's data)
+                    for es in ecc_stream:
+                        ecc_entry += es
+                    # -- Commit the ecc entry into the database
+                    entrymarker_pos = db.tell() # backup the position of the start of this ecc entry
+                    # -- Committing the hash/ecc encoding of the file's content
+                    db.write(b(ecc_entry)) # commit to the ecc file, and replicate the number of times required
+                    # -- External indexes backup: calculate the position of the entrymarker and of each field delimiter, and compute their ecc, and save into the index backup file. This will allow later to retrieve the position of each marker in the ecc file, and repair them if necessary, while just incurring a very cheap storage cost.
+                    # Also, the index backup file is fixed delimited fields sizes, which means that each field has a very specifically delimited size, so that we don't need any marker: we can just compute the total size for each entry, and thus find all entries independently even if one or several are corrupted beyond repair, so that this won't affect other index entries.
+                    markers_pos = [entrymarker_pos,
+                                                entrymarker_pos+len(entrymarker)+len(relfilepath),
+                                                entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize)),
+                                                entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize))+len(field_delim)+len(relfilepath_ecc),
+                                                entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize))+len(field_delim)+len(relfilepath_ecc)+len(field_delim)+len(filesize_ecc),
+                                                ] # Make the list of all markers positions for this ecc entry. The first and last indexes are the most important (first is the entrymarker, the last is the field_delim just before the ecc track start)
+                    markers_pos = [struct.pack('>Q', x) for x in markers_pos] # Convert to a binary representation in 8 bytes using unsigned long long (up to 16 EB, this should be more than sufficient)
+                    markers_types = [b'1', b'2', b'2', b'2', b'2']
+                    markers_pos_ecc = [ecc_manager_idx.encode(x+y) for x,y in _izip(markers_types,markers_pos)] # compute the ecc for each number
+                    # Couple each marker's position with its type and with its ecc, and write them all consecutively into the index backup file
+                    for items in _izip(markers_types,markers_pos,markers_pos_ecc):
+                        for item in items:
+                            dbidx.write(b(item))
+                files_done += 1
+        ptee.write("All done! Total number of files processed: %i, skipped: %i" % (files_done, files_skipped))
+        ptee.close()
+        return 0
+
+    # == Error Correction (and checking by hash) mode
+    # For each file, check their headers by block by checking each block against a hash, and if the hash does not match, try to correct with Reed-Solomon and then check the hash again to see if we correctly repaired the block (else the ecc entry might have been corrupted, whether it's the hash or the ecc field, in both cases it's highly unlikely that a wrong repair will match the hash after this wrong repair)
+    elif correct:
+        ptee.write("====================================")
+        ptee.write("Header ECC verification and correction, started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        # Prepare the list of files with errors to reduce the scan (only if provided)
+        errors_filelist = []
+        if errors_file:
+            with open(errors_file, 'rb') as efile:
+                for row in csv.DictReader(efile, lineterminator='\n', delimiter='|', quotechar='"', fieldnames=['filepath', 'error']): # need to specify the fieldnames, else the first row in the csv file will be skipped (it will be used as the columns names)
+                    errors_filelist.append(row['filepath'])
+
+        # Read the ecc file
+        dbsize = os.stat(database).st_size # must get db file size before opening it in order not to move the cursor
+        with open(database, 'rb') as db:
+            # Counters
+            files_count = 0
+            files_corrupted = 0
+            files_repaired_partially = 0
+            files_repaired_completely = 0
+            files_skipped = 0
+
+            # Main loop: process each ecc entry
+            entry = 1 # to start the while loop
+            bardisp = tqdm.tqdm(total=dbsize, file=ptee, leave=True, desc='DBREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
+            while entry:
+
+                # -- Read the next ecc entry (extract the raw string from the ecc file)
+                #if replication_rate == 1:
+                entry = get_next_entry(db, entrymarker, False)
+                if entry: bardisp.update(len(entry)) # update progress bar
+
+                # No entry? Then we finished because this is the end of file (stop condition)
+                if not entry: break
+
+                # -- Get position of current entry (for debugging purposes)
+                entry_pos = [db.tell(), db.tell()-len(entry)]
+
+                # -- Extract the fields from the ecc entry
+                entry_p = entry_fields(entry, b(field_delim))
+
+                # -- Get file path, check its correctness and correct it by using intra-ecc if necessary
+                relfilepath = entry_p["relfilepath"] # Relative file path
+                relfilepath, fpcorrupted, fpcorrected, fperrmsg = ecc_correct_intra(ecc_manager_intra, ecc_params_intra, relfilepath, entry_p["relfilepath_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
+
+                # Report errors
+                if fpcorrupted:
+                    if fpcorrected: ptee.write("\n- Fixed error in metadata field at offset %i filepath %s." % (db.tell()-len(entry), relfilepath))
+                    else: ptee.write("\n- Error in filepath, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], relfilepath))
+                ptee.write(fperrmsg)
+
+                # Convert to str (so that we can use os.path funcs)
+                relfilepath = relfilepath.decode('latin-1')
+                # Update entry_p
+                entry_p["relfilepath"] = relfilepath
+                # -- End of intra-ecc on filepath
+
+                # -- Get file size, check its correctness and correct it by using intra-ecc if necessary
+                filesize = str(entry_p["filesize"])
+                filesize, fscorrupted, fscorrected, fserrmsg = ecc_correct_intra(ecc_manager_intra, ecc_params_intra, filesize, entry_p["filesize_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
+
+                # Report errors
+                if fscorrupted:
+                    if fscorrected: ptee.write("\n- Fixed error in metadata field at offset %i filesize %s." % (db.tell()-len(entry), filesize))
+                    else: ptee.write("\n- Error in filesize, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], filesize))
+                ptee.write(fserrmsg)
+
+                # Convert filesize intra-field into an int
+                filesize = int(filesize)
+
+                # Update entry_p
+                entry_p["filesize"] = filesize # need to update entry_p because various funcs will directly access filesize this way...
+                # -- End of intra-ecc on filesize
+
+                # Build the absolute file path
+                filepath = os.path.join(rootfolderpath, relfilepath) # Get full absolute filepath from given input folder (because the files may be specified in any folder, in the ecc file the paths are relative, so that the files can be moved around or burnt on optical discs)
+                if errors_filelist and relfilepath not in errors_filelist: continue # if a list of files with errors was supplied (for example by rfigc.py), then we will check only those files and skip the others
+
+                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
+
+                # -- Check filepath
+                # Check that the filepath isn't corrupted (if a silent error erase a character (not only flip a bit), then it will also be detected this way)
+                if relfilepath.find("\x00") >= 0:
+                    ptee.write("Error: ecc entry corrupted on filepath field, please try to manually repair the filepath (filepath: %s - missing/corrupted character at %i)." % (filepath, relfilepath.find("\x00")))
+                    files_skipped += 1
+                    continue
+                # Check that file still exists before checking it
+                if not os.path.isfile(filepath):
+                    if not skip_missing: ptee.write("Error: file %s could not be found: either file was moved or the ecc entry was corrupted. You may try to fix manually the entry." % filepath)
+                    files_skipped += 1
+                    continue
+
+                # -- Checking file size: if the size has changed, the blocks may not match anymore!
+                real_filesize = os.stat(filepath).st_size
+                if filesize != real_filesize:
+                    if ignore_size:
+                        ptee.write("Warning: file %s has a different size: %s (before: %s). Will still try to correct it (but the blocks may not match!)." % (relfilepath, real_filesize, filesize))
+                    else:
+                        ptee.write("Error: file %s has a different size: %s (before: %s). Skipping the file correction because blocks may not match (you can set --ignore_size to still correct even if size is different, maybe just the entry was corrupted)." % (relfilepath, real_filesize, filesize))
+                        files_skipped += 1
+                        continue
+
+                files_count += 1
+                # -- Check blocks and repair if necessary
+                entry_asm = entry_assemble(entry_p, ecc_params, header_size, filepath) # Extract and assemble each message block from the original file with its corresponding ecc and hash
+                corrupted = False # flag to signal that the file was corrupted and we need to reconstruct it afterwards
+                repaired_partially = False # flag to signal if a file was repaired only partially
+                err_consecutive = True # flag to check if the ecc track is misaligned/misdetected (we only encounter corrupted blocks that we can't fix)
+                # For each message block, check the message with hash and repair with ecc if necessary
+                for i, e in enumerate(entry_asm):
+                    # If the message block has a different hash, it was corrupted (or the hash is corrupted, or both)
+                    if hasher.hash(e["message"]) != e["hash"] or (not fast_check and not ecc_manager.check(e["message"], e["ecc"])):
+                        corrupted = True
+                        # Try to repair the block using ECC
+                        ptee.write("File %s: corruption in block %i. Trying to fix it." % (relfilepath, i))
+                        try:
+                            repaired_block, repaired_ecc = ecc_manager.decode(e["message"], e["ecc"], enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
+                        except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
+                            repaired_block = None
+                            repaired_ecc = None
+                            print("Error: file %s: block %i: %s" % (relfilepath, i, exc))
+                        # Check if the repair was successful.
+                        hash_ok = None
+                        ecc_ok = None
+                        if repaired_block is not None:
+                            hash_ok = (hasher.hash(repaired_block) == e["hash"])
+                            ecc_ok = ecc_manager.check(repaired_block, repaired_ecc)
+                        if repaired_block is not None and (hash_ok or ecc_ok): # If either the hash or the ecc check now match the repaired message block, we commit the new block
+                            entry_asm[i]["message_repaired"] = repaired_block # save the repaired block
+                            # Show a precise report about the repair
+                            if hash_ok and ecc_ok: ptee.write("File %s: block %i repaired!" % (relfilepath, i))
+                            elif not hash_ok: ptee.write("File %s: block %i probably repaired with matching ecc check but with a hash error (assume the hash was corrupted)." % (relfilepath, i))
+                            elif not ecc_ok: ptee.write("File %s: block %i probably repaired with matching hash but with ecc check error (assume the ecc was partially corrupted)." % (relfilepath, i))
+                            err_consecutive = False
+                        else: # Else the hash and the ecc check do not match: the repair failed (either because the ecc is too much tampered, or because the hash is corrupted. Either way, we don't commit).
+                            ptee.write("Error: file %s could not repair block %i (both hash and ecc check mismatch)." % (relfilepath, i)) # you need to code yourself to use bit-recover, it's in perl but it should work given the hash computed by this script and the corresponding message block.
+                            repaired_partially = True
+                            # Detect if the ecc track is misaligned/misdetected (we encounter only errors that we can't fix)
+                            if err_consecutive and i >= 10: # threshold is ten consecutive uncorrectable errors
+                                ptee.write("Failure: Too many consecutive uncorrectable errors for %s. Most likely, the ecc track was misdetected (try to repair the entrymarkers and field delimiters). Skipping this track/file." % relfilepath)
+                                break
+                    else:
+                        err_consecutive = False
+                # -- Reconstruct/Copying the repaired file
+                # If this file had a corruption in one of its header blocks, then we will reconstruct the file header and then append the rest of the file (which can then be further repaired by other tools such as PAR2).
+                if corrupted:
+                    # Counters...
+                    files_corrupted += 1
+                    if repaired_partially:
+                        files_repaired_partially += 1
+                    else:
+                        files_repaired_completely += 1
+                    # Reconstructing the file
+                    outfilepath = os.path.join(outputpath, relfilepath) # get the full path to the output file
+                    outfiledir = os.path.dirname(outfilepath)
+                    if not os.path.isdir(outfiledir): os.makedirs(outfiledir) # if the target directory does not exist, create it (and create recursively all parent directories too)
+                    with open(outfilepath, 'wb') as out:
+                        # Reconstruct the header using repaired blocks (and the other non corrupted blocks)
+                        for e in entry_asm:
+                            if "message_repaired" in e:
+                                out.write(e["message_repaired"])
+                            else:
+                                out.write(e["message"])
+                        # Append the rest of the file by copying from the original
+                        with open(filepath, 'rb') as originalfile:
+                            blocksize = 65535
+                            originalfile.seek(header_size)
+                            buf = originalfile.read(blocksize)
+                            while buf:
+                                out.write(buf)
+                                buf = originalfile.read(blocksize)
+                    # Copying the last access time and last modification time from the original file TODO: a more reliable way would be to use the db computed by rfigc.py, because if a software maliciously tampered the data, then the modification date may also have changed (but not if it's a silent error, in that case we're ok).
+                    filestats = os.stat(filepath)
+                    os.utime(outfilepath, (filestats.st_atime, filestats.st_mtime))
+        # All ecc entries processed for checking and potentally repairing, we're done correcting!
+        bardisp.close() # at the end, the bar may not be 100% because of the headers that are skipped by get_next_entry() and are not accounted in bardisp.
+        ptee.write("All done! Stats:\n- Total files processed: %i\n- Total files corrupted: %i\n- Total files repaired completely: %i\n- Total files repaired partially: %i\n- Total files corrupted but not repaired at all: %i\n- Total files skipped: %i" % (files_count, files_corrupted, files_repaired_completely, files_repaired_partially, files_corrupted - (files_repaired_partially + files_repaired_completely), files_skipped) )
+        ptee.close()
+        if files_corrupted == 0 or files_repaired_completely == files_corrupted:
+            return 0
+        else:
+            return 1
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":  # pragma: no cover
+    global __package__
+    if __package__ is None:
+        #sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+        print('HOHO')
+        __package__ = 'pyFileFixity.header_ecc'
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/lib/_compat.py` & `pyFileFixity-3.1.4/pyFileFixity/lib/_compat.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,71 +1,72 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-
-import sys
-
-try: # compatibility with Python 3+
-    _range = xrange
-except NameError:
-    _range = range
-
-try:
-    from cStringIO import StringIO
-    _StringIO = StringIO
-except (ImportError, NameError): #python3.x
-    from io import StringIO
-    _StringIO = StringIO
-
-try:
-    from itertools import izip
-    _izip = izip
-except ImportError:  #python3.x
-    _izip = zip
-
-try:
-    _str = basestring
-except NameError:
-    _str = str
-
-if sys.version_info < (3,):
-    def b(x):
-        return x
-else:
-    import codecs
-    def b(x):
-        if isinstance(x, _str):
-            return codecs.latin_1_encode(x)[0]
-        else:
-            return x
-
-if sys.version_info < (3,):
-    def _open_csv(x, mode='r'):
-        return open(x, mode+'b')
-else:
-    def _open_csv(x, mode='r'):
-        return open(x, mode+'t', newline='')
-
-if sys.version_info < (3,):
-    def _ord(x):
-        return ord(x)
-else:
-    def _ord(x):
-        if isinstance(x, int):
-            return x
-        else:
-            return ord(x)
-
-if sys.version_info < (3,):
-    def _bytes(x):
-        return bytes(x)
-else:
-    def _bytes(x):
-        if isinstance(x, (bytes, bytearray)):
-            return x
-        else:
-            return bytes(x, 'latin-1')
-
-try:
-    from itertools import izip
-    _izip = izip
-except ImportError:
-    _izip = zip
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import sys
+
+try: # compatibility with Python 3+
+    _range = xrange
+except NameError:
+    _range = range
+
+try:
+    from cStringIO import StringIO
+    _StringIO = StringIO
+except (ImportError, NameError): #python3.x
+    from io import StringIO
+    _StringIO = StringIO
+
+try:
+    from itertools import izip
+    _izip = izip
+except ImportError:  #python3.x
+    _izip = zip
+
+try:
+    _str = basestring
+except NameError:
+    _str = str
+
+if sys.version_info < (3,):
+    def b(x):
+        return x
+else:
+    import codecs
+    def b(x):
+        if isinstance(x, _str):
+            return codecs.latin_1_encode(x)[0]
+        else:
+            return x
+
+if sys.version_info < (3,):
+    import io
+    def _open_csv(x, mode='r'):
+        return io.open(x, mode+'b')  # on Py3, io.open() is the same as open(), see: https://stackoverflow.com/questions/5250744/difference-between-open-and-codecs-open-in-python
+else:
+    def _open_csv(x, mode='r'):
+        return open(x, mode+'t', newline='', encoding='utf-8')  # for csv module, open() mode needed to be binary for Python 2, but on Py3 it needs to be text mode, no binary! https://stackoverflow.com/a/34283957/1121352
+
+if sys.version_info < (3,):
+    def _ord(x):
+        return ord(x)
+else:
+    def _ord(x):
+        if isinstance(x, int):
+            return x
+        else:
+            return ord(x)
+
+if sys.version_info < (3,):
+    def _bytes(x):
+        return bytes(x)
+else:
+    def _bytes(x):
+        if isinstance(x, (bytes, bytearray)):
+            return x
+        else:
+            return bytes(x, 'latin-1')
+
+try:
+    from itertools import izip
+    _izip = izip
+except ImportError:
+    _izip = zip
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/lib/aux_funcs.py` & `pyFileFixity-3.1.4/pyFileFixity/lib/aux_funcs.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,427 +1,427 @@
-#!/usr/bin/env python
-#
-# Auxiliary functions library
-# Copyright (C) 2015 Larroque Stephen
-#
-
-import codecs
-import os
-import posixpath # to generate unix paths
-import shutil
-
-from ._compat import b
-
-from argparse import ArgumentTypeError
-from pathlib2 import PurePath, PureWindowsPath, PurePosixPath # opposite operation of os.path.join (split a path into parts)
-
-try:
-    from scandir import walk # use the faster scandir module if available (Python >= 3.5), see https://github.com/benhoyt/scandir
-except ImportError:
-    from os import walk # else, default to os.walk()
-
-
-def is_file(dirname):
-    '''Checks if a path is an actual file that exists'''
-    if not os.path.isfile(dirname):
-        msg = "{0} is not an existing file".format(dirname)
-        raise ArgumentTypeError(msg)
-    else:
-        return dirname
-
-def is_dir(dirname):
-    '''Checks if a path is an actual directory that exists'''
-    if not os.path.isdir(dirname):
-        msg = "{0} is not a directory".format(dirname)
-        raise ArgumentTypeError(msg)
-    else:
-        return dirname
-
-def is_dir_or_file(dirname):
-    '''Checks if a path is an actual directory that exists or a file'''
-    if not os.path.isdir(dirname) and not os.path.isfile(dirname):
-        msg = "{0} is not a directory nor a file".format(dirname)
-        raise ArgumentTypeError(msg)
-    else:
-        return dirname
-
-def fullpath(relpath):
-    '''Relative path to absolute'''
-    if (type(relpath) is object or hasattr(relpath, 'read')): # relpath is either an object or file-like, try to get its name
-        relpath = relpath.name
-    return os.path.abspath(os.path.expanduser(relpath))
-
-def recwalk(inputpath, sorting=True):
-    '''Recursively walk through a folder. This provides a mean to flatten out the files restitution (necessary to show a progress bar). This is a generator.'''
-    # If it's only a single file, return this single file
-    if os.path.isfile(inputpath):
-        abs_path = fullpath(inputpath)
-        yield os.path.dirname(abs_path), os.path.basename(abs_path)
-    # Else if it's a folder, walk recursively and return every files
-    else:
-        for dirpath, dirs, files in walk(inputpath):	
-            if sorting:
-                files.sort()
-                dirs.sort() # sort directories in-place for ordered recursive walking
-            for filename in files:
-                yield (dirpath, filename) # return directory (full path) and filename
-
-def sizeof_fmt(num, suffix='B', mod=1024.0):
-    '''Readable size format, courtesy of Sridhar Ratnakumar'''
-    for unit in ['','K','M','G','T','P','E','Z']:
-        if abs(num) < mod:
-            return "%3.1f%s%s" % (num, unit, suffix)
-        num /= mod
-    return "%.1f%s%s" % (num, 'Y', suffix)
-
-def path2unix(path, nojoin=False, fromwinpath=False):
-    '''From a path given in any format, converts to posix path format
-    fromwinpath=True forces the input path to be recognized as a Windows path (useful on Unix machines to unit test Windows paths)'''
-    if fromwinpath:
-        pathparts = list(PureWindowsPath(path).parts)
-    else:
-        pathparts = list(PurePath(path).parts)
-    if nojoin:
-        return pathparts
-    else:
-        return posixpath.join(*pathparts)
-
-def get_next_entry(file, entrymarker="\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF", only_coord=True, blocksize=65535):
-    '''Find or read the next ecc entry in a given ecc file.
-    Call this function multiple times with the same file handle to get subsequent markers positions (this is not a generator but it works very similarly, because it will continue reading from the file's current cursor position -- this can be used advantageously if you want to read only a specific entry by seeking before supplying the file handle).
-    This will read any string length between two entrymarkers.
-    The reading is very tolerant, so it will always return any valid entry (but also scrambled entries if any, but the decoding will ensure everything's ok).
-    `file` is a file handle, not the path to the file.'''
-    # TODO: use mmap native module instead of manually reading using blocksize?
-
-    entrymarker = bytearray(b(entrymarker))
-    found = False
-    start = None # start and end vars are the relative position of the starting/ending entrymarkers in the current buffer
-    end = None
-    startcursor = None # startcursor and endcursor are the absolute position of the starting/ending entrymarkers inside the database file
-    endcursor = None
-    buf = 1
-    # Sanity check: cannot screen the file's content if the window is of the same size as the pattern to match (the marker)
-    if blocksize <= len(entrymarker): blocksize = len(entrymarker) + 1
-    # Continue the search as long as we did not find at least one starting marker and one ending marker (or end of file)
-    while (not found and buf):
-        # Read a long block at once, we will readjust the file cursor after
-        buf = bytearray(file.read(blocksize))
-        # Find the start marker (if not found already)
-        if start is None or start == -1:
-            start = buf.find(entrymarker); # relative position of the starting marker in the currently read string
-            if start >= 0 and not startcursor: # assign startcursor only if it's empty (meaning that we did not find the starting entrymarker, else if found we are only looking for 
-                startcursor = file.tell() - len(buf) + start # absolute position of the starting marker in the file
-            if start >= 0: start = start + len(entrymarker)
-        # If we have a starting marker, we try to find a subsequent marker which will be the ending of our entry (if the entry is corrupted we don't care: it won't pass the entry_to_dict() decoding or subsequent steps of decoding and we will just pass to the next ecc entry). This allows to process any valid entry, no matter if previous ones were scrambled.
-        if startcursor is not None and startcursor >= 0:
-            end = buf.find(entrymarker, start)
-            if end < 0 and len(buf) < blocksize: # Special case: we didn't find any ending marker but we reached the end of file, then we are probably in fact just reading the last entry (thus there's no ending marker for this entry)
-                end = len(buf) # It's ok, we have our entry, the ending marker is just the end of file
-            # If we found an ending marker (or if end of file is reached), then we compute the absolute cursor value and put the file reading cursor back in position, just before the next entry (where the ending marker is if any)
-            if end >= 0:
-                endcursor = file.tell() - len(buf) + end
-                # Make sure we are not redetecting the same marker as the start marker
-                if endcursor > startcursor:
-                    file.seek(endcursor)
-                    found = True
-                else:
-                    end = -1
-                    encursor = None
-        #print("Start:", start, startcursor)
-        #print("End: ", end, endcursor)
-        # Stop criterion to avoid infinite loop: in the case we could not find any entry in the rest of the file and we reached the EOF, we just quit now
-        if len(buf) < blocksize: break
-        # Did not find the full entry in one buffer? Reinit variables for next iteration, but keep in memory startcursor.
-        if start > 0: start = 0 # reset the start position for the end buf find at next iteration (ie: in the arithmetic operations to compute the absolute endcursor position, the start entrymarker won't be accounted because it was discovered in a previous buffer).
-        if not endcursor: file.seek(file.tell()-len(entrymarker)) # Try to fix edge case where blocksize stops the buffer exactly in the middle of the ending entrymarker. The starting marker should always be ok because it should be quite close (or generally immediately after) the previous entry, but the end depends on the end of the current entry (size of the original file), thus the buffer may miss the ending entrymarker. should offset file.seek(-len(entrymarker)) before searching for ending.
-
-    if found: # if an entry was found, we seek to the beginning of the entry and then either read the entry from file or just return the markers positions (aka the entry bounds)
-        file.seek(startcursor + len(entrymarker))
-        if only_coord:
-            # Return only coordinates of the start and end markers
-            # Note: it is useful to just return the reading positions and not the entry itself because it can get quite huge and may overflow memory, thus we will read each ecc blocks on request using a generator.
-            return [startcursor + len(entrymarker), endcursor]
-        else:
-            # Return the full entry's content
-            return file.read(endcursor - startcursor - len(entrymarker))
-    else:
-        # Nothing found (or no new entry to find, we've already found them all), so we return None
-        return None
-
-def create_dir_if_not_exist(path):  # pragma: no cover
-    """Create a directory if it does not already exist, else nothing is done and no error is return"""
-    if not os.path.exists(path):
-        os.makedirs(path)
-
-def remove_if_exist(path):  # pragma: no cover
-    """Delete a file or a directory recursively if it exists, else no exception is raised"""
-    if os.path.exists(path):
-        if os.path.isdir(path):
-            shutil.rmtree(path)
-            return True
-        elif os.path.isfile(path):
-            os.remove(path)
-            return True
-    return False
-
-def copy_any(src, dst, only_missing=False):  # pragma: no cover
-    """Copy a file or a directory tree, deleting the destination before processing"""
-    if not only_missing:
-        remove_if_exist(dst)
-    if os.path.exists(src):
-        if os.path.isdir(src):
-            if not only_missing:
-                shutil.copytree(src, dst, symlinks=False, ignore=None)
-            else:
-                for dirpath, filepath in recwalk(src):
-                    srcfile = os.path.join(dirpath, filepath)
-                    relpath = os.path.relpath(srcfile, src)
-                    dstfile = os.path.join(dst, relpath)
-                    if not os.path.exists(dstfile):
-                        create_dir_if_not_exist(os.path.dirname(dstfile))
-                        shutil.copyfile(srcfile, dstfile)
-                        shutil.copystat(srcfile, dstfile)
-            return True
-        elif os.path.isfile(src) and (not only_missing or not os.path.exists(dst)):
-            shutil.copyfile(src, dst)
-            shutil.copystat(src, dst)
-            return True
-    return False
-
-def simple_read(rel_path, root_path=None):
-    # Helper function to ease reading a single-sourced __version__ attribute from a mypackage.__init__ file
-    # It accesses the specified relative path, relative to the calling script (so that it workes relative to the current package if installed in site-scripts)
-    # From: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/#single-sourcing-the-package-version
-    # See also: https://stackoverflow.com/a/75962009/1121352
-
-    #if root_path is not None:
-    here = root_path
-    #else:  # this branch was commented out to remove dead code that can cause the function to return strange unexpected results in edge cases, we prefer to fail explicitly in these cases
-    #    try:
-    #        here = os.path.abspath(os.path.dirname(__file__))
-    #    except UnboundLocalError as exc:
-    #        __file__ = os.path.join(os.getcwd(), 'dummyfile.ext')  # get current working directory if using this function under an interactive prompt
-    #        here = os.path.abspath(os.path.dirname(__file__))
-    with codecs.open(os.path.join(here, rel_path), 'r') as fp:
-        return fp.read()
-
-def get_version(rel_path, root_path=None):
-    # Helper function to ease reading a single-sourced __version__ attribute from a mypackage.__init__ file
-    # Read instead of importing, so that there is no risk of infinite imports loop
-    # From: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/#single-sourcing-the-package-version
-    # See also: https://stackoverflow.com/a/75962009/1121352
-    for line in simple_read(rel_path, root_path).splitlines():
-        if line.startswith('__version__'):
-            delim = '"' if '"' in line else "'"
-            return line.split(delim)[1]
-    else:
-        raise RuntimeError("Unable to find version string.")
-
-#### MULTIFILES AUX FUNCTIONS ####
-# Here are the aux functions to cluster files of similar sizes in order to generate multifiles ecc tracks in header_ecc.py and structural_variable_ecc.py
-# TODO in hecc and saecc:
-#  * make intra-fields filepath with multiple files paths, separated by "|" (eg: "filepath1|filepath2|filepath3"
-#  * make intra-fields multiple file sizes, just like filepaths: "filesize1|filesize2|filesize3"
-#  * intra-ecc should work just the same
-#  * adapt stream encoding and decoding to dispatch the bytes to the corresponding files.
-#  * stream encoding/decoding should be robust: when a file ecc track is either ended (file was smaller than the others) or corrupted, then fill with null bytes, the rest should work the same (at decoding, maybe can use erasures decoding to double the recovery rate, at encoding maybe try to not store the null bytes? But how? Normally, the files in a group should be in descending size order, so the first file will always have a track, but if the other files are smaller then nvm their ecc just won't be appended, thus we end up with a shorter ecc track, but we know that and we can just computationally append the missing null bytes, no harm and no risk of corruption since these bytes aren't stored! So at encoding this doubles the recovery rate towards the end of the file for the bigger file)..
-# TODO in group_files_by_size: try to use sortedcontainers.SortedList() ? https://pypi.python.org/pypi/sortedcontainers
-
-from collections import OrderedDict
-from sortedcontainers import SortedList
-from random import randint
-try:
-    from itertools import izip_longest
-except ImportError:
-    from itertools import zip_longest as izip_longest
-
-def grouper(n, iterable, fillvalue=None):
-    "grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx"
-    args = [iter(iterable)] * n
-    return izip_longest(fillvalue=fillvalue, *args)
-
-def group_files_by_size(fileslist, multi):  # pragma: no cover
-    ''' Cluster files into the specified number of groups, where each groups total size is as close as possible to each other.
-
-    Pseudo-code (O(n^g) time complexity):
-    Input: number of groups G per cluster, list of files F with respective sizes
-    - Order F by descending size
-    - Until F is empty:
-        - Create a cluster X
-        - A = Pop first item in F
-        - Put A in X[0] (X[0] is thus the first group in cluster X)
-        For g in 1..len(G)-1 :
-            - B = Pop first item in F
-            - Put B in X[g]
-            - group_size := size(B)
-            If group_size != size(A):
-                While group_size < size(A):
-                    - Find next item C in F which size(C) <= size(A) - group_size
-                    - Put C in X[g]
-                    - group_size := group_size + size(C)
-    '''
-    flord = OrderedDict(sorted(fileslist.items(), key=lambda x: x[1], reverse=True))
-    if multi <= 1:
-        fgrouped = {}
-        i = 0
-        for x in flord.keys():
-            i += 1
-            fgrouped[i] = [[x]]
-        return fgrouped
-
-    fgrouped = {}
-    i = 0
-    while flord:
-        i += 1
-        fgrouped[i] = []
-        big_key, big_value = flord.popitem(0)
-        fgrouped[i].append([big_key])
-        for j in xrange(multi-1):
-            cluster = []
-            if not flord: break
-            child_key, child_value = flord.popitem(0)
-            cluster.append(child_key)
-            if child_value == big_value:
-                fgrouped[i].append(cluster)
-                continue
-            else:
-                diff = big_value - child_value
-                for key, value in flord.iteritems():
-                    if value <= diff:
-                        cluster.append(key)
-                        del flord[key]
-                        if value == diff:
-                            break
-                        else:
-                            child_value += value
-                            diff = big_value - child_value
-                fgrouped[i].append(cluster)
-    return fgrouped
-
-def group_files_by_size_fast(fileslist, nbgroups, mode=1):  # pragma: no cover
-    '''Given a files list with sizes, output a list where the files are grouped in nbgroups per cluster.
-
-    Pseudo-code for algorithm in O(n log(g)) (thank's to insertion sort or binary search trees)
-    See for more infos: http://cs.stackexchange.com/questions/44406/fast-algorithm-for-clustering-groups-of-elements-given-their-size-time/44614#44614
-    For each file:
-        - If to-fill list is empty or file.size > first-key(to-fill):
-          * Create cluster c with file in first group g1
-          * Add to-fill[file.size].append([c, g2], [c, g3], ..., [c, gn])
-        - Else:
-          * ksize = first-key(to-fill)
-          * c, g = to-fill[ksize].popitem(0)
-          * Add file to cluster c in group g
-          * nsize = ksize - file.size
-          * if nsize > 0:
-            . to-fill[nsize].append([c, g])
-            . sort to-fill if not an automatic ordering structure
-        '''
-    ftofill = SortedList()
-    ftofill_pointer = {}
-    fgrouped = [] # [] or {}
-    ford = sorted(fileslist.iteritems(), key=lambda x: x[1])
-    last_cid = -1
-    while ford:
-        fname, fsize = ford.pop()
-        #print "----\n"+fname, fsize
-        #if ftofill: print "beforebranch", fsize, ftofill[-1]
-        #print ftofill
-        if not ftofill or fsize > ftofill[-1]:
-            last_cid += 1
-            #print "Branch A: create cluster %i" % last_cid
-            fgrouped.append([])
-            #fgrouped[last_cid] = []
-            fgrouped[last_cid].append([fname])
-            if mode==0:
-                for g in xrange(nbgroups-1, 0, -1):
-                    fgrouped[last_cid].append([])
-                    if not fsize in ftofill_pointer:
-                        ftofill_pointer[fsize] = []
-                    ftofill_pointer[fsize].append((last_cid, g))
-                    ftofill.add(fsize)
-            else:
-                for g in xrange(1, nbgroups):
-                    try:
-                        fgname, fgsize = ford.pop()
-                        #print "Added to group %i: %s %i" % (g, fgname, fgsize)
-                    except IndexError:
-                        break
-                    fgrouped[last_cid].append([fgname])
-                    diff_size = fsize - fgsize
-                    if diff_size > 0:
-                        if not diff_size in ftofill_pointer:
-                            ftofill_pointer[diff_size] = []
-                        ftofill_pointer[diff_size].append((last_cid, g))
-                        ftofill.add(diff_size)
-        else:
-            #print "Branch B"
-            ksize = ftofill.pop()
-            c, g = ftofill_pointer[ksize].pop()
-            #print "Assign to cluster %i group %i" % (c, g)
-            fgrouped[c][g].append(fname)
-            nsize = ksize - fsize
-            if nsize > 0:
-                if not nsize in ftofill_pointer:
-                    ftofill_pointer[nsize] = []
-                ftofill_pointer[nsize].append((c, g))
-                ftofill.add(nsize)
-    return fgrouped
-
-def group_files_by_size_simple(fileslist, nbgroups):  # pragma: no cover
-    """ Simple and fast files grouping strategy: just order by size, and group files n-by-n, so that files with the closest sizes are grouped together.
-    In this strategy, there is only one file per subgroup, and thus there will often be remaining space left because there is no filling strategy here, but it's very fast. """
-    ford = sorted(fileslist.iteritems(), key=lambda x: x[1], reverse=True)
-    ford = [[x[0]] for x in ford]
-    return [group for group in grouper(nbgroups, ford)]
-
-def grouped_count_sizes(fileslist, fgrouped):  # pragma: no cover
-    '''Compute the total size per group and total number of files. Useful to check that everything is OK.'''
-    fsizes = {}
-    total_files = 0
-    allitems = None
-    if isinstance(fgrouped, dict):
-        allitems = fgrouped.iteritems()
-    elif isinstance(fgrouped, list):
-        allitems = enumerate(fgrouped)
-    for fkey, cluster in allitems:
-        fsizes[fkey] = []
-        for subcluster in cluster:
-            tot = 0
-            if subcluster is not None:
-                for fname in subcluster:
-                    tot += fileslist[fname]
-                    total_files += 1
-            fsizes[fkey].append(tot)
-    return fsizes, total_files
-
-def gen_rand_fileslist(nbfiles=100, maxvalue=100):  # pragma: no cover
-    fileslist = {}
-    for i in xrange(nbfiles):
-        fileslist["file_%i" % i] = randint(1, maxvalue)
-    return fileslist
-
-def gen_rand_fileslist2(nbfiles=100, maxvalue=100):  # pragma: no cover
-    fileslist = []
-    for i in xrange(nbfiles):
-        fileslist.append( ("file_%i" % i, randint(1, maxvalue)) )
-    return fileslist
-
-def grouped_test(nbfiles=100, nbgroups=3):  # pragma: no cover
-    fileslist = gen_rand_fileslist(nbfiles)
-    fgrouped = group_files_by_size(fileslist, nbgroups)
-    fsizes, total_files = grouped_count_sizes(fileslist, fgrouped)
-    return [fgrouped, fsizes, total_files]
-
-def grouped_fast_test(nbfiles=100, nbgroups=3, mode=1):  # pragma: no cover
-    nbfiles = 100
-    nbgroups = 3
-    fileslist = gen_rand_fileslist(nbfiles)
-    fgrouped = group_files_by_size_fast(fileslist, nbgroups, mode=mode)
-    fsizes, total_files = grouped_count_sizes(fileslist, fgrouped)
-    return [fgrouped, fsizes, total_files]
-
-def grouped_simple_test(nbfiles=100, nbgroups=3):  # pragma: no cover
-    fileslist = gen_rand_fileslist(nbfiles)
-    fgrouped = group_files_by_size_simple(fileslist, nbgroups)
-    fsizes, total_files = grouped_count_sizes(fileslist, fgrouped)
-    return [fgrouped, fsizes, total_files]
+#!/usr/bin/env python
+#
+# Auxiliary functions library
+# Copyright (C) 2015 Larroque Stephen
+#
+
+import codecs
+import os
+import posixpath # to generate unix paths
+import shutil
+
+from ._compat import b
+
+from argparse import ArgumentTypeError
+from pathlib2 import PurePath, PureWindowsPath, PurePosixPath # opposite operation of os.path.join (split a path into parts)
+
+try:
+    from scandir import walk # use the faster scandir module if available (Python >= 3.5), see https://github.com/benhoyt/scandir
+except ImportError:
+    from os import walk # else, default to os.walk()
+
+
+def is_file(dirname):
+    '''Checks if a path is an actual file that exists'''
+    if not os.path.isfile(dirname):
+        msg = "{0} is not an existing file".format(dirname)
+        raise ArgumentTypeError(msg)
+    else:
+        return dirname
+
+def is_dir(dirname):
+    '''Checks if a path is an actual directory that exists'''
+    if not os.path.isdir(dirname):
+        msg = "{0} is not a directory".format(dirname)
+        raise ArgumentTypeError(msg)
+    else:
+        return dirname
+
+def is_dir_or_file(dirname):
+    '''Checks if a path is an actual directory that exists or a file'''
+    if not os.path.isdir(dirname) and not os.path.isfile(dirname):
+        msg = "{0} is not a directory nor a file".format(dirname)
+        raise ArgumentTypeError(msg)
+    else:
+        return dirname
+
+def fullpath(relpath):
+    '''Relative path to absolute'''
+    if (type(relpath) is object or hasattr(relpath, 'read')): # relpath is either an object or file-like, try to get its name
+        relpath = relpath.name
+    return os.path.abspath(os.path.expanduser(relpath))
+
+def recwalk(inputpath, sorting=True):
+    '''Recursively walk through a folder. This provides a mean to flatten out the files restitution (necessary to show a progress bar). This is a generator.'''
+    # If it's only a single file, return this single file
+    if os.path.isfile(inputpath):
+        abs_path = fullpath(inputpath)
+        yield os.path.dirname(abs_path), os.path.basename(abs_path)
+    # Else if it's a folder, walk recursively and return every files
+    else:
+        for dirpath, dirs, files in walk(inputpath):	
+            if sorting:
+                files.sort()
+                dirs.sort() # sort directories in-place for ordered recursive walking
+            for filename in files:
+                yield (dirpath, filename) # return directory (full path) and filename
+
+def sizeof_fmt(num, suffix='B', mod=1024.0):
+    '''Readable size format, courtesy of Sridhar Ratnakumar'''
+    for unit in ['','K','M','G','T','P','E','Z']:
+        if abs(num) < mod:
+            return "%3.1f%s%s" % (num, unit, suffix)
+        num /= mod
+    return "%.1f%s%s" % (num, 'Y', suffix)
+
+def path2unix(path, nojoin=False, fromwinpath=False):
+    '''From a path given in any format, converts to posix path format
+    fromwinpath=True forces the input path to be recognized as a Windows path (useful on Unix machines to unit test Windows paths)'''
+    if fromwinpath:
+        pathparts = list(PureWindowsPath(path).parts)
+    else:
+        pathparts = list(PurePath(path).parts)
+    if nojoin:
+        return pathparts
+    else:
+        return posixpath.join(*pathparts)
+
+def get_next_entry(file, entrymarker="\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF", only_coord=True, blocksize=65535):
+    '''Find or read the next ecc entry in a given ecc file.
+    Call this function multiple times with the same file handle to get subsequent markers positions (this is not a generator but it works very similarly, because it will continue reading from the file's current cursor position -- this can be used advantageously if you want to read only a specific entry by seeking before supplying the file handle).
+    This will read any string length between two entrymarkers.
+    The reading is very tolerant, so it will always return any valid entry (but also scrambled entries if any, but the decoding will ensure everything's ok).
+    `file` is a file handle, not the path to the file.'''
+    # TODO: use mmap native module instead of manually reading using blocksize?
+
+    entrymarker = bytearray(b(entrymarker))
+    found = False
+    start = None # start and end vars are the relative position of the starting/ending entrymarkers in the current buffer
+    end = None
+    startcursor = None # startcursor and endcursor are the absolute position of the starting/ending entrymarkers inside the database file
+    endcursor = None
+    buf = 1
+    # Sanity check: cannot screen the file's content if the window is of the same size as the pattern to match (the marker)
+    if blocksize <= len(entrymarker): blocksize = len(entrymarker) + 1
+    # Continue the search as long as we did not find at least one starting marker and one ending marker (or end of file)
+    while (not found and buf):
+        # Read a long block at once, we will readjust the file cursor after
+        buf = bytearray(file.read(blocksize))
+        # Find the start marker (if not found already)
+        if start is None or start == -1:
+            start = buf.find(entrymarker); # relative position of the starting marker in the currently read string
+            if start >= 0 and not startcursor: # assign startcursor only if it's empty (meaning that we did not find the starting entrymarker, else if found we are only looking for 
+                startcursor = file.tell() - len(buf) + start # absolute position of the starting marker in the file
+            if start >= 0: start = start + len(entrymarker)
+        # If we have a starting marker, we try to find a subsequent marker which will be the ending of our entry (if the entry is corrupted we don't care: it won't pass the entry_to_dict() decoding or subsequent steps of decoding and we will just pass to the next ecc entry). This allows to process any valid entry, no matter if previous ones were scrambled.
+        if startcursor is not None and startcursor >= 0:
+            end = buf.find(entrymarker, start)
+            if end < 0 and len(buf) < blocksize: # Special case: we didn't find any ending marker but we reached the end of file, then we are probably in fact just reading the last entry (thus there's no ending marker for this entry)
+                end = len(buf) # It's ok, we have our entry, the ending marker is just the end of file
+            # If we found an ending marker (or if end of file is reached), then we compute the absolute cursor value and put the file reading cursor back in position, just before the next entry (where the ending marker is if any)
+            if end >= 0:
+                endcursor = file.tell() - len(buf) + end
+                # Make sure we are not redetecting the same marker as the start marker
+                if endcursor > startcursor:
+                    file.seek(endcursor)
+                    found = True
+                else:
+                    end = -1
+                    encursor = None
+        #print("Start:", start, startcursor)
+        #print("End: ", end, endcursor)
+        # Stop criterion to avoid infinite loop: in the case we could not find any entry in the rest of the file and we reached the EOF, we just quit now
+        if len(buf) < blocksize: break
+        # Did not find the full entry in one buffer? Reinit variables for next iteration, but keep in memory startcursor.
+        if start > 0: start = 0 # reset the start position for the end buf find at next iteration (ie: in the arithmetic operations to compute the absolute endcursor position, the start entrymarker won't be accounted because it was discovered in a previous buffer).
+        if not endcursor: file.seek(file.tell()-len(entrymarker)) # Try to fix edge case where blocksize stops the buffer exactly in the middle of the ending entrymarker. The starting marker should always be ok because it should be quite close (or generally immediately after) the previous entry, but the end depends on the end of the current entry (size of the original file), thus the buffer may miss the ending entrymarker. should offset file.seek(-len(entrymarker)) before searching for ending.
+
+    if found: # if an entry was found, we seek to the beginning of the entry and then either read the entry from file or just return the markers positions (aka the entry bounds)
+        file.seek(startcursor + len(entrymarker))
+        if only_coord:
+            # Return only coordinates of the start and end markers
+            # Note: it is useful to just return the reading positions and not the entry itself because it can get quite huge and may overflow memory, thus we will read each ecc blocks on request using a generator.
+            return [startcursor + len(entrymarker), endcursor]
+        else:
+            # Return the full entry's content
+            return file.read(endcursor - startcursor - len(entrymarker))
+    else:
+        # Nothing found (or no new entry to find, we've already found them all), so we return None
+        return None
+
+def create_dir_if_not_exist(path):  # pragma: no cover
+    """Create a directory if it does not already exist, else nothing is done and no error is return"""
+    if not os.path.exists(path):
+        os.makedirs(path)
+
+def remove_if_exist(path):  # pragma: no cover
+    """Delete a file or a directory recursively if it exists, else no exception is raised"""
+    if os.path.exists(path):
+        if os.path.isdir(path):
+            shutil.rmtree(path)
+            return True
+        elif os.path.isfile(path):
+            os.remove(path)
+            return True
+    return False
+
+def copy_any(src, dst, only_missing=False):  # pragma: no cover
+    """Copy a file or a directory tree, deleting the destination before processing"""
+    if not only_missing:
+        remove_if_exist(dst)
+    if os.path.exists(src):
+        if os.path.isdir(src):
+            if not only_missing:
+                shutil.copytree(src, dst, symlinks=False, ignore=None)
+            else:
+                for dirpath, filepath in recwalk(src):
+                    srcfile = os.path.join(dirpath, filepath)
+                    relpath = os.path.relpath(srcfile, src)
+                    dstfile = os.path.join(dst, relpath)
+                    if not os.path.exists(dstfile):
+                        create_dir_if_not_exist(os.path.dirname(dstfile))
+                        shutil.copyfile(srcfile, dstfile)
+                        shutil.copystat(srcfile, dstfile)
+            return True
+        elif os.path.isfile(src) and (not only_missing or not os.path.exists(dst)):
+            shutil.copyfile(src, dst)
+            shutil.copystat(src, dst)
+            return True
+    return False
+
+def simple_read(rel_path, root_path=None):
+    # Helper function to ease reading a single-sourced __version__ attribute from a mypackage.__init__ file
+    # It accesses the specified relative path, relative to the calling script (so that it workes relative to the current package if installed in site-scripts)
+    # From: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/#single-sourcing-the-package-version
+    # See also: https://stackoverflow.com/a/75962009/1121352
+
+    #if root_path is not None:
+    here = root_path
+    #else:  # this branch was commented out to remove dead code that can cause the function to return strange unexpected results in edge cases, we prefer to fail explicitly in these cases
+    #    try:
+    #        here = os.path.abspath(os.path.dirname(__file__))
+    #    except UnboundLocalError as exc:
+    #        __file__ = os.path.join(os.getcwd(), 'dummyfile.ext')  # get current working directory if using this function under an interactive prompt
+    #        here = os.path.abspath(os.path.dirname(__file__))
+    with codecs.open(os.path.join(here, rel_path), 'r') as fp:
+        return fp.read()
+
+def get_version(rel_path, root_path=None):
+    # Helper function to ease reading a single-sourced __version__ attribute from a mypackage.__init__ file
+    # Read instead of importing, so that there is no risk of infinite imports loop
+    # From: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/#single-sourcing-the-package-version
+    # See also: https://stackoverflow.com/a/75962009/1121352
+    for line in simple_read(rel_path, root_path).splitlines():
+        if line.startswith('__version__'):
+            delim = '"' if '"' in line else "'"
+            return line.split(delim)[1]
+    else:
+        raise RuntimeError("Unable to find version string.")
+
+#### MULTIFILES AUX FUNCTIONS ####
+# Here are the aux functions to cluster files of similar sizes in order to generate multifiles ecc tracks in header_ecc.py and structural_variable_ecc.py
+# TODO in hecc and saecc:
+#  * make intra-fields filepath with multiple files paths, separated by "|" (eg: "filepath1|filepath2|filepath3"
+#  * make intra-fields multiple file sizes, just like filepaths: "filesize1|filesize2|filesize3"
+#  * intra-ecc should work just the same
+#  * adapt stream encoding and decoding to dispatch the bytes to the corresponding files.
+#  * stream encoding/decoding should be robust: when a file ecc track is either ended (file was smaller than the others) or corrupted, then fill with null bytes, the rest should work the same (at decoding, maybe can use erasures decoding to double the recovery rate, at encoding maybe try to not store the null bytes? But how? Normally, the files in a group should be in descending size order, so the first file will always have a track, but if the other files are smaller then nvm their ecc just won't be appended, thus we end up with a shorter ecc track, but we know that and we can just computationally append the missing null bytes, no harm and no risk of corruption since these bytes aren't stored! So at encoding this doubles the recovery rate towards the end of the file for the bigger file)..
+# TODO in group_files_by_size: try to use sortedcontainers.SortedList() ? https://pypi.python.org/pypi/sortedcontainers
+
+from collections import OrderedDict
+from sortedcontainers import SortedList
+from random import randint
+try:
+    from itertools import izip_longest
+except ImportError:
+    from itertools import zip_longest as izip_longest
+
+def grouper(n, iterable, fillvalue=None):
+    "grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx"
+    args = [iter(iterable)] * n
+    return izip_longest(fillvalue=fillvalue, *args)
+
+def group_files_by_size(fileslist, multi):  # pragma: no cover
+    ''' Cluster files into the specified number of groups, where each groups total size is as close as possible to each other.
+
+    Pseudo-code (O(n^g) time complexity):
+    Input: number of groups G per cluster, list of files F with respective sizes
+    - Order F by descending size
+    - Until F is empty:
+        - Create a cluster X
+        - A = Pop first item in F
+        - Put A in X[0] (X[0] is thus the first group in cluster X)
+        For g in 1..len(G)-1 :
+            - B = Pop first item in F
+            - Put B in X[g]
+            - group_size := size(B)
+            If group_size != size(A):
+                While group_size < size(A):
+                    - Find next item C in F which size(C) <= size(A) - group_size
+                    - Put C in X[g]
+                    - group_size := group_size + size(C)
+    '''
+    flord = OrderedDict(sorted(fileslist.items(), key=lambda x: x[1], reverse=True))
+    if multi <= 1:
+        fgrouped = {}
+        i = 0
+        for x in flord.keys():
+            i += 1
+            fgrouped[i] = [[x]]
+        return fgrouped
+
+    fgrouped = {}
+    i = 0
+    while flord:
+        i += 1
+        fgrouped[i] = []
+        big_key, big_value = flord.popitem(0)
+        fgrouped[i].append([big_key])
+        for j in xrange(multi-1):
+            cluster = []
+            if not flord: break
+            child_key, child_value = flord.popitem(0)
+            cluster.append(child_key)
+            if child_value == big_value:
+                fgrouped[i].append(cluster)
+                continue
+            else:
+                diff = big_value - child_value
+                for key, value in flord.iteritems():
+                    if value <= diff:
+                        cluster.append(key)
+                        del flord[key]
+                        if value == diff:
+                            break
+                        else:
+                            child_value += value
+                            diff = big_value - child_value
+                fgrouped[i].append(cluster)
+    return fgrouped
+
+def group_files_by_size_fast(fileslist, nbgroups, mode=1):  # pragma: no cover
+    '''Given a files list with sizes, output a list where the files are grouped in nbgroups per cluster.
+
+    Pseudo-code for algorithm in O(n log(g)) (thank's to insertion sort or binary search trees)
+    See for more infos: http://cs.stackexchange.com/questions/44406/fast-algorithm-for-clustering-groups-of-elements-given-their-size-time/44614#44614
+    For each file:
+        - If to-fill list is empty or file.size > first-key(to-fill):
+          * Create cluster c with file in first group g1
+          * Add to-fill[file.size].append([c, g2], [c, g3], ..., [c, gn])
+        - Else:
+          * ksize = first-key(to-fill)
+          * c, g = to-fill[ksize].popitem(0)
+          * Add file to cluster c in group g
+          * nsize = ksize - file.size
+          * if nsize > 0:
+            . to-fill[nsize].append([c, g])
+            . sort to-fill if not an automatic ordering structure
+        '''
+    ftofill = SortedList()
+    ftofill_pointer = {}
+    fgrouped = [] # [] or {}
+    ford = sorted(fileslist.iteritems(), key=lambda x: x[1])
+    last_cid = -1
+    while ford:
+        fname, fsize = ford.pop()
+        #print "----\n"+fname, fsize
+        #if ftofill: print "beforebranch", fsize, ftofill[-1]
+        #print ftofill
+        if not ftofill or fsize > ftofill[-1]:
+            last_cid += 1
+            #print "Branch A: create cluster %i" % last_cid
+            fgrouped.append([])
+            #fgrouped[last_cid] = []
+            fgrouped[last_cid].append([fname])
+            if mode==0:
+                for g in xrange(nbgroups-1, 0, -1):
+                    fgrouped[last_cid].append([])
+                    if not fsize in ftofill_pointer:
+                        ftofill_pointer[fsize] = []
+                    ftofill_pointer[fsize].append((last_cid, g))
+                    ftofill.add(fsize)
+            else:
+                for g in xrange(1, nbgroups):
+                    try:
+                        fgname, fgsize = ford.pop()
+                        #print "Added to group %i: %s %i" % (g, fgname, fgsize)
+                    except IndexError:
+                        break
+                    fgrouped[last_cid].append([fgname])
+                    diff_size = fsize - fgsize
+                    if diff_size > 0:
+                        if not diff_size in ftofill_pointer:
+                            ftofill_pointer[diff_size] = []
+                        ftofill_pointer[diff_size].append((last_cid, g))
+                        ftofill.add(diff_size)
+        else:
+            #print "Branch B"
+            ksize = ftofill.pop()
+            c, g = ftofill_pointer[ksize].pop()
+            #print "Assign to cluster %i group %i" % (c, g)
+            fgrouped[c][g].append(fname)
+            nsize = ksize - fsize
+            if nsize > 0:
+                if not nsize in ftofill_pointer:
+                    ftofill_pointer[nsize] = []
+                ftofill_pointer[nsize].append((c, g))
+                ftofill.add(nsize)
+    return fgrouped
+
+def group_files_by_size_simple(fileslist, nbgroups):  # pragma: no cover
+    """ Simple and fast files grouping strategy: just order by size, and group files n-by-n, so that files with the closest sizes are grouped together.
+    In this strategy, there is only one file per subgroup, and thus there will often be remaining space left because there is no filling strategy here, but it's very fast. """
+    ford = sorted(fileslist.iteritems(), key=lambda x: x[1], reverse=True)
+    ford = [[x[0]] for x in ford]
+    return [group for group in grouper(nbgroups, ford)]
+
+def grouped_count_sizes(fileslist, fgrouped):  # pragma: no cover
+    '''Compute the total size per group and total number of files. Useful to check that everything is OK.'''
+    fsizes = {}
+    total_files = 0
+    allitems = None
+    if isinstance(fgrouped, dict):
+        allitems = fgrouped.iteritems()
+    elif isinstance(fgrouped, list):
+        allitems = enumerate(fgrouped)
+    for fkey, cluster in allitems:
+        fsizes[fkey] = []
+        for subcluster in cluster:
+            tot = 0
+            if subcluster is not None:
+                for fname in subcluster:
+                    tot += fileslist[fname]
+                    total_files += 1
+            fsizes[fkey].append(tot)
+    return fsizes, total_files
+
+def gen_rand_fileslist(nbfiles=100, maxvalue=100):  # pragma: no cover
+    fileslist = {}
+    for i in xrange(nbfiles):
+        fileslist["file_%i" % i] = randint(1, maxvalue)
+    return fileslist
+
+def gen_rand_fileslist2(nbfiles=100, maxvalue=100):  # pragma: no cover
+    fileslist = []
+    for i in xrange(nbfiles):
+        fileslist.append( ("file_%i" % i, randint(1, maxvalue)) )
+    return fileslist
+
+def grouped_test(nbfiles=100, nbgroups=3):  # pragma: no cover
+    fileslist = gen_rand_fileslist(nbfiles)
+    fgrouped = group_files_by_size(fileslist, nbgroups)
+    fsizes, total_files = grouped_count_sizes(fileslist, fgrouped)
+    return [fgrouped, fsizes, total_files]
+
+def grouped_fast_test(nbfiles=100, nbgroups=3, mode=1):  # pragma: no cover
+    nbfiles = 100
+    nbgroups = 3
+    fileslist = gen_rand_fileslist(nbfiles)
+    fgrouped = group_files_by_size_fast(fileslist, nbgroups, mode=mode)
+    fsizes, total_files = grouped_count_sizes(fileslist, fgrouped)
+    return [fgrouped, fsizes, total_files]
+
+def grouped_simple_test(nbfiles=100, nbgroups=3):  # pragma: no cover
+    fileslist = gen_rand_fileslist(nbfiles)
+    fgrouped = group_files_by_size_simple(fileslist, nbgroups)
+    fsizes, total_files = grouped_count_sizes(fileslist, fgrouped)
+    return [fgrouped, fsizes, total_files]
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/lib/eccman.py` & `pyFileFixity-3.1.4/pyFileFixity/lib/eccman.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,269 +1,269 @@
-#!/usr/bin/env python
-#
-# ECC manager facade api
-# Allows to easily use different kinds of ECC algorithms and libraries under one single class.
-# Copyright (C) 2015-2023 Stephen Karl Larroque
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-
-# Compatibility with Python 3
-from ._compat import _str, _range, b, _bytes
-
-from distance import hamming
-
-# ECC libraries
-try: # Try to automatically load speed-optimized Cython implementations if compiled
-    # If Python 3, we can't just import __pypy__ to check if there is an ImportError, because it raises a ModuleNotFoundError on Travis CI that is never caught, dunno why
-    # So we test manually without raising any exception
-    import platform, os
-    inpypy = platform.python_implementation().lower().startswith("pypy")
-    if inpypy:  # we are under PyPy, don't use the Cython extensions, it won't play well
-        raise ImportError()
-
-    from unireedsolomon import rs as brownanrs
-    #from .brownanrs import crs as brownanrs  # cythonized extension performances are worse than pure python under pypy, so it's removed. But with Cython v3, which supports cythonized methods, it may now be a more worthy endeavor!
-    import creedsolo as reedsolo
-except ImportError:
-    from unireedsolomon import rs as brownanrs # Pure python implementation of Reed-Solomon with configurable max_block_size and automatic error detection (you don't have to specify where they are). This is a base 3 implementation that is formally correct and with unit tests.
-    import reedsolo # Faster pure python implementation of Reed-Solomon, with a base 3 compatible encoder (but not yet decoder! But you can use brownanrs to decode).
-
-rs_encode_msg = reedsolo.rs_encode_msg # local reference for small speed boost
-#rs_encode_msg_precomp = reedsolo.rs_encode_msg_precomp
-
-
-### Auxiliary ECC functions ###
-
-
-def compute_ecc_params(max_block_size, rate, hasher):
-    '''Compute the ecc parameters (size of the message, size of the hash, size of the ecc). This is an helper function to easily compute the parameters from a resilience rate to instanciate an ECCMan object.'''
-    #message_size = max_block_size - int(round(max_block_size * rate * 2, 0)) # old way to compute, wasn't really correct because we applied the rate on the total message+ecc size, when we should apply the rate to the message size only (that is not known beforehand, but we want the ecc size (k) = 2*rate*message_size or in other words that k + k * 2 * rate = n)
-    message_size = int(round(float(max_block_size) / (1 + 2*rate), 0))
-    ecc_size = max_block_size - message_size
-    hash_size = len(hasher) # 32 when we use MD5
-    return {"message_size": message_size, "ecc_size": ecc_size, "hash_size": hash_size}
-
-def detect_reedsolomon_parameters(message, mesecc_orig, gen_list=[2, 3, 5], c_exp=8):
-    '''Use an exhaustive search to automatically find the correct parameters for the ReedSolomon codec from a sample message and its encoded RS code.
-    Arguments: message is the sample message, eg, "hello world" ; mesecc_orig is the message variable encoded with RS block appended at the end.
-    '''
-    # Description: this is basically an exhaustive search where we will try every possible RS parameter, then try to encode the sample message, and see if the resulting RS code is close to the supplied code.
-    # All variables except the Galois Field's exponent are automatically generated and searched.
-    # To compare with the supplied RS code, we compute the Hamming distance, so that even if the RS code is tampered, we can still find the closest set of RS parameters to decode this message.
-    # The goal is to provide users a function so that they can use the "hello world" sample string in generated ECC files to recover their RS parameters in case they forget them. But users can use any sample message: for example, if they have an untampered file and its relative ecc track, they can use the ecc track as the mesecc_orig and their original file as the sample message.
-
-    import reedsolo as reedsolop # need to import another time the reedsolo library for detect_reedsolomon_parameters to work (because we need to reinit all the tables, and they are declared module-wide, so this would conflict with decoding)
-
-    # Init the variables
-    n = len(mesecc_orig)
-    k = len(message)
-    field_charac = int((2**c_exp) - 1)
-    maxval1 = max([ord(x) if isinstance(x, _str) else x for x in message ])
-    maxval2 = max([ord(x) if isinstance(x, _str) else x for x in mesecc_orig])
-    maxval = max([maxval1, maxval2])
-    if (maxval > field_charac):
-        raise ValueError("The specified field's exponent is wrong, the message contains values (%i) above the field's cardinality (%i)!" % (maxval, field_charac))
-
-    # Prepare the variable that will store the result
-    best_match = {"hscore": -1, "params": [{"gen_nb": 0, "prim": 0, "fcr": 0}]}
-
-    if isinstance(message, _str):
-        message = b(message)
-
-    # Exhaustively search by generating every combination of values for the RS parameters and test the Hamming distance
-    for gen_nb in gen_list:
-        prim_list = reedsolop.find_prime_polys(generator=gen_nb, c_exp=c_exp, fast_primes=False, single=False)
-        for prim in prim_list:
-            reedsolop.init_tables(prim)
-            for fcr in _range(field_charac):
-                #g = reedsolop.rs_generator_poly_all(n, fcr=fcr, generator=gen_nb)
-                # Generate a RS code from the sample message using the current combination of RS parameters
-                mesecc = reedsolop.rs_encode_msg(message, n-k, fcr=fcr)
-                # Compute the Hamming distance
-                h = hamming(mesecc, mesecc_orig)
-                # If the Hamming distance is lower than the previous best match (or if it's the first try), save this set of parameters
-                if best_match["hscore"] == -1 or h <= best_match["hscore"]:
-                    # If the distance is strictly lower than for the previous match, then we replace the previous match with the current one
-                    if best_match["hscore"] == -1 or h < best_match["hscore"]:
-                        best_match["hscore"] = h
-                        best_match["params"] = [{"gen_nb": gen_nb, "prim": prim, "fcr": fcr}]
-                    # Else there is an ambiguity: the Hamming distance is the same as for the previous best match, so we keep the previous set of parameters but we append the current set
-                    elif h == best_match["hscore"]:
-                        best_match["params"].append({"gen_nb": gen_nb, "prim": prim, "fcr": fcr})
-                    # If Hamming distance is 0, then we have found a perfect match (the current set of parameters allow to generate the exact same RS code from the sample message), so we stop here
-                    if h == 0: break
-
-    # Printing the results to the user
-    if best_match["hscore"] >= 0 and best_match["hscore"] < len(mesecc_orig):
-        perfect_match_str = " (0=perfect match)" if best_match["hscore"]==0 else ""
-        result = ''
-        result += "Found closest set of parameters, with Hamming distance %i%s:\n" % (best_match["hscore"], perfect_match_str)
-        for param in best_match["params"]:
-            result += "gen_nb=%s prim=%s(%s) fcr=%s\n" % (param["gen_nb"], param["prim"], hex(param["prim"]), param["fcr"])
-        return result
-    else:
-        return "Parameters could not be automatically detected..."
-
-
-### Main ECCMan Class to manage ECC codecs ###
-
-
-class ECCMan(object):
-    '''Error correction code manager, which provides a facade API to use different kinds of ecc algorithms or libraries/codecs.'''
-
-    def __init__(self, n, k, algo=1):
-        self.c_exp = 8 # we stay in GF(2^8) for this software
-        self.field_charac = int((2**self.c_exp) - 1)
-
-        if algo == 1 or algo == 2: # brownanrs library implementations: fully correct base 3 implementation, and mode 2 is for fast encoding
-            self.gen_nb = 3
-            self.prim = 0x11b
-            self.fcr = 1
-
-            self.ecc_manager = brownanrs.RSCoder(n, k, generator=self.gen_nb, prim=self.prim, fcr=self.fcr)
-        elif algo == 3: # reedsolo fast implementation, compatible with brownanrs in base 3
-            self.gen_nb = 3
-            self.prim = 0x11b
-            self.fcr = 1
-
-            reedsolo.init_tables(generator=self.gen_nb, prim=self.prim)
-            self.g = reedsolo.rs_generator_poly_all(n, fcr=self.fcr, generator=self.gen_nb)
-            #self.gf_mul_arr, self.gf_add_arr = reedsolo.gf_precomp_tables()
-        elif algo == 4: # reedsolo fast implementation, incompatible with any other implementation
-            self.gen_nb = 2
-            self.prim = 0x187
-            self.fcr = 120
-
-            reedsolo.init_tables(self.prim) # parameters for US FAA ADSB UAT RS FEC
-            self.g = reedsolo.rs_generator_poly_all(n, fcr=self.fcr, generator=self.gen_nb)
-        else:
-            raise Exception("Specified algorithm %i is not supported!" % algo)
-
-        self.algo = algo
-        self.n = n
-        self.k = k
-
-    def encode(self, message, k=None):
-        '''Encode one message block (up to 255) into an ecc'''
-        if not k: k = self.k
-        if self.algo == 1:
-            message, _ = self.pad(b(message), k=k)
-            mesecc = self.ecc_manager.encode(message, k=k)
-        elif self.algo == 2:
-            message, _ = self.pad(b(message), k=k)
-            mesecc = self.ecc_manager.encode_fast(message, k=k)
-        elif self.algo == 3 or self.algo == 4:
-            message, _ = self.pad(bytearray(b(message)), k=k)  # TODO: need to use bytearray to be fully compatible with cythonized extension (the fastest!)
-            mesecc = rs_encode_msg(message, self.n-k, fcr=self.fcr, gen=self.g[self.n-k])
-            #mesecc = rs_encode_msg_precomp(message, self.n-k, fcr=self.fcr, gen=self.g[self.n-k])
-
-        ecc = mesecc[len(message):]
-        return _bytes(ecc)
-
-    def decode(self, message, ecc, k=None, enable_erasures=False, erasures_char="\x00", only_erasures=False):
-        '''Repair a message and its ecc also, given the message and its ecc (both can be corrupted, we will still try to fix both of them)'''
-        if not k: k = self.k
-
-        # Optimization, use bytearray
-        if isinstance(message, _str):
-            message = bytearray([ord(x) for x in message])
-        if isinstance(ecc, _str):
-            ecc = bytearray([ord(x) for x in ecc])
-
-        # Detect erasures positions and replace with null bytes (replacing erasures with null bytes is necessary for correct syndrome computation)
-        # Note that this must be done before padding, else we risk counting the padded null bytes as erasures!
-        erasures_pos = None
-        if enable_erasures:
-            # Concatenate to find erasures in the whole codeword
-            mesecc = message + ecc
-            # Convert char to a int (because we use a bytearray)
-            if isinstance(erasures_char, _str): erasures_char = ord(erasures_char)
-            # Find the positions of the erased characters
-            erasures_pos = bytearray([i for i in _range(len(mesecc)) if mesecc[i] == erasures_char])
-            # Failing case: no erasures could be found and we want to only correct erasures, then we return the message as-is
-            if only_erasures and not erasures_pos: return message, ecc
-
-        # Pad with null bytes if necessary
-        message, pad = self.pad(message, k=k)
-        ecc, _ = self.rpad(ecc, k=k) # fill ecc with null bytes if too small (maybe the field delimiters were misdetected and this truncated the ecc? But we maybe still can correct if the truncation is less than the resilience rate)
-
-        # If the message was left padded, then we need to update the positions of the erasures
-        if erasures_pos and pad:
-            len_pad = len(pad)
-            erasures_pos = bytearray([x+len_pad for x in erasures_pos])
-
-        # Decoding
-        if self.algo == 1:
-            msg_repaired, ecc_repaired = self.ecc_manager.decode(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures) # Avoid automatic stripping because we are working with binary streams, thus we should manually strip padding only when we know we padded
-        elif self.algo == 2:
-            msg_repaired, ecc_repaired = self.ecc_manager.decode_fast(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures)
-        elif self.algo == 3:
-            #msg_repaired, ecc_repaired = self.ecc_manager.decode_fast(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures)
-            msg_repaired, ecc_repaired, _ = reedsolo.rs_correct_msg_nofsynd(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb, erase_pos=erasures_pos, only_erasures=only_erasures)
-            msg_repaired = bytearray(msg_repaired)
-            ecc_repaired = bytearray(ecc_repaired)
-        elif self.algo == 4:
-            msg_repaired, ecc_repaired, _ = reedsolo.rs_correct_msg(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb, erase_pos=erasures_pos, only_erasures=only_erasures)
-            msg_repaired = bytearray(msg_repaired)
-            ecc_repaired = bytearray(ecc_repaired)
-
-        if pad: # Strip the null bytes if we padded the message before decoding
-            msg_repaired = msg_repaired[len(pad):len(msg_repaired)]
-        return _bytes(msg_repaired), _bytes(ecc_repaired)
-
-    def pad(self, message, k=None):
-        '''Automatically left pad with null bytes a message if too small, or leave unchanged if not necessary. This allows to keep track of padding and strip the null bytes after decoding reliably with binary data. Equivalent to shortening (shortened reed-solomon code).'''
-        if not k: k = self.k
-        pad = None
-        if len(message) < k:
-            #pad = "\x00" * (k-len(message))
-            pad = bytearray(k-len(message))
-            message = pad + bytearray(b(message))
-        return [message, pad]
-
-    def rpad(self, ecc, k=None):
-        '''Automatically right pad with null bytes an ecc to fill for missing bytes if too small, or leave unchanged if not necessary. This can be used as a workaround for field delimiter misdetection. Equivalent to puncturing (punctured reed-solomon code).'''
-        if not k: k = self.k
-        pad = None
-        if len(ecc) < self.n-k:
-            print("Warning: the ecc field may have been truncated (entrymarker or field_delim misdetection?).")
-            #pad = "\x00" * (self.n-k-len(ecc))
-            pad = bytearray(self.n-k-len(ecc))
-            ecc = bytearray(ecc) + pad
-        return [ecc, pad]
-
-    def check(self, message, ecc, k=None):
-        '''Check if there's any error in a message+ecc. Can be used before decoding, in addition to hashes to detect if the message was tampered, or after decoding to check that the message was fully recovered.'''
-        if not k: k = self.k
-        message, _ = self.pad(message, k=k)
-        ecc, _ = self.rpad(ecc, k=k)
-        if self.algo == 1 or self.algo == 2:
-            return self.ecc_manager.check_fast(message + ecc, k=k)
-        elif self.algo == 3 or self.algo == 4:
-            return reedsolo.rs_check(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb)
-
-    def description(self):
-        '''Provide a description for each algorithm available, useful to print in ecc file'''
-        if 0 < self.algo <= 3:
-            return "Reed-Solomon with polynomials in Galois field of characteristic %i (2^%i) with generator=%s, prime poly=%s and first consecutive root=%s." % (self.field_charac, self.c_exp, self.gen_nb, hex(self.prim), self.fcr)
-        elif self.algo == 4:
-            return "Reed-Solomon with polynomials in Galois field of characteristic %i (2^%i) under US FAA ADSB UAT RS FEC standard with generator=%s, prime poly=%s and first consecutive root=%s." % (self.field_charac, self.c_exp, self.gen_nb, hex(self.prim), self.fcr)
-        else:
-            return "No description for this ECC algorithm."
+#!/usr/bin/env python
+#
+# ECC manager facade api
+# Allows to easily use different kinds of ECC algorithms and libraries under one single class.
+# Copyright (C) 2015-2023 Stephen Karl Larroque
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+
+# Compatibility with Python 3
+from ._compat import _str, _range, b, _bytes
+
+from distance import hamming
+
+# ECC libraries
+try: # Try to automatically load speed-optimized Cython implementations if compiled
+    # If Python 3, we can't just import __pypy__ to check if there is an ImportError, because it raises a ModuleNotFoundError on Travis CI that is never caught, dunno why
+    # So we test manually without raising any exception
+    import platform, os
+    inpypy = platform.python_implementation().lower().startswith("pypy")
+    if inpypy:  # we are under PyPy, don't use the Cython extensions, it won't play well
+        raise ImportError()
+
+    from unireedsolomon import rs as brownanrs
+    #from .brownanrs import crs as brownanrs  # cythonized extension performances are worse than pure python under pypy, so it's removed. But with Cython v3, which supports cythonized methods, it may now be a more worthy endeavor!
+    import creedsolo as reedsolo
+except ImportError:
+    from unireedsolomon import rs as brownanrs # Pure python implementation of Reed-Solomon with configurable max_block_size and automatic error detection (you don't have to specify where they are). This is a base 3 implementation that is formally correct and with unit tests.
+    import reedsolo # Faster pure python implementation of Reed-Solomon, with a base 3 compatible encoder (but not yet decoder! But you can use brownanrs to decode).
+
+rs_encode_msg = reedsolo.rs_encode_msg # local reference for small speed boost
+#rs_encode_msg_precomp = reedsolo.rs_encode_msg_precomp
+
+
+### Auxiliary ECC functions ###
+
+
+def compute_ecc_params(max_block_size, rate, hasher):
+    '''Compute the ecc parameters (size of the message, size of the hash, size of the ecc). This is an helper function to easily compute the parameters from a resilience rate to instanciate an ECCMan object.'''
+    #message_size = max_block_size - int(round(max_block_size * rate * 2, 0)) # old way to compute, wasn't really correct because we applied the rate on the total message+ecc size, when we should apply the rate to the message size only (that is not known beforehand, but we want the ecc size (k) = 2*rate*message_size or in other words that k + k * 2 * rate = n)
+    message_size = int(round(float(max_block_size) / (1 + 2*rate), 0))
+    ecc_size = max_block_size - message_size
+    hash_size = len(hasher) # 32 when we use MD5
+    return {"message_size": message_size, "ecc_size": ecc_size, "hash_size": hash_size}
+
+def detect_reedsolomon_parameters(message, mesecc_orig, gen_list=[2, 3, 5], c_exp=8):
+    '''Use an exhaustive search to automatically find the correct parameters for the ReedSolomon codec from a sample message and its encoded RS code.
+    Arguments: message is the sample message, eg, "hello world" ; mesecc_orig is the message variable encoded with RS block appended at the end.
+    '''
+    # Description: this is basically an exhaustive search where we will try every possible RS parameter, then try to encode the sample message, and see if the resulting RS code is close to the supplied code.
+    # All variables except the Galois Field's exponent are automatically generated and searched.
+    # To compare with the supplied RS code, we compute the Hamming distance, so that even if the RS code is tampered, we can still find the closest set of RS parameters to decode this message.
+    # The goal is to provide users a function so that they can use the "hello world" sample string in generated ECC files to recover their RS parameters in case they forget them. But users can use any sample message: for example, if they have an untampered file and its relative ecc track, they can use the ecc track as the mesecc_orig and their original file as the sample message.
+
+    import reedsolo as reedsolop # need to import another time the reedsolo library for detect_reedsolomon_parameters to work (because we need to reinit all the tables, and they are declared module-wide, so this would conflict with decoding)
+
+    # Init the variables
+    n = len(mesecc_orig)
+    k = len(message)
+    field_charac = int((2**c_exp) - 1)
+    maxval1 = max([ord(x) if isinstance(x, _str) else x for x in message ])
+    maxval2 = max([ord(x) if isinstance(x, _str) else x for x in mesecc_orig])
+    maxval = max([maxval1, maxval2])
+    if (maxval > field_charac):
+        raise ValueError("The specified field's exponent is wrong, the message contains values (%i) above the field's cardinality (%i)!" % (maxval, field_charac))
+
+    # Prepare the variable that will store the result
+    best_match = {"hscore": -1, "params": [{"gen_nb": 0, "prim": 0, "fcr": 0}]}
+
+    if isinstance(message, _str):
+        message = b(message)
+
+    # Exhaustively search by generating every combination of values for the RS parameters and test the Hamming distance
+    for gen_nb in gen_list:
+        prim_list = reedsolop.find_prime_polys(generator=gen_nb, c_exp=c_exp, fast_primes=False, single=False)
+        for prim in prim_list:
+            reedsolop.init_tables(prim)
+            for fcr in _range(field_charac):
+                #g = reedsolop.rs_generator_poly_all(n, fcr=fcr, generator=gen_nb)
+                # Generate a RS code from the sample message using the current combination of RS parameters
+                mesecc = reedsolop.rs_encode_msg(message, n-k, fcr=fcr)
+                # Compute the Hamming distance
+                h = hamming(mesecc, mesecc_orig)
+                # If the Hamming distance is lower than the previous best match (or if it's the first try), save this set of parameters
+                if best_match["hscore"] == -1 or h <= best_match["hscore"]:
+                    # If the distance is strictly lower than for the previous match, then we replace the previous match with the current one
+                    if best_match["hscore"] == -1 or h < best_match["hscore"]:
+                        best_match["hscore"] = h
+                        best_match["params"] = [{"gen_nb": gen_nb, "prim": prim, "fcr": fcr}]
+                    # Else there is an ambiguity: the Hamming distance is the same as for the previous best match, so we keep the previous set of parameters but we append the current set
+                    elif h == best_match["hscore"]:
+                        best_match["params"].append({"gen_nb": gen_nb, "prim": prim, "fcr": fcr})
+                    # If Hamming distance is 0, then we have found a perfect match (the current set of parameters allow to generate the exact same RS code from the sample message), so we stop here
+                    if h == 0: break
+
+    # Printing the results to the user
+    if best_match["hscore"] >= 0 and best_match["hscore"] < len(mesecc_orig):
+        perfect_match_str = " (0=perfect match)" if best_match["hscore"]==0 else ""
+        result = ''
+        result += "Found closest set of parameters, with Hamming distance %i%s:\n" % (best_match["hscore"], perfect_match_str)
+        for param in best_match["params"]:
+            result += "gen_nb=%s prim=%s(%s) fcr=%s\n" % (param["gen_nb"], param["prim"], hex(param["prim"]), param["fcr"])
+        return result
+    else:
+        return "Parameters could not be automatically detected..."
+
+
+### Main ECCMan Class to manage ECC codecs ###
+
+
+class ECCMan(object):
+    '''Error correction code manager, which provides a facade API to use different kinds of ecc algorithms or libraries/codecs.'''
+
+    def __init__(self, n, k, algo=1):
+        self.c_exp = 8 # we stay in GF(2^8) for this software
+        self.field_charac = int((2**self.c_exp) - 1)
+
+        if algo == 1 or algo == 2: # brownanrs library implementations: fully correct base 3 implementation, and mode 2 is for fast encoding
+            self.gen_nb = 3
+            self.prim = 0x11b
+            self.fcr = 1
+
+            self.ecc_manager = brownanrs.RSCoder(n, k, generator=self.gen_nb, prim=self.prim, fcr=self.fcr)
+        elif algo == 3: # reedsolo fast implementation, compatible with brownanrs in base 3
+            self.gen_nb = 3
+            self.prim = 0x11b
+            self.fcr = 1
+
+            reedsolo.init_tables(generator=self.gen_nb, prim=self.prim)
+            self.g = reedsolo.rs_generator_poly_all(n, fcr=self.fcr, generator=self.gen_nb)
+            #self.gf_mul_arr, self.gf_add_arr = reedsolo.gf_precomp_tables()
+        elif algo == 4: # reedsolo fast implementation, incompatible with any other implementation
+            self.gen_nb = 2
+            self.prim = 0x187
+            self.fcr = 120
+
+            reedsolo.init_tables(self.prim) # parameters for US FAA ADSB UAT RS FEC
+            self.g = reedsolo.rs_generator_poly_all(n, fcr=self.fcr, generator=self.gen_nb)
+        else:
+            raise Exception("Specified algorithm %i is not supported!" % algo)
+
+        self.algo = algo
+        self.n = n
+        self.k = k
+
+    def encode(self, message, k=None):
+        '''Encode one message block (up to 255) into an ecc'''
+        if not k: k = self.k
+        if self.algo == 1:
+            message, _ = self.pad(b(message), k=k)
+            mesecc = self.ecc_manager.encode(message, k=k)
+        elif self.algo == 2:
+            message, _ = self.pad(b(message), k=k)
+            mesecc = self.ecc_manager.encode_fast(message, k=k)
+        elif self.algo == 3 or self.algo == 4:
+            message, _ = self.pad(bytearray(b(message)), k=k)  # TODO: need to use bytearray to be fully compatible with cythonized extension (the fastest!)
+            mesecc = rs_encode_msg(message, self.n-k, fcr=self.fcr, gen=self.g[self.n-k])
+            #mesecc = rs_encode_msg_precomp(message, self.n-k, fcr=self.fcr, gen=self.g[self.n-k])
+
+        ecc = mesecc[len(message):]
+        return _bytes(ecc)
+
+    def decode(self, message, ecc, k=None, enable_erasures=False, erasures_char="\x00", only_erasures=False):
+        '''Repair a message and its ecc also, given the message and its ecc (both can be corrupted, we will still try to fix both of them)'''
+        if not k: k = self.k
+
+        # Optimization, use bytearray
+        if isinstance(message, _str):
+            message = bytearray([ord(x) for x in message])
+        if isinstance(ecc, _str):
+            ecc = bytearray([ord(x) for x in ecc])
+
+        # Detect erasures positions and replace with null bytes (replacing erasures with null bytes is necessary for correct syndrome computation)
+        # Note that this must be done before padding, else we risk counting the padded null bytes as erasures!
+        erasures_pos = None
+        if enable_erasures:
+            # Concatenate to find erasures in the whole codeword
+            mesecc = message + ecc
+            # Convert char to a int (because we use a bytearray)
+            if isinstance(erasures_char, _str): erasures_char = ord(erasures_char)
+            # Find the positions of the erased characters
+            erasures_pos = bytearray([i for i in _range(len(mesecc)) if mesecc[i] == erasures_char])
+            # Failing case: no erasures could be found and we want to only correct erasures, then we return the message as-is
+            if only_erasures and not erasures_pos: return message, ecc
+
+        # Pad with null bytes if necessary
+        message, pad = self.pad(message, k=k)
+        ecc, _ = self.rpad(ecc, k=k) # fill ecc with null bytes if too small (maybe the field delimiters were misdetected and this truncated the ecc? But we maybe still can correct if the truncation is less than the resilience rate)
+
+        # If the message was left padded, then we need to update the positions of the erasures
+        if erasures_pos and pad:
+            len_pad = len(pad)
+            erasures_pos = bytearray([x+len_pad for x in erasures_pos])
+
+        # Decoding
+        if self.algo == 1:
+            msg_repaired, ecc_repaired = self.ecc_manager.decode(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures) # Avoid automatic stripping because we are working with binary streams, thus we should manually strip padding only when we know we padded
+        elif self.algo == 2:
+            msg_repaired, ecc_repaired = self.ecc_manager.decode_fast(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures)
+        elif self.algo == 3:
+            #msg_repaired, ecc_repaired = self.ecc_manager.decode_fast(message + ecc, nostrip=True, k=k, erasures_pos=erasures_pos, only_erasures=only_erasures)
+            msg_repaired, ecc_repaired, _ = reedsolo.rs_correct_msg_nofsynd(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb, erase_pos=erasures_pos, only_erasures=only_erasures)
+            msg_repaired = bytearray(msg_repaired)
+            ecc_repaired = bytearray(ecc_repaired)
+        elif self.algo == 4:
+            msg_repaired, ecc_repaired, _ = reedsolo.rs_correct_msg(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb, erase_pos=erasures_pos, only_erasures=only_erasures)
+            msg_repaired = bytearray(msg_repaired)
+            ecc_repaired = bytearray(ecc_repaired)
+
+        if pad: # Strip the null bytes if we padded the message before decoding
+            msg_repaired = msg_repaired[len(pad):len(msg_repaired)]
+        return _bytes(msg_repaired), _bytes(ecc_repaired)
+
+    def pad(self, message, k=None):
+        '''Automatically left pad with null bytes a message if too small, or leave unchanged if not necessary. This allows to keep track of padding and strip the null bytes after decoding reliably with binary data. Equivalent to shortening (shortened reed-solomon code).'''
+        if not k: k = self.k
+        pad = None
+        if len(message) < k:
+            #pad = "\x00" * (k-len(message))
+            pad = bytearray(k-len(message))
+            message = pad + bytearray(b(message))
+        return [message, pad]
+
+    def rpad(self, ecc, k=None):
+        '''Automatically right pad with null bytes an ecc to fill for missing bytes if too small, or leave unchanged if not necessary. This can be used as a workaround for field delimiter misdetection. Equivalent to puncturing (punctured reed-solomon code).'''
+        if not k: k = self.k
+        pad = None
+        if len(ecc) < self.n-k:
+            print("Warning: the ecc field may have been truncated (entrymarker or field_delim misdetection?).")
+            #pad = "\x00" * (self.n-k-len(ecc))
+            pad = bytearray(self.n-k-len(ecc))
+            ecc = bytearray(ecc) + pad
+        return [ecc, pad]
+
+    def check(self, message, ecc, k=None):
+        '''Check if there's any error in a message+ecc. Can be used before decoding, in addition to hashes to detect if the message was tampered, or after decoding to check that the message was fully recovered.'''
+        if not k: k = self.k
+        message, _ = self.pad(message, k=k)
+        ecc, _ = self.rpad(ecc, k=k)
+        if self.algo == 1 or self.algo == 2:
+            return self.ecc_manager.check_fast(message + ecc, k=k)
+        elif self.algo == 3 or self.algo == 4:
+            return reedsolo.rs_check(bytearray(message + ecc), self.n-k, fcr=self.fcr, generator=self.gen_nb)
+
+    def description(self):
+        '''Provide a description for each algorithm available, useful to print in ecc file'''
+        if 0 < self.algo <= 3:
+            return "Reed-Solomon with polynomials in Galois field of characteristic %i (2^%i) with generator=%s, prime poly=%s and first consecutive root=%s." % (self.field_charac, self.c_exp, self.gen_nb, hex(self.prim), self.fcr)
+        elif self.algo == 4:
+            return "Reed-Solomon with polynomials in Galois field of characteristic %i (2^%i) under US FAA ADSB UAT RS FEC standard with generator=%s, prime poly=%s and first consecutive root=%s." % (self.field_charac, self.c_exp, self.gen_nb, hex(self.prim), self.fcr)
+        else:
+            return "No description for this ECC algorithm."
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/lib/hasher.py` & `pyFileFixity-3.1.4/pyFileFixity/lib/hasher.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-#!/usr/bin/env python
-#
-# Hash manager facade api
-# Allows to easily use different kinds of hashing algorithms, size and libraries under one single class.
-# Copyright (C) 2015 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-
-from ._compat import b
-
-import sys
-import hashlib
-#import zlib
-from base64 import b64encode, b64decode  # using b64encode is about 3x faster than using encode('base64_codec')
-# alternative to base64: from codecs import encode
-
-class Hasher(object):
-    '''Class to provide a hasher object with various hashing algorithms. What's important is to provide the __len__ so that we can easily compute the block size of ecc entries. Must only use fixed size hashers for the rest of the script to work properly.'''
-    
-    known_algo = ["md5", "shortmd5", "shortsha256", "minimd5", "minisha256", "none"]
-    __slots__ = ['algo', 'length']
-
-    def __init__(self, algo="md5"):
-        # Store the selected hashing algo
-        self.algo = algo.lower()
-        # Precompute length so that it's very fast to access it later
-        if self.algo == "md5":
-            self.length = 32
-        elif self.algo == "shortmd5" or self.algo == "shortsha256":
-            self.length = 8
-        elif self.algo == "minimd5" or self.algo == "minisha256":
-            self.length = 4
-        elif self.algo == "none":
-            self.length = 0
-        else:
-            raise NameError('Hashing algorithm %s is unknown!' % algo)
-
-    def hash(self, mes):
-        # use hashlib.algorithms_guaranteed to list algorithms
-        mes = b(mes)
-        if self.algo == "md5":
-            return b(hashlib.md5(mes).hexdigest())
-        elif self.algo == "shortmd5": # from: http://www.peterbe.com/plog/best-hashing-function-in-python
-            return b64encode(b(hashlib.md5(mes).hexdigest()))[:8]
-        elif self.algo == "shortsha256":
-            return b64encode(b(hashlib.sha256(mes).hexdigest()))[:8]
-        elif self.algo == "minimd5":
-            return b64encode(b(hashlib.md5(mes).hexdigest()))[:4]
-        elif self.algo == "minisha256":
-            return b64encode(b(hashlib.sha256(mes).hexdigest()))[:4]
-        elif self.algo == "none":
-            return ''
-        else:
-            raise NameError('Hashing algorithm %s is unknown!' % self.algo)
-
-    def __len__(self):
+#!/usr/bin/env python
+#
+# Hash manager facade api
+# Allows to easily use different kinds of hashing algorithms, size and libraries under one single class.
+# Copyright (C) 2015 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+
+from ._compat import b
+
+import sys
+import hashlib
+#import zlib
+from base64 import b64encode, b64decode  # using b64encode is about 3x faster than using encode('base64_codec')
+# alternative to base64: from codecs import encode
+
+class Hasher(object):
+    '''Class to provide a hasher object with various hashing algorithms. What's important is to provide the __len__ so that we can easily compute the block size of ecc entries. Must only use fixed size hashers for the rest of the script to work properly.'''
+    
+    known_algo = ["md5", "shortmd5", "shortsha256", "minimd5", "minisha256", "none"]
+    __slots__ = ['algo', 'length']
+
+    def __init__(self, algo="md5"):
+        # Store the selected hashing algo
+        self.algo = algo.lower()
+        # Precompute length so that it's very fast to access it later
+        if self.algo == "md5":
+            self.length = 32
+        elif self.algo == "shortmd5" or self.algo == "shortsha256":
+            self.length = 8
+        elif self.algo == "minimd5" or self.algo == "minisha256":
+            self.length = 4
+        elif self.algo == "none":
+            self.length = 0
+        else:
+            raise NameError('Hashing algorithm %s is unknown!' % algo)
+
+    def hash(self, mes):
+        # use hashlib.algorithms_guaranteed to list algorithms
+        mes = b(mes)
+        if self.algo == "md5":
+            return b(hashlib.md5(mes).hexdigest())
+        elif self.algo == "shortmd5": # from: http://www.peterbe.com/plog/best-hashing-function-in-python
+            return b64encode(b(hashlib.md5(mes).hexdigest()))[:8]
+        elif self.algo == "shortsha256":
+            return b64encode(b(hashlib.sha256(mes).hexdigest()))[:8]
+        elif self.algo == "minimd5":
+            return b64encode(b(hashlib.md5(mes).hexdigest()))[:4]
+        elif self.algo == "minisha256":
+            return b64encode(b(hashlib.sha256(mes).hexdigest()))[:4]
+        elif self.algo == "none":
+            return ''
+        else:
+            raise NameError('Hashing algorithm %s is unknown!' % self.algo)
+
+    def __len__(self):
         return self.length
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/lib/md5py.py` & `pyFileFixity-3.1.4/pyFileFixity/lib/md5py.py`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/lib/tee.py` & `pyFileFixity-3.1.4/pyFileFixity/lib/tee.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,78 +1,78 @@
-#!/usr/bin/env python
-#
-
-import sys
-
-from ._compat import b
-
-class Tee(object):
-    """ Redirect print output to the terminal as well as in a log file """
-
-    def __init__(self, name=None, mode=None, nostdout=False, silent=False):
-        self.file = None
-        self.nostdout = nostdout
-        self.silent = silent
-        if not nostdout:
-            self.stdout = sys.stdout
-            sys.stdout = self
-        if name is not None and mode is not None:
-            self.filename = name
-            self.filemode = mode
-            self.file = open(name, mode)
-
-    def close(self):
-        """ Restore stdout and close file when Tee is closed """
-        try:
-            self.flush() # commit all latest changes before exiting
-        except:
-            pass  # sometimes it's already closed, just skip
-        if not self.nostdout and hasattr(self, 'stdout'):
-            sys.stdout = self.stdout
-            self.stdout = None
-        if self.file: self.file.close()
-
-    def __del__(self):
-        self.close()
-
-    def write(self, data, end="\n", flush=True):
-        """ Output data to stdout and/or file """
-        if not self.silent:
-            if not self.nostdout:
-                self.stdout.write(data)
-                self.stdout.write(end)
-            if self.file is not None:
-                # Binary mode: need to convert to byte objects if Python 3
-                if 'b' in self.filemode:
-                    data = b(data)
-                    end = b(end)
-                self.file.write(data)
-                self.file.write(end)
-            if flush:
-                self.flush()
-
-    def flush(self):
-        """ Force commit changes to the file and stdout """
-        if not self.silent:
-            if not self.nostdout:
-                self.stdout.flush()
-            if self.file is not None:
-                self.file.flush()
-
-    # def disable(self):
-        # """ Temporarily disable Tee's redirection """
-        # self.flush() # commit all latest changes before exiting
-        # if not self.nostdout and hasattr(self, 'stdout'):
-            # sys.stdout = self.stdout
-            # self.stdout = None
-        # if self.file:
-            # self.file.close()
-            # self.file = None
-
-    # def enable(self):
-        # """ Reenable Tee's redirection after being temporarily disabled """
-        # if not self.nostdout and not self.stdout:
-            # self.__del__.stdout = sys.stdout
-            # self.stdout = self.__del__.stdout # The weakref proxy is to prevent Python, or yourself from deleting the self.files variable somehow (if it is deleted, then it will not affect the original file list). If it is not the case that this is being deleted even though there are more references to the variable, then you can remove the proxy encapsulation. http://stackoverflow.com/questions/865115/how-do-i-correctly-clean-up-a-python-object
-            # sys.stdout = self
-        # if not self.file and self.filename is not None and self.filemode is not None:
-            # self.file = open(self.filename, self.filemode)
+#!/usr/bin/env python
+#
+
+import sys
+
+from ._compat import b
+
+class Tee(object):
+    """ Redirect print output to the terminal as well as in a log file """
+
+    def __init__(self, name=None, mode=None, nostdout=False, silent=False):
+        self.file = None
+        self.nostdout = nostdout
+        self.silent = silent
+        if not nostdout:
+            self.stdout = sys.stdout
+            sys.stdout = self
+        if name is not None and mode is not None:
+            self.filename = name
+            self.filemode = mode
+            self.file = open(name, mode)
+
+    def close(self):
+        """ Restore stdout and close file when Tee is closed """
+        try:
+            self.flush() # commit all latest changes before exiting
+        except:
+            pass  # sometimes it's already closed, just skip
+        if not self.nostdout and hasattr(self, 'stdout'):
+            sys.stdout = self.stdout
+            self.stdout = None
+        if self.file: self.file.close()
+
+    def __del__(self):
+        self.close()
+
+    def write(self, data, end="\n", flush=True):
+        """ Output data to stdout and/or file """
+        if not self.silent:
+            if not self.nostdout:
+                self.stdout.write(data)
+                self.stdout.write(end)
+            if self.file is not None:
+                # Binary mode: need to convert to byte objects if Python 3
+                if 'b' in self.filemode:
+                    data = b(data)
+                    end = b(end)
+                self.file.write(data)
+                self.file.write(end)
+            if flush:
+                self.flush()
+
+    def flush(self):
+        """ Force commit changes to the file and stdout """
+        if not self.silent:
+            if not self.nostdout:
+                self.stdout.flush()
+            if self.file is not None:
+                self.file.flush()
+
+    # def disable(self):
+        # """ Temporarily disable Tee's redirection """
+        # self.flush() # commit all latest changes before exiting
+        # if not self.nostdout and hasattr(self, 'stdout'):
+            # sys.stdout = self.stdout
+            # self.stdout = None
+        # if self.file:
+            # self.file.close()
+            # self.file = None
+
+    # def enable(self):
+        # """ Reenable Tee's redirection after being temporarily disabled """
+        # if not self.nostdout and not self.stdout:
+            # self.__del__.stdout = sys.stdout
+            # self.stdout = self.__del__.stdout # The weakref proxy is to prevent Python, or yourself from deleting the self.files variable somehow (if it is deleted, then it will not affect the original file list). If it is not the case that this is being deleted even though there are more references to the variable, then you can remove the proxy encapsulation. http://stackoverflow.com/questions/865115/how-do-i-correctly-clean-up-a-python-object
+            # sys.stdout = self
+        # if not self.file and self.filename is not None and self.filemode is not None:
+            # self.file = open(self.filename, self.filemode)
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/pff.py` & `pyFileFixity-3.1.4/pyFileFixity/pff.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,133 +1,133 @@
-#!/usr/bin/env python
-#
-# Main script entry point for pyFileFixity, provides an interface with subcommands
-# Copyright (C) 2023 Stephen Karl Larroque
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#=================================
-#     pyFileFixity Main Subcommands Facade API
-#                by Stephen Larroque
-#                       License: MIT
-#              Creation date: 2023-08-04
-#=================================
-# Inspired by Adam Johnson's template for a script with subcommands: https://adamj.eu/tech/2021/10/15/a-python-script-template-with-sub-commands-and-type-hints/
-#
-
-from __future__ import annotations
-
-# Import tools for argument parsing and typing
-import argparse
-from collections.abc import Sequence
-import sys
-
-# Include the lib folder in the python import path to be able to do relative imports
-# DEPRECATED: unnecessary since PEP328, but need to use the "from .a import x" form, not "import .x" https://fortierq.github.io/python-import/ -- but note that editable mode is very fine and accepted nowadays, a subsequent PEP fixed the issue!
-#import os, sys
-#thispathname = os.path.dirname(__file__)
-#sys.path.append(os.path.join(thispathname))
-
-# Import all pyFileFixity subcommands tools
-from .rfigc import main as rfigc_main
-from .header_ecc import main as hecc_main
-from .structural_adaptive_ecc import main as saecc_main
-from .repair_ecc import main as recc_main
-from .replication_repair import main as replication_repair_main
-from .resiliency_tester import main as restest_main
-from .filetamper import main as filetamper_main
-from .ecc_speedtest import main as ecc_speedtest_main
-
-def main(argv: Sequence[str] | None = None) -> int:
-    parser = argparse.ArgumentParser()
-    subparsers = parser.add_subparsers(dest="subcommand", required=True)
-
-    # Add sub-commands
-    rfigc_parser = subparsers.add_parser("hash", aliases=["rfigc"], help="Check files integrity fast by hash, size, modification date or by data structure integrity.", add_help=False)  # disable help, so that we can redefine it and propagate as an argument downstream to the called module
-    rfigc_parser.add_argument('-h', '--help', action='store_true')  # redefine help argument so that we can pass it downstream to submodules' argparse parsers
-
-    hecc_parser = subparsers.add_parser("header", aliases=["header_ecc", "hecc"], help="Protect/repair files headers with error correction codes", add_help=False)
-    hecc_parser.add_argument('-h', '--help', action='store_true')
-
-    saecc_parser = subparsers.add_parser("whole", aliases=["structural_adaptive_ecc", "saecc", "protect", "repair"], help="Protect/repair whole files with error correction codes", add_help=False)
-    saecc_parser.add_argument('-h', '--help', action='store_true')
-
-    recc_parser = subparsers.add_parser("recover", aliases=["repair_ecc", "recc"], help="Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery mode (note: this does NOT recover your files, only the ecc files, which may then be used to recover your files!)", add_help=False)
-    recc_parser.add_argument('-h', '--help', action='store_true')
-
-    replication_repair_parser = subparsers.add_parser("dup", aliases=["replication_repair"], help="Repair files from multiple copies of various storage mediums using a majority vote", add_help=False)
-    replication_repair_parser.add_argument('-h', '--help', action='store_true')
-
-    restest_parser = subparsers.add_parser("restest", aliases=["resilience_tester"], help="Run tests to quantify robustness of a file protection scheme (can be used on any, not just pyFileFixity)", add_help=False)
-    restest_parser.add_argument('-h', '--help', action='store_true')
-
-    filetamper_parser = subparsers.add_parser("filetamper", help="Tamper files using various schemes", add_help=False)
-    filetamper_parser.add_argument('-h', '--help', action='store_true')
-
-    ecc_speedtest_parser = subparsers.add_parser("speedtest", aliases=["ecc_speedtest"], help="Run error correction encoding and decoding speedtests", add_help=False)
-    ecc_speedtest_parser.add_argument('-h', '--help', action='store_true')
-
-    # Parse known arguments, but we have almost none, this is done on purpose so that we can pass all arguments (except helps) downstream for submodules to handle with their own Argparse
-    args, args_remainder = parser.parse_known_args(argv)
-    #print(type(args_remainder))  # DEBUGLINE
-    #print(args)  # DEBUGLINE
-
-    if len(sys.argv) >= 2:
-        # Prepare subarguments
-        subargs = []
-        if args.help is True:
-            # Manage custom case of manually propagating --help to downstream module, we prepend to the string of the remainder of arguments
-            subargs.append("--help")
-        # Add the rest of the arguments, so that the downstream module can handle them with their own Argparse parser
-        subargs.extend(args_remainder)  # args_remainder is a list, so we can extend subargs with it
-
-        fullcommand = "pff.py " + args.subcommand
-
-        if args.subcommand in ["hash", "rfigc"]:
-            return rfigc_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["header", "header_ecc", "hecc"]:
-            return hecc_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["whole", "structural_adaptive_ecc", "saecc", "protect", "repair"]:
-            return saecc_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["recover", "repair_ecc", "recc"]:
-            return recc_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["dup", "replication_repair"]:
-            return replication_repair_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["restest", "resilience_tester"]:
-            return restest_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["filetamper"]:
-            return filetamper_main(argv=subargs, command=fullcommand)
-        elif args.subcommand in ["speedtest", "ecc_speedtest"]:
-            return ecc_speedtest_main(argv=subargs, command=fullcommand)
-        else:
-            # Unreachable
-            raise NotImplementedError(
-                f"Command {args.command} is not implemented (dev forgot!).",
-            )
-
-
-def subcommand1(string: str) -> int:
-    # Implement behaviour
-
-    return 0
-
-
-if __name__ == "__main__":
-    raise SystemExit(main())
+#!/usr/bin/env python
+#
+# Main script entry point for pyFileFixity, provides an interface with subcommands
+# Copyright (C) 2023 Stephen Karl Larroque
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#=================================
+#     pyFileFixity Main Subcommands Facade API
+#                by Stephen Larroque
+#                       License: MIT
+#              Creation date: 2023-08-04
+#=================================
+# Inspired by Adam Johnson's template for a script with subcommands: https://adamj.eu/tech/2021/10/15/a-python-script-template-with-sub-commands-and-type-hints/
+#
+
+from __future__ import annotations
+
+# Import tools for argument parsing and typing
+import argparse
+from collections.abc import Sequence
+import sys
+
+# Include the lib folder in the python import path to be able to do relative imports
+# DEPRECATED: unnecessary since PEP328, but need to use the "from .a import x" form, not "import .x" https://fortierq.github.io/python-import/ -- but note that editable mode is very fine and accepted nowadays, a subsequent PEP fixed the issue!
+#import os, sys
+#thispathname = os.path.dirname(__file__)
+#sys.path.append(os.path.join(thispathname))
+
+# Import all pyFileFixity subcommands tools
+from .rfigc import main as rfigc_main
+from .header_ecc import main as hecc_main
+from .structural_adaptive_ecc import main as saecc_main
+from .repair_ecc import main as recc_main
+from .replication_repair import main as replication_repair_main
+from .resiliency_tester import main as restest_main
+from .filetamper import main as filetamper_main
+from .ecc_speedtest import main as ecc_speedtest_main
+
+def main(argv: Sequence[str] | None = None) -> int:
+    parser = argparse.ArgumentParser()
+    subparsers = parser.add_subparsers(dest="subcommand", required=True)
+
+    # Add sub-commands
+    rfigc_parser = subparsers.add_parser("hash", aliases=["rfigc"], help="Check files integrity fast by hash, size, modification date or by data structure integrity.", add_help=False)  # disable help, so that we can redefine it and propagate as an argument downstream to the called module
+    rfigc_parser.add_argument('-h', '--help', action='store_true')  # redefine help argument so that we can pass it downstream to submodules' argparse parsers
+
+    hecc_parser = subparsers.add_parser("header", aliases=["header_ecc", "hecc"], help="Protect/repair files headers with error correction codes", add_help=False)
+    hecc_parser.add_argument('-h', '--help', action='store_true')
+
+    saecc_parser = subparsers.add_parser("whole", aliases=["structural_adaptive_ecc", "saecc", "protect", "repair"], help="Protect/repair whole files with error correction codes", add_help=False)
+    saecc_parser.add_argument('-h', '--help', action='store_true')
+
+    recc_parser = subparsers.add_parser("recover", aliases=["repair_ecc", "recc"], help="Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery mode (note: this does NOT recover your files, only the ecc files, which may then be used to recover your files!)", add_help=False)
+    recc_parser.add_argument('-h', '--help', action='store_true')
+
+    replication_repair_parser = subparsers.add_parser("dup", aliases=["replication_repair"], help="Repair files from multiple copies of various storage mediums using a majority vote", add_help=False)
+    replication_repair_parser.add_argument('-h', '--help', action='store_true')
+
+    restest_parser = subparsers.add_parser("restest", aliases=["resilience_tester"], help="Run tests to quantify robustness of a file protection scheme (can be used on any, not just pyFileFixity)", add_help=False)
+    restest_parser.add_argument('-h', '--help', action='store_true')
+
+    filetamper_parser = subparsers.add_parser("filetamper", help="Tamper files using various schemes", add_help=False)
+    filetamper_parser.add_argument('-h', '--help', action='store_true')
+
+    ecc_speedtest_parser = subparsers.add_parser("speedtest", aliases=["ecc_speedtest"], help="Run error correction encoding and decoding speedtests", add_help=False)
+    ecc_speedtest_parser.add_argument('-h', '--help', action='store_true')
+
+    # Parse known arguments, but we have almost none, this is done on purpose so that we can pass all arguments (except helps) downstream for submodules to handle with their own Argparse
+    args, args_remainder = parser.parse_known_args(argv)
+    #print(type(args_remainder))  # DEBUGLINE
+    #print(args)  # DEBUGLINE
+
+    if len(sys.argv) >= 2:
+        # Prepare subarguments
+        subargs = []
+        if args.help is True:
+            # Manage custom case of manually propagating --help to downstream module, we prepend to the string of the remainder of arguments
+            subargs.append("--help")
+        # Add the rest of the arguments, so that the downstream module can handle them with their own Argparse parser
+        subargs.extend(args_remainder)  # args_remainder is a list, so we can extend subargs with it
+
+        fullcommand = "pff.py " + args.subcommand
+
+        if args.subcommand in ["hash", "rfigc"]:
+            return rfigc_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["header", "header_ecc", "hecc"]:
+            return hecc_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["whole", "structural_adaptive_ecc", "saecc", "protect", "repair"]:
+            return saecc_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["recover", "repair_ecc", "recc"]:
+            return recc_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["dup", "replication_repair"]:
+            return replication_repair_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["restest", "resilience_tester"]:
+            return restest_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["filetamper"]:
+            return filetamper_main(argv=subargs, command=fullcommand)
+        elif args.subcommand in ["speedtest", "ecc_speedtest"]:
+            return ecc_speedtest_main(argv=subargs, command=fullcommand)
+        else:
+            # Unreachable
+            raise NotImplementedError(
+                f"Command {args.command} is not implemented (dev forgot!).",
+            )
+
+
+def subcommand1(string: str) -> int:
+    # Implement behaviour
+
+    return 0
+
+
+if __name__ == "__main__":
+    raise SystemExit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/repair_ecc.py` & `pyFileFixity-3.1.4/pyFileFixity/repair_ecc.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,369 +1,369 @@
-#!/usr/bin/env python
-#
-# ECC repairer
-# Copyright (C) 2015 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#
-
-from __future__ import print_function
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, _range, b
-from lib.aux_funcs import fullpath
-import argparse
-import datetime, time
-import tqdm
-import itertools
-import math
-#import operator # to get the max out of a dict
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-import struct # to support indexes backup file
-import shutil
-from distance import hamming
-#import pprint # Unnecessary, used only for debugging purposes
-
-# ECC and hashing facade libraries
-from lib.eccman import ECCMan, compute_ecc_params
-from lib.hasher import Hasher
-from reedsolo import ReedSolomonError
-
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''ECC file repairer
-Description: Repair the structure of an ecc file, mainly the ecc markers, so that at least the ecc correction can align correctly the ecc entries and fields.
-Note: An ecc structure repair does NOT allow to recover from more errors on your files, it only allows to repair an ecc file so that its structure is valid and can be read correctly.
-    '''
-    ep = ''' '''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='eccfile.txt', type=str, required=True,
-                        help='Path to the ecc file to repair.', **widget_file)
-    main_parser.add_argument('-o', '--output', metavar='eccfile_repaired.txt', type=str, required=True, #type=argparse.FileType('rt')
-                        help='Output path where to save the repaired ecc file.', **widget_filesave)
-    main_parser.add_argument('-t', '--threshold', type=float, default=0.3, required=False,
-                        help='Distance threshold for the heuristic hamming distance repair. This must be a float, eg, 0.2 means that if there are 20%% characters different between an ecc marker and a substring in the ecc file, it will be detected as a marker and corrected.', **widget_text)
-
-    # Optional general arguments
-    main_parser.add_argument('--index', metavar='eccfile.txt.idx', type=str, required=False,
-                        help='Path to the index backup file corresponding to the ecc file (optional but helps a lot).', **widget_file)
-    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
-                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
-                        help='Force overwriting the ecc file even if it already exists (if --generate).')
-
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set hard-coded variables
-    entrymarker = "\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF" # marker that will signal the beginning of an ecc entry - use an alternating pattern of several characters, this avoids confusion (eg: if you use "AAA" as a pattern, if the ecc block of the previous file ends with "EGA" for example, then the full string for example will be "EGAAAAC:\yourfolder\filea.jpg" and then the entry reader will detect the first "AAA" occurrence as the entry start - this should not make the next entry bug because there is an automatic trim - but the previous ecc block will miss one character that could be used to repair the block because it will be "EG" instead of "EGA"!)
-    field_delim = "\xFA\xFF\xFA\xFF\xFA" # delimiter between fields (filepath, filesize, hash+ecc blocks) inside an ecc entry
-    markers = [entrymarker, field_delim] # put them in a list for easy reference
-    max_block_size = 27
-    resilience_rate = 1
-
-    #-- Set variables from arguments
-    inputpath = fullpath(args.input)
-    outputpath = fullpath(args.output)
-    distance_threshold = args.threshold
-    indexpath = None
-    if args.index: indexpath = fullpath(args.index)
-    force = args.force
-    ecc_algo = args.ecc_algo
-    verbose = args.verbose
-    silent = args.silent
-
-    # -- Checking arguments
-    if not os.path.isfile(inputpath):
-        raise NameError('Specified database ecc file %s does not exist!' % inputpath)
-    if os.path.isfile(outputpath) and not force:
-        raise NameError('Specified output path for the repaired ecc file %s already exists! Use --force if you want to overwrite.' % outputpath)
-    if indexpath and not os.path.isfile(indexpath):
-        raise NameError('Specified index backup file %s does not exist!' % indexpath)
-
-    if max_block_size < 2 or max_block_size > 255:
-        raise ValueError('RS max block size must be between 2 and 255.')
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log, 'a', nostdout=silent)
-        #sys.stdout = Tee(args.log, 'a')
-        sys.stderr = Tee(args.log, 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-
-    # == PROCESSING BRANCHING == #
-
-    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
-    hasher_none = Hasher('none') # for index ecc we don't use any hash
-    ecc_params_idx = compute_ecc_params(max_block_size, resilience_rate, hasher_none)
-    ecc_manager_idx = ECCMan(max_block_size, ecc_params_idx["message_size"], algo=ecc_algo)
-
-    # == Main loop
-    ptee.write("====================================")
-    ptee.write("ECC repair, started on %s" % datetime.datetime.now().isoformat())
-    ptee.write("====================================")
-    ptee.write("Please note that this tool may not know if it found all the markers, so it may miss too much corrupted markers but it will repair the ones it finds (except if you have a fully valid index file, then you are guaranteed to always find all markers).")
-
-    ecc_size = os.stat(inputpath).st_size
-    if indexpath: idx_size = os.stat(indexpath).st_size
-    shutil.copy2(inputpath, outputpath)
-    blocksize = 65535
-    with open(outputpath, 'r+b') as db:
-
-        # == Index backup repair
-        # This repair needs an index backup file which is normally generated at the same time as the ecc file. The index backup file is a file that stores the position of all ecc markers in the corresponding ecc file, and protects those positions using ecc.
-        if indexpath:
-            ptee.write("Using the index backup file %s to repair ecc markers, please wait..." % args.index)
-            db.seek(0) # seek to the beginning of the database file
-            idx_corrupted = 0
-            idx_corrected = 0
-            idx_total = 0
-            markers_repaired = [0] * len(markers) # create one list for each marker type
-            bardisp = tqdm.tqdm(total=idx_size, file=ptee, leave=True, desc='IDXREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
-            with open(indexpath, 'rb') as dbidx:
-                buf = 1
-                while buf:
-                    # The format of the index backup file is pretty simple: for each entrymarker or field_delim, a block is appended. Each such block is made of: the type on one byte (1 for entrymarker, 2 for field_delim), then the marker's position in the ecc file encoded in an unsigned long long (thus it's on a fixed 8 bytes), and finally an ecc for both the type and marker's position, and which is of fixed size (since we know that the marker's type + position = 9 bytes). Each such block is appended right after the precedent, so we know easily read them and such structure cannot be tampered by a soft error (there's no way a hard drive failure can chance the structure of the data, but a malicious user can! But it's then easy to fix that for a human user, you can clearly see the patterns, where the marker's positions begins and ends).
-                    # Note that this constant sized structure of blocks is made on purpose, so that the structure of the index backup file is implicit, while the structure of the ecc file is explicit (it needs uncorrupted markers, which is a weak point that we try to address with the index backup file).
-                    # eg of two blocks: 10000008Aecceccecc2000000F2ecceccecc
-                    #
-                    # Read one index block
-                    curpos = dbidx.tell() # backup current position for error messages
-                    buf = dbidx.read(max_block_size)
-                    # Update progress bar
-                    bardisp.update(dbidx.tell()-bardisp.n)
-                    # If we have reached EOF, then we stop here
-                    if not buf: break
-
-                    # Else it's ok we have an index block, we process it
-                    idx_total += 1
-                    # Extract the marker's infos and the ecc
-                    marker_str = buf[:ecc_params_idx["message_size"]]
-                    ecc = buf[ecc_params_idx["message_size"]:]
-                    # Check if the marker's infos are corrupted, if yes, then we will try to fix that using the ecc
-                    if not ecc_manager_idx.check(marker_str, ecc):
-                        # Trying to fix the marker's infos using the ecc
-                        idx_corrupted += 1
-                        marker_repaired, repaired_ecc = ecc_manager_idx.decode(marker_str, ecc)
-                        # Repaired the marker's infos, all is good!
-                        if ecc_manager_idx.check(marker_repaired, repaired_ecc):
-                            marker_str = marker_repaired
-                            idx_corrected += 1
-                        # Else it's corrupted beyond repair, just skip
-                        else:
-                            ptee.write("\n- Index backup file: error on block starting at %i, corrupted and could not fix it. Skipping." % curpos)
-                            marker_str = None
-                            continue
-                    if not marker_str: continue
-
-                    # Repair ecc file's marker using our correct (or repaired) marker's infos
-                    marker_type = int(chr(marker_str[0]) if isinstance(marker_str[0], int) else marker_str[0]) # marker's type is always stored on the first byte/character
-                    marker_pos = struct.unpack('>Q', marker_str[1:]) # marker's position is encoded as a big-endian unsigned long long, in a 8 bytes/chars string
-                    db.seek(marker_pos[0]) # move the ecc reading cursor to the beginning of the marker
-                    current_marker = db.read(len(markers[marker_type-1])) # read the current marker (potentially corrupted)
-                    db.seek(marker_pos[0])
-                    if verbose:
-                        ptee.write("- Found marker by index file: type=%i content=" % (marker_type))
-                        ptee.write(db.read(len(markers[marker_type-1])+4))
-                        db.seek(marker_pos[0]) # replace the reading cursor back in place before the marker
-                    if current_marker != markers[marker_type-1]: # check if we really need to repair this marker
-                        # Rewrite the marker over the ecc file
-                        db.write(b(markers[marker_type-1]))
-                        markers_repaired[marker_type-1] += 1
-                    else:
-                        ptee.write("skipped, no need to repair")
-            # Done the index backup repair
-            if bardisp.n > bardisp.total: bardisp.n = bardisp.total # just a workaround in case there's one byte more than the predicted total
-            bardisp.close()
-            ptee.write("Done. Total: %i/%i markers repaired (%i entrymarkers and %i field_delim), %i indexes corrupted and %i indexes repaired (%i indexes lost).\n" % (markers_repaired[0]+markers_repaired[1], idx_total, markers_repaired[0], markers_repaired[1], idx_corrupted, idx_corrected, idx_corrupted-idx_corrected) )
-
-        # == Heuristical Greedy Hamming distance repair
-        # This is a heuristical (doesn't need any other file than the ecc file) 2-pass algorithm: the first pass tries to find the markers positions, and then the second pass simply reads the original ecc file and copies it while repairing the found markers.
-        # The first pass is obviously the most interesting, here's a description: we use a kind of greedy algorithm but with backtracking, meaning that we simply read through all the strings sequentially and just compare with the markers and compute the Hamming distance: if the Hamming distance gets below the threshold, we trigger the found marker flag. Then if the Hamming distance decreases, we save this marker position and disable the found marker flag. However, there can be false positives like this (eg, the marker is corrupted in the middle), so we have a backtracking mechanism: if a later string is found to have a Hamming distance that is below the threshold, then we check if the just previously found marker is in the range (ie, the new marker's position is smaller than the previous marker's length) and if the Hamming distance is smaller, then we replace the previous marker with the new marker's position, because the previous one was most likely a false positive.
-        # This method doesn't require any other file than the ecc file, but it may not work on ecc markers that are too much tampered, and if the detection threshold is too low or the markers are too small, there may be lots of false positives.
-        # So try to use long markers (consisting of many character, preferably an alternating pattern different than the null byte \x00) and a high enough detection threshold.
-        ptee.write("Using heuristics (Hamming distance) to fix markers with a threshold of %i%%, please wait..." % (round(distance_threshold*100, 0)) )
-
-        # Main loop for heuristical repair, try to find the substrings that minimize the hamming distance to one of the ecc markers
-        markers_repaired = [0] * len(markers) # stat counter
-        already_valid = 0 # stat counter
-        db.seek(0) # seek to the beginning of the database file
-        buf = 1 # init the buffer to 1 to initiate the while loop
-        markers_pos = [[] for i in _range(len(markers))] # will contain the list of positions where a corrupted marker has been detected (not valid markers, they will be skipped)
-        distance_thresholds = [round(len(x)*distance_threshold, 0) for x in markers] # calculate the number of characters maximum for distance
-        skip_until = -1 # when a valid marker (non corrupted) is found, we use this variable to skip to after the marker length (to avoid detecting partial parts of this marker, which will have a hamming distance even if the marker is completely valid because the reading window will be after the beginning of the marker)
-        bardisp = tqdm.tqdm(total=ecc_size, file=ptee, leave=True, desc='DBREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
-        while buf: # until we have walked through the whole ecc file
-            # Read a part of the ecc file into a buffer, this allows to process more quickly than just loading the size of a marker
-            curpos = db.tell() # keep the current reading position
-            buf = db.read(blocksize)
-            # Update progress bar
-            bardisp.update(db.tell()-bardisp.n)
-            if not buf: break # reached EOF? quitting here
-
-            # Scan the buffer, by splitting the buffer into substrings the length of the ecc markers
-            for i in _range(len(buf)-max(len(entrymarker),len(field_delim))):
-                # If we just came accross a non corrupted ecc marker, we skip until we are after this ecc marker (to avoid misdetections)
-                if i < skip_until: continue
-                # Compare each ecc marker type to this substring and compute the Hamming distance
-                for m in _range(len(markers)):
-                    d = hamming(b(buf[i:i+len(markers[m])]), b(markers[m])) # Compute the Hamming distance (simply the number of different characters)
-                    mcurpos = curpos+i # current absolute position of this ecc marker
-                    
-                    # If there's no difference, then it's a valid, non-corrupted ecc marker
-                    if d == 0:
-                        already_valid += 1 # stats...
-                        # If we previously wrongly detected a corrupted ecc marker near here, then it's probably a misdetection (because we just had a partial view on this marker until now), thus we just remove it from our list of markers to repair
-                        if len(markers_pos[m]) > 0 and (mcurpos - markers_pos[m][-1][0]) <= len(markers[m]): # to detect that, we just check if the latest marker to repair is near the current marker (if its position is at maximum the length of the marker). This works because in the other condition below, we update the latest marker to repair if we find another one with a lower hamming distance very near.
-                            del markers_pos[m][-1]
-                        # Skip scanning until we are after the current marker to avoid misdetections
-                        su = i+len(markers[m])
-                        if su > skip_until: skip_until = su # update with the biggest marker (because both markers can be detected here if the pattern is similar)
-                        break
-                    # Else there's a difference/distance but it's below the threshold: we have a corrupted marker!
-                    elif d > 0 and d <= distance_thresholds[m]:
-                        # Updating case: If the latest marker to repair is quite close to the current one, but the current detection has a lower distance, we probably are detecting the same marker but we are better positionned now, so we update the previous marker's position with this one now.
-                        if len(markers_pos[m]) > 0 and (mcurpos - markers_pos[m][-1][0]) <= len(markers[m]):
-                            if d < markers_pos[m][-1][1]: # Update only if the distance is less
-                                markers_pos[m][-1] = [mcurpos, d]
-                            else: # Else, we probably are detecting the same marker as the last detected one, but since our scanning window has moved forward, we have increased the distance. Just skip it, we should not repair at this position (else we will probably be overwriting over real content).
-                                continue
-                        # Adding case: Else we just add this marker as a new one to repair by appending to the list
-                        else:
-                            markers_pos[m].append([mcurpos, d])
-                    # Else the distance is too great for the threshold, it's not a marker at all, we go on to the next substring
-            if db.tell() < ecc_size: db.seek(db.tell()-max(len(entrymarker),len(field_delim)))
-        if bardisp.n > bardisp.total: bardisp.n = bardisp.total # just a workaround in case there's one byte more than the predicted total
-        bardisp.close()
-
-        # Committing the repair into the ecc file
-        for m in _range(len(markers)): # for each type of markers
-            marker = markers[m]
-            if len(markers_pos[m]) > 0: # If there is any detected marker to repair for this type
-                for pos in markers_pos[m]: # for each detected marker to repair, we rewrite it over into the file at the detected position
-                    if verbose: ptee.write("- Detected marker type %i at position %i with distance %i (%i%%): repairing." % (m+1, pos[0], pos[1], (float(pos[1])/len(markers[m]))*100) )
-                    db.seek(pos[0])
-                    db.write(b(marker))
-
-        #print(markers_pos)
-        ptee.write("Done. Hamming heuristic with threshold %i%% repaired %i entrymarkers and %i field_delim (%i total) and %i were already valid.\n" % (round(distance_threshold*100, 0), len(markers_pos[0]), len(markers_pos[1]), len(markers_pos[0])+len(markers_pos[1]), already_valid) )
-        ptee.close()
-        return 0
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":  # pragma: no cover
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# ECC repairer
+# Copyright (C) 2015 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#
+
+from __future__ import print_function
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, _range, b
+from lib.aux_funcs import fullpath
+import argparse
+import datetime, time
+import tqdm
+import itertools
+import math
+#import operator # to get the max out of a dict
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+import struct # to support indexes backup file
+import shutil
+from distance import hamming
+#import pprint # Unnecessary, used only for debugging purposes
+
+# ECC and hashing facade libraries
+from lib.eccman import ECCMan, compute_ecc_params
+from lib.hasher import Hasher
+from reedsolo import ReedSolomonError
+
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''ECC file repairer
+Description: Repair the structure of an ecc file, mainly the ecc markers, so that at least the ecc correction can align correctly the ecc entries and fields.
+Note: An ecc structure repair does NOT allow to recover from more errors on your files, it only allows to repair an ecc file so that its structure is valid and can be read correctly.
+    '''
+    ep = ''' '''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='eccfile.txt', type=str, required=True,
+                        help='Path to the ecc file to repair.', **widget_file)
+    main_parser.add_argument('-o', '--output', metavar='eccfile_repaired.txt', type=str, required=True, #type=argparse.FileType('rt')
+                        help='Output path where to save the repaired ecc file.', **widget_filesave)
+    main_parser.add_argument('-t', '--threshold', type=float, default=0.3, required=False,
+                        help='Distance threshold for the heuristic hamming distance repair. This must be a float, eg, 0.2 means that if there are 20%% characters different between an ecc marker and a substring in the ecc file, it will be detected as a marker and corrected.', **widget_text)
+
+    # Optional general arguments
+    main_parser.add_argument('--index', metavar='eccfile.txt.idx', type=str, required=False,
+                        help='Path to the index backup file corresponding to the ecc file (optional but helps a lot).', **widget_file)
+    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
+                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
+                        help='Force overwriting the ecc file even if it already exists (if --generate).')
+
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set hard-coded variables
+    entrymarker = "\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF" # marker that will signal the beginning of an ecc entry - use an alternating pattern of several characters, this avoids confusion (eg: if you use "AAA" as a pattern, if the ecc block of the previous file ends with "EGA" for example, then the full string for example will be "EGAAAAC:\yourfolder\filea.jpg" and then the entry reader will detect the first "AAA" occurrence as the entry start - this should not make the next entry bug because there is an automatic trim - but the previous ecc block will miss one character that could be used to repair the block because it will be "EG" instead of "EGA"!)
+    field_delim = "\xFA\xFF\xFA\xFF\xFA" # delimiter between fields (filepath, filesize, hash+ecc blocks) inside an ecc entry
+    markers = [entrymarker, field_delim] # put them in a list for easy reference
+    max_block_size = 27
+    resilience_rate = 1
+
+    #-- Set variables from arguments
+    inputpath = fullpath(args.input)
+    outputpath = fullpath(args.output)
+    distance_threshold = args.threshold
+    indexpath = None
+    if args.index: indexpath = fullpath(args.index)
+    force = args.force
+    ecc_algo = args.ecc_algo
+    verbose = args.verbose
+    silent = args.silent
+
+    # -- Checking arguments
+    if not os.path.isfile(inputpath):
+        raise NameError('Specified database ecc file %s does not exist!' % inputpath)
+    if os.path.isfile(outputpath) and not force:
+        raise NameError('Specified output path for the repaired ecc file %s already exists! Use --force if you want to overwrite.' % outputpath)
+    if indexpath and not os.path.isfile(indexpath):
+        raise NameError('Specified index backup file %s does not exist!' % indexpath)
+
+    if max_block_size < 2 or max_block_size > 255:
+        raise ValueError('RS max block size must be between 2 and 255.')
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log, 'a', nostdout=silent)
+        #sys.stdout = Tee(args.log, 'a')
+        sys.stderr = Tee(args.log, 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+
+    # == PROCESSING BRANCHING == #
+
+    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
+    hasher_none = Hasher('none') # for index ecc we don't use any hash
+    ecc_params_idx = compute_ecc_params(max_block_size, resilience_rate, hasher_none)
+    ecc_manager_idx = ECCMan(max_block_size, ecc_params_idx["message_size"], algo=ecc_algo)
+
+    # == Main loop
+    ptee.write("====================================")
+    ptee.write("ECC repair, started on %s" % datetime.datetime.now().isoformat())
+    ptee.write("====================================")
+    ptee.write("Please note that this tool may not know if it found all the markers, so it may miss too much corrupted markers but it will repair the ones it finds (except if you have a fully valid index file, then you are guaranteed to always find all markers).")
+
+    ecc_size = os.stat(inputpath).st_size
+    if indexpath: idx_size = os.stat(indexpath).st_size
+    shutil.copy2(inputpath, outputpath)
+    blocksize = 65535
+    with open(outputpath, 'r+b') as db:
+
+        # == Index backup repair
+        # This repair needs an index backup file which is normally generated at the same time as the ecc file. The index backup file is a file that stores the position of all ecc markers in the corresponding ecc file, and protects those positions using ecc.
+        if indexpath:
+            ptee.write("Using the index backup file %s to repair ecc markers, please wait..." % args.index)
+            db.seek(0) # seek to the beginning of the database file
+            idx_corrupted = 0
+            idx_corrected = 0
+            idx_total = 0
+            markers_repaired = [0] * len(markers) # create one list for each marker type
+            bardisp = tqdm.tqdm(total=idx_size, file=ptee, leave=True, desc='IDXREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
+            with open(indexpath, 'rb') as dbidx:
+                buf = 1
+                while buf:
+                    # The format of the index backup file is pretty simple: for each entrymarker or field_delim, a block is appended. Each such block is made of: the type on one byte (1 for entrymarker, 2 for field_delim), then the marker's position in the ecc file encoded in an unsigned long long (thus it's on a fixed 8 bytes), and finally an ecc for both the type and marker's position, and which is of fixed size (since we know that the marker's type + position = 9 bytes). Each such block is appended right after the precedent, so we know easily read them and such structure cannot be tampered by a soft error (there's no way a hard drive failure can chance the structure of the data, but a malicious user can! But it's then easy to fix that for a human user, you can clearly see the patterns, where the marker's positions begins and ends).
+                    # Note that this constant sized structure of blocks is made on purpose, so that the structure of the index backup file is implicit, while the structure of the ecc file is explicit (it needs uncorrupted markers, which is a weak point that we try to address with the index backup file).
+                    # eg of two blocks: 10000008Aecceccecc2000000F2ecceccecc
+                    #
+                    # Read one index block
+                    curpos = dbidx.tell() # backup current position for error messages
+                    buf = dbidx.read(max_block_size)
+                    # Update progress bar
+                    bardisp.update(dbidx.tell()-bardisp.n)
+                    # If we have reached EOF, then we stop here
+                    if not buf: break
+
+                    # Else it's ok we have an index block, we process it
+                    idx_total += 1
+                    # Extract the marker's infos and the ecc
+                    marker_str = buf[:ecc_params_idx["message_size"]]
+                    ecc = buf[ecc_params_idx["message_size"]:]
+                    # Check if the marker's infos are corrupted, if yes, then we will try to fix that using the ecc
+                    if not ecc_manager_idx.check(marker_str, ecc):
+                        # Trying to fix the marker's infos using the ecc
+                        idx_corrupted += 1
+                        marker_repaired, repaired_ecc = ecc_manager_idx.decode(marker_str, ecc)
+                        # Repaired the marker's infos, all is good!
+                        if ecc_manager_idx.check(marker_repaired, repaired_ecc):
+                            marker_str = marker_repaired
+                            idx_corrected += 1
+                        # Else it's corrupted beyond repair, just skip
+                        else:
+                            ptee.write("\n- Index backup file: error on block starting at %i, corrupted and could not fix it. Skipping." % curpos)
+                            marker_str = None
+                            continue
+                    if not marker_str: continue
+
+                    # Repair ecc file's marker using our correct (or repaired) marker's infos
+                    marker_type = int(chr(marker_str[0]) if isinstance(marker_str[0], int) else marker_str[0]) # marker's type is always stored on the first byte/character
+                    marker_pos = struct.unpack('>Q', marker_str[1:]) # marker's position is encoded as a big-endian unsigned long long, in a 8 bytes/chars string
+                    db.seek(marker_pos[0]) # move the ecc reading cursor to the beginning of the marker
+                    current_marker = db.read(len(markers[marker_type-1])) # read the current marker (potentially corrupted)
+                    db.seek(marker_pos[0])
+                    if verbose:
+                        ptee.write("- Found marker by index file: type=%i content=" % (marker_type))
+                        ptee.write(db.read(len(markers[marker_type-1])+4))
+                        db.seek(marker_pos[0]) # replace the reading cursor back in place before the marker
+                    if current_marker != markers[marker_type-1]: # check if we really need to repair this marker
+                        # Rewrite the marker over the ecc file
+                        db.write(b(markers[marker_type-1]))
+                        markers_repaired[marker_type-1] += 1
+                    else:
+                        ptee.write("skipped, no need to repair")
+            # Done the index backup repair
+            if bardisp.n > bardisp.total: bardisp.n = bardisp.total # just a workaround in case there's one byte more than the predicted total
+            bardisp.close()
+            ptee.write("Done. Total: %i/%i markers repaired (%i entrymarkers and %i field_delim), %i indexes corrupted and %i indexes repaired (%i indexes lost).\n" % (markers_repaired[0]+markers_repaired[1], idx_total, markers_repaired[0], markers_repaired[1], idx_corrupted, idx_corrected, idx_corrupted-idx_corrected) )
+
+        # == Heuristical Greedy Hamming distance repair
+        # This is a heuristical (doesn't need any other file than the ecc file) 2-pass algorithm: the first pass tries to find the markers positions, and then the second pass simply reads the original ecc file and copies it while repairing the found markers.
+        # The first pass is obviously the most interesting, here's a description: we use a kind of greedy algorithm but with backtracking, meaning that we simply read through all the strings sequentially and just compare with the markers and compute the Hamming distance: if the Hamming distance gets below the threshold, we trigger the found marker flag. Then if the Hamming distance decreases, we save this marker position and disable the found marker flag. However, there can be false positives like this (eg, the marker is corrupted in the middle), so we have a backtracking mechanism: if a later string is found to have a Hamming distance that is below the threshold, then we check if the just previously found marker is in the range (ie, the new marker's position is smaller than the previous marker's length) and if the Hamming distance is smaller, then we replace the previous marker with the new marker's position, because the previous one was most likely a false positive.
+        # This method doesn't require any other file than the ecc file, but it may not work on ecc markers that are too much tampered, and if the detection threshold is too low or the markers are too small, there may be lots of false positives.
+        # So try to use long markers (consisting of many character, preferably an alternating pattern different than the null byte \x00) and a high enough detection threshold.
+        ptee.write("Using heuristics (Hamming distance) to fix markers with a threshold of %i%%, please wait..." % (round(distance_threshold*100, 0)) )
+
+        # Main loop for heuristical repair, try to find the substrings that minimize the hamming distance to one of the ecc markers
+        markers_repaired = [0] * len(markers) # stat counter
+        already_valid = 0 # stat counter
+        db.seek(0) # seek to the beginning of the database file
+        buf = 1 # init the buffer to 1 to initiate the while loop
+        markers_pos = [[] for i in _range(len(markers))] # will contain the list of positions where a corrupted marker has been detected (not valid markers, they will be skipped)
+        distance_thresholds = [round(len(x)*distance_threshold, 0) for x in markers] # calculate the number of characters maximum for distance
+        skip_until = -1 # when a valid marker (non corrupted) is found, we use this variable to skip to after the marker length (to avoid detecting partial parts of this marker, which will have a hamming distance even if the marker is completely valid because the reading window will be after the beginning of the marker)
+        bardisp = tqdm.tqdm(total=ecc_size, file=ptee, leave=True, desc='DBREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
+        while buf: # until we have walked through the whole ecc file
+            # Read a part of the ecc file into a buffer, this allows to process more quickly than just loading the size of a marker
+            curpos = db.tell() # keep the current reading position
+            buf = db.read(blocksize)
+            # Update progress bar
+            bardisp.update(db.tell()-bardisp.n)
+            if not buf: break # reached EOF? quitting here
+
+            # Scan the buffer, by splitting the buffer into substrings the length of the ecc markers
+            for i in _range(len(buf)-max(len(entrymarker),len(field_delim))):
+                # If we just came accross a non corrupted ecc marker, we skip until we are after this ecc marker (to avoid misdetections)
+                if i < skip_until: continue
+                # Compare each ecc marker type to this substring and compute the Hamming distance
+                for m in _range(len(markers)):
+                    d = hamming(b(buf[i:i+len(markers[m])]), b(markers[m])) # Compute the Hamming distance (simply the number of different characters)
+                    mcurpos = curpos+i # current absolute position of this ecc marker
+                    
+                    # If there's no difference, then it's a valid, non-corrupted ecc marker
+                    if d == 0:
+                        already_valid += 1 # stats...
+                        # If we previously wrongly detected a corrupted ecc marker near here, then it's probably a misdetection (because we just had a partial view on this marker until now), thus we just remove it from our list of markers to repair
+                        if len(markers_pos[m]) > 0 and (mcurpos - markers_pos[m][-1][0]) <= len(markers[m]): # to detect that, we just check if the latest marker to repair is near the current marker (if its position is at maximum the length of the marker). This works because in the other condition below, we update the latest marker to repair if we find another one with a lower hamming distance very near.
+                            del markers_pos[m][-1]
+                        # Skip scanning until we are after the current marker to avoid misdetections
+                        su = i+len(markers[m])
+                        if su > skip_until: skip_until = su # update with the biggest marker (because both markers can be detected here if the pattern is similar)
+                        break
+                    # Else there's a difference/distance but it's below the threshold: we have a corrupted marker!
+                    elif d > 0 and d <= distance_thresholds[m]:
+                        # Updating case: If the latest marker to repair is quite close to the current one, but the current detection has a lower distance, we probably are detecting the same marker but we are better positionned now, so we update the previous marker's position with this one now.
+                        if len(markers_pos[m]) > 0 and (mcurpos - markers_pos[m][-1][0]) <= len(markers[m]):
+                            if d < markers_pos[m][-1][1]: # Update only if the distance is less
+                                markers_pos[m][-1] = [mcurpos, d]
+                            else: # Else, we probably are detecting the same marker as the last detected one, but since our scanning window has moved forward, we have increased the distance. Just skip it, we should not repair at this position (else we will probably be overwriting over real content).
+                                continue
+                        # Adding case: Else we just add this marker as a new one to repair by appending to the list
+                        else:
+                            markers_pos[m].append([mcurpos, d])
+                    # Else the distance is too great for the threshold, it's not a marker at all, we go on to the next substring
+            if db.tell() < ecc_size: db.seek(db.tell()-max(len(entrymarker),len(field_delim)))
+        if bardisp.n > bardisp.total: bardisp.n = bardisp.total # just a workaround in case there's one byte more than the predicted total
+        bardisp.close()
+
+        # Committing the repair into the ecc file
+        for m in _range(len(markers)): # for each type of markers
+            marker = markers[m]
+            if len(markers_pos[m]) > 0: # If there is any detected marker to repair for this type
+                for pos in markers_pos[m]: # for each detected marker to repair, we rewrite it over into the file at the detected position
+                    if verbose: ptee.write("- Detected marker type %i at position %i with distance %i (%i%%): repairing." % (m+1, pos[0], pos[1], (float(pos[1])/len(markers[m]))*100) )
+                    db.seek(pos[0])
+                    db.write(b(marker))
+
+        #print(markers_pos)
+        ptee.write("Done. Hamming heuristic with threshold %i%% repaired %i entrymarkers and %i field_delim (%i total) and %i were already valid.\n" % (round(distance_threshold*100, 0), len(markers_pos[0]), len(markers_pos[1]), len(markers_pos[0])+len(markers_pos[1]), already_valid) )
+        ptee.close()
+        return 0
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":  # pragma: no cover
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/replication_repair.py` & `pyFileFixity-3.1.4/pyFileFixity/replication_repair.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,624 +1,624 @@
-#!/usr/bin/env python
-#
-# Replication repair
-# Copyright (C) 2015-2023 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#=================================
-#                   Replication repair
-#                by Stephen Larroque
-#                      License: MIT
-#              Creation date: 2015-11-16
-#=================================
-#
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, _range, _open_csv, _ord, b
-from . import rfigc # optional
-import shutil
-from lib.aux_funcs import recwalk, path2unix, fullpath, is_dir_or_file, is_dir, is_file, create_dir_if_not_exist
-import argparse
-import datetime, time
-import tqdm
-import itertools
-import math
-#import operator # to get the max out of a dict
-import csv # to process the database file from rfigc.py
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-#import pprint # Unnecessary, used only for debugging purposes
-
-
-
-#***********************************
-#     AUXILIARY FUNCTIONS
-#***********************************
-
-def relpath_posix(recwalk_result, pardir, fromwinpath=False):
-    ''' Helper function to convert all paths to relative posix like paths (to ease comparison) '''
-    return recwalk_result[0], path2unix(os.path.join(os.path.relpath(recwalk_result[0], pardir),recwalk_result[1]), nojoin=True, fromwinpath=fromwinpath)
-
-#def checkAllEqual(lst):
-#    return not lst or [lst[0]]*len(lst) == lst
-
-def sort_dict_of_paths(d):
-    """ Sort a dict containing paths parts (ie, paths divided in parts and stored as a list). Top paths will be given precedence over deeper paths. """
-    def nonesorter(a):
-        """ Make None a sortable type (necessary for Python 3) """
-        if not a:
-            return ['']
-        return a
-
-    # Find the path that is the deepest, and count the number of parts
-    max_rec = max(len(x) if x else 0 for x in d.values())
-    # Pad other paths with empty parts to fill in, so that all paths will have the same number of parts (necessary to compare correctly, else deeper paths may get precedence over top ones, since the folder name will be compared to filenames!)
-    for key in d.keys():
-        if d[key]:
-            d[key] = ['']*(max_rec-len(d[key])) + d[key]
-    # Sort the dict relatively to the paths alphabetical order
-    d_sort = sorted(d.items(), key=lambda x: nonesorter(x[1]))
-    return d_sort
-
-def sort_group(d, return_only_first=False):
-    ''' Sort a dictionary of relative paths and cluster equal paths together at the same time '''
-    # First, sort the paths in order (this must be a couple: (parent_dir, filename), so that there's no ambiguity because else a file at root will be considered as being after a folder/file since the ordering is done alphabetically without any notion of tree structure).
-    d_sort = sort_dict_of_paths(d)
-    # Pop the first item in the ordered list
-    base_elt = (-1, None)
-    while (base_elt[1] is None and d_sort):
-        base_elt = d_sort.pop(0)
-    # No element, then we just return
-    if base_elt[1] is None:
-        return None
-    # Else, we will now group equivalent files together (remember we are working on multiple directories, so we can have multiple equivalent relative filepaths, but of course the absolute filepaths are different).
-    else:
-        # Init by creating the first group and pushing the first ordered filepath into the first group
-        lst = []
-        lst.append([base_elt])
-        if d_sort:
-            # For each subsequent filepath
-            for elt in d_sort:
-                # If the filepath is not empty (generator died)
-                if elt[1] is not None:
-                    # If the filepath is the same to the latest grouped filepath, we add it to the same group
-                    if elt[1] == base_elt[1]:
-                        lst[-1].append(elt)
-                    # Else the filepath is different: we create a new group, add the filepath to this group, and replace the latest grouped filepath
-                    else:
-                        if return_only_first: break  # break here if we only need the first group
-                        lst.append([elt])
-                        base_elt = elt # replace the latest grouped filepath
-        return lst
-
-def majority_vote_byte_scan(relfilepath, fileslist, outpath, blocksize=65535, default_char_null=False):
-    '''Takes a list of files in string format representing the same data, and disambiguate by majority vote: for position in string, if the character is not the same accross all entries, we keep the major one. If none, it will be replaced by a null byte (because we can't know if any of the entries are correct about this character).
-    relfilepath is the filename or the relative file path relative to the parent directory (ie, this is the relative path so that we can compare the files from several directories).'''
-    # The idea of replication combined with ECC was a bit inspired by this paper: Friedman, Roy, Yoav Kantor, and Amir Kantor. "Combining Erasure-Code and Replication Redundancy Schemes for Increased Storage and Repair Efficiency in P2P Storage Systems.", 2013, Technion, Computer Science Department, Technical Report CS-2013-03
-    # But it is a very well known concept in redundancy engineering, usually called triple-modular redundancy (which is here extended to n-modular since we can supply any number of files we want, not just three).
-    # Preference in case of ambiguity is always given to the file of the first folder.
-
-    # Store the return code and return message in a variable, so that we can continue executing code to clean up and close open file handles, otherwise we will have a leak!
-    rtncodemesg = None
-
-    # Open all files and store file handles in a list
-    fileshandles = []
-    for filepath in fileslist:
-        if filepath:
-            # Already a file handle? Just store it in the fileshandles list
-            if hasattr(filepath, 'read'):
-                fileshandles.append(filepath)
-            # Else it's a string filepath, open the file
-            else:
-                fileshandles.append(open(filepath, 'rb'))
-
-    # Create and open output (merged) file, except if we were already given a file handle
-    if hasattr(outpath, 'write'):
-        outfile = outpath
-    else:
-        outpathfull = os.path.join(outpath, relfilepath)
-        pardir = os.path.dirname(outpathfull)
-        if not os.path.exists(pardir):
-            os.makedirs(pardir)
-        outfile = open(outpathfull, 'wb')
-
-    # Cannot vote if there's not at least 3 files!
-    # In this case, just copy the file from the first folder, verbatim
-    if len(fileshandles) < 3:
-        # If there's at least one input file, then copy it verbatim to the output folder
-        if fileshandles:
-            create_dir_if_not_exist(os.path.dirname(outpathfull))
-            buf = 1
-            while (buf):
-                buf = fileshandles[0].read()
-                outfile.write(buf)
-                outfile.flush()
-        rtncodemesg = (1, "Error with file %s: only %i copies available, cannot vote (need at least 3)! Copied the first file from the first folder, verbatim." % (relfilepath, len(fileshandles)))
-
-    if rtncodemesg is None:  # skip if error
-        # Main majority vote routine
-        errors = []
-        entries = [1]*len(fileshandles)  # init with 0 to start the while loop
-        while (entries.count(b('')) < len(fileshandles)):
-            final_entry = []
-            # Read a block from all input files into memory
-            for i in _range(len(fileshandles)):
-                entries[i] = fileshandles[i].read(blocksize)
-
-            # End of file for all files, we exit
-            if entries.count(b('')) == len(fileshandles):
-                break
-            # Else if there's only one file, just copy the file's content over
-            elif entries.count(b('')) == (len(fileshandles) - 1):
-                final_entry = entries[0]
-
-            # Else, do the majority vote
-            else:
-                # Walk along each column (imagine the strings being rows in a matrix, then we pick one column at each iteration = all characters at position i of each string), so that we can compare these characters easily
-                for i in _range(max(len(entry) for entry in entries)):
-                    hist = {} # kind of histogram, we just memorize how many times a character is presented at the position i in each string TODO: use collections.Counter instead of dict()?
-                    # Extract the character at position i of each string and compute the histogram at the same time (number of time this character appear among all strings at this position i)
-                    for entry in entries:
-                        # Check if we are not beyond the current entry's length
-                        if i < len(entry): # TODO: check this line, this should allow the vote to continue even if some files are shorter than others
-                            # Extract the character and use it to contribute to the histogram
-                            # TODO: add warning message when one file is not of the same size as the others
-                            key = str(_ord(entry[i])) # convert to the ascii value to avoid any funky problem with encoding in dict keys
-                            hist[key] = hist.get(key, 0) + 1 # increment histogram for this value. If it does not exists, use 0. (essentially equivalent to hist[key] += 1 but with exception management if key did not already exists)
-                    # If there's only one character (it's the same accross all strings at position i), then it's an exact match, we just save the character and we can skip to the next iteration
-                    if len(hist) == 1:
-                        final_entry.append(int(list(hist.keys())[0]))
-                        continue
-                    # Else, the character is different among different entries, we will pick the major one (mode)
-                    elif len(hist) > 1:
-                        # Sort the dict by value (and reverse because we want the most frequent first)
-                        skeys = sorted(hist, key=hist.get, reverse=True)
-                        # Ambiguity! If each entries present a different character (thus the major has only an occurrence of 1), then it's too ambiguous and we just set a null byte to signal that
-                        if hist[skeys[0]] == 1:
-                            if default_char_null:
-                                if default_char_null is True:
-                                    final_entry.append(0)
-                                else:
-                                    final_entry.append(_ord(default_char_null))
-                            else:
-                                # Use the entry of the first file that is still open
-                                first_char = ''
-                                for entry in entries:
-                                    # Found the first file that has a character at this position: store it and break loop
-                                    if i < len(entry):
-                                        first_char = entry[i]
-                                        break
-                                # Use this character in spite of ambiguity
-                                final_entry.append(_ord(first_char))
-                            errors.append(outfile.tell() + i) # Print an error indicating the characters that failed
-                        # Else if there is a tie (at least two characters appear with the same frequency), then we just pick one of them
-                        elif hist[skeys[0]] == hist[skeys[1]]:
-                            final_entry.append(int(skeys[0])) # TODO: find a way to account for both characters. Maybe return two different strings that will both have to be tested? (eg: maybe one has a tampered hash, both will be tested and if one correction pass the hash then it's ok we found the correct one)
-                        # Else we have a clear major character that appear in more entries than any other character, then we keep this one
-                        else:
-                            final_entry.append(int(skeys[0])) # alternative one-liner: max(hist.iteritems(), key=operator.itemgetter(1))[0]
-                        continue
-                # Concatenate to a string (this is faster than using a string from the start and concatenating at each iteration because Python strings are immutable so Python has to copy over the whole string, it's in O(n^2)
-                final_entry = ''.join([chr(x) for x in final_entry])
-
-            # Commit to output file
-            outfile.write(b(final_entry))
-            outfile.flush()
-
-        # Errors signaling
-        if errors:
-            error_msg = "Unrecoverable corruptions (because of ambiguity) in file %s on characters: %s." % (relfilepath, [hex(int(x)) for x in errors]) # Signal to user that this file has unrecoverable corruptions (he may try to fix the bits manually or with his own script)
-            rtncodemesg = (1, error_msg) # return an error
-
-    # CLEAN UP! Avoid tracemalloc warnings!
-    # Close all input files
-    for fh in fileshandles:
-        fh.close()
-    # Close output file
-    if outfile != outpath:  # close only if we were not given a file handle in the first place
-        outfile.flush()
-        outfile.close()
-
-    # Return!
-    # We choose to return a tuple, so that the calling function can know not only that there was an error, but what kind of error more precisely.
-    if rtncodemesg is None:
-        # No error? We return a no error tuple then!
-        return (0, None)
-    else:
-        # Else we return the error code and error message tuple we have
-        return rtncodemesg
-
-def synchronize_files(inputpaths, outpath, database=None, tqdm_bar=None, report_file=None, ptee=None, verbose=False):
-    ''' Main function to synchronize files contents by majority vote
-    The main job of this function is to walk through the input folders and align the files, so that we can compare every files across every folders, one by one.
-    The whole trick here is to align files, so that we don't need to memorize all the files in memory and we compare all equivalent files together: to do that, we ensure that we walk through the input directories in alphabetical order, and we pick the relative filepath at the top of the alphabetical order, this ensures the alignment of files between different folders, without memorizing  the whole trees structures.
-    '''
-    # (Generator) Files Synchronization Algorithm:
-    # Needs a function stable_dir_walking, which will walk through directories recursively but in always the same order on all platforms (same order for files but also for folders), whatever order it is, as long as it is stable.
-    # Until there's no file in any of the input folders to be processed:
-    # - curfiles <- load first file for each folder by using stable_dir_walking on each input folder.
-    # - curfiles_grouped <- group curfiles_ordered:
-    #    * curfiles_ordered <- order curfiles alphabetically (need to separate the relative parent directory and the filename, to account for both without ambiguity)
-    #    * curfiles_grouped <- empty list
-    #    * curfiles_grouped[0] = add first element in curfiles_ordered
-    #    * last_group = 0
-    #    * for every subsequent element nextelt in curfiles_ordered:
-    #        . if nextelt == curfiles_grouped[last_group][0]: add nextelt into curfiles_grouped[last_group] (the latest group in curfiles_grouped)
-    #        . else: create a new group in curfiles_grouped (last_group += 1) and add nextelt into curfiles_grouped[last_group]
-    # At this stage, curfiles_grouped[0] should contain a group of files with the same relative filepath from different input folders, and since we used stable_dir_walking, we are guaranteed that this file is the next to be processed in alphabetical order.
-    # - Majority vote byte-by-byte for each of curfiles_grouped[0], and output winning byte to the output file.
-    # - Update files list alignment: we will now ditch files in curfiles_grouped[0] from curfiles, and replace by the next files respectively from each respective folder. Since we processed in alphabetical (or whatever) order, the next loaded files will match the files in other curfiles_grouped groups that we could not process before.
-    # At this point (after the loop), all input files have been processed in order, without maintaining the whole files list in memory, just one file per input folder.
-
-    # Init files walking generator for each inputpaths
-    recgen = [recwalk(path, sorting=True) for path in inputpaths]
-    curfiles = {}
-    recgen_exhausted = {}
-    recgen_exhausted_count = 0
-    nbpaths = len(inputpaths)
-    retcode = 0
-
-    if not ptee: ptee = sys.stdout  # allow to specify an output to log, such as a StringIO as used in unit tests
-
-    # Open report file and write header
-    if report_file is not None:
-        rfile = _open_csv(report_file, 'w')
-        r_writer = csv.writer(rfile, delimiter='|', lineterminator='\n', quotechar='"')
-        r_header = ['filepath'] + ["dir%i" % (i+1) for i in _range(nbpaths)] + ['hash-correct', 'error_code', 'errors']
-        r_length = len(r_header)
-        r_writer.writerow(r_header)
-
-    # Initialization: load the first batch of files, one for each folder
-    for i in _range(len(recgen)):
-        recgen_exhausted[i] = False
-        try:
-            if curfiles.get(i, None) is None:
-                curfiles[i] = relpath_posix(next(recgen[i]), inputpaths[i])[1]
-        except StopIteration:
-            recgen_exhausted[i] = True
-            recgen_exhausted_count += 1
-
-    # Files lists alignment loop
-    while recgen_exhausted_count < nbpaths:
-        errcode = 0
-        errmsg = None
-
-        # Init a new report's row
-        if report_file: r_row = ["-"] * r_length
-
-        # -- Group equivalent relative filepaths together
-        #print curfiles # debug
-        curfiles_grouped = sort_group(curfiles, True)
-
-        # -- Extract first group of equivalent filepaths (this allows us to process with the same alphabetical order on all platforms)
-        # Note that the remaining files in other groups will be processed later, because their alphabetical order is higher to the first group, this means that the first group is to be processed now
-        to_process = curfiles_grouped[0]
-        #print to_process # debug
-
-        # -- Byte-by-byte majority vote on the first group of files
-        # Need the relative filepath also (note that there's only one since it's a group of equivalent relative filepaths, only the absolute path is different between files of a same group)
-        relfilepath = path2unix(os.path.join(*to_process[0][1]))
-        if report_file: r_row[0] = relfilepath
-        if verbose: ptee.write("- Processing file %s." % relfilepath)
-        # Generate output path
-        outpathfull = os.path.join(outpath, relfilepath)
-        create_dir_if_not_exist(os.path.dirname(outpathfull))
-        # Initialize the list of absolute filepaths
-        fileslist = []
-        for elt in to_process:
-            i = elt[0]
-            fileslist.append(os.path.join(inputpaths[i], os.path.join(*elt[1])))
-            if report_file: r_row[i+1] = 'X' # put an X in the report file below each folder that contains this file
-        # If there's only one file, just copy it over
-        if len(to_process) == 1:
-            shutil.copyfile(fileslist[0], outpathfull)
-            id = to_process[0][0]
-            if report_file: r_row[id+1] = 'O'
-        # Else, merge by majority vote
-        else:
-            # Before-merge check using rfigc database, if provided
-            # If one of the files in the input folders is already correct, just copy it over
-            correct_file = None
-            if database:
-                for id, filepath in enumerate(fileslist):
-                    if rfigc.main("-i \"%s\" -d \"%s\" -m --silent" % (filepath, database)) == 0:
-                        correct_file = filepath
-                        correct_id = to_process[id][0]
-                        break
-
-            # If one correct file was found, copy it over
-            if correct_file:
-                create_dir_if_not_exist(os.path.dirname(outpathfull))
-                shutil.copyfile(correct_file, outpathfull)
-                if report_file:
-                    r_row[correct_id+1] = "O"
-                    r_row[-3] = "OK"
-            # Else, we need to do the majority vote merge
-            else:
-                # Do the majority vote merge
-                errcode, errmsg = majority_vote_byte_scan(relfilepath, fileslist, outpath)
-
-        # After-merge/move check using rfigc database, if provided
-        if database:
-            if rfigc.main("-i \"%s\" -d \"%s\" -m --silent" % (outpathfull, database)) == 1:
-                errcode = 1
-                r_row[-3] = "KO"
-                if not errmsg: errmsg = ''
-                errmsg += " File could not be totally repaired according to rfigc database."
-            else:
-                if report_file:
-                    r_row[-3] = "OK"
-                    if errmsg: errmsg += " But merged file is correct according to rfigc database."
-
-        # Display errors if any
-        if errcode:
-            if report_file:
-                r_row[-2] = "KO"
-                r_row[-1] = errmsg
-            ptee.write(errmsg)
-            retcode = 1
-        else:
-            if report_file: r_row[-2] = "OK"
-
-        # Save current report's row
-        if report_file:
-            r_writer.writerow(r_row)
-
-        # -- Update files lists alignment (ie, retrieve new files but while trying to keep the alignment)
-        for elt in to_process:  # for files of the first group (the ones we processed)
-            i = elt[0]
-            # Walk their respective folders and load up the next file
-            try:
-                if not recgen_exhausted.get(i, False):
-                    curfiles[i] = relpath_posix(next(recgen[i]), inputpaths[i])[1]
-            # If there's no file left in this folder, mark this input folder as exhausted and continue with the others
-            except StopIteration:
-                curfiles[i] = None
-                recgen_exhausted[i] = True
-                recgen_exhausted_count += 1
-        if tqdm_bar: tqdm_bar.update()
-    if tqdm_bar: tqdm_bar.close()
-
-    # Closing report file
-    if report_file:
-        # Write list of directories and legend
-        rfile.write("\n=> Input directories:")
-        for id, ipath in enumerate(inputpaths):
-            rfile.write("\n\t- dir%i = %s" % ((id+1), ipath))
-        rfile.write("\n=> Output directory: %s" % outpath)
-        rfile.write("\n=> Legend: X=existing/selected for majority vote, O=only used this file, - = not existing, OK = check correct, KO = check incorrect (file was not recovered)\n")
-        # Close the report file handle
-        rfile.close()
-
-    return retcode
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-#@conditional_decorator(check_gui_arg(), gooey.Gooey) # alternative to AutoGooey which also correctly works
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Replication Repair
-Description: Given a set of directories (or files), try to repair your files by scanning each byte, cast a majority vote among all copies, and then output the winning byte. This process is usually called triple-modular redundancy (but here it should be called n-modular redundancy since you can use as many copies as you have).
-It is recommended for long term storage to store several copies of your files on different storage mediums. Everything's fine until all your copies are partially corrupted. In this case, this script can help you, by taking advantage of your multiple copies, without requiring a pregenerated ecc file. Just specify the path to every copies, and the script will try to recover them.
-Replication can repair exactly r-2 errors using majority vote (you need at least 2 blocks for majority vote to work), where r is the number of replications: if r=3, you get a redundancy rate of 1/3, if r=4, rate is 2/4, etc.
-This script can also take advantage of a database generated by rfigc.py to make sure that the recovered files are correct, or to select files that are already correct.
-
-Note: in case the end result is not what you expected, you can try a different order of input directories: in case of ambiguity, the first input folder has precedence over subsequent folders.
-Note2: in case some files with the same names are of different length, the merging will continue until the longest file is exhausted.
-Note3: last modification date is not (yet) accounted for.
-    '''
-    ep = '''Use --gui as the first argument to use with a GUI (via Gooey).
-'''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-        widget_multidir = {"widget": "MultiDirChooser"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-        widget_multidir = {}
-
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='"/path/to/copy1/" "/path/to/copy2/" "etc."', type=is_dir_or_file, nargs='+', required=True,
-                        help='Specify the paths to every copies you have (minimum 3 copies, else it won\'t work!). Can be folders or files (if you want to repair only one file). Order matters: in case of ambiguity, the first folder where the file exists will be chosen.', **widget_multidir)
-    main_parser.add_argument('-o', '--output', metavar='/ouput/folder/', nargs=1, required=True,
-                        help='Where the recovered files will be stored.', **widget_dir)
-
-    # Optional general arguments
-    main_parser.add_argument('-d', '--database', metavar='database.csv', type=is_file, required=False,
-                        help='Path to a previously generated rfigc.py database. If provided, this will be used to check that the repaired files are correct (and also to find already correct files in copies).', **widget_file)
-    main_parser.add_argument('-r', '--report', metavar='/some/folder/report.csv', type=str, required=False,
-                        help='Save all results of the repair process in a report file, with detailed descriptions of ambiguous repairs (ie, when majority vote came to a draw).', **widget_filesave)
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
-                        help='Force overwriting the output folder even if it already exists.')
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set variables from arguments
-    inputpaths = [fullpath(x) for x in args.input] # path to the files to repair (ie, paths to all the different copies the user has)
-    outputpath = fullpath(args.output[0])
-    force = args.force
-    verbose = args.verbose
-    silent = args.silent
-
-    if len(inputpaths) < 3:
-        raise Exception('Need at least 3 copies to do a replication repair/majority vote!')
-
-    #if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
-        #rootfolderpath = os.path.dirname(inputpath)
-
-    report_file = None
-    if args.report: report_file = os.path.basename(fullpath(args.report))
-    database = None
-    if args.database: database = args.database
-
-    # -- Checking arguments
-    if os.path.exists(outputpath) and not force:
-        raise NameError('Specified output path %s already exists! Use --force if you want to overwrite.' % outputpath)
-
-    if database and not os.path.isfile(database):
-        raise NameError('Specified rfigc database file %s does not exist!' % database)
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log[0], 'a', nostdout=silent)
-        #sys.stdout = Tee(args.log[0], 'a')
-        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-
-    # == PROCESSING BRANCHING == #
-
-    # == Precomputation of ecc file size
-    # Precomputing is important so that the user can know what size to expect before starting (and how much time it will take...).
-    filescount = 0
-    sizetotal = 0
-    sizeheaders = 0
-    visitedfiles = {}
-    ptee.write("Precomputing list of files and predicted statistics...")
-    prebar = tqdm.tqdm(file=ptee, disable=silent)
-    for inputpath in inputpaths:
-        for (dirpath, filename) in recwalk(inputpath):
-            # Get full absolute filepath
-            filepath = os.path.join(dirpath, filename)
-            relfilepath = path2unix(os.path.relpath(filepath, inputpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
-
-            # Only increase the files count if we didn't see this file before
-            if not visitedfiles.get(relfilepath, None):
-                # Counting the total number of files we will process (so that we can show a progress bar with ETA)
-                filescount = filescount + 1
-                # Add the file to the list of already visited files
-                visitedfiles[relfilepath] = True
-                # Get the current file's size
-                size = os.stat(filepath).st_size
-                # Compute total size of all files
-                sizetotal = sizetotal + size
-            prebar.update()
-    prebar.close()
-    ptee.write("Precomputing done.")
-
-    # == Majority vote repair
-    # For each folder, align the files lists and then majority vote over each byte to repair
-    ptee.write("====================================")
-    ptee.write("Replication repair, started on %s" % datetime.datetime.now().isoformat())
-    ptee.write("====================================")
-
-    # Prepare progress bar if necessary
-    if silent:
-        tqdm_bar = None
-    else:
-        tqdm_bar = tqdm.tqdm(total=filescount, file=ptee, leave=True, unit="files")
-    # Call the main function to synchronize files using majority vote
-    errcode = synchronize_files(inputpaths, outputpath, database=database, tqdm_bar=tqdm_bar, report_file=report_file, ptee=ptee, verbose=verbose)
-    #ptee.write("All done! Stats:\n- Total files processed: %i\n- Total files corrupted: %i\n- Total files repaired completely: %i\n- Total files repaired partially: %i\n- Total files corrupted but not repaired at all: %i\n- Total files skipped: %i" % (files_count, files_corrupted, files_repaired_completely, files_repaired_partially, files_corrupted - (files_repaired_partially + files_repaired_completely), files_skipped) )
-    if tqdm_bar: tqdm_bar.close()
-    ptee.write("All done!")
-    if report_file: ptee.write("Saved replication repair results in report file: %s" % report_file)
-    ptee.close()
-    return errcode
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":  # pragma: no cover
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Replication repair
+# Copyright (C) 2015-2023 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#=================================
+#                   Replication repair
+#                by Stephen Larroque
+#                      License: MIT
+#              Creation date: 2015-11-16
+#=================================
+#
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, _range, _open_csv, _ord, b
+from . import rfigc # optional
+import shutil
+from lib.aux_funcs import recwalk, path2unix, fullpath, is_dir_or_file, is_dir, is_file, create_dir_if_not_exist
+import argparse
+import datetime, time
+import tqdm
+import itertools
+import math
+#import operator # to get the max out of a dict
+import csv # to process the database file from rfigc.py
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+#import pprint # Unnecessary, used only for debugging purposes
+
+
+
+#***********************************
+#     AUXILIARY FUNCTIONS
+#***********************************
+
+def relpath_posix(recwalk_result, pardir, fromwinpath=False):
+    ''' Helper function to convert all paths to relative posix like paths (to ease comparison) '''
+    return recwalk_result[0], path2unix(os.path.join(os.path.relpath(recwalk_result[0], pardir),recwalk_result[1]), nojoin=True, fromwinpath=fromwinpath)
+
+#def checkAllEqual(lst):
+#    return not lst or [lst[0]]*len(lst) == lst
+
+def sort_dict_of_paths(d):
+    """ Sort a dict containing paths parts (ie, paths divided in parts and stored as a list). Top paths will be given precedence over deeper paths. """
+    def nonesorter(a):
+        """ Make None a sortable type (necessary for Python 3) """
+        if not a:
+            return ['']
+        return a
+
+    # Find the path that is the deepest, and count the number of parts
+    max_rec = max(len(x) if x else 0 for x in d.values())
+    # Pad other paths with empty parts to fill in, so that all paths will have the same number of parts (necessary to compare correctly, else deeper paths may get precedence over top ones, since the folder name will be compared to filenames!)
+    for key in d.keys():
+        if d[key]:
+            d[key] = ['']*(max_rec-len(d[key])) + d[key]
+    # Sort the dict relatively to the paths alphabetical order
+    d_sort = sorted(d.items(), key=lambda x: nonesorter(x[1]))
+    return d_sort
+
+def sort_group(d, return_only_first=False):
+    ''' Sort a dictionary of relative paths and cluster equal paths together at the same time '''
+    # First, sort the paths in order (this must be a couple: (parent_dir, filename), so that there's no ambiguity because else a file at root will be considered as being after a folder/file since the ordering is done alphabetically without any notion of tree structure).
+    d_sort = sort_dict_of_paths(d)
+    # Pop the first item in the ordered list
+    base_elt = (-1, None)
+    while (base_elt[1] is None and d_sort):
+        base_elt = d_sort.pop(0)
+    # No element, then we just return
+    if base_elt[1] is None:
+        return None
+    # Else, we will now group equivalent files together (remember we are working on multiple directories, so we can have multiple equivalent relative filepaths, but of course the absolute filepaths are different).
+    else:
+        # Init by creating the first group and pushing the first ordered filepath into the first group
+        lst = []
+        lst.append([base_elt])
+        if d_sort:
+            # For each subsequent filepath
+            for elt in d_sort:
+                # If the filepath is not empty (generator died)
+                if elt[1] is not None:
+                    # If the filepath is the same to the latest grouped filepath, we add it to the same group
+                    if elt[1] == base_elt[1]:
+                        lst[-1].append(elt)
+                    # Else the filepath is different: we create a new group, add the filepath to this group, and replace the latest grouped filepath
+                    else:
+                        if return_only_first: break  # break here if we only need the first group
+                        lst.append([elt])
+                        base_elt = elt # replace the latest grouped filepath
+        return lst
+
+def majority_vote_byte_scan(relfilepath, fileslist, outpath, blocksize=65535, default_char_null=False):
+    '''Takes a list of files in string format representing the same data, and disambiguate by majority vote: for position in string, if the character is not the same accross all entries, we keep the major one. If none, it will be replaced by a null byte (because we can't know if any of the entries are correct about this character).
+    relfilepath is the filename or the relative file path relative to the parent directory (ie, this is the relative path so that we can compare the files from several directories).'''
+    # The idea of replication combined with ECC was a bit inspired by this paper: Friedman, Roy, Yoav Kantor, and Amir Kantor. "Combining Erasure-Code and Replication Redundancy Schemes for Increased Storage and Repair Efficiency in P2P Storage Systems.", 2013, Technion, Computer Science Department, Technical Report CS-2013-03
+    # But it is a very well known concept in redundancy engineering, usually called triple-modular redundancy (which is here extended to n-modular since we can supply any number of files we want, not just three).
+    # Preference in case of ambiguity is always given to the file of the first folder.
+
+    # Store the return code and return message in a variable, so that we can continue executing code to clean up and close open file handles, otherwise we will have a leak!
+    rtncodemesg = None
+
+    # Open all files and store file handles in a list
+    fileshandles = []
+    for filepath in fileslist:
+        if filepath:
+            # Already a file handle? Just store it in the fileshandles list
+            if hasattr(filepath, 'read'):
+                fileshandles.append(filepath)
+            # Else it's a string filepath, open the file
+            else:
+                fileshandles.append(open(filepath, 'rb'))
+
+    # Create and open output (merged) file, except if we were already given a file handle
+    if hasattr(outpath, 'write'):
+        outfile = outpath
+    else:
+        outpathfull = os.path.join(outpath, relfilepath)
+        pardir = os.path.dirname(outpathfull)
+        if not os.path.exists(pardir):
+            os.makedirs(pardir)
+        outfile = open(outpathfull, 'wb')
+
+    # Cannot vote if there's not at least 3 files!
+    # In this case, just copy the file from the first folder, verbatim
+    if len(fileshandles) < 3:
+        # If there's at least one input file, then copy it verbatim to the output folder
+        if fileshandles:
+            create_dir_if_not_exist(os.path.dirname(outpathfull))
+            buf = 1
+            while (buf):
+                buf = fileshandles[0].read()
+                outfile.write(buf)
+                outfile.flush()
+        rtncodemesg = (1, "Error with file %s: only %i copies available, cannot vote (need at least 3)! Copied the first file from the first folder, verbatim." % (relfilepath, len(fileshandles)))
+
+    if rtncodemesg is None:  # skip if error
+        # Main majority vote routine
+        errors = []
+        entries = [1]*len(fileshandles)  # init with 0 to start the while loop
+        while (entries.count(b('')) < len(fileshandles)):
+            final_entry = []
+            # Read a block from all input files into memory
+            for i in _range(len(fileshandles)):
+                entries[i] = fileshandles[i].read(blocksize)
+
+            # End of file for all files, we exit
+            if entries.count(b('')) == len(fileshandles):
+                break
+            # Else if there's only one file, just copy the file's content over
+            elif entries.count(b('')) == (len(fileshandles) - 1):
+                final_entry = entries[0]
+
+            # Else, do the majority vote
+            else:
+                # Walk along each column (imagine the strings being rows in a matrix, then we pick one column at each iteration = all characters at position i of each string), so that we can compare these characters easily
+                for i in _range(max(len(entry) for entry in entries)):
+                    hist = {} # kind of histogram, we just memorize how many times a character is presented at the position i in each string TODO: use collections.Counter instead of dict()?
+                    # Extract the character at position i of each string and compute the histogram at the same time (number of time this character appear among all strings at this position i)
+                    for entry in entries:
+                        # Check if we are not beyond the current entry's length
+                        if i < len(entry): # TODO: check this line, this should allow the vote to continue even if some files are shorter than others
+                            # Extract the character and use it to contribute to the histogram
+                            # TODO: add warning message when one file is not of the same size as the others
+                            key = str(_ord(entry[i])) # convert to the ascii value to avoid any funky problem with encoding in dict keys
+                            hist[key] = hist.get(key, 0) + 1 # increment histogram for this value. If it does not exists, use 0. (essentially equivalent to hist[key] += 1 but with exception management if key did not already exists)
+                    # If there's only one character (it's the same accross all strings at position i), then it's an exact match, we just save the character and we can skip to the next iteration
+                    if len(hist) == 1:
+                        final_entry.append(int(list(hist.keys())[0]))
+                        continue
+                    # Else, the character is different among different entries, we will pick the major one (mode)
+                    elif len(hist) > 1:
+                        # Sort the dict by value (and reverse because we want the most frequent first)
+                        skeys = sorted(hist, key=hist.get, reverse=True)
+                        # Ambiguity! If each entries present a different character (thus the major has only an occurrence of 1), then it's too ambiguous and we just set a null byte to signal that
+                        if hist[skeys[0]] == 1:
+                            if default_char_null:
+                                if default_char_null is True:
+                                    final_entry.append(0)
+                                else:
+                                    final_entry.append(_ord(default_char_null))
+                            else:
+                                # Use the entry of the first file that is still open
+                                first_char = ''
+                                for entry in entries:
+                                    # Found the first file that has a character at this position: store it and break loop
+                                    if i < len(entry):
+                                        first_char = entry[i]
+                                        break
+                                # Use this character in spite of ambiguity
+                                final_entry.append(_ord(first_char))
+                            errors.append(outfile.tell() + i) # Print an error indicating the characters that failed
+                        # Else if there is a tie (at least two characters appear with the same frequency), then we just pick one of them
+                        elif hist[skeys[0]] == hist[skeys[1]]:
+                            final_entry.append(int(skeys[0])) # TODO: find a way to account for both characters. Maybe return two different strings that will both have to be tested? (eg: maybe one has a tampered hash, both will be tested and if one correction pass the hash then it's ok we found the correct one)
+                        # Else we have a clear major character that appear in more entries than any other character, then we keep this one
+                        else:
+                            final_entry.append(int(skeys[0])) # alternative one-liner: max(hist.iteritems(), key=operator.itemgetter(1))[0]
+                        continue
+                # Concatenate to a string (this is faster than using a string from the start and concatenating at each iteration because Python strings are immutable so Python has to copy over the whole string, it's in O(n^2)
+                final_entry = ''.join([chr(x) for x in final_entry])
+
+            # Commit to output file
+            outfile.write(b(final_entry))
+            outfile.flush()
+
+        # Errors signaling
+        if errors:
+            error_msg = "Unrecoverable corruptions (because of ambiguity) in file %s on characters: %s." % (relfilepath, [hex(int(x)) for x in errors]) # Signal to user that this file has unrecoverable corruptions (he may try to fix the bits manually or with his own script)
+            rtncodemesg = (1, error_msg) # return an error
+
+    # CLEAN UP! Avoid tracemalloc warnings!
+    # Close all input files
+    for fh in fileshandles:
+        fh.close()
+    # Close output file
+    if outfile != outpath:  # close only if we were not given a file handle in the first place
+        outfile.flush()
+        outfile.close()
+
+    # Return!
+    # We choose to return a tuple, so that the calling function can know not only that there was an error, but what kind of error more precisely.
+    if rtncodemesg is None:
+        # No error? We return a no error tuple then!
+        return (0, None)
+    else:
+        # Else we return the error code and error message tuple we have
+        return rtncodemesg
+
+def synchronize_files(inputpaths, outpath, database=None, tqdm_bar=None, report_file=None, ptee=None, verbose=False):
+    ''' Main function to synchronize files contents by majority vote
+    The main job of this function is to walk through the input folders and align the files, so that we can compare every files across every folders, one by one.
+    The whole trick here is to align files, so that we don't need to memorize all the files in memory and we compare all equivalent files together: to do that, we ensure that we walk through the input directories in alphabetical order, and we pick the relative filepath at the top of the alphabetical order, this ensures the alignment of files between different folders, without memorizing  the whole trees structures.
+    '''
+    # (Generator) Files Synchronization Algorithm:
+    # Needs a function stable_dir_walking, which will walk through directories recursively but in always the same order on all platforms (same order for files but also for folders), whatever order it is, as long as it is stable.
+    # Until there's no file in any of the input folders to be processed:
+    # - curfiles <- load first file for each folder by using stable_dir_walking on each input folder.
+    # - curfiles_grouped <- group curfiles_ordered:
+    #    * curfiles_ordered <- order curfiles alphabetically (need to separate the relative parent directory and the filename, to account for both without ambiguity)
+    #    * curfiles_grouped <- empty list
+    #    * curfiles_grouped[0] = add first element in curfiles_ordered
+    #    * last_group = 0
+    #    * for every subsequent element nextelt in curfiles_ordered:
+    #        . if nextelt == curfiles_grouped[last_group][0]: add nextelt into curfiles_grouped[last_group] (the latest group in curfiles_grouped)
+    #        . else: create a new group in curfiles_grouped (last_group += 1) and add nextelt into curfiles_grouped[last_group]
+    # At this stage, curfiles_grouped[0] should contain a group of files with the same relative filepath from different input folders, and since we used stable_dir_walking, we are guaranteed that this file is the next to be processed in alphabetical order.
+    # - Majority vote byte-by-byte for each of curfiles_grouped[0], and output winning byte to the output file.
+    # - Update files list alignment: we will now ditch files in curfiles_grouped[0] from curfiles, and replace by the next files respectively from each respective folder. Since we processed in alphabetical (or whatever) order, the next loaded files will match the files in other curfiles_grouped groups that we could not process before.
+    # At this point (after the loop), all input files have been processed in order, without maintaining the whole files list in memory, just one file per input folder.
+
+    # Init files walking generator for each inputpaths
+    recgen = [recwalk(path, sorting=True) for path in inputpaths]
+    curfiles = {}
+    recgen_exhausted = {}
+    recgen_exhausted_count = 0
+    nbpaths = len(inputpaths)
+    retcode = 0
+
+    if not ptee: ptee = sys.stdout  # allow to specify an output to log, such as a StringIO as used in unit tests
+
+    # Open report file and write header
+    if report_file is not None:
+        rfile = _open_csv(report_file, 'w')
+        r_writer = csv.writer(rfile, delimiter='|', lineterminator='\n', quotechar='"')
+        r_header = ['filepath'] + ["dir%i" % (i+1) for i in _range(nbpaths)] + ['hash-correct', 'error_code', 'errors']
+        r_length = len(r_header)
+        r_writer.writerow(r_header)
+
+    # Initialization: load the first batch of files, one for each folder
+    for i in _range(len(recgen)):
+        recgen_exhausted[i] = False
+        try:
+            if curfiles.get(i, None) is None:
+                curfiles[i] = relpath_posix(next(recgen[i]), inputpaths[i])[1]
+        except StopIteration:
+            recgen_exhausted[i] = True
+            recgen_exhausted_count += 1
+
+    # Files lists alignment loop
+    while recgen_exhausted_count < nbpaths:
+        errcode = 0
+        errmsg = None
+
+        # Init a new report's row
+        if report_file: r_row = ["-"] * r_length
+
+        # -- Group equivalent relative filepaths together
+        #print curfiles # debug
+        curfiles_grouped = sort_group(curfiles, True)
+
+        # -- Extract first group of equivalent filepaths (this allows us to process with the same alphabetical order on all platforms)
+        # Note that the remaining files in other groups will be processed later, because their alphabetical order is higher to the first group, this means that the first group is to be processed now
+        to_process = curfiles_grouped[0]
+        #print to_process # debug
+
+        # -- Byte-by-byte majority vote on the first group of files
+        # Need the relative filepath also (note that there's only one since it's a group of equivalent relative filepaths, only the absolute path is different between files of a same group)
+        relfilepath = path2unix(os.path.join(*to_process[0][1]))
+        if report_file: r_row[0] = relfilepath
+        if verbose: ptee.write("- Processing file %s." % relfilepath)
+        # Generate output path
+        outpathfull = os.path.join(outpath, relfilepath)
+        create_dir_if_not_exist(os.path.dirname(outpathfull))
+        # Initialize the list of absolute filepaths
+        fileslist = []
+        for elt in to_process:
+            i = elt[0]
+            fileslist.append(os.path.join(inputpaths[i], os.path.join(*elt[1])))
+            if report_file: r_row[i+1] = 'X' # put an X in the report file below each folder that contains this file
+        # If there's only one file, just copy it over
+        if len(to_process) == 1:
+            shutil.copyfile(fileslist[0], outpathfull)
+            id = to_process[0][0]
+            if report_file: r_row[id+1] = 'O'
+        # Else, merge by majority vote
+        else:
+            # Before-merge check using rfigc database, if provided
+            # If one of the files in the input folders is already correct, just copy it over
+            correct_file = None
+            if database:
+                for id, filepath in enumerate(fileslist):
+                    if rfigc.main("-i \"%s\" -d \"%s\" -m --silent" % (filepath, database)) == 0:
+                        correct_file = filepath
+                        correct_id = to_process[id][0]
+                        break
+
+            # If one correct file was found, copy it over
+            if correct_file:
+                create_dir_if_not_exist(os.path.dirname(outpathfull))
+                shutil.copyfile(correct_file, outpathfull)
+                if report_file:
+                    r_row[correct_id+1] = "O"
+                    r_row[-3] = "OK"
+            # Else, we need to do the majority vote merge
+            else:
+                # Do the majority vote merge
+                errcode, errmsg = majority_vote_byte_scan(relfilepath, fileslist, outpath)
+
+        # After-merge/move check using rfigc database, if provided
+        if database:
+            if rfigc.main("-i \"%s\" -d \"%s\" -m --silent" % (outpathfull, database)) == 1:
+                errcode = 1
+                r_row[-3] = "KO"
+                if not errmsg: errmsg = ''
+                errmsg += " File could not be totally repaired according to rfigc database."
+            else:
+                if report_file:
+                    r_row[-3] = "OK"
+                    if errmsg: errmsg += " But merged file is correct according to rfigc database."
+
+        # Display errors if any
+        if errcode:
+            if report_file:
+                r_row[-2] = "KO"
+                r_row[-1] = errmsg
+            ptee.write(errmsg)
+            retcode = 1
+        else:
+            if report_file: r_row[-2] = "OK"
+
+        # Save current report's row
+        if report_file:
+            r_writer.writerow(r_row)
+
+        # -- Update files lists alignment (ie, retrieve new files but while trying to keep the alignment)
+        for elt in to_process:  # for files of the first group (the ones we processed)
+            i = elt[0]
+            # Walk their respective folders and load up the next file
+            try:
+                if not recgen_exhausted.get(i, False):
+                    curfiles[i] = relpath_posix(next(recgen[i]), inputpaths[i])[1]
+            # If there's no file left in this folder, mark this input folder as exhausted and continue with the others
+            except StopIteration:
+                curfiles[i] = None
+                recgen_exhausted[i] = True
+                recgen_exhausted_count += 1
+        if tqdm_bar: tqdm_bar.update()
+    if tqdm_bar: tqdm_bar.close()
+
+    # Closing report file
+    if report_file:
+        # Write list of directories and legend
+        rfile.write("\n=> Input directories:")
+        for id, ipath in enumerate(inputpaths):
+            rfile.write("\n\t- dir%i = %s" % ((id+1), ipath))
+        rfile.write("\n=> Output directory: %s" % outpath)
+        rfile.write("\n=> Legend: X=existing/selected for majority vote, O=only used this file, - = not existing, OK = check correct, KO = check incorrect (file was not recovered)\n")
+        # Close the report file handle
+        rfile.close()
+
+    return retcode
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+#@conditional_decorator(check_gui_arg(), gooey.Gooey) # alternative to AutoGooey which also correctly works
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Replication Repair
+Description: Given a set of directories (or files), try to repair your files by scanning each byte, cast a majority vote among all copies, and then output the winning byte. This process is usually called triple-modular redundancy (but here it should be called n-modular redundancy since you can use as many copies as you have).
+It is recommended for long term storage to store several copies of your files on different storage mediums. Everything's fine until all your copies are partially corrupted. In this case, this script can help you, by taking advantage of your multiple copies, without requiring a pregenerated ecc file. Just specify the path to every copies, and the script will try to recover them.
+Replication can repair exactly r-2 errors using majority vote (you need at least 2 blocks for majority vote to work), where r is the number of replications: if r=3, you get a redundancy rate of 1/3, if r=4, rate is 2/4, etc.
+This script can also take advantage of a database generated by rfigc.py to make sure that the recovered files are correct, or to select files that are already correct.
+
+Note: in case the end result is not what you expected, you can try a different order of input directories: in case of ambiguity, the first input folder has precedence over subsequent folders.
+Note2: in case some files with the same names are of different length, the merging will continue until the longest file is exhausted.
+Note3: last modification date is not (yet) accounted for.
+    '''
+    ep = '''Use --gui as the first argument to use with a GUI (via Gooey).
+'''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+        widget_multidir = {"widget": "MultiDirChooser"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+        widget_multidir = {}
+
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='"/path/to/copy1/" "/path/to/copy2/" "etc."', type=is_dir_or_file, nargs='+', required=True,
+                        help='Specify the paths to every copies you have (minimum 3 copies, else it won\'t work!). Can be folders or files (if you want to repair only one file). Order matters: in case of ambiguity, the first folder where the file exists will be chosen.', **widget_multidir)
+    main_parser.add_argument('-o', '--output', metavar='/ouput/folder/', nargs=1, required=True,
+                        help='Where the recovered files will be stored.', **widget_dir)
+
+    # Optional general arguments
+    main_parser.add_argument('-d', '--database', metavar='database.csv', type=is_file, required=False,
+                        help='Path to a previously generated rfigc.py database. If provided, this will be used to check that the repaired files are correct (and also to find already correct files in copies).', **widget_file)
+    main_parser.add_argument('-r', '--report', metavar='/some/folder/report.csv', type=str, required=False,
+                        help='Save all results of the repair process in a report file, with detailed descriptions of ambiguous repairs (ie, when majority vote came to a draw).', **widget_filesave)
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
+                        help='Force overwriting the output folder even if it already exists.')
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set variables from arguments
+    inputpaths = [fullpath(x) for x in args.input] # path to the files to repair (ie, paths to all the different copies the user has)
+    outputpath = fullpath(args.output[0])
+    force = args.force
+    verbose = args.verbose
+    silent = args.silent
+
+    if len(inputpaths) < 3:
+        raise Exception('Need at least 3 copies to do a replication repair/majority vote!')
+
+    #if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
+        #rootfolderpath = os.path.dirname(inputpath)
+
+    report_file = None
+    if args.report: report_file = os.path.basename(fullpath(args.report))
+    database = None
+    if args.database: database = args.database
+
+    # -- Checking arguments
+    if os.path.exists(outputpath) and not force:
+        raise NameError('Specified output path %s already exists! Use --force if you want to overwrite.' % outputpath)
+
+    if database and not os.path.isfile(database):
+        raise NameError('Specified rfigc database file %s does not exist!' % database)
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log[0], 'a', nostdout=silent)
+        #sys.stdout = Tee(args.log[0], 'a')
+        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+
+    # == PROCESSING BRANCHING == #
+
+    # == Precomputation of ecc file size
+    # Precomputing is important so that the user can know what size to expect before starting (and how much time it will take...).
+    filescount = 0
+    sizetotal = 0
+    sizeheaders = 0
+    visitedfiles = {}
+    ptee.write("Precomputing list of files and predicted statistics...")
+    prebar = tqdm.tqdm(file=ptee, disable=silent)
+    for inputpath in inputpaths:
+        for (dirpath, filename) in recwalk(inputpath):
+            # Get full absolute filepath
+            filepath = os.path.join(dirpath, filename)
+            relfilepath = path2unix(os.path.relpath(filepath, inputpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
+
+            # Only increase the files count if we didn't see this file before
+            if not visitedfiles.get(relfilepath, None):
+                # Counting the total number of files we will process (so that we can show a progress bar with ETA)
+                filescount = filescount + 1
+                # Add the file to the list of already visited files
+                visitedfiles[relfilepath] = True
+                # Get the current file's size
+                size = os.stat(filepath).st_size
+                # Compute total size of all files
+                sizetotal = sizetotal + size
+            prebar.update()
+    prebar.close()
+    ptee.write("Precomputing done.")
+
+    # == Majority vote repair
+    # For each folder, align the files lists and then majority vote over each byte to repair
+    ptee.write("====================================")
+    ptee.write("Replication repair, started on %s" % datetime.datetime.now().isoformat())
+    ptee.write("====================================")
+
+    # Prepare progress bar if necessary
+    if silent:
+        tqdm_bar = None
+    else:
+        tqdm_bar = tqdm.tqdm(total=filescount, file=ptee, leave=True, unit="files")
+    # Call the main function to synchronize files using majority vote
+    errcode = synchronize_files(inputpaths, outputpath, database=database, tqdm_bar=tqdm_bar, report_file=report_file, ptee=ptee, verbose=verbose)
+    #ptee.write("All done! Stats:\n- Total files processed: %i\n- Total files corrupted: %i\n- Total files repaired completely: %i\n- Total files repaired partially: %i\n- Total files corrupted but not repaired at all: %i\n- Total files skipped: %i" % (files_count, files_corrupted, files_repaired_completely, files_repaired_partially, files_corrupted - (files_repaired_partially + files_repaired_completely), files_skipped) )
+    if tqdm_bar: tqdm_bar.close()
+    ptee.write("All done!")
+    if report_file: ptee.write("Saved replication repair results in report file: %s" % report_file)
+    ptee.close()
+    return errcode
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":  # pragma: no cover
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/resiliency_tester.py` & `pyFileFixity-3.1.4/pyFileFixity/resiliency_tester.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,583 +1,583 @@
-#!/usr/bin/env python
-#
-# Resiliency tester
-# Copyright (C) 2015-2023 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#=================================
-#                 Resiliency Tester
-#                by Stephen Larroque
-#                      License: MIT
-#              Creation date: 2015-11-28
-#=================================
-#
-
-#from __future__ import division  # if I try to optimize this script, Py3 division is required (eg, in Cython)
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, _range, _StringIO, _izip
-import subprocess # to execute commands
-import itertools
-from lib.aux_funcs import recwalk, path2unix, fullpath, is_dir_or_file, is_dir, is_file, fullpath, copy_any, create_dir_if_not_exist, remove_if_exist
-import argparse
-import datetime, time
-import tqdm
-#import operator # to get the max out of a dict
-import csv # to process the database file from rfigc.py
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-from collections import OrderedDict
-#import pprint # Unnecessary, used only for debugging purposes
-
-# To read the config file
-try:
-    import configparser as ConfigParser
-except ImportError:
-    import ConfigParser
-
-
-
-#***********************************
-#     AUXILIARY FUNCTIONS
-#***********************************
-
-######## Config parsing and command execution ########
-
-def parse_configfile(filepath, comments_prefix='#'):
-    '''
-    Parse a makefile to find commands and substitute variables. Expects a
-    makefile with only aliases and a line return between each command.
-
-    Returns a dict, with a list of commands for each alias.
-    '''
-
-    # -- Parsing the Makefile using ConfigParser
-    # Adding a fake section to make the Makefile a valid Ini file
-    ini_str = '[root]\n'
-    if not hasattr(filepath, 'read'):
-        fd = open(filepath, 'r')
-    else:
-        fd = filepath
-    ini_str = ini_str + fd.read().replace('\t@', '\t').\
-        replace('\t+', '\t').replace('\tmake ', '\t')
-    if fd != filepath: fd.close()
-    ini_fp = _StringIO(ini_str)
-    # Parse using ConfigParser
-    config = ConfigParser.RawConfigParser()
-    config.read_file(ini_fp)
-    # Fetch the list of aliases
-    aliases = config.options('root')
-
-    # -- Extracting commands for each alias
-    commands = {}
-    for alias in aliases:
-        commands[alias] = []
-        # strip the first line return, and then split by any line return
-        for cmd in config.get('root', alias).lstrip('\n').split('\n'):
-            if not cmd.startswith(comments_prefix):
-                commands[alias].append(cmd)
-    return commands
-
-def get_filename_no_ext(filepath):
-    return os.path.splitext(os.path.basename(filepath))[0]
-
-def interpolate_dict(s, interp_args={}):
-    # String interpolate the command to fill in the variables if necessary
-    return s.format(**interp_args)
-
-def execute_command(cmd, ptee=None, verbose=False):  # pragma: no cover
-    # Parse string in a shell-like fashion
-    # (incl quoted strings and comments)
-    parsed_cmd = shlex.split(cmd, comments=True)
-    # Execute command if not empty (ie, not just a comment)
-    if parsed_cmd:
-        if verbose:
-            print("Running command: " + cmd)
-        #if ptee: ptee.disable()
-        if parsed_cmd[0].lower().startswith('python'):
-            mod = import_module(get_filename_no_ext(parsed_cmd[1]))
-            if mod and hasattr(mod, 'main'):
-                mod.main(parsed_cmd[2:])
-            else:
-                return subprocess.check_call(parsed_cmd)
-        else:
-            # Launch the command and wait to finish (synchronized call)
-            return subprocess.check_call(parsed_cmd)
-        #if ptee: ptee.enable()
-
-######## Diff functions ########
-
-def diff_bytes_files(path1, path2, blocksize=65535, startpos1=0, startpos2=0):
-    """ Compare two files byte-wise, and return the total number of differing bytes """
-    diff_count = 0
-    total_size = 0
-    with open(path1, 'rb') as f1, open(path2, 'rb') as f2:
-        buf1 = 1
-        buf2 = 1
-        f1.seek(startpos1)
-        f2.seek(startpos2)
-        while 1:
-            buf1 = f1.read(blocksize)
-            buf2 = f2.read(blocksize)
-            if (buf1 and not buf2) or (buf2 and not buf1):
-                # Reached end of file or the content is different, then add the rest of the other file as a difference
-                size_remaining = 0
-                if not buf2:
-                    curpos = f1.tell()
-                    size_remaining = f1.fstat(f1.fileno()).st_size - curpos - len(buf1)
-                elif not buf1:
-                    curpos = f2.tell()
-                    size_remaining = f2.fstat(f2.fileno()).st_size - curpos - len(buf2)
-                diff_count += size_remaining
-                total_size += size_remaining
-                break
-            elif (not buf1 and not buf2):
-                # End of file for both files
-                break
-            else:
-                for char1, char2 in _izip(buf1, buf2):
-                    if char1 != char2:
-                        diff_count += 1
-                    total_size += 1
-    return diff_count, total_size
-
-def diff_bytes_dir(dir1, dir2):
-    total_diff = 0
-    total_size = 0
-    for dirpath, filepath in recwalk(dir1):
-        filepath1 = os.path.join(dirpath, filepath)
-        relpath = os.path.relpath(filepath1, dir1)
-        filepath2 = os.path.join(dir2, relpath)
-        if not os.path.exists(filepath2):
-            fsize = os.stat(filepath1).st_size
-            total_diff += fsize
-            total_size += fsize
-        else:
-            fdiff, fsize = diff_bytes_files(filepath1, filepath2)
-            total_diff += fdiff
-            total_size += fsize
-    return total_diff, total_size
-
-def diff_count_files(path1, path2, blocksize=65535, startpos1=0, startpos2=0):
-    """ Return True if both files are identical, False otherwise """
-    flag = True
-    with open(path1, 'rb') as f1, open(path2, 'rb') as f2:
-        buf1 = 1
-        buf2 = 1
-        f1.seek(startpos1)
-        f2.seek(startpos2)
-        while 1:
-            buf1 = f1.read(blocksize)
-            buf2 = f2.read(blocksize)
-            if buf1 != buf2 or (buf1 and not buf2) or (buf2 and not buf1):
-                # Reached end of file or the content is different, then return false
-                flag = False
-                break
-            elif (not buf1 and not buf2):
-                # End of file for both files
-                break
-    return flag
-    #return filecmp.cmp(path1, path2, shallow=False)  # does not work on Travis
-
-def diff_count_dir(dir1, dir2):
-    diff_count = 0
-    total_count = 0
-    for dirpath, filepath in recwalk(dir1):
-        filepath1 = os.path.join(dirpath, filepath)
-        relpath = os.path.relpath(filepath1, dir1)
-        filepath2 = os.path.join(dir2, relpath)
-        if not os.path.exists(filepath2):
-            diff_count += 1
-        else:
-            if not diff_count_files(filepath1, filepath2):
-                diff_count += 1
-        total_count += 1
-    return diff_count, total_count
-
-######## Stats functions ########
-
-def compute_repair_power(new_error, old_error):
-    if old_error != 0.0:
-        return (1 - (new_error / old_error)) * 100
-    else:
-        return new_error
-
-def compute_diff_stats(orig, dir1, dir2):
-    stats = OrderedDict()
-    stats["diff_bytes"] = diff_bytes_dir(orig, dir2)
-    stats["diff_count"] = diff_count_dir(orig, dir2)
-    stats["diff_bytes_prev"] = diff_bytes_dir(dir1, dir2)
-    stats["diff_count_prev"] = diff_count_dir(dir1, dir2)
-    stats["error"] = stats["diff_bytes"][0] / stats["diff_bytes"][1] * 100
-    stats["repair_power"] = 0 # can only be defined when compared to the previous stage's stats
-    return stats
-
-def compute_all_diff_stats(commands, origpath, tamperdir, repairdir, finalrepairdir):
-    stats = OrderedDict() # keep order just to more easily print the stats aftewards
-
-    # compute diff for each steps:
-    # - origpath/tampered
-    stats["tamper"] = compute_diff_stats(origpath, origpath, tamperdir)
-
-    # - tampered/result0,1,... in loop
-    indir = tamperdir
-    for i in _range(len(commands["repair"])):
-        outdir = "%s%i" % (repairdir, i)
-        stats["repair%i"%i] = compute_diff_stats(origpath, indir, outdir)
-        if i == 0:
-            stats["repair0"]["repair_power"] = compute_repair_power(stats["repair0"]["error"], stats["tamper"]["error"])
-        else:
-            stats["repair%i"%i]["repair_power"] = compute_repair_power(stats["repair%i"%i]["error"], stats["repair%i"%(i-1)]["error"])
-        indir = outdir
-
-    # - final diff origpath/tampered + tampered/finalresult
-    stats["final"] = compute_diff_stats(origpath, origpath, finalrepairdir)
-    stats["final"]["repair_power"] = compute_repair_power(stats["final"]["error"], stats["tamper"]["error"])
-
-    return stats
-
-def pretty_print_stats(stat):  # pragma: no cover
-    out = ''
-    for key, value in stat.items():
-        if key == 'diff_bytes':
-            out += "\t- Differing bytes from original: %i/%i\n" % (value[0], value[1])
-        elif key == 'diff_bytes_prev':
-            out += "\t- Differing bytes from previous stage: %i/%i\n" % (value[0], value[1])
-        elif key == 'diff_count':
-            out += "\t- Differing files from original: %i/%i\n" % (value[0], value[1])
-        elif key == 'diff_count_prev':
-            out += "\t- Differing files from previous stage: %i/%i\n" % (value[0], value[1])
-        elif key == 'error':
-            out += "\t- Error rate (from original): %g\n" % value
-        elif key == 'repair_power':
-            out += "\t- Repair power (from original): %g\n" % value
-        else:
-            out += "\t- %s: %s\n" % (key, value)
-    return out
-
-def stats_running_average(stats, new_stats, weight):
-    """ Compute the running average between two stats dictionaries """
-    def running_average(old, new, weight):
-        return (old*weight + new) / (weight+1)
-
-    nstats = {}
-    for stage in stats.keys():
-        if stage in new_stats:
-            nstats[stage] = {}
-            for key in stats[stage].keys():
-                if key in new_stats[stage]:
-                    # List
-                    if isinstance(stats[stage][key], (list, tuple)):
-                        nstats[stage][key] = [running_average(x, y, weight) for x,y in _izip(stats[stage][key], new_stats[stage][key])]
-                        #nstats[stage][key] = [[x, y] for x,y in _izip(stats[stage][key], new_stats[stage][key])]
-
-                    # Scalar
-                    elif not hasattr(stats[stage][key], '__len__') and (not isinstance(stats[stage][key], _str)):
-                        nstats[stage][key] = running_average(stats[stage][key], new_stats[stage][key], weight)
-
-    return nstats
-
-######## Helper functions (importing, path construction, etc.) ########
-
-def import_module(module_name):  # pragma: no cover
-    ''' Reliable import, courtesy of Armin Ronacher '''
-    try:
-        __import__(module_name)
-    except ImportError:
-        exc_type, exc_value, tb_root = sys.exc_info()
-        tb = tb_root
-        while tb is not None:
-            if tb.tb_frame.f_globals.get('__name__') == module_name:
-                raise(exc_type, exc_value, tb_root)
-            tb = tb.tb_next
-        return None
-    return sys.modules[module_name]
-
-def get_dbfile(dbdir, id):
-    return fullpath(os.path.join(dbdir, 'db%i' % id))
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-#@conditional_decorator(check_gui_arg(), gooey.Gooey) # alternative to AutoGooey which also correctly works
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Resiliency Tester
-Description: Given a directory and a configuration file (containing the commands to execute before and after file tampering), this script will generate a testing tree, where the files will be corrupted randomly and then the supplied repair commands will be executed, and repair stats will be computed at each step (for each stage and repair commands).
-
-The testing process works in stages:
-1- Before_tamper stage: Run preparatory commands before tampering (useful to generate ecc/database files).
-2- Tamper stage: Tamper the files and/or databases.
-3- After_tamper stage: Run after tampering commands, aka preparatory commands before repair stage.
-4- Repair stage: Run repair commands, each repair command reusing the files generated (partially repaired) by the previous stage. This is indeed your repair workchain that you define here.
-5- Statistics are generated for each stage.
-
-Note that the original files are never tampered, we tamper only the copy we did inside the test folder.
-Also note that the test folder will not be removed at the end, so that you can see for yourself the files resulting of each stage, and eventually use other tools to compute additional stats.
-    '''
-    ep = '''Use --gui as the first argument to use with a GUI (via Gooey).
-'''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-        widget_multidir = {"widget": "MultiDirChooser"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-        widget_multidir = {}
-
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='"/path/to/original/files/"', type=is_dir_or_file, nargs=1, required=True,
-                        help='Specify the path to the directory containing the sample data.', **widget_dir)
-    main_parser.add_argument('-o', '--output', metavar='/test/folder/', nargs=1, required=True,
-                        help='Path to the test folder that will be created to store temporary test files.', **widget_dir)
-    main_parser.add_argument('-c', '--config', metavar='/some/folder/config.txt', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
-                        help='Path to the configuration file (containing the commands to execute, Makefile format). Possible entries: before_tamper, tamper, after_tamper, repair. Note that you can use a few special tags to trigger string interpolation: {inputdir}, {dbdir}, {outputdir}.', **widget_file)
-
-    # Optional arguments
-    main_parser.add_argument('-p', '--parallel', action='store_true', required=False,
-                        help='If true, repair commands will be run on the tampered files, not on the previous repair results. Useful if you want to try different strategies/commands/programs. By default, false, thus the repair commands will take advantage of the results of previous repair commands.')
-    main_parser.add_argument('-m', '--multiple', metavar=1, type=int, default=1, required=False,
-                        help='Run multiple times the resiliency test, and average the stats.', **widget_text)
-
-    # Optional general arguments
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
-                        help='Force overwriting the output folder even if it already exists.')
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set variables from arguments
-    origpath = fullpath(args.input[0]) # path to the input directory (where the original, sample data is)
-    outputpath = fullpath(args.output[0])
-    configfile = fullpath(args.config[0])
-    parallel = args.parallel
-    multiple = args.multiple
-    force = args.force
-    verbose = args.verbose
-    silent = args.silent
-
-    #if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
-        #rootfolderpath = os.path.dirname(inputpath)
-
-    # -- Checking arguments
-    if not os.path.isdir(origpath):
-        raise NameError("Input path needs to be a directory!")
-
-    if not os.path.exists(configfile):
-        raise NameError("Please provide a configuration file in order to run a test!")
-    else:
-        commands = parse_configfile(configfile)
-
-    if os.path.exists(outputpath) and not force:
-        raise NameError("Specified test folder (output path) %s already exists! Use --force to overwrite this directory." % outputpath)
-    else:
-        remove_if_exist(outputpath)
-
-    if multiple < 1:
-        multiple = 1
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log[0], 'a', nostdout=silent)
-        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-    # == PROCESSING BRANCHING == #
-
-    # == Main branch
-    ptee.write("====================================")
-    ptee.write("Resiliency tester, started on %s" % datetime.datetime.now().isoformat())
-    ptee.write("====================================")
-    
-    ptee.write("Testing folder %s into test folder %s for %i run(s)." % (origpath, outputpath, multiple))
-
-    fstats = {}
-    for m in _range(multiple):
-        run_nb = m + 1
-
-        ptee.write("===== Resiliency tester: starting run %i =====" % run_nb)
-
-        # -- Define directories tree for this test run
-        # testpath is the basepath for the current run
-        # Generate a specific subdirectory for the current run
-        testpath = os.path.join(outputpath, "run%i" % run_nb)
-        dbdir = fullpath(os.path.join(testpath, "db"))
-        origdbdir = fullpath(os.path.join(testpath, "origdb"))
-        tamperdir = fullpath(os.path.join(testpath, "tampered"))
-        repairdir = fullpath(os.path.join(testpath, "repair"))
-
-        # == START TEST RUN
-        # Create test folder
-        create_dir_if_not_exist(testpath)
-
-        # Before tampering
-        ptee.write("=== BEFORE TAMPERING ===")
-        create_dir_if_not_exist(dbdir)
-        for i, cmd in enumerate(commands["before_tamper"]):
-            scmd = interpolate_dict(cmd, interp_args={"inputdir": origpath, "dbdir": dbdir})
-            ptee.write("Executing command: %s" % scmd)
-            execute_command(scmd, ptee=ptee)
-        copy_any(dbdir, origdbdir) # make a copy because we may tamper the db files
-
-        # Tampering
-        ptee.write("=== TAMPERING ===")
-        copy_any(origpath, tamperdir)
-        for i, cmd in enumerate(commands["tamper"]):
-            scmd = interpolate_dict(cmd, interp_args={"inputdir": tamperdir, "dbdir": dbdir})
-            ptee.write("- RTEST: Executing command: %s" % scmd)
-            execute_command(scmd, ptee=ptee)
-
-        # After tampering
-        ptee.write("=== AFTER TAMPERING ===")
-        for i, cmd in enumerate(commands["after_tamper"]):
-            scmd = interpolate_dict(cmd, interp_args={"inputdir": tamperdir, "dbdir": dbdir})
-            ptee.write("- RTEST: Executing command: %s" % scmd)
-            execute_command(scmd, ptee=ptee)
-
-        # Repairing
-        ptee.write("=== REPAIRING ===")
-        indir = tamperdir
-        finalrepairdir = ''
-        for i, cmd in enumerate(commands["repair"]):
-            outdir = "%s%i" % (repairdir, i)
-            scmd = interpolate_dict(cmd, interp_args={"inputdir": indir, "dbdir": dbdir, "outputdir": outdir})
-            ptee.write("- RTEST: Executing command: %s" % scmd)
-            create_dir_if_not_exist(outdir)
-            execute_command(scmd, ptee=ptee)
-            copy_any(indir, outdir, only_missing=True) # copy the files that did not need any repair (or could not be repaired at all!)
-            finalrepairdir = outdir
-            # If parallel, do not reuse the previous repair resulting files, repair from the tampered files directly
-            if not parallel: indir = outdir
-
-        # Stats
-        stats = compute_all_diff_stats(commands, origpath, tamperdir, repairdir, finalrepairdir)
-        ptee.write("========== Resiliency tester results for run %i ==========" % run_nb)
-        for key, stat in stats.items():
-            ptee.write("=> Stage: %s" % key)
-            ptee.write(pretty_print_stats(stat))
-
-        if run_nb == 1:
-            fstats = stats
-        else:
-            fstats = stats_running_average(fstats, stats, run_nb-1)
-
-    ptee.write("============================")
-    ptee.write("RESILIENCY TESTER FINAL AVERAGED RESULTS OVER %i RUNS" % multiple)
-    ptee.write("============================")
-    for key, stat in fstats.items():
-        ptee.write("=> Stage: %s" % key)
-        ptee.write(pretty_print_stats(stat))
-
-    # Shutting down
-    ptee.close()
-    # Completely repair all the files? Return OK
-    if stats["final"]["error"] == 0:
-        return 0
-    else:
-        return 1
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":  # pragma: no cover
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Resiliency tester
+# Copyright (C) 2015-2023 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#=================================
+#                 Resiliency Tester
+#                by Stephen Larroque
+#                      License: MIT
+#              Creation date: 2015-11-28
+#=================================
+#
+
+#from __future__ import division  # if I try to optimize this script, Py3 division is required (eg, in Cython)
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, _range, _StringIO, _izip
+import subprocess # to execute commands
+import itertools
+from lib.aux_funcs import recwalk, path2unix, fullpath, is_dir_or_file, is_dir, is_file, fullpath, copy_any, create_dir_if_not_exist, remove_if_exist
+import argparse
+import datetime, time
+import tqdm
+#import operator # to get the max out of a dict
+import csv # to process the database file from rfigc.py
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+from collections import OrderedDict
+#import pprint # Unnecessary, used only for debugging purposes
+
+# To read the config file
+try:
+    import configparser as ConfigParser
+except ImportError:
+    import ConfigParser
+
+
+
+#***********************************
+#     AUXILIARY FUNCTIONS
+#***********************************
+
+######## Config parsing and command execution ########
+
+def parse_configfile(filepath, comments_prefix='#'):
+    '''
+    Parse a makefile to find commands and substitute variables. Expects a
+    makefile with only aliases and a line return between each command.
+
+    Returns a dict, with a list of commands for each alias.
+    '''
+
+    # -- Parsing the Makefile using ConfigParser
+    # Adding a fake section to make the Makefile a valid Ini file
+    ini_str = '[root]\n'
+    if not hasattr(filepath, 'read'):
+        fd = open(filepath, 'r')
+    else:
+        fd = filepath
+    ini_str = ini_str + fd.read().replace('\t@', '\t').\
+        replace('\t+', '\t').replace('\tmake ', '\t')
+    if fd != filepath: fd.close()
+    ini_fp = _StringIO(ini_str)
+    # Parse using ConfigParser
+    config = ConfigParser.RawConfigParser()
+    config.read_file(ini_fp)
+    # Fetch the list of aliases
+    aliases = config.options('root')
+
+    # -- Extracting commands for each alias
+    commands = {}
+    for alias in aliases:
+        commands[alias] = []
+        # strip the first line return, and then split by any line return
+        for cmd in config.get('root', alias).lstrip('\n').split('\n'):
+            if not cmd.startswith(comments_prefix):
+                commands[alias].append(cmd)
+    return commands
+
+def get_filename_no_ext(filepath):
+    return os.path.splitext(os.path.basename(filepath))[0]
+
+def interpolate_dict(s, interp_args={}):
+    # String interpolate the command to fill in the variables if necessary
+    return s.format(**interp_args)
+
+def execute_command(cmd, ptee=None, verbose=False):  # pragma: no cover
+    # Parse string in a shell-like fashion
+    # (incl quoted strings and comments)
+    parsed_cmd = shlex.split(cmd, comments=True)
+    # Execute command if not empty (ie, not just a comment)
+    if parsed_cmd:
+        if verbose:
+            print("Running command: " + cmd)
+        #if ptee: ptee.disable()
+        if parsed_cmd[0].lower().startswith('python'):
+            mod = import_module(get_filename_no_ext(parsed_cmd[1]))
+            if mod and hasattr(mod, 'main'):
+                mod.main(parsed_cmd[2:])
+            else:
+                return subprocess.check_call(parsed_cmd)
+        else:
+            # Launch the command and wait to finish (synchronized call)
+            return subprocess.check_call(parsed_cmd)
+        #if ptee: ptee.enable()
+
+######## Diff functions ########
+
+def diff_bytes_files(path1, path2, blocksize=65535, startpos1=0, startpos2=0):
+    """ Compare two files byte-wise, and return the total number of differing bytes """
+    diff_count = 0
+    total_size = 0
+    with open(path1, 'rb') as f1, open(path2, 'rb') as f2:
+        buf1 = 1
+        buf2 = 1
+        f1.seek(startpos1)
+        f2.seek(startpos2)
+        while 1:
+            buf1 = f1.read(blocksize)
+            buf2 = f2.read(blocksize)
+            if (buf1 and not buf2) or (buf2 and not buf1):
+                # Reached end of file or the content is different, then add the rest of the other file as a difference
+                size_remaining = 0
+                if not buf2:
+                    curpos = f1.tell()
+                    size_remaining = f1.fstat(f1.fileno()).st_size - curpos - len(buf1)
+                elif not buf1:
+                    curpos = f2.tell()
+                    size_remaining = f2.fstat(f2.fileno()).st_size - curpos - len(buf2)
+                diff_count += size_remaining
+                total_size += size_remaining
+                break
+            elif (not buf1 and not buf2):
+                # End of file for both files
+                break
+            else:
+                for char1, char2 in _izip(buf1, buf2):
+                    if char1 != char2:
+                        diff_count += 1
+                    total_size += 1
+    return diff_count, total_size
+
+def diff_bytes_dir(dir1, dir2):
+    total_diff = 0
+    total_size = 0
+    for dirpath, filepath in recwalk(dir1):
+        filepath1 = os.path.join(dirpath, filepath)
+        relpath = os.path.relpath(filepath1, dir1)
+        filepath2 = os.path.join(dir2, relpath)
+        if not os.path.exists(filepath2):
+            fsize = os.stat(filepath1).st_size
+            total_diff += fsize
+            total_size += fsize
+        else:
+            fdiff, fsize = diff_bytes_files(filepath1, filepath2)
+            total_diff += fdiff
+            total_size += fsize
+    return total_diff, total_size
+
+def diff_count_files(path1, path2, blocksize=65535, startpos1=0, startpos2=0):
+    """ Return True if both files are identical, False otherwise """
+    flag = True
+    with open(path1, 'rb') as f1, open(path2, 'rb') as f2:
+        buf1 = 1
+        buf2 = 1
+        f1.seek(startpos1)
+        f2.seek(startpos2)
+        while 1:
+            buf1 = f1.read(blocksize)
+            buf2 = f2.read(blocksize)
+            if buf1 != buf2 or (buf1 and not buf2) or (buf2 and not buf1):
+                # Reached end of file or the content is different, then return false
+                flag = False
+                break
+            elif (not buf1 and not buf2):
+                # End of file for both files
+                break
+    return flag
+    #return filecmp.cmp(path1, path2, shallow=False)  # does not work on Travis
+
+def diff_count_dir(dir1, dir2):
+    diff_count = 0
+    total_count = 0
+    for dirpath, filepath in recwalk(dir1):
+        filepath1 = os.path.join(dirpath, filepath)
+        relpath = os.path.relpath(filepath1, dir1)
+        filepath2 = os.path.join(dir2, relpath)
+        if not os.path.exists(filepath2):
+            diff_count += 1
+        else:
+            if not diff_count_files(filepath1, filepath2):
+                diff_count += 1
+        total_count += 1
+    return diff_count, total_count
+
+######## Stats functions ########
+
+def compute_repair_power(new_error, old_error):
+    if old_error != 0.0:
+        return (1 - (new_error / old_error)) * 100
+    else:
+        return new_error
+
+def compute_diff_stats(orig, dir1, dir2):
+    stats = OrderedDict()
+    stats["diff_bytes"] = diff_bytes_dir(orig, dir2)
+    stats["diff_count"] = diff_count_dir(orig, dir2)
+    stats["diff_bytes_prev"] = diff_bytes_dir(dir1, dir2)
+    stats["diff_count_prev"] = diff_count_dir(dir1, dir2)
+    stats["error"] = stats["diff_bytes"][0] / stats["diff_bytes"][1] * 100
+    stats["repair_power"] = 0 # can only be defined when compared to the previous stage's stats
+    return stats
+
+def compute_all_diff_stats(commands, origpath, tamperdir, repairdir, finalrepairdir):
+    stats = OrderedDict() # keep order just to more easily print the stats aftewards
+
+    # compute diff for each steps:
+    # - origpath/tampered
+    stats["tamper"] = compute_diff_stats(origpath, origpath, tamperdir)
+
+    # - tampered/result0,1,... in loop
+    indir = tamperdir
+    for i in _range(len(commands["repair"])):
+        outdir = "%s%i" % (repairdir, i)
+        stats["repair%i"%i] = compute_diff_stats(origpath, indir, outdir)
+        if i == 0:
+            stats["repair0"]["repair_power"] = compute_repair_power(stats["repair0"]["error"], stats["tamper"]["error"])
+        else:
+            stats["repair%i"%i]["repair_power"] = compute_repair_power(stats["repair%i"%i]["error"], stats["repair%i"%(i-1)]["error"])
+        indir = outdir
+
+    # - final diff origpath/tampered + tampered/finalresult
+    stats["final"] = compute_diff_stats(origpath, origpath, finalrepairdir)
+    stats["final"]["repair_power"] = compute_repair_power(stats["final"]["error"], stats["tamper"]["error"])
+
+    return stats
+
+def pretty_print_stats(stat):  # pragma: no cover
+    out = ''
+    for key, value in stat.items():
+        if key == 'diff_bytes':
+            out += "\t- Differing bytes from original: %i/%i\n" % (value[0], value[1])
+        elif key == 'diff_bytes_prev':
+            out += "\t- Differing bytes from previous stage: %i/%i\n" % (value[0], value[1])
+        elif key == 'diff_count':
+            out += "\t- Differing files from original: %i/%i\n" % (value[0], value[1])
+        elif key == 'diff_count_prev':
+            out += "\t- Differing files from previous stage: %i/%i\n" % (value[0], value[1])
+        elif key == 'error':
+            out += "\t- Error rate (from original): %g\n" % value
+        elif key == 'repair_power':
+            out += "\t- Repair power (from original): %g\n" % value
+        else:
+            out += "\t- %s: %s\n" % (key, value)
+    return out
+
+def stats_running_average(stats, new_stats, weight):
+    """ Compute the running average between two stats dictionaries """
+    def running_average(old, new, weight):
+        return (old*weight + new) / (weight+1)
+
+    nstats = {}
+    for stage in stats.keys():
+        if stage in new_stats:
+            nstats[stage] = {}
+            for key in stats[stage].keys():
+                if key in new_stats[stage]:
+                    # List
+                    if isinstance(stats[stage][key], (list, tuple)):
+                        nstats[stage][key] = [running_average(x, y, weight) for x,y in _izip(stats[stage][key], new_stats[stage][key])]
+                        #nstats[stage][key] = [[x, y] for x,y in _izip(stats[stage][key], new_stats[stage][key])]
+
+                    # Scalar
+                    elif not hasattr(stats[stage][key], '__len__') and (not isinstance(stats[stage][key], _str)):
+                        nstats[stage][key] = running_average(stats[stage][key], new_stats[stage][key], weight)
+
+    return nstats
+
+######## Helper functions (importing, path construction, etc.) ########
+
+def import_module(module_name):  # pragma: no cover
+    ''' Reliable import, courtesy of Armin Ronacher '''
+    try:
+        __import__(module_name)
+    except ImportError:
+        exc_type, exc_value, tb_root = sys.exc_info()
+        tb = tb_root
+        while tb is not None:
+            if tb.tb_frame.f_globals.get('__name__') == module_name:
+                raise(exc_type, exc_value, tb_root)
+            tb = tb.tb_next
+        return None
+    return sys.modules[module_name]
+
+def get_dbfile(dbdir, id):
+    return fullpath(os.path.join(dbdir, 'db%i' % id))
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+#@conditional_decorator(check_gui_arg(), gooey.Gooey) # alternative to AutoGooey which also correctly works
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Resiliency Tester
+Description: Given a directory and a configuration file (containing the commands to execute before and after file tampering), this script will generate a testing tree, where the files will be corrupted randomly and then the supplied repair commands will be executed, and repair stats will be computed at each step (for each stage and repair commands).
+
+The testing process works in stages:
+1- Before_tamper stage: Run preparatory commands before tampering (useful to generate ecc/database files).
+2- Tamper stage: Tamper the files and/or databases.
+3- After_tamper stage: Run after tampering commands, aka preparatory commands before repair stage.
+4- Repair stage: Run repair commands, each repair command reusing the files generated (partially repaired) by the previous stage. This is indeed your repair workchain that you define here.
+5- Statistics are generated for each stage.
+
+Note that the original files are never tampered, we tamper only the copy we did inside the test folder.
+Also note that the test folder will not be removed at the end, so that you can see for yourself the files resulting of each stage, and eventually use other tools to compute additional stats.
+    '''
+    ep = '''Use --gui as the first argument to use with a GUI (via Gooey).
+'''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+        widget_multidir = {"widget": "MultiDirChooser"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+        widget_multidir = {}
+
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='"/path/to/original/files/"', type=is_dir_or_file, nargs=1, required=True,
+                        help='Specify the path to the directory containing the sample data.', **widget_dir)
+    main_parser.add_argument('-o', '--output', metavar='/test/folder/', nargs=1, required=True,
+                        help='Path to the test folder that will be created to store temporary test files.', **widget_dir)
+    main_parser.add_argument('-c', '--config', metavar='/some/folder/config.txt', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
+                        help='Path to the configuration file (containing the commands to execute, Makefile format). Possible entries: before_tamper, tamper, after_tamper, repair. Note that you can use a few special tags to trigger string interpolation: {inputdir}, {dbdir}, {outputdir}.', **widget_file)
+
+    # Optional arguments
+    main_parser.add_argument('-p', '--parallel', action='store_true', required=False,
+                        help='If true, repair commands will be run on the tampered files, not on the previous repair results. Useful if you want to try different strategies/commands/programs. By default, false, thus the repair commands will take advantage of the results of previous repair commands.')
+    main_parser.add_argument('-m', '--multiple', metavar=1, type=int, default=1, required=False,
+                        help='Run multiple times the resiliency test, and average the stats.', **widget_text)
+
+    # Optional general arguments
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
+                        help='Force overwriting the output folder even if it already exists.')
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set variables from arguments
+    origpath = fullpath(args.input[0]) # path to the input directory (where the original, sample data is)
+    outputpath = fullpath(args.output[0])
+    configfile = fullpath(args.config[0])
+    parallel = args.parallel
+    multiple = args.multiple
+    force = args.force
+    verbose = args.verbose
+    silent = args.silent
+
+    #if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
+        #rootfolderpath = os.path.dirname(inputpath)
+
+    # -- Checking arguments
+    if not os.path.isdir(origpath):
+        raise NameError("Input path needs to be a directory!")
+
+    if not os.path.exists(configfile):
+        raise NameError("Please provide a configuration file in order to run a test!")
+    else:
+        commands = parse_configfile(configfile)
+
+    if os.path.exists(outputpath) and not force:
+        raise NameError("Specified test folder (output path) %s already exists! Use --force to overwrite this directory." % outputpath)
+    else:
+        remove_if_exist(outputpath)
+
+    if multiple < 1:
+        multiple = 1
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log[0], 'a', nostdout=silent)
+        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+    # == PROCESSING BRANCHING == #
+
+    # == Main branch
+    ptee.write("====================================")
+    ptee.write("Resiliency tester, started on %s" % datetime.datetime.now().isoformat())
+    ptee.write("====================================")
+    
+    ptee.write("Testing folder %s into test folder %s for %i run(s)." % (origpath, outputpath, multiple))
+
+    fstats = {}
+    for m in _range(multiple):
+        run_nb = m + 1
+
+        ptee.write("===== Resiliency tester: starting run %i =====" % run_nb)
+
+        # -- Define directories tree for this test run
+        # testpath is the basepath for the current run
+        # Generate a specific subdirectory for the current run
+        testpath = os.path.join(outputpath, "run%i" % run_nb)
+        dbdir = fullpath(os.path.join(testpath, "db"))
+        origdbdir = fullpath(os.path.join(testpath, "origdb"))
+        tamperdir = fullpath(os.path.join(testpath, "tampered"))
+        repairdir = fullpath(os.path.join(testpath, "repair"))
+
+        # == START TEST RUN
+        # Create test folder
+        create_dir_if_not_exist(testpath)
+
+        # Before tampering
+        ptee.write("=== BEFORE TAMPERING ===")
+        create_dir_if_not_exist(dbdir)
+        for i, cmd in enumerate(commands["before_tamper"]):
+            scmd = interpolate_dict(cmd, interp_args={"inputdir": origpath, "dbdir": dbdir})
+            ptee.write("Executing command: %s" % scmd)
+            execute_command(scmd, ptee=ptee)
+        copy_any(dbdir, origdbdir) # make a copy because we may tamper the db files
+
+        # Tampering
+        ptee.write("=== TAMPERING ===")
+        copy_any(origpath, tamperdir)
+        for i, cmd in enumerate(commands["tamper"]):
+            scmd = interpolate_dict(cmd, interp_args={"inputdir": tamperdir, "dbdir": dbdir})
+            ptee.write("- RTEST: Executing command: %s" % scmd)
+            execute_command(scmd, ptee=ptee)
+
+        # After tampering
+        ptee.write("=== AFTER TAMPERING ===")
+        for i, cmd in enumerate(commands["after_tamper"]):
+            scmd = interpolate_dict(cmd, interp_args={"inputdir": tamperdir, "dbdir": dbdir})
+            ptee.write("- RTEST: Executing command: %s" % scmd)
+            execute_command(scmd, ptee=ptee)
+
+        # Repairing
+        ptee.write("=== REPAIRING ===")
+        indir = tamperdir
+        finalrepairdir = ''
+        for i, cmd in enumerate(commands["repair"]):
+            outdir = "%s%i" % (repairdir, i)
+            scmd = interpolate_dict(cmd, interp_args={"inputdir": indir, "dbdir": dbdir, "outputdir": outdir})
+            ptee.write("- RTEST: Executing command: %s" % scmd)
+            create_dir_if_not_exist(outdir)
+            execute_command(scmd, ptee=ptee)
+            copy_any(indir, outdir, only_missing=True) # copy the files that did not need any repair (or could not be repaired at all!)
+            finalrepairdir = outdir
+            # If parallel, do not reuse the previous repair resulting files, repair from the tampered files directly
+            if not parallel: indir = outdir
+
+        # Stats
+        stats = compute_all_diff_stats(commands, origpath, tamperdir, repairdir, finalrepairdir)
+        ptee.write("========== Resiliency tester results for run %i ==========" % run_nb)
+        for key, stat in stats.items():
+            ptee.write("=> Stage: %s" % key)
+            ptee.write(pretty_print_stats(stat))
+
+        if run_nb == 1:
+            fstats = stats
+        else:
+            fstats = stats_running_average(fstats, stats, run_nb-1)
+
+    ptee.write("============================")
+    ptee.write("RESILIENCY TESTER FINAL AVERAGED RESULTS OVER %i RUNS" % multiple)
+    ptee.write("============================")
+    for key, stat in fstats.items():
+        ptee.write("=> Stage: %s" % key)
+        ptee.write(pretty_print_stats(stat))
+
+    # Shutting down
+    ptee.close()
+    # Completely repair all the files? Return OK
+    if stats["final"]["error"] == 0:
+        return 0
+    else:
+        return 1
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":  # pragma: no cover
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/resiliency_tester_config.txt` & `pyFileFixity-3.1.4/pyFileFixity/resiliency_tester_config.txt`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-# IMPORTANT: to be compatible with `python setup.py make alias`, you must make
-# sure that you only put one command per line, and ALWAYS put a line return
-# after an alias and before a command, eg:
-#
-#```
-#all:
-#	test
-#	install
-#test:
-#	nosetest
-#install:
-#	python setup.py install
-#    ```
-#
-# resiliency_tester.py supports a templating system: you can use the following special tags, they will be interpolated at runtime:
-#   - {inputdir}: input directory. Depending on the stage, this is either the untampered files (a copy of the original files), the tampered folder, or even previous repair folders during the repair stage.
-#   - {dbdir}: database directory, where the generated databases will be placed.
-#   - {outputdir}: output directory, where the files generated after executing the current command will be placed in.
-
-before_tamper: # this will be executed before files tampering. Generate your ecc/database files here.
-    python header_ecc.py -i "{inputdir}" -d "{dbdir}/hecc.txt" --size 4096 --ecc_algo 3 -g -f
-    python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/ecc.txt" -r1 0.3 -r2 0.2 -r3 0.1 -g -f --ecc_algo 3
-
-tamper: # parameters to tamper the files and even the database files.
-    python filetamper.py -i "{inputdir}" -m "n" -p 0.005 -b "3|6"
-    python filetamper.py -i "{dbdir}" -m "n" -p 0.001 -b "4|9"
-
-after_tamper: # execute commands after tampering. Can be used to recover
-    python repair_ecc.py -i "{dbdir}/hecc.txt" --index "{dbdir}/hecc.txt.idx" -o "{dbdir}/heccrep.txt" -t 0.4 -f
-    python repair_ecc.py -i "{dbdir}/ecc.txt" --index "{dbdir}/ecc.txt.idx" -o "{dbdir}/eccrep.txt" -t 0.4 -f
-
-repair:
-    python header_ecc.py -i "{inputdir}" -d "{dbdir}/heccrep.txt" -o "{outputdir}" -c --size 4096 --no_fast_check --ecc_algo 3
-    python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/eccrep.txt" -o "{outputdir}" -c -r1 0.3 -r2 0.2 -r3 0.1 -f --ecc_algo 3
-
-none:
-	# used for unit testing
+# IMPORTANT: to be compatible with `python setup.py make alias`, you must make
+# sure that you only put one command per line, and ALWAYS put a line return
+# after an alias and before a command, eg:
+#
+#```
+#all:
+#	test
+#	install
+#test:
+#	nosetest
+#install:
+#	python setup.py install
+#    ```
+#
+# resiliency_tester.py supports a templating system: you can use the following special tags, they will be interpolated at runtime:
+#   - {inputdir}: input directory. Depending on the stage, this is either the untampered files (a copy of the original files), the tampered folder, or even previous repair folders during the repair stage.
+#   - {dbdir}: database directory, where the generated databases will be placed.
+#   - {outputdir}: output directory, where the files generated after executing the current command will be placed in.
+
+before_tamper: # this will be executed before files tampering. Generate your ecc/database files here.
+    python header_ecc.py -i "{inputdir}" -d "{dbdir}/hecc.txt" --size 4096 --ecc_algo 3 -g -f
+    python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/ecc.txt" -r1 0.3 -r2 0.2 -r3 0.1 -g -f --ecc_algo 3
+
+tamper: # parameters to tamper the files and even the database files.
+    python filetamper.py -i "{inputdir}" -m "n" -p 0.005 -b "3|6"
+    python filetamper.py -i "{dbdir}" -m "n" -p 0.001 -b "4|9"
+
+after_tamper: # execute commands after tampering. Can be used to recover
+    python repair_ecc.py -i "{dbdir}/hecc.txt" --index "{dbdir}/hecc.txt.idx" -o "{dbdir}/heccrep.txt" -t 0.4 -f
+    python repair_ecc.py -i "{dbdir}/ecc.txt" --index "{dbdir}/ecc.txt.idx" -o "{dbdir}/eccrep.txt" -t 0.4 -f
+
+repair:
+    python header_ecc.py -i "{inputdir}" -d "{dbdir}/heccrep.txt" -o "{outputdir}" -c --size 4096 --no_fast_check --ecc_algo 3
+    python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/eccrep.txt" -o "{outputdir}" -c -r1 0.3 -r2 0.2 -r3 0.1 -f --ecc_algo 3
+
+none:
+	# used for unit testing
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/rfigc.py` & `pyFileFixity-3.1.4/pyFileFixity/rfigc.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,598 +1,597 @@
-#!/usr/bin/env python
-#
-# Recursive/Relative Files Integrity Generator and Checker
-# Copyright (C) 2015-2023 Stephen Karl Larroque
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#=================================
-#  Recursive/Relative Files Integrity Generator and Checker
-#                by Stephen Larroque
-#                       License: MIT
-#              Creation date: 2015-02-27
-#=================================
-#
-# TODO:
-# - Copy corrupted/error files to another location (while preserving folders tree) so that user can try to repair them.
-# - Add interface to a repair software/library? (at least for images and maybe videos and archives zip/tar.gz)
-# - Check similar files: make list of hashes and compare each new file to the hashes database to see if a file that is not in our db ressembles a file in our db, maybe indicating that the file was moved (problem: the check currently is based on database: we walk through database entries, so we cannot detect new files. We would need to do another branch for this special check, or at least do it after the normal check.)
-# - multidisks option: allow to continue checking files on removable drives so that you can spread your archives on several disks and check them all with only one database file.
-# - moviepy try to open video file that is tampered?
-# - add replace md5 and sha1 by farmhash and murmurhash? (non-cryptographic hashes, but it would require importing third-party libraries! no pure python anymore!)
-#
-# NOTE: this software is similar in purpose to the (more advanced) MD5deep / HashDeep for hash set auditing: http://md5deep.sourceforge.net/
-#
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, b, _open_csv
-from lib.aux_funcs import is_dir, is_dir_or_file, fullpath, recwalk, path2unix
-import argparse
-import os, datetime, time, sys
-import hashlib
-import csv
-import tqdm
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-#import pprint # Unnecessary, used only for debugging purposes
-try:
-    import PIL.Image # It's advised that you use PILLOW instead of PIL, to get a wider array of supported filetypes.
-    structure_check_import = True
-except ImportError:
-    structure_check_import = False
-
-#***********************************
-#                   FUNCTIONS
-#***********************************
-
-# Prepare the image filter once and for all in a global variable
-if structure_check_import:
-    PIL.Image.init() # Init PIL to access its supported formats
-    img_filter = ['.'+x.lower() for x in PIL.Image.OPEN.keys()] # Load the supported formats
-    img_filter = img_filter + ['.jpg', '.jpe'] # Add some extensions variations
-def check_structure(filepath):
-    """Returns False if the file is okay, None if file format is unsupported by PIL/PILLOW, or returns an error string if the file is corrupt."""
-    #http://stackoverflow.com/questions/1401527/how-do-i-programmatically-check-whether-an-image-png-jpeg-or-gif-is-corrupted/1401565#1401565
-    
-    # Check structure only for images (not supported for other types currently)
-    if filepath.lower().endswith(tuple(img_filter)):
-        try:
-            #try:
-            im = PIL.Image.open(filepath)
-            #except IOError: # File format not supported by PIL, we skip the check_structure - ARG this is also raised if a supported image file is corrupted...
-                #print("File: %s: DETECTNOPE" % filepath)
-                #return None
-            im.verify()
-        # If an error occurred, the structure is corrupted
-        except Exception as e:
-            return str(e)
-        # Else no exception, there's no corruption
-        return False
-    # Else the format does not currently support structure checking, we just return None to signal we didin't check
-    else:
-        return None
-
-def generate_hashes(filepath, blocksize=65536):
-    '''Generate several hashes (md5 and sha1) in a single sweep of the file. Using two hashes lowers the probability of collision and false negative (file modified but the hash is the same). Supports big files by streaming blocks by blocks to the hasher automatically. Blocksize can be any multiple of 128.'''
-    # Init hashers
-    hasher_md5 = hashlib.md5()
-    hasher_sha1 = hashlib.sha1()
-    # Read the file blocks by blocks
-    with open(filepath, 'rb') as afile:
-        buf = afile.read(blocksize)
-        while len(buf) > 0:
-            # Compute both hashes at the same time
-            hasher_md5.update(buf)
-            hasher_sha1.update(buf)
-            # Load the next data block from file
-            buf = afile.read(blocksize)
-    return (hasher_md5.hexdigest(), hasher_sha1.hexdigest())
-
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Recursive/Relative Files Integrity Generator and Checker
-Description: Recursively generate or check the integrity of files by MD5 and SHA1 hashes, size, modification date (any file) or by data structure integrity (only for images).
-
-This script is originally meant to be used for data archival, by allowing an easy way to check for silent file corruption. Thus, this script uses relative paths so that you can easily compute and check the same redundant data copied on different mediums (hard drives, optical discs, etc.). This script is not meant for system files corruption notification, but is more meant to be used from times-to-times to check up on your data archives integrity.
-    '''
-    ep = '''Example usage:
-- To generate the database (only needed once):
-pff hash -i "folderimages" -d "dbhash.csv" -g
-- To check:
-pff hash -i "folderimages" -d "dbhash.csv" -l log.txt -s
-- To update your database by appending new files:
-pff hash -i "folderimages" -d "dbhash.csv" -u -a 
-- To update your database by appending new files AND removing inexistent files:
-pff hash -i "folderimages" -d "dbhash.csv" -u -a -r
-- (Deprecated) To use with a Gooey gui:
-pff hash --gui
-
-Note that by default, the script is by default in check mode, to avoid wrong manipulations. It will also alert you if you generate over an already existing database file.
-Note2: you can use PyPy to speed the generation, but you should avoid using PyPy when in checking mode (from our tests, it will slow things down a lot).
-'''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='/path/to/root/folder', type=is_dir_or_file, nargs=1, required=True,
-                        help='Path to the root folder (or a single file) from where the scanning will occur.', **widget_dir)
-    main_parser.add_argument('-d', '--database', metavar='/some/folder/databasefile.csv', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
-                        help='Path to the csv file containing the hash informations.', **widget_filesave)
-
-    # Optional general arguments
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('--skip_hash', action='store_true', required=False, default=False,
-                        help='Skip hash computation/checking (checks only the other metadata, this is a lot quicker).')
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    # Checking mode arguments
-    main_parser.add_argument('-s', '--structure_check', action='store_true', required=False, default=False,
-                        help='Check images structures for corruption?')
-    main_parser.add_argument('-e', '--errors_file', metavar='/some/folder/errorsfile.csv', type=str, nargs=1, required=False, #type=argparse.FileType('rt')
-                        help='Path to the error file, where errors at checking will be stored in CSV for further processing by other softwares (such as file repair softwares).', **widget_filesave)
-    main_parser.add_argument('-m', '--disable_modification_date_checking', action='store_true', required=False, default=False,
-                        help='Disable modification date checking.')
-    main_parser.add_argument('--skip_missing', action='store_true', required=False, default=False,
-                        help='Skip missing files when checking (useful if you split your files into several mediums, for example on optical discs with limited capacity).')
-
-    # Generate mode arguments
-    main_parser.add_argument('-g', '--generate', action='store_true', required=False, default=False,
-                        help='Generate the database? (omit this parameter to check instead of generating).')
-    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
-                        help='Force overwriting the database file even if it already exists (if --generate).')
-
-    # Update mode arguments
-    main_parser.add_argument('-u', '--update', action='store_true', required=False, default=False,
-                        help='Update database (you must also specify --append or --remove).')
-    main_parser.add_argument('-a', '--append', action='store_true', required=False, default=False,
-                        help='Append new files (if --update).')
-    main_parser.add_argument('-r', '--remove', action='store_true', required=False, default=False,
-                        help='Remove missing files (if --update).')
-
-    # Recover from file scraping
-    main_parser.add_argument('--filescraping_recovery', action='store_true', required=False, default=False,
-                        help='Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.')
-    main_parser.add_argument('-o', '--output', metavar='/path/to/root/folder', type=is_dir, nargs=1, required=False,
-                        help='Path to the output folder where to output (copy) the files reorganized after --recover_from_filescraping.', **widget_dir)
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set variables from arguments
-    inputpath = fullpath(args.input[0]) # path to the files to protect (either a folder or a single file)
-    rootfolderpath = inputpath # path to the root folder (to compute relative paths)
-    #database = os.path.basename(fullpath(args.database[0])) # Take only the filename.
-    database = fullpath(args.database[0])
-    generate = args.generate
-    structure_check = args.structure_check
-    force = args.force
-    disable_modification_date_checking = args.disable_modification_date_checking
-    skip_missing = args.skip_missing
-    skip_hash = args.skip_hash
-    update = args.update
-    append = args.append
-    remove = args.remove
-    outputpath = None
-    if args.output: outputpath = fullpath(args.output[0])
-    filescraping = args.filescraping_recovery
-    verbose = args.verbose
-    silent = args.silent
-
-    if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
-        rootfolderpath = os.path.dirname(inputpath)
-
-    errors_file = None
-    if args.errors_file: errors_file = fullpath(args.errors_file[0])
-
-    # -- Checking arguments
-    if structure_check and not structure_check_import:
-        raise ImportError('PIL (Python Imaging Library) could not be imported. PIL is needed to do structure check, please install PIL (or you can disable structure check to continue).');
-
-    if update and (not append and not remove):
-        raise ValueError('--update specified but not --append nor --remove. You must specify at least one of these modes when using --update!')
-
-    if filescraping and not outputpath:
-        raise ValueError('Output path needed when --recover_from_filescraping.')
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log[0], 'a', nostdout=silent)
-        #sys.stdout = Tee(args.log[0], 'a')
-        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-
-    # == PROCESSING BRANCHING == #
-    retval = 0 # Returned value: 0 OK, 1 KO (files in error), -1 Error
-
-    # -- Update the database file by removing missing files
-    if update and remove:
-        if not os.path.isfile(database):
-            raise NameError('Specified database file does not exist, can\'t update!')
-
-        ptee.write("====================================")
-        ptee.write("RIFGC Database Update Removal of missing files, started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        # Precompute the total number of lines to process (this should be fairly quick)
-        filestodocount = 0
-        with _open_csv(database, 'r') as dbf:
-            for row in csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"'):
-                filestodocount = filestodocount + 1
-
-            # Preparing CSV writer for the temporary file that will have the lines removed
-            with _open_csv(database+'.rem', 'w') as dbfilerem:
-                csv_writer = csv.writer(dbfilerem, lineterminator='\n', delimiter='|', quotechar='"')
-
-                # Printing CSV headers
-                csv_headers = ['path', 'md5', 'sha1', 'last_modification_timestamp', 'last_modification_date', 'size', 'ext']
-                csv_writer.writerow(csv_headers)
-
-                dbf.seek(0)
-                dbfile = csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"') # we need to reopen the file to put the reading cursor (the generator position) back to the beginning
-                delcount = 0
-                filescount = 0
-                for row in tqdm.tqdm(dbfile, file=ptee, total=filestodocount, leave=True):
-                    filescount = filescount + 1
-                    filepath = os.path.join(rootfolderpath, row['path']) # Build the absolute file path
-
-                    # Single-file mode: skip if this is not the file we are looking for
-                    if inputpath != rootfolderpath and inputpath != filepath: continue
-
-                    if verbose: ptee.write("\n- Processing file %s" % row['path'])
-                    errors = []
-                    if not os.path.isfile(filepath):
-                        delcount = delcount + 1
-                        ptee.write("\n- File %s is missing, removed from database." % row['path'])
-                    else:
-                        csv_writer.writerow( [ path2unix(row['path']), row['md5'], row['sha1'], row['last_modification_timestamp'], row['last_modification_date'], row['size'], row['ext'] ] )
-
-        # REMOVE UPDATE DONE, we remove the old database file and replace it with the new
-        os.remove(database) # delete old database
-        os.rename(database+'.rem', database) # rename new database to match old name
-        # Show some stats
-        ptee.write("----------------------------------------------------")
-        ptee.write("All files processed: Total: %i - Removed/Missing: %i.\n\n" % (filescount, delcount))
-
-    # -- Generate the database file or update/append (both will walk through the filesystem to get new files, contrary to other branchs which walk through the database csv)
-    if generate or (update and append):
-        if not force and os.path.isfile(database) and not update:
-            raise NameError('Database file already exists. Please choose another name to generate your database file.')
-
-        if generate:
-            dbmode = 'w'
-        elif (update and append):
-            dbmode = 'a'
-        with _open_csv(database, dbmode) as dbfile: # Must open in write + binary, because on Windows it will do weird things otherwise (at least with Python 2.7)
-            ptee.write("====================================")
-            if generate:
-                ptee.write("RIFGC Database Generation started on %s" % datetime.datetime.now().isoformat())
-            elif update and append:
-                ptee.write("RIFGC Database Update Append new files, started on %s" % datetime.datetime.now().isoformat())
-            ptee.write("====================================")
-
-            # Preparing CSV writer
-            csv_writer = csv.writer(dbfile, lineterminator='\n', delimiter='|', quotechar='"')
-
-            if generate:
-                # Printing CSV headers
-                csv_headers = ['path', 'md5', 'sha1', 'last_modification_timestamp', 'last_modification_date', 'size', 'ext']
-                csv_writer.writerow(csv_headers)
-
-            if (update and append):
-                # Extract all paths already stored in database to avoid readding them
-                db_paths = {}
-                with _open_csv(database, 'r') as dbf:
-                    for row in csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"'):
-                        db_paths[row['path']] = True
-
-            # Counting the total number of files that we will have to process
-            ptee.write("Counting total number of files to process, please wait...")
-            filestodocount = 0
-            for _ in tqdm.tqdm(recwalk(inputpath), file=ptee):
-                filestodocount = filestodocount + 1
-            ptee.write("Counting done.")
-
-            # Recursively traversing the root directory and save the metadata in the db for each file
-            ptee.write("Processing files to compute metadata to store in database, please wait...")
-            filescount = 0
-            addcount = 0
-            for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee, total=filestodocount, leave=True):
-                    filescount = filescount + 1
-                    # Get full absolute filepath
-                    filepath = os.path.join(dirpath, filename)
-                    # Get database relative path (from scanning root folder)
-                    relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (so that we can easily check the files later even if the absolute path is different)
-                    if verbose: ptee.write("\n- Processing file %s" % relfilepath)
-
-                    # If update + append mode, then if the file is already in the database we skip it (we continue computing metadata only for new files)
-                    if update and append and relfilepath in db_paths:
-                        if verbose: ptee.write("... skipped")
-                        continue
-                    else:
-                        addcount = addcount + 1
-
-                    # Compute the hashes (leave it outside the with command because generate_hashes() open the file by itself, so that both hashes can be computed in a single sweep of the file at the same time)
-                    if not skip_hash:
-                        md5hash, sha1hash = generate_hashes(filepath)
-                    else:
-                        md5hash = sha1hash = 0
-                    # Compute other metadata
-                    with open(filepath) as thisfile:
-                        # Check file structure if option is enabled
-                        if structure_check:
-                            struct_result = check_structure(filepath)
-                            # Print/Log an error only if there's one (else we won't say anything)
-                            if struct_result:
-                                ptee.write("\n- Structure error with file "+filepath+": "+struct_result)
-                        ext = os.path.splitext(filepath)[1] # File's extension
-                        statinfos = os.stat(filepath) # Various OS filesystem infos about the file
-                        size = statinfos.st_size # File size
-                        lastmodif = statinfos.st_mtime # File last modified date (as a timestamp)
-                        lastmodif_readable = datetime.datetime.fromtimestamp(lastmodif).strftime("%Y-%m-%d %H:%M:%S") # File last modified date as a human readable date (ISO universal time)
-
-                        csv_row = [path2unix(relfilepath), md5hash, sha1hash, lastmodif, lastmodif_readable, size, ext] # Prepare the CSV row
-                        csv_writer.writerow(csv_row) # Save to the file
-        ptee.write("----------------------------------------------------")
-        ptee.write("All files processed: Total: %i - Added: %i.\n\n" % (filescount, addcount))
-
-    # -- Filescraping recovery mode
-    # We will compare all files from the input path and reorganize the ones that are recognized into the output path
-    elif filescraping:
-        import shutil
-        ptee.write("====================================")
-        ptee.write("RIFGC File Scraping Recovery started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        ptee.write("Loading the database into memory, please wait...")
-        md5list = {}
-        sha1list = {}
-        dbrows = {} # TODO: instead of memorizing everything in memory, store just the reading cursor position at the beginning of the line with the size and then just read when necessary from the db file directly
-        id = 0
-        with _open_csv(database, 'r') as db:
-            for row in csv.DictReader(db, lineterminator='\n', delimiter='|', quotechar='"'):
-                id += 1
-                if (len(row['md5']) > 0 and len(row['sha1']) > 0):
-                    md5list[row['md5']] = id
-                    sha1list[row['sha1']] = id
-                    dbrows[id] = row
-        ptee.write("Loading done.")
-
-        if len(dbrows) == 0:
-            ptee.write("Nothing to do, there's no md5 nor sha1 hashes in the database file!")
-            ptee.close()
-            return 1 # return with an error
-
-        # Counting the total number of files that we will have to process
-        ptee.write("Counting total number of files to process, please wait...")
-        filestodocount = 0
-        for _ in tqdm.tqdm(recwalk(inputpath), file=ptee):
-            filestodocount = filestodocount + 1
-        ptee.write("Counting done.")
-        
-        # Recursively traversing the root directory and save the metadata in the db for each file
-        ptee.write("Processing file scraping recovery, walking through all files from input folder...")
-        filescount = 0
-        copiedcount = 0
-        for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee, total=filestodocount, leave=True):
-                filescount = filescount + 1
-                # Get full absolute filepath
-                filepath = os.path.join(dirpath,filename)
-                # Get database relative path (from scanning root folder)
-                relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
-                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
-
-                # Generate the hashes from the currently inspected file
-                md5hash, sha1hash = generate_hashes(filepath)
-                # If it match with a file in the database, we will copy it over with the correct name, directory structure, file extension and last modification date
-                if md5hash in md5list and sha1hash in sha1list and md5list[md5hash] == sha1list[sha1hash]:
-                    # Load the db infos for this file
-                    row = dbrows[md5list[md5hash]]
-                    ptee.write("- Found: %s --> %s.\n" % (filepath, row['path']))
-                    # Generate full absolute filepath of the output file
-                    outfilepath = os.path.join(outputpath, row['path'])
-                    # Recursively create the directory tree structure
-                    outfiledir = os.path.dirname(outfilepath)
-                    if not os.path.isdir(outfiledir): os.makedirs(outfiledir) # if the target directory does not exist, create it (and create recursively all parent directories too)
-                    # Copy over and set attributes
-                    shutil.copy2(filepath, outfilepath)
-                    filestats = os.stat(filepath)
-                    os.utime(outfilepath, (filestats.st_atime, float(row['last_modification_timestamp'])))
-                    # Counter...
-                    copiedcount += 1
-        ptee.write("----------------------------------------------------")
-        ptee.write("All files processed: Total: %i - Recovered: %i.\n\n" % (filescount, copiedcount))
-
-    # -- Check mode: check the files using a database file
-    elif not update and not generate and not filescraping:
-        ptee.write("====================================")
-        ptee.write("RIFGC Check started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        # Open errors file if supplied (where we will store every errors in a formatted csv so that it can later be easily processed by other softwares, such as repair softwares)
-        if errors_file is not None:
-            efile = _open_csv(errors_file, 'w')
-            e_writer = csv.writer(efile, delimiter='|', lineterminator='\n', quotechar='"')
-
-        # Precompute the total number of lines to process (this should be fairly quick)
-        filestodocount = 0
-        with _open_csv(database, 'r') as dbf:
-            for row in csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"'):
-                filestodocount = filestodocount + 1
-
-            # Processing the files using the database list
-            ptee.write("Checking for files corruption based on database %s on input path %s, please wait..." % (database, inputpath))
-            dbf.seek(0)
-            dbfile = csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"') # we need to reopen the file to put the reading cursor (the generator position) back to the beginning
-            errorscount = 0
-            filescount = 0
-            for row in tqdm.tqdm(dbfile, file=ptee, total=filestodocount, leave=True):
-                filescount = filescount + 1
-                filepath = os.path.join(rootfolderpath, row['path'])
-
-                # Single-file mode: skip if this is not the file we are looking for
-                if inputpath != rootfolderpath and inputpath != filepath: continue
-
-                if verbose: ptee.write("\n- Processing file %s" % row['path'])
-                errors = []
-                if not os.path.isfile(filepath):
-                    if not skip_missing: errors.append('file is missing')
-                # First generate the current file's metadata given the filepath from the CSV, and then we will check the differences from database
-                else:
-                    try: # Try to be resilient to various file access errors
-                        # Generate hash
-                        if not skip_hash:
-                            md5hash, sha1hash = generate_hashes(filepath)
-                        else:
-                            md5hash = sha1hash = 0
-                        # Check structure integrity if enabled
-                        if structure_check:
-                            struct_result = check_structure(filepath)
-                            if struct_result:
-                                errors.append("structure error (%s)" % struct_result)
-                        # Compute other metadata
-                        with open(filepath) as thisfile:
-                            ext = os.path.splitext(filepath)[1]
-                            statinfos = os.stat(filepath)
-                            size = statinfos.st_size
-                            lastmodif = statinfos.st_mtime
-                            lastmodif_readable = datetime.datetime.fromtimestamp(lastmodif).strftime("%Y-%m-%d %H:%M:%S")
-
-                            # CHECK THE DIFFERENCES
-                            if not skip_hash and md5hash != row['md5'] and sha1hash != row['sha1']:
-                                errors.append('both md5 and sha1 hash failed')
-                            elif not skip_hash and ((md5hash == row['md5'] and sha1hash != row['sha1']) or (md5hash != row['md5'] and sha1hash == row['sha1'])):
-                                errors.append('one of the hash failed but not the other (which may indicate that the database file is corrupted)')
-                            if ext != row['ext']:
-                                errors.append('extension has changed')
-                            if size != int(row['size']):
-                                errors.append("size has changed (before: %s - now: %s)" % (row['size'], size))
-                            if not disable_modification_date_checking and (lastmodif != float(row['last_modification_timestamp']) and round(lastmodif,0) != round(float(row['last_modification_timestamp']),0)): # for usage with PyPy: last modification time is differently managed (rounded), thus we need to round here manually to compare against PyPy.
-                                errors.append("modification date has changed (before: %s - now: %s)" % (row['last_modification_date'], lastmodif_readable))
-                    except IOError as e: # Catch IOError as a file error
-                        errors.append('file can\'t be read, IOError (inaccessible, maybe bad sector?)')
-                    except Exception as e: # Any other exception when accessing the file will also be caught as a file error
-                        errors.append('file can\'t be accessed: %s' % e)
-                # Print/Log all errors for this file if any happened
-                if errors:
-                    errorscount = errorscount + 1
-                    ptee.write("\n- Error for file %s: %s." % (row['path'], ', '.join(errors)))
-                    if errors_file is not None: # Write error in a csv file if supplied (for easy processing later by other softwares such as file repair softwares)
-                        e_writer.writerow( [row['path'], ', '.join(errors)] )
-        # END OF CHECKING: show some stats
-        ptee.write("----------------------------------------------------")
-        ptee.write("All files checked: Total: %i - Files with errors: %i.\n\n" % (filescount, errorscount))
-        retval = (errorscount > 0)
-
-    ptee.close()
-    if errors_file is not None:
-        efile.close()
-    return retval # return error code if any
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":  # pragma: no cover
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Recursive/Relative Files Integrity Generator and Checker
+# Copyright (C) 2015-2023 Stephen Karl Larroque
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#=================================
+#  Recursive/Relative Files Integrity Generator and Checker
+#                by Stephen Larroque
+#                       License: MIT
+#              Creation date: 2015-02-27
+#=================================
+#
+# TODO:
+# - Copy corrupted/error files to another location (while preserving folders tree) so that user can try to repair them.
+# - Add interface to a repair software/library? (at least for images and maybe videos and archives zip/tar.gz)
+# - Check similar files: make list of hashes and compare each new file to the hashes database to see if a file that is not in our db ressembles a file in our db, maybe indicating that the file was moved (problem: the check currently is based on database: we walk through database entries, so we cannot detect new files. We would need to do another branch for this special check, or at least do it after the normal check.)
+# - multidisks option: allow to continue checking files on removable drives so that you can spread your archives on several disks and check them all with only one database file.
+# - moviepy try to open video file that is tampered?
+# - add replace md5 and sha1 by farmhash and murmurhash? (non-cryptographic hashes, but it would require importing third-party libraries! no pure python anymore!)
+#
+# NOTE: this software is similar in purpose to the (more advanced) MD5deep / HashDeep for hash set auditing: http://md5deep.sourceforge.net/
+#
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, b, _open_csv
+from lib.aux_funcs import is_dir, is_dir_or_file, fullpath, recwalk, path2unix
+import argparse
+import os, datetime, time, sys
+import hashlib
+import csv
+import tqdm
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+#import pprint # Unnecessary, used only for debugging purposes
+try:
+    import PIL.Image # It's advised that you use PILLOW instead of PIL, to get a wider array of supported filetypes.
+    structure_check_import = True
+except ImportError:
+    structure_check_import = False
+
+#***********************************
+#                   FUNCTIONS
+#***********************************
+
+# Prepare the image filter once and for all in a global variable
+if structure_check_import:
+    PIL.Image.init() # Init PIL to access its supported formats
+    img_filter = ['.'+x.lower() for x in PIL.Image.OPEN.keys()] # Load the supported formats
+    img_filter = img_filter + ['.jpg', '.jpe'] # Add some extensions variations
+def check_structure(filepath):
+    """Returns False if the file is okay, None if file format is unsupported by PIL/PILLOW, or returns an error string if the file is corrupt."""
+    #http://stackoverflow.com/questions/1401527/how-do-i-programmatically-check-whether-an-image-png-jpeg-or-gif-is-corrupted/1401565#1401565
+    
+    # Check structure only for images (not supported for other types currently)
+    if filepath.lower().endswith(tuple(img_filter)):
+        try:
+            #try:
+            im = PIL.Image.open(filepath)
+            #except IOError: # File format not supported by PIL, we skip the check_structure - ARG this is also raised if a supported image file is corrupted...
+                #print("File: %s: DETECTNOPE" % filepath)
+                #return None
+            im.verify()
+        # If an error occurred, the structure is corrupted
+        except Exception as e:
+            return str(e)
+        # Else no exception, there's no corruption
+        return False
+    # Else the format does not currently support structure checking, we just return None to signal we didin't check
+    else:
+        return None
+
+def generate_hashes(filepath, blocksize=65536):
+    '''Generate several hashes (md5 and sha1) in a single sweep of the file. Using two hashes lowers the probability of collision and false negative (file modified but the hash is the same). Supports big files by streaming blocks by blocks to the hasher automatically. Blocksize can be any multiple of 128.'''
+    # Init hashers
+    hasher_md5 = hashlib.md5()
+    hasher_sha1 = hashlib.sha1()
+    # Read the file blocks by blocks
+    with open(filepath, 'rb') as afile:
+        buf = afile.read(blocksize)
+        while len(buf) > 0:
+            # Compute both hashes at the same time
+            hasher_md5.update(buf)
+            hasher_sha1.update(buf)
+            # Load the next data block from file
+            buf = afile.read(blocksize)
+    return (hasher_md5.hexdigest(), hasher_sha1.hexdigest())
+
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Recursive/Relative Files Integrity Generator and Checker
+Description: Recursively generate or check the integrity of files by MD5 and SHA1 hashes, size, modification date (any file) or by data structure integrity (only for images).
+
+This script is originally meant to be used for data archival, by allowing an easy way to check for silent file corruption. Thus, this script uses relative paths so that you can easily compute and check the same redundant data copied on different mediums (hard drives, optical discs, etc.). This script is not meant for system files corruption notification, but is more meant to be used from times-to-times to check up on your data archives integrity.
+    '''
+    ep = '''Example usage:
+- To generate the database (only needed once):
+pff hash -i "folderimages" -d "dbhash.csv" -g
+- To check:
+pff hash -i "folderimages" -d "dbhash.csv" -l log.txt -s
+- To update your database by appending new files:
+pff hash -i "folderimages" -d "dbhash.csv" -u -a 
+- To update your database by appending new files AND removing inexistent files:
+pff hash -i "folderimages" -d "dbhash.csv" -u -a -r
+- (Deprecated) To use with a Gooey gui:
+pff hash --gui
+
+Note that by default, the script is by default in check mode, to avoid wrong manipulations. It will also alert you if you generate over an already existing database file.
+Note2: you can use PyPy to speed the generation, but you should avoid using PyPy when in checking mode (from our tests, it will slow things down a lot).
+'''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='/path/to/root/folder', type=is_dir_or_file, nargs=1, required=True,
+                        help='Path to the root folder (or a single file) from where the scanning will occur.', **widget_dir)
+    main_parser.add_argument('-d', '--database', metavar='/some/folder/databasefile.csv', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
+                        help='Path to the csv file containing the hash informations.', **widget_filesave)
+
+    # Optional general arguments
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('--skip_hash', action='store_true', required=False, default=False,
+                        help='Skip hash computation/checking (checks only the other metadata, this is a lot quicker).')
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    # Checking mode arguments
+    main_parser.add_argument('-s', '--structure_check', action='store_true', required=False, default=False,
+                        help='Check images structures for corruption?')
+    main_parser.add_argument('-e', '--errors_file', metavar='/some/folder/errorsfile.csv', type=str, nargs=1, required=False, #type=argparse.FileType('rt')
+                        help='Path to the error file, where errors at checking will be stored in CSV for further processing by other softwares (such as file repair softwares).', **widget_filesave)
+    main_parser.add_argument('-m', '--disable_modification_date_checking', action='store_true', required=False, default=False,
+                        help='Disable modification date checking.')
+    main_parser.add_argument('--skip_missing', action='store_true', required=False, default=False,
+                        help='Skip missing files when checking (useful if you split your files into several mediums, for example on optical discs with limited capacity).')
+
+    # Generate mode arguments
+    main_parser.add_argument('-g', '--generate', action='store_true', required=False, default=False,
+                        help='Generate the database? (omit this parameter to check instead of generating).')
+    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
+                        help='Force overwriting the database file even if it already exists (if --generate).')
+
+    # Update mode arguments
+    main_parser.add_argument('-u', '--update', action='store_true', required=False, default=False,
+                        help='Update database (you must also specify --append or --remove).')
+    main_parser.add_argument('-a', '--append', action='store_true', required=False, default=False,
+                        help='Append new files (if --update).')
+    main_parser.add_argument('-r', '--remove', action='store_true', required=False, default=False,
+                        help='Remove missing files (if --update).')
+
+    # Recover from file scraping
+    main_parser.add_argument('--filescraping_recovery', action='store_true', required=False, default=False,
+                        help='Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.')
+    main_parser.add_argument('-o', '--output', metavar='/path/to/root/folder', type=is_dir, nargs=1, required=False,
+                        help='Path to the output folder where to output (copy) the files reorganized after --recover_from_filescraping.', **widget_dir)
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set variables from arguments
+    inputpath = fullpath(args.input[0]) # path to the files to protect (either a folder or a single file)
+    rootfolderpath = inputpath # path to the root folder (to compute relative paths)
+    #database = os.path.basename(fullpath(args.database[0])) # Take only the filename.
+    database = fullpath(args.database[0])
+    generate = args.generate
+    structure_check = args.structure_check
+    force = args.force
+    disable_modification_date_checking = args.disable_modification_date_checking
+    skip_missing = args.skip_missing
+    skip_hash = args.skip_hash
+    update = args.update
+    append = args.append
+    remove = args.remove
+    outputpath = None
+    if args.output: outputpath = fullpath(args.output[0])
+    filescraping = args.filescraping_recovery
+    verbose = args.verbose
+    silent = args.silent
+
+    if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
+        rootfolderpath = os.path.dirname(inputpath)
+
+    errors_file = None
+    if args.errors_file: errors_file = fullpath(args.errors_file[0])
+
+    # -- Checking arguments
+    if structure_check and not structure_check_import:
+        raise ImportError('PIL (Python Imaging Library) could not be imported. PIL is needed to do structure check, please install PIL (or you can disable structure check to continue).');
+
+    if update and (not append and not remove):
+        raise ValueError('--update specified but not --append nor --remove. You must specify at least one of these modes when using --update!')
+
+    if filescraping and not outputpath:
+        raise ValueError('Output path needed when --recover_from_filescraping.')
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log[0], 'a', nostdout=silent)
+        #sys.stdout = Tee(args.log[0], 'a')
+        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+
+    # == PROCESSING BRANCHING == #
+    retval = 0 # Returned value: 0 OK, 1 KO (files in error), -1 Error
+    csv_headers = ['path', 'md5', 'sha1', 'last_modification_timestamp', 'last_modification_date', 'size', 'ext']  # preconfigure csv_headers
+
+    # -- Update the database file by removing missing files
+    if update and remove:
+        if not os.path.isfile(database):
+            raise NameError('Specified database file does not exist, can\'t update!')
+
+        ptee.write("====================================")
+        ptee.write("RIFGC Database Update Removal of missing files, started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        # Precompute the total number of lines to process (this should be fairly quick)
+        filestodocount = 0
+        with _open_csv(database, 'r') as dbf:
+            for row in csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"'):
+                filestodocount = filestodocount + 1
+
+            # Preparing CSV writer for the temporary file that will have the lines removed
+            with _open_csv(database+'.rem', 'w') as dbfilerem:
+                csv_writer = csv.writer(dbfilerem, lineterminator='\n', delimiter='|', quotechar='"')
+
+                # Printing CSV headers
+                csv_writer.writerow(csv_headers)
+
+                dbf.seek(0)
+                dbfile = csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"') # we need to reopen the file to put the reading cursor (the generator position) back to the beginning
+                delcount = 0
+                filescount = 0
+                for row in tqdm.tqdm(dbfile, file=ptee, total=filestodocount, leave=True):
+                    filescount = filescount + 1
+                    filepath = os.path.join(rootfolderpath, row['path']) # Build the absolute file path
+
+                    # Single-file mode: skip if this is not the file we are looking for
+                    if inputpath != rootfolderpath and inputpath != filepath: continue
+
+                    if verbose: ptee.write("\n- Processing file %s" % row['path'])
+                    errors = []
+                    if not os.path.isfile(filepath):
+                        delcount = delcount + 1
+                        ptee.write("\n- File %s is missing, removed from database." % row['path'])
+                    else:
+                        csv_writer.writerow( [ path2unix(row['path']), row['md5'], row['sha1'], row['last_modification_timestamp'], row['last_modification_date'], row['size'], row['ext'] ] )
+
+        # REMOVE UPDATE DONE, we remove the old database file and replace it with the new
+        os.remove(database) # delete old database
+        os.rename(database+'.rem', database) # rename new database to match old name
+        # Show some stats
+        ptee.write("----------------------------------------------------")
+        ptee.write("All files processed: Total: %i - Removed/Missing: %i.\n\n" % (filescount, delcount))
+
+    # -- Generate the database file or update/append (both will walk through the filesystem to get new files, contrary to other branchs which walk through the database csv)
+    if generate or (update and append):
+        if not force and os.path.isfile(database) and not update:
+            raise NameError('Database file already exists. Please choose another name to generate your database file.')
+
+        if generate:
+            dbmode = 'w'
+        elif (update and append):
+            dbmode = 'a'
+        with _open_csv(database, dbmode) as dbfile: # Must open in write + binary, because on Windows it will do weird things otherwise (at least with Python 2.7)
+            ptee.write("====================================")
+            if generate:
+                ptee.write("RIFGC Database Generation started on %s" % datetime.datetime.now().isoformat())
+            elif update and append:
+                ptee.write("RIFGC Database Update Append new files, started on %s" % datetime.datetime.now().isoformat())
+            ptee.write("====================================")
+
+            # Preparing CSV writer
+            csv_writer = csv.writer(dbfile, lineterminator='\n', delimiter='|', quotechar='"')
+
+            if generate:
+                # Printing CSV headers
+                csv_writer.writerow(csv_headers)
+
+            if (update and append):
+                # Extract all paths already stored in database to avoid readding them
+                db_paths = {}
+                with _open_csv(database, 'r') as dbf:
+                    for row in csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"'):
+                        db_paths[row['path']] = True
+
+            # Counting the total number of files that we will have to process
+            ptee.write("Counting total number of files to process, please wait...")
+            filestodocount = 0
+            for _ in tqdm.tqdm(recwalk(inputpath), file=ptee):
+                filestodocount = filestodocount + 1
+            ptee.write("Counting done.")
+
+            # Recursively traversing the root directory and save the metadata in the db for each file
+            ptee.write("Processing files to compute metadata to store in database, please wait...")
+            filescount = 0
+            addcount = 0
+            for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee, total=filestodocount, leave=True):
+                    filescount = filescount + 1
+                    # Get full absolute filepath
+                    filepath = os.path.join(dirpath, filename)
+                    # Get database relative path (from scanning root folder)
+                    relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (so that we can easily check the files later even if the absolute path is different)
+                    if verbose: ptee.write("\n- Processing file %s" % relfilepath)
+
+                    # If update + append mode, then if the file is already in the database we skip it (we continue computing metadata only for new files)
+                    if update and append and relfilepath in db_paths:
+                        if verbose: ptee.write("... skipped")
+                        continue
+                    else:
+                        addcount = addcount + 1
+
+                    # Compute the hashes (leave it outside the with command because generate_hashes() open the file by itself, so that both hashes can be computed in a single sweep of the file at the same time)
+                    if not skip_hash:
+                        md5hash, sha1hash = generate_hashes(filepath)
+                    else:
+                        md5hash = sha1hash = 0
+                    # Compute other metadata
+                    with open(filepath) as _:
+                        # Check file structure if option is enabled
+                        if structure_check:
+                            struct_result = check_structure(filepath)
+                            # Print/Log an error only if there's one (else we won't say anything)
+                            if struct_result:
+                                ptee.write("\n- Structure error with file "+filepath+": "+struct_result)
+                        ext = os.path.splitext(filepath)[1] # File's extension
+                        statinfos = os.stat(filepath) # Various OS filesystem infos about the file
+                        size = statinfos.st_size # File size
+                        lastmodif = statinfos.st_mtime # File last modified date (as a timestamp)
+                        lastmodif_readable = datetime.datetime.fromtimestamp(lastmodif).strftime("%Y-%m-%d %H:%M:%S") # File last modified date as a human readable date (ISO universal time)
+
+                        csv_row = [path2unix(relfilepath), md5hash, sha1hash, lastmodif, lastmodif_readable, size, ext] # Prepare the CSV row
+                        csv_writer.writerow(csv_row) # Save to the file
+        ptee.write("----------------------------------------------------")
+        ptee.write("All files processed: Total: %i - Added: %i.\n\n" % (filescount, addcount))
+
+    # -- Filescraping recovery mode
+    # We will compare all files from the input path and reorganize the ones that are recognized into the output path
+    elif filescraping:
+        import shutil
+        ptee.write("====================================")
+        ptee.write("RIFGC File Scraping Recovery started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        ptee.write("Loading the database into memory, please wait...")
+        md5list = {}
+        sha1list = {}
+        dbrows = {} # TODO: instead of memorizing everything in memory, store just the reading cursor position at the beginning of the line with the size and then just read when necessary from the db file directly
+        id = 0
+        with _open_csv(database, 'r') as db:
+            for row in csv.DictReader(db, lineterminator='\n', delimiter='|', quotechar='"'):
+                id += 1
+                if (len(row['md5']) > 0 and len(row['sha1']) > 0):
+                    md5list[row['md5']] = id
+                    sha1list[row['sha1']] = id
+                    dbrows[id] = row
+        ptee.write("Loading done.")
+
+        if len(dbrows) == 0:
+            ptee.write("Nothing to do, there's no md5 nor sha1 hashes in the database file!")
+            ptee.close()
+            return 1 # return with an error
+
+        # Counting the total number of files that we will have to process
+        ptee.write("Counting total number of files to process, please wait...")
+        filestodocount = 0
+        for _ in tqdm.tqdm(recwalk(inputpath), file=ptee):
+            filestodocount = filestodocount + 1
+        ptee.write("Counting done.")
+        
+        # Recursively traversing the root directory and save the metadata in the db for each file
+        ptee.write("Processing file scraping recovery, walking through all files from input folder...")
+        filescount = 0
+        copiedcount = 0
+        for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee, total=filestodocount, leave=True):
+                filescount = filescount + 1
+                # Get full absolute filepath
+                filepath = os.path.join(dirpath,filename)
+                # Get database relative path (from scanning root folder)
+                relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
+                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
+
+                # Generate the hashes from the currently inspected file
+                md5hash, sha1hash = generate_hashes(filepath)
+                # If it match with a file in the database, we will copy it over with the correct name, directory structure, file extension and last modification date
+                if md5hash in md5list and sha1hash in sha1list and md5list[md5hash] == sha1list[sha1hash]:
+                    # Load the db infos for this file
+                    row = dbrows[md5list[md5hash]]
+                    ptee.write("- Found: %s --> %s.\n" % (filepath, row['path']))
+                    # Generate full absolute filepath of the output file
+                    outfilepath = os.path.join(outputpath, row['path'])
+                    # Recursively create the directory tree structure
+                    outfiledir = os.path.dirname(outfilepath)
+                    if not os.path.isdir(outfiledir): os.makedirs(outfiledir) # if the target directory does not exist, create it (and create recursively all parent directories too)
+                    # Copy over and set attributes
+                    shutil.copy2(filepath, outfilepath)
+                    filestats = os.stat(filepath)
+                    os.utime(outfilepath, (filestats.st_atime, float(row['last_modification_timestamp'])))
+                    # Counter...
+                    copiedcount += 1
+        ptee.write("----------------------------------------------------")
+        ptee.write("All files processed: Total: %i - Recovered: %i.\n\n" % (filescount, copiedcount))
+
+    # -- Check mode: check the files using a database file
+    elif not update and not generate and not filescraping:
+        ptee.write("====================================")
+        ptee.write("RIFGC Check started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        # Open errors file if supplied (where we will store every errors in a formatted csv so that it can later be easily processed by other softwares, such as repair softwares)
+        if errors_file is not None:
+            efile = _open_csv(errors_file, 'w')
+            e_writer = csv.writer(efile, delimiter='|', lineterminator='\n', quotechar='"')
+
+        # Precompute the total number of lines to process (this should be fairly quick)
+        filestodocount = 0
+        with _open_csv(database, 'r') as dbf:
+            for row in csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"'):
+                filestodocount = filestodocount + 1
+
+            # Processing the files using the database list
+            ptee.write("Checking for files corruption based on database %s on input path %s, please wait..." % (database, inputpath))
+            dbf.seek(0)
+            dbfile = csv.DictReader(dbf, lineterminator='\n', delimiter='|', quotechar='"') # we need to reopen the file to put the reading cursor (the generator position) back to the beginning
+            errorscount = 0
+            filescount = 0
+            for row in tqdm.tqdm(dbfile, file=ptee, total=filestodocount, leave=True):
+                filescount = filescount + 1
+                filepath = os.path.join(rootfolderpath, row['path'])
+
+                # Single-file mode: skip if this is not the file we are looking for
+                if inputpath != rootfolderpath and inputpath != filepath: continue
+
+                if verbose: ptee.write("\n- Processing file %s" % row['path'])
+                errors = []
+                if not os.path.isfile(filepath):
+                    if not skip_missing: errors.append('file is missing')
+                # First generate the current file's metadata given the filepath from the CSV, and then we will check the differences from database
+                else:
+                    try: # Try to be resilient to various file access errors
+                        # Generate hash
+                        if not skip_hash:
+                            md5hash, sha1hash = generate_hashes(filepath)
+                        else:
+                            md5hash = sha1hash = 0
+                        # Check structure integrity if enabled
+                        if structure_check:
+                            struct_result = check_structure(filepath)
+                            if struct_result:
+                                errors.append("structure error (%s)" % struct_result)
+                        # Compute other metadata
+                        with open(filepath) as _:
+                            ext = os.path.splitext(filepath)[1]
+                            statinfos = os.stat(filepath)
+                            size = statinfos.st_size
+                            lastmodif = statinfos.st_mtime
+                            lastmodif_readable = datetime.datetime.fromtimestamp(lastmodif).strftime("%Y-%m-%d %H:%M:%S")
+
+                            # CHECK THE DIFFERENCES
+                            if not skip_hash and md5hash != row['md5'] and sha1hash != row['sha1']:
+                                errors.append('both md5 and sha1 hash failed')
+                            elif not skip_hash and ((md5hash == row['md5'] and sha1hash != row['sha1']) or (md5hash != row['md5'] and sha1hash == row['sha1'])):
+                                errors.append('one of the hash failed but not the other (which may indicate that the database file is corrupted)')
+                            if ext != row['ext']:
+                                errors.append('extension has changed')
+                            if size != int(row['size']):
+                                errors.append("size has changed (before: %s - now: %s)" % (row['size'], size))
+                            if not disable_modification_date_checking and (lastmodif != float(row['last_modification_timestamp']) and round(lastmodif,0) != round(float(row['last_modification_timestamp']),0)): # for usage with PyPy: last modification time is differently managed (rounded), thus we need to round here manually to compare against PyPy.
+                                errors.append("modification date has changed (before: %s - now: %s)" % (row['last_modification_date'], lastmodif_readable))
+                    except IOError as e: # Catch IOError as a file error
+                        errors.append('file can\'t be read, IOError (inaccessible, maybe bad sector?)')
+                    except Exception as e: # Any other exception when accessing the file will also be caught as a file error
+                        errors.append('file can\'t be accessed: %s' % e)
+                # Print/Log all errors for this file if any happened
+                if errors:
+                    errorscount = errorscount + 1
+                    ptee.write("\n- Error for file %s: %s." % (row['path'], ', '.join(errors)))
+                    if errors_file is not None: # Write error in a csv file if supplied (for easy processing later by other softwares such as file repair softwares)
+                        e_writer.writerow( [row['path'], ', '.join(errors)] )
+        # END OF CHECKING: show some stats
+        ptee.write("----------------------------------------------------")
+        ptee.write("All files checked: Total: %i - Files with errors: %i.\n\n" % (filescount, errorscount))
+        retval = (errorscount > 0)
+
+    ptee.close()
+    if errors_file is not None:
+        efile.close()
+    return retval # return error code if any
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":  # pragma: no cover
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/structural_adaptive_ecc.py` & `pyFileFixity-3.1.4/pyFileFixity/structural_adaptive_ecc.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,792 +1,792 @@
-#!/usr/bin/env python
-#
-# Structural Adaptive Error Correction Code
-# Copyright (C) 2015-2023 Larroque Stephen
-#
-# Licensed under the MIT License (MIT)
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-# THE SOFTWARE.
-#
-#=================================
-#  Structural Adaptive Error Correction Code generator and checker
-#                by Stephen Larroque
-#                       License: MIT
-#              Creation date: 2015-03-10
-#=================================
-#
-# From : http://simple.wikipedia.org/wiki/Reed-Solomon_error_correction
-# The key idea behind a Reed-Solomon code is that the data encoded is first visualized as a polynomial. The code relies on a theorem from algebra that states that any k distinct points uniquely determine a polynomial of degree at most k-1.
-# The sender determines a degree k-1 polynomial, over a finite field, that represents the k data points. The polynomial is then "encoded" by its evaluation at various points, and these values are what is actually sent. During transmission, some of these values may become corrupted. Therefore, more than k points are actually sent. As long as sufficient values are received correctly, the receiver can deduce what the original polynomial was, and decode the original data.
-# In the same sense that one can correct a curve by interpolating past a gap, a Reed-Solomon code can bridge a series of errors in a block of data to recover the coefficients of the polynomial that drew the original curve.
-# This is done automatically using a simple trick: the matrix inversion. For more infos, see this well-explained blog post by Richard Kiss: How to be Minimally Redundant (or "A Splitting Headache") http://blog.richardkiss.com/?p=264
-#
-# codeword_rate: rs = @(n,k) k / n
-# @(n,k) (n - k) / k
-#
-# To get higher resilience against corruption: make the codeword bigger (decrease k and increase n = higher resilience ratio), and use smaller packets (n should be smaller). Source: Kumar, Sanjeev, and Ragini Gupta. "Bit error rate analysis of Reed-Solomon code for efficient communication system." International Journal of Computer Applications 30.12 (2011): 11-15.
-# Note: in the paper, they told that their results indicate to use bigger blocks, but if you look at the Figure 4, this shows the opposite: the best performing parameters is: RS(246, 164) with code rate 0.66667 = resilience ratio 0.25
-# Note2: however, logically, it's more sensible to use bigger blocks to be more resilient. Indeed, since data stream is hashed and ecc'ed per block, this means that if an error burst corrupt a block twice the size of max_block_size (or just the size of max_block_size but exactly overlapping on one block, no shift), then the block will be unrecoverable. In other words, if we have bigger blocks, we raise the required size for an error burst to permanently corrupt our data (thus we diminish the risk). Thus, it would make sense to use error correcting codes with very big blocks. However, this would probably incur a huge performance drop, at least for Reed-Solomon which is basically grounded on matrix operations (thus the complexity would be about O(max_block_size^2), a quadratic complexity).
-#
-#  If 2s + r < 2t (s errors, r erasures) t
-#
-# TODO:
-# - Performance boost in Reed-Solomon libraries. A big work was done, and it's quite fast when using PyPy 2.5.0, but 10x more speedup (to attain 10MB/s encoding rate) would just be perfect! Decoding rate is sufficiently speedy as it is, no need to optimize that part.
-# Use Cauchy Reed-Solomon, which significantly outperforms simple Reed-Solomon?
-# or https://bitbucket.org/kmgreen2/pyeclib with jerasure
-# or http://www.bth.se/fou/cuppsats.nsf/all/bcb2fd16e55a96c2c1257c5e00666323/$file/BTH2013KARLSSON.pdf
-# or https://www.usenix.org/legacy/events/fast09/tech/full_papers/plank/plank_html/
-# - Also backup folders meta-data? (to reconstruct the tree in case a folder is truncated by bit rot)
-#
-
-# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
-import sys, os
-thispathname = os.path.dirname(__file__)
-sys.path.append(os.path.join(thispathname))
-
-# Import necessary libraries
-from lib._compat import _str, _range, _StringIO, b # to support intra-ecc
-from lib.aux_funcs import get_next_entry, is_dir, is_dir_or_file, fullpath, recwalk, sizeof_fmt, path2unix, get_version
-import argparse
-import datetime, time
-import tqdm
-#import itertools
-import math
-#import operator # to get the max out of a dict
-import csv # to process the errors_file from rfigc.py
-import shlex # for string parsing as argv argument to main(), unnecessary otherwise
-from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
-import struct # to support indexes backup file
-from io import BytesIO
-#import pprint # Unnecessary, used only for debugging purposes
-
-# ECC and hashing facade libraries
-from lib.eccman import ECCMan, compute_ecc_params
-from lib.hasher import Hasher
-from reedsolo import ReedSolomonError
-from unireedsolomon.rs import RSCodecError
-
-# Get package version, necessary to input in the ECC the version of pyFileFixity we used, it will be stored in the ecc files,, this can be helpful to disambiguate in the future which version of the software to use for optimal recovery
-__version__ = get_version("__init__.py", thispathname)  # aux function will return root_path as path to lib/ subfolder, so we need to provide our own path here
-
-
-
-#***********************************
-#     AUXILIARY FUNCTIONS
-#***********************************
-
-def feature_scaling(x, xmin, xmax, a=0, b=1):
-    '''Generalized feature scaling (useful for variable error correction rate calculation)'''
-    return a + float(x - xmin) * (b - a) / (xmax - xmin)
-
-#--------------------------------
-
-def entry_fields(file, entry_pos, field_delim="\xFF"):
-    '''From an ecc entry position (a list with starting and ending positions), extract the metadata fields (filename, filesize, ecc for both), and the starting/ending positions of the ecc stream (containing variably encoded blocks of hash and ecc per blocks of the original file's header)'''
-    # Read the the beginning of the ecc entry
-    blocksize = 65535
-    file.seek(entry_pos[0])
-    entry = file.read(blocksize)
-    entry = entry.lstrip(field_delim) # if there was some slight adjustment error (example: the last ecc block of the last file was the field_delim, then we will start with a field_delim, and thus we need to remove the trailing field_delim which is useless and will make the field detection buggy). This is not really a big problem for the previous file's ecc block: the missing ecc characters (which were mistaken for a field_delim), will just be missing (so we will lose a bit of resiliency for the last block of the previous file, but that's not a huge issue, the correction can still rely on the other characters).
-    # TODO: do in a while loop in case the filename is really big (bigger than blocksize) - or in case we add intra-ecc for filename
-
-    # Find metadata fields delimiters positions
-    # TODO: automate this part, just give in argument the number of field_delim to find, and the func will find the x field_delims (the number needs to be specified in argument because the field_delim can maybe be found wrongly inside the ecc stream, which we don't want)
-    first = entry.find(field_delim)
-    second = entry.find(field_delim, first+len(field_delim))
-    third = entry.find(field_delim, second+len(field_delim))
-    fourth = entry.find(field_delim, third+len(field_delim))
-    # Note: we do not try to find all the field delimiters because we optimize here: we just walk the string to find the exact number of field_delim we are looking for, and after we stop, no need to walk through the whole string.
-
-    # Extract the content of the fields
-    # Metadata fields
-    relfilepath = entry[:first]
-    filesize = entry[first+len(field_delim):second]
-    relfilepath_ecc = entry[second+len(field_delim):third]
-    filesize_ecc = entry[third+len(field_delim):fourth]
-    # Ecc stream field (aka ecc blocks)
-    ecc_field_pos = [entry_pos[0]+fourth+len(field_delim), entry_pos[1]] # return the starting and ending position of the rest of the ecc track, which contains blocks of hash/ecc of the original file's content.
-
-    # Place the cursor at the beginning of the ecc_field
-    file.seek(ecc_field_pos[0])
-
-    # Try to convert to an int, an error may happen
-    try:
-        filesize = int(filesize)
-    except Exception as e:
-        print("Exception when trying to detect the filesize in ecc field (it may be corrupted), skipping: ")
-        print(e)
-        #filesize = 0 # avoid setting to 0, we keep as an int so that we can try to fix using intra-ecc
-
-    # entries = [ {"message":, "ecc":, "hash":}, etc.]
-    return {"relfilepath": relfilepath, "relfilepath_ecc": relfilepath_ecc, "filesize": filesize, "filesize_ecc": filesize_ecc, "ecc_field_pos": ecc_field_pos}
-
-def stream_entry_assemble(hasher, file, eccfile, entry_fields, max_block_size, header_size, resilience_rates, constantmode=False):
-    '''From an entry with its parameters (filename, filesize), assemble a list of each block from the original file along with the relative hash and ecc for easy processing later.'''
-    # Cut the header and the ecc entry into blocks, and then assemble them so that we can easily process block by block
-    eccfile.seek(entry_fields["ecc_field_pos"][0])
-    curpos = file.tell()
-    ecc_curpos = eccfile.tell()
-    while (ecc_curpos < entry_fields["ecc_field_pos"][1]): # continue reading the input file until we reach the position of the previously detected ending marker
-        # Compute the current rate, depending on where we are inside the input file (headers? later stage?)
-        if curpos < header_size or constantmode: # header stage: constant rate
-            rate = resilience_rates[0]
-        else: # later stage 2 or 3: progressive rate
-            rate = feature_scaling(curpos, header_size, entry_fields["filesize"], resilience_rates[1], resilience_rates[2]) # find the rate for the current stream of data (interpolate between stage 2 and stage 3 rates depending on the cursor position in the file)
-        # From the rate, compute the ecc parameters
-        ecc_params = compute_ecc_params(max_block_size, rate, hasher)
-        # Extract the message block from input file, given the computed ecc parameters
-        mes = file.read(ecc_params["message_size"])
-        if len(mes) == 0: return # quit if message is empty (reached end-of-file), this is a safeguard if ecc pos ending was miscalculated (we thus only need the starting position to be correct)
-        buf = eccfile.read(ecc_params["hash_size"]+ecc_params["ecc_size"])
-        hash = buf[:ecc_params["hash_size"]]
-        ecc = buf[ecc_params["hash_size"]:]
-
-        yield {"message": mes, "hash": hash, "ecc": ecc, "rate": rate, "ecc_params": ecc_params, "curpos": curpos, "ecc_curpos": ecc_curpos}
-        # Prepare for the next iteration of the loop
-        curpos = file.tell()
-        ecc_curpos = eccfile.tell()
-    # Just a quick safe guard against ecc ending marker misdetection
-    file.seek(0, os.SEEK_END) # alternative way of finding the total size: go to the end of the file
-    size = file.tell()
-    if curpos < size: print("WARNING: end of ecc track reached but not the end of file! Either the ecc ending marker was misdetected, or either the file hash changed! Some blocks maybe may not have been properly checked!")
-
-def stream_compute_ecc_hash(ecc_manager, hasher, file, max_block_size, header_size, resilience_rates):
-    '''Generate a stream of hash/ecc blocks, of variable encoding rate and size, given a file.'''
-    curpos = file.tell() # init the reading cursor at the beginning of the file
-    # Find the total size to know when to stop
-    #size = os.fstat(file.fileno()).st_size # old way of doing it, doesn't work with _StringIO objects
-    file.seek(0, os.SEEK_END) # alternative way of finding the total size: go to the end of the file
-    size = file.tell()
-    file.seek(0, curpos) # place the reading cursor back at the beginning of the file
-    # Main encoding loop
-    while curpos < size: # Continue encoding while we do not reach the end of the file
-        # Calculating the encoding rate
-        if curpos < header_size: # if we are still reading the file's header, we use a constant rate
-            rate = resilience_rates[0]
-        else: # else we use a progressive rate for the rest of the file the we calculate on-the-fly depending on our current reading cursor position in the file
-            rate = feature_scaling(curpos, header_size, size, resilience_rates[1], resilience_rates[2]) # find the rate for the current stream of data (interpolate between stage 2 and stage 3 rates depending on the cursor position in the file)
-
-        # Compute the ecc parameters given the calculated rate
-        ecc_params = compute_ecc_params(max_block_size, rate, hasher)
-        #ecc_manager = ECCMan(max_block_size, ecc_params["message_size"]) # not necessary to create an ecc manager anymore, as it is very costly. Now we can specify a value for k on the fly (tables for all possible values of k are pre-generated in the reed-solomon libraries)
-
-        # Compute the ecc and hash for the current message block
-        mes = file.read(ecc_params["message_size"])
-        hash = hasher.hash(mes)
-        ecc = ecc_manager.encode(mes, k=ecc_params["message_size"])
-        #print("mes %i (%i) - ecc %i (%i) - hash %i (%i)" % (len(mes), message_size, len(ecc), ecc_params["ecc_size"], len(hash), ecc_params["hash_size"])) # DEBUGLINE
-
-        # Return the result
-        yield [b(hash), b(ecc), ecc_params]
-        # Prepare for next iteration
-        curpos = file.tell()
-
-def compute_ecc_hash_from_string(string, ecc_manager, hasher, max_block_size, resilience_rate):
-    '''Generate a concatenated string of ecc stream of hash/ecc blocks, of constant encoding rate, given a string.
-    NOTE: resilience_rate here is constant, you need to supply only one rate, between 0.0 and 1.0. The encoding rate will then be constant, like in header_ecc.py.'''
-    fpfile = BytesIO(b(string))
-    ecc_stream = b''.join( [b(x[1]) for x in stream_compute_ecc_hash(ecc_manager, hasher, fpfile, max_block_size, len(string), [resilience_rate])] ) # "hack" the function by tricking it to always use a constant rate, by setting the header_size=len(relfilepath), and supplying the resilience_rate_intra instead of resilience_rate_s1 (the one for header)
-    return ecc_stream
-
-def ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, field, ecc, entry_pos, enable_erasures=False, erasures_char="\x00", only_erasures=False, max_block_size=65535):
-    """ Correct an intra-field with its corresponding intra-ecc if necessary """
-    # convert strings to _StringIO object so that we can trick our ecc reading functions that normally works only on files
-    fpfile = BytesIO(b(field))
-    fpfile_ecc = BytesIO(b(ecc))
-    fpentry_p = {"ecc_field_pos": [0, len(field)]} # create a fake entry_pos so that the ecc reading function works correctly
-    # Prepare variables
-    field_correct = [] # will store each block of the corrected (or already correct) filepath
-    fcorrupted = False # check if field was corrupted
-    fcorrected = True # check if field was corrected (if it was corrupted)
-    errmsg = ''
-    # Decode each block of the filepath
-    for e in stream_entry_assemble(hasher_intra, fpfile, fpfile_ecc, fpentry_p, max_block_size, len(field), [resilience_rate_intra], constantmode=True):
-        # Check if this block of the filepath is OK, if yes then we just copy it over
-        if ecc_manager_intra.check(e["message"], e["ecc"]):
-            field_correct.append(e["message"])
-        else: # Else this block is corrupted, we will try to fix it using the ecc
-            fcorrupted = True
-            # Repair the message block and the ecc
-            try:
-                repaired_block, repaired_ecc = ecc_manager_intra.decode(e["message"], e["ecc"], enable_erasures=enable_erasures, erasures_char=erasures_char, only_erasures=only_erasures)
-            except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
-                repaired_block = None
-                repaired_ecc = None
-                errmsg += "- Error: unrecoverable corrupted metadata field at offset %i: %s\n" % (entry_pos[0], exc)
-            # Check if the block was successfully repaired: if yes then we copy the repaired block...
-            if repaired_block is not None and ecc_manager_intra.check(repaired_block, repaired_ecc):
-                field_correct.append(repaired_block)
-            else: # ... else it failed, then we copy the original corrupted block and report an error later
-                field_correct.append(e["message"])
-                fcorrected = False
-    # Join all the blocks into one string to build the final filepath
-    field_correct = [b(x) for x in field_correct] # workaround when using --ecc_algo 3 or 4, because we get a list of bytearrays instead of str
-    field = b''.join(field_correct)
-    # Report errors
-    return (field, fcorrupted, fcorrected, errmsg)
-
-
-
-#***********************************
-#        GUI AUX FUNCTIONS
-#***********************************
-
-# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
-try:  # pragma: no cover
-    import gooey
-except ImportError as exc:
-    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
-    class gooey(object):  # pragma: no cover
-        def Gooey(func):
-            return func
-    # If --gui was specified, then there's a problem
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
-        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
-        raise(exc)
-
-def conditional_decorator(flag, dec):  # pragma: no cover
-    def decorate(fn):
-        if flag:
-            return dec(fn)
-        else:
-            return fn
-    return decorate
-
-def check_gui_arg():  # pragma: no cover
-    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
-    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
-        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
-        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
-        return True
-    else:
-        return False
-
-def AutoGooey(fn):  # pragma: no cover
-    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
-    if check_gui_arg():
-        return gooey.Gooey(fn)
-    else:
-        return fn
-
-
-
-#***********************************
-#                       MAIN
-#***********************************
-
-@AutoGooey
-def main(argv=None, command=None):
-    if argv is None: # if argv is empty, fetch from the commandline
-        argv = sys.argv[1:]
-    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
-        argv = shlex.split(argv) # Parse string just like argv using shlex
-
-    #==== COMMANDLINE PARSER ====
-
-    #== Commandline description
-    desc = '''Structural Adaptive Error Correction Code generator and checker
-Description: Given a directory, this application will generate error correcting codes or correct corrupt files, using a structural adaptive approach (the headers will be protected by more ecc bits than subsequent parts, progressively decreasing in the resilience).
-Note: Folders meta-data is NOT accounted, only the files! Use DVDisaster or a similar tool to also cover folders meta-data.
-    '''
-    ep = '''
-Note1: this is a pure-python implementation (except for MD5 hash but a pure-python alternative is provided in lib/md5py.py), thus it may be VERY slow to generate an ecc file. To speed-up things considerably, you can use PyPy v2.5.0 or above, there will be a speed-up of at least 100x from our experiments (you can expect an encoding rate of more than 1MB/s). Feel free to profile using easy_profiler.py and try to optimize the encoding parts of the reed-solomon libraries.
-
-Note2: that Reed-Solomon can correct up to 2*resilience_rate erasures (eg, null bytes, you know where they are) or resilience_rate errors (an error is a corrupted character but you don't know its position) and amount to an additional storage of 2*resilience_rate storage compared to the original total files size.
-'''
-
-    #== Commandline arguments
-    #-- Constructing the parser
-    # Use GooeyParser if we want the GUI because it will provide better widgets
-    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
-        # Initialize the Gooey parser
-        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
-        widget_dir = {"widget": "DirChooser"}
-        widget_filesave = {"widget": "FileSaver"}
-        widget_file = {"widget": "FileChooser"}
-        widget_text = {"widget": "TextField"}
-    else: # Else in command-line usage, use the standard argparse
-        # Delete the special argument to avoid unrecognized argument error in argparse
-        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
-        # Initialize the normal argparse parser
-        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
-        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
-        # Define dummy dict to keep compatibile with command-line usage
-        widget_dir = {}
-        widget_filesave = {}
-        widget_file = {}
-        widget_text = {}
-    # Required arguments
-    main_parser.add_argument('-i', '--input', metavar='/path/to/root/folder', type=is_dir_or_file, nargs=1, required=True,
-                        help='Path to the root folder (or a single file) from where the scanning will occur.', **widget_dir)
-    main_parser.add_argument('-d', '--database', metavar='/some/folder/ecc.txt', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
-                        help='Path to the file containing the ECC informations.', **widget_filesave)
-
-    # Optional general arguments
-    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
-                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
-    main_parser.add_argument('--max_block_size', type=int, default=255, required=False,
-                        help='Reed-Solomon max block size (maximum = 255). It is advised to keep it at the maximum for more resilience (see comments at the top of the script for more info). However, if encoding it too slow, using a smaller value will speed things up greatly, at the expense of more storage space (because hash will relatively take more space - you can use --hash "shortmd5" or --hash "minimd5" to counter balance).', **widget_text)
-    main_parser.add_argument('-s', '--size', type=int, default=1024, required=False,
-                        help='Headers block size to protect with resilience rate stage 1 (eg: 1024 meants that the first 1k of each file will be protected by stage 1).', **widget_text)
-    main_parser.add_argument('-r1', '--resilience_rate_stage1', type=float, default=0.3, required=False,
-                        help='Resilience rate for files headers (eg: 0.3 = 30%% of errors can be recovered but size of codeword will be 60%% of the data block).', **widget_text)
-    main_parser.add_argument('-r2', '--resilience_rate_stage2', type=float, default=0.2, required=False,
-                        help='Resilience rate for stage 2 (after headers, this is the starting rate applied to the rest of the file, which will be gradually lessened towards the end of the file to the stage 3 rate).', **widget_text)
-    main_parser.add_argument('-r3', '--resilience_rate_stage3', type=float, default=0.1, required=False,
-                        help='Resilience rate for stage 3 (rate that will be applied towards the end of the files).', **widget_text)
-    main_parser.add_argument('-ri', '--resilience_rate_intra', type=float, default=0.5, required=False,
-                        help='Resilience rate for intra-ecc (ecc on meta-data, such as filepath, thus this defines the ecc for the critical spots!).', **widget_text)
-    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
-                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
-    main_parser.add_argument('--stats_only', action='store_true', required=False, default=False,
-                        help='Only show the predicted total size of the ECC file given the parameters.')
-    main_parser.add_argument('--hash', metavar='md5;shortmd5;shortsha256...', type=str, required=False,
-                        help='Hash algorithm to use. Choose between: md5, shortmd5, shortsha256, minimd5, minisha256.', **widget_text)
-    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
-                        help='Verbose mode (show more output).')
-    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
-                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
-
-    # Correction mode arguments
-    main_parser.add_argument('-c', '--correct', action='store_true', required=False, default=False,
-                        help='Check/Correct the files')
-    main_parser.add_argument('-o', '--output', metavar='/path/to/output/folder', type=is_dir, nargs=1, required=False,
-                        help='Path of the folder where the repaired files will be copied (only repaired corrupted files will be copied there, files that weren\'t corrupted at all won\'t be copied so you have to copy them by yourself afterwards).', **widget_dir)
-    main_parser.add_argument('-e', '--errors_file', metavar='/some/folder/errorsfile.csv', type=str, nargs=1, required=False, #type=argparse.FileType('rt')
-                        help='Path to the error file generated by RFIGC.py. The software will automatically correct those files and only those files.', **widget_file)
-    main_parser.add_argument('--ignore_size', action='store_true', required=False, default=False,
-                        help='On correction, if the file size differs from when the ecc file was generated, ignore and try to correct anyway (this may work with file where data was appended without changing the rest. For compressed formats like zip, this will probably fail).')
-    main_parser.add_argument('--no_fast_check', action='store_true', required=False, default=False,
-                        help='On correction, block corruption is only checked with the hash (the ecc will still be checked after correction, but not before). If no_fast_check is enabled, then ecc will also be checked before. This allows to find blocks corrupted by malicious intent (the block is corrupted but the hash has been corrupted as well to match the corrupted block, because it\'s almost impossible that following a hardware or logical fault, the hash match the corrupted block).')
-    main_parser.add_argument('--skip_missing', action='store_true', required=False, default=False,
-                        help='Skip missing files (no warning).')
-    main_parser.add_argument('--enable_erasures', action='store_true', required=False, default=False,
-                        help='Enable errors-and-erasures correction. Reed-Solomon can correct twice more erasures than errors (eg, if resilience rate is 0.3, then you can correct 30%% errors and 60%% erasures and any combination of errors and erasures between 30%%-60%% corruption). An erasure is a corrupted symbol where we know the position, while errors are not known at all. To find erasures, we will find any symbol that is equal to --erasure_symbol and flag it as an erasure. This is particularly useful if the software you use (eg, a disk scraper) can mark bad sectors with a constant character (eg, null byte). Misdetected erasures will just eat one ecc symbol, and won\'t change the decoded message.')
-    main_parser.add_argument('--only_erasures', action='store_true', required=False, default=False,
-                        help='Enable only erasures correction (no errors). Use this only if you are sure that all corrupted symbols have the same value (eg, if your disk scraper replace bad sectors by null bytes). This will ensure that you can correct up to 2*resilience_rate corrupted symbols.')
-    main_parser.add_argument('--erasure_symbol', type=int, default=0, required=False,
-                        help='Symbol that will be flagged as an erasure. Default: null byte 0. (value must be an integer)', **widget_text)
-
-    # Generate mode arguments
-    main_parser.add_argument('-g', '--generate', action='store_true', required=False, default=False,
-                        help='Generate the ecc file?')
-    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
-                        help='Force overwriting the ecc file even if it already exists (if --generate).')
-    main_parser.add_argument('--skip_size_below', type=int, default=None, required=False,
-                        help='Skip files below the specified size (in bytes).', **widget_text)
-    main_parser.add_argument('--always_include_ext', metavar='txt|jpg|png', type=str, default=None, required=False,
-                        help='Always include files with the specified extensions, useful in combination with --skip_size_below to keep files of certain types even if they are below the size. Format: extensions separated by |.', **widget_text)
-
-    #== Parsing the arguments
-    args = main_parser.parse_args(argv) # Storing all arguments to args
-
-    #-- Set hard-coded variables
-    entrymarker = "\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF" # marker that will signal the beginning of an ecc entry - use an alternating pattern of several characters, this avoids confusion (eg: if you use "AAA" as a pattern, if the ecc block of the previous file ends with "EGA" for example, then the full string for example will be "EGAAAAC:\yourfolder\filea.jpg" and then the entry reader will detect the first "AAA" occurrence as the entry start - this should not make the next entry bug because there is an automatic trim - but the previous ecc block will miss one character that could be used to repair the block because it will be "EG" instead of "EGA"!)
-    field_delim = "\xFA\xFF\xFA\xFF\xFA" # delimiter between fields (filepath, filesize, hash+ecc blocks) inside an ecc entry
-
-    #-- Set variables from arguments
-    inputpath = fullpath(args.input[0]) # path to the files to protect (either a folder or a single file)
-    rootfolderpath = inputpath # path to the root folder (to compute relative paths)
-    database = fullpath(args.database[0])
-    generate = args.generate
-    correct = args.correct
-    force = args.force
-    stats_only = args.stats_only
-    max_block_size = args.max_block_size
-    header_size = args.size
-    resilience_rate_s1 = args.resilience_rate_stage1
-    resilience_rate_s2 = args.resilience_rate_stage2
-    resilience_rate_s3 = args.resilience_rate_stage3
-    resilience_rate_intra = args.resilience_rate_intra
-    enable_erasures = args.enable_erasures
-    only_erasures = args.only_erasures
-    erasure_symbol = args.erasure_symbol
-    ignore_size = args.ignore_size
-    skip_missing = args.skip_missing
-    skip_size_below = args.skip_size_below
-    always_include_ext = args.always_include_ext
-    if always_include_ext: always_include_ext = tuple(['.'+ext for ext in always_include_ext.split('|')]) # prepare a tuple of extensions (prepending with a dot) so that str.endswith() works (it doesn't with a list, only a tuple)
-    hash_algo = args.hash
-    if not hash_algo: hash_algo = "md5"
-    ecc_algo = args.ecc_algo
-    fast_check = not args.no_fast_check
-    verbose = args.verbose
-    silent = args.silent
-
-    if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
-        rootfolderpath = os.path.dirname(inputpath)
-
-    if correct:
-        if not args.output:
-            raise NameError('Output path is necessary when in correction mode!')
-        outputpath = fullpath(args.output[0])
-
-    errors_file = None
-    if args.errors_file: errors_file = os.path.basename(fullpath(args.errors_file[0]))
-
-    # -- Checking arguments
-    if not stats_only and not generate and not os.path.isfile(database):
-        raise NameError('Specified database ecc file %s does not exist!' % database)
-    elif generate and os.path.isfile(database) and not force:
-        raise NameError('Specified database ecc file %s already exists! Use --force if you want to overwrite.' % database)
-
-    if resilience_rate_s1 <= 0 or resilience_rate_s2 <= 0 or resilience_rate_s3 <= 0 or resilience_rate_intra <= 0:
-        raise ValueError('Resilience rates cannot be negative nor zero and they must be floating numbers.');
-
-    if max_block_size < 2 or max_block_size > 255:
-        raise ValueError('RS max block size must be between 2 and 255.')
-
-    if header_size < 1:
-        raise ValueError('Header size cannot be negative.')
-
-    if hash_algo not in Hasher.known_algo:
-        raise ValueError("Specified hash algorithm %s is unknown!" % hash_algo)
-
-    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
-    if args.log:
-        ptee = Tee(args.log[0], 'a', nostdout=silent)
-        #sys.stdout = Tee(args.log[0], 'a')
-        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
-    else:
-        ptee = Tee(nostdout=silent)
-
-
-    # == PROCESSING BRANCHING == #
-
-    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
-    ptee.write("Initializing the ECC codecs, please wait...")
-    resilience_rates = [resilience_rate_s1, resilience_rate_s2, resilience_rate_s3]
-    hasher = Hasher(hash_algo)
-    hasher_intra = Hasher('none') # for intra_ecc we don't use any hash
-    ecc_params_header = compute_ecc_params(max_block_size, resilience_rate_s1, hasher)
-    ecc_manager_header = ECCMan(max_block_size, ecc_params_header["message_size"], algo=ecc_algo)
-    ecc_manager_variable = ECCMan(max_block_size, 1, algo=ecc_algo)
-    ecc_params_intra = compute_ecc_params(max_block_size, resilience_rate_intra, hasher_intra)
-    ecc_manager_intra = ECCMan(max_block_size, ecc_params_intra["message_size"], algo=ecc_algo)
-    ecc_params_idx = compute_ecc_params(27, 1, hasher_intra)
-    ecc_manager_idx = ECCMan(27, ecc_params_idx["message_size"], algo=ecc_algo)
-    # for stats only
-    ecc_params_variable_average = compute_ecc_params(max_block_size, (resilience_rate_s2 + resilience_rate_s3)/2, hasher) # compute the average variable rate to compute statistics
-    ecc_params_s2 = compute_ecc_params(max_block_size, resilience_rate_s2, hasher)
-    ecc_params_s3 = compute_ecc_params(max_block_size, resilience_rate_s3, hasher)
-
-    # == Precomputation of ecc file size
-    # Precomputing is important so that the user can know what size to expect before starting (and how much time it will take...).
-    filescount = 0
-    sizetotal = 0
-    sizeecc = 0
-    ptee.write("Precomputing list of files and predicted statistics...")
-    for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee):
-        filescount = filescount + 1 # counting the total number of files we will process (so that we can show a progress bar with ETA)
-        # Get full absolute filepath
-        filepath = os.path.join(dirpath, filename)
-        relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
-        # Get the current file's size
-        size = os.stat(filepath).st_size
-        # Check if we must skip this file because size is too small, and then if we still keep it because it's extension is always to be included
-        if skip_size_below and size < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)): continue
-
-        # Compute total size of all files
-        sizetotal = sizetotal + size
-        # Compute predicted size of their headers
-        if size >= header_size: # for big files, we limit the size to the header size
-            filesize_header = header_size
-            filesize_content = size - header_size
-        else: # else for size smaller than the defined header size, it will just be the size of the file
-            filesize_header = size
-            filesize_content = 0
-        # Size of the ecc entry for this file will be: entrymarker-bytes + field_delim-bytes*occurrence + length-filepath-string + length-size-string + length-filepath-ecc + size of the ecc per block for all blocks in file header + size of the hash per block for all blocks in file header.
-        sizeecc += (len(entrymarker) + len(field_delim)*3 + len(relfilepath) + len(str(size)) + int(float(len(relfilepath))*resilience_rate_intra) + (int(math.ceil(float(filesize_header) / ecc_params_header["message_size"])) * (ecc_params_header["ecc_size"]+ecc_params_header["hash_size"])) + (int(math.ceil(float(filesize_content) / ecc_params_variable_average["message_size"])) * (ecc_params_variable_average["ecc_size"]+ecc_params_variable_average["hash_size"])) ) # Compute the total number of bytes we will add with ecc + hash (accounting for the padding of the remaining characters at the end of the sequence in case it doesn't fit with the message_size, by using ceil() )
-    ptee.write("Precomputing done.")
-    if generate: # show statistics only if generating an ecc file
-        # TODO: add the size of the ecc format header? (arguments string + PYHEADERECC identifier)
-        total_pred_percentage = sizeecc * 100 / sizetotal
-        ptee.write("Total ECC size estimation: %s = %g%% of total files size %s." % (sizeof_fmt(sizeecc), total_pred_percentage, sizeof_fmt(sizetotal)))
-        ptee.write("Details per stage:")
-        ptee.write("- Resiliency stage1 of %i%%: For the header (first %i characters) of each file: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate_s1*100, header_size, ecc_params_header["message_size"], ecc_params_header["ecc_size"], int(ecc_params_header["ecc_size"] / 2), ecc_params_header["ecc_size"]))
-        ptee.write("- Resiliency stage2 of %i%%: for the rest of the file, the parameters will start with: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate_s2*100, ecc_params_s2["message_size"], ecc_params_s2["ecc_size"], int(ecc_params_s2["ecc_size"] / 2), ecc_params_s2["ecc_size"]))
-        ptee.write("- Resiliency stage3 of %i%%: progressively towards the end, the parameters will gradually become: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate_s3*100, ecc_params_s3["message_size"], ecc_params_s3["ecc_size"], int(ecc_params_s3["ecc_size"] / 2), ecc_params_s3["ecc_size"]))
-        if max_block_size > 100: ptee.write("Note: current max_block_size (size of message+ecc blocks) is %i. Consider using a smaller value to greatly speedup the processing (because Reed-Solomon encoding complexity is about O(max_block_size^2)) at the expense of generating a bigger ecc file and less burst error resiliency (because the ecc blocks will be smaller)." % max_block_size)
-
-    if stats_only:
-        ptee.close()
-        return 0
-
-    # == Generation mode
-    # Generate an ecc file, containing ecc entries for every files recursively in the specified root folder.
-    # The file header will be split by blocks depending on max_block_size and resilience_rate, and each of those blocks will be hashed and a Reed-Solomon code will be produced.
-    if generate:
-        ptee.write("====================================")
-        ptee.write("Structural adaptive ECC generation, started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        with open(database, 'wb') as db, open(database+".idx", 'wb') as dbidx:
-            # Write ECC file header identifier (unique string + version)
-            db.write( b("**PYSTRUCTADAPTECCv%s**\n" % (''.join([x * 3 for x in __version__]))) ) # each character in the version will be repeated 3 times, so that in case of tampering, a majority vote can try to disambiguate)
-            # Write the parameters (they are NOT reloaded automatically, you have to specify them at commandline! It's the user role to memorize those parameters (using any means: own brain memory, keep a copy on paper, on email, etc.), so that the parameters are NEVER tampered. The parameters MUST be ultra reliable so that errors in the ECC file can be more efficiently recovered.
-            for i in _range(3): db.write( ("** Parameters: "+" ".join(argv) + "\n").encode() ) # copy them 3 times just to be redundant in case of ecc file corruption
-            db.write( b("** Generated under %s\n" % ecc_manager_variable.description()) )
-            # NOTE: there's NO HEADER for the ecc file! Ecc entries are all independent of each others, you just need to supply the decoding arguments at commandline, and the ecc entries can be decoded. This is done on purpose to be remove the risk of critical spots in ecc file.
-
-            # Compile the list of files to put in the header
-            #filesheader = [':'.join([str(i), str(item[0]), str(item[1])]) for i, item in enumerate(itertools.izip(fileslist, filessizes))]
-            #for i in _range(4): # replicate the headers several times as a safeguard for corruption
-                #db.write("**" + '|'.join(filesheader) + "**\n")
-
-            # Processing ecc on files
-            files_done = 0
-            files_skipped = 0
-            bardisp = tqdm.tqdm(total=sizetotal, file=ptee, leave=True, unit='B', unit_scale=True, mininterval=1)
-            for (dirpath, filename) in recwalk(inputpath):
-                # Get full absolute filepath
-                filepath = os.path.join(dirpath,filename)
-                # Get database relative path (from scanning root folder)
-                relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
-                # Get file size
-                filesize = os.stat(filepath).st_size
-                # If skip size is enabled and size is below the skip size, we skip UNLESS the file extension is in the always include list
-                if skip_size_below and filesize < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)):
-                    files_skipped += 1
-                    continue
-
-                # Opening the input file's to read its header and compute the ecc/hash blocks
-                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
-                with open(os.path.join(rootfolderpath, filepath), 'rb') as file:
-                    entrymarker_pos = db.tell() # backup the position of the start of this ecc entry
-                    # -- Intra-ecc generation: Compute an ecc for the filepath, to avoid a critical spot here (so that we don't care that the filepath gets corrupted, we have an ecc to fix it!)
-                    relfilepath_ecc = compute_ecc_hash_from_string(relfilepath, ecc_manager_intra, hasher_intra, max_block_size, resilience_rate_intra)
-                    filesize_ecc = compute_ecc_hash_from_string(b(str(filesize)), ecc_manager_intra, hasher_intra, max_block_size, resilience_rate_intra)
-                    db.write( b''.join([b(entrymarker), b(relfilepath), b(field_delim), b(str(filesize)), b(field_delim), b(relfilepath_ecc), b(field_delim), b(filesize_ecc), b(field_delim)]) ) # first save the file's metadata (filename, filesize, ecc for filename, ...), separated with field_delim
-                    # -- External indexes backup: calculate the position of the entrymarker and of each field delimiter, and compute their ecc, and save into the index backup file. This will allow later to retrieve the position of each marker in the ecc file, and repair them if necessary, while just incurring a very cheap storage cost.
-                    # Also, the index backup file is fixed delimited fields sizes, which means that each field has a very specifically delimited size, so that we don't need any marker: we can just compute the total size for each entry, and thus find all entries independently even if one or several are corrupted beyond repair, so that this won't affect other index entries.
-                    markers_pos = [
-                                                    entrymarker_pos,
-                                                    entrymarker_pos+len(entrymarker)+len(relfilepath),
-                                                    entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize)),
-                                                    entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize))+len(field_delim)+len(relfilepath_ecc),
-                                                    db.tell()-len(field_delim)
-                                                    ] # Make the list of all markers positions for this ecc entry. The first and last indexes are the most important (first is the entrymarker, the last is the field_delim just before the ecc track start)
-                    markers_pos = [struct.pack('>Q', x) for x in markers_pos] # Convert to a binary representation in 8 bytes using unsigned long long (up to 16 EB, this should be more than sufficient)
-                    markers_types = [b'1', b'2', b'2', b'2', b'2']
-                    markers_pos_ecc = [ecc_manager_idx.encode(x+y) for x,y in zip(markers_types,markers_pos)] # compute the ecc for each number
-                    # Couple each marker's position with its type and with its ecc, and write them all consecutively into the index backup file
-                    for items in zip(markers_types,markers_pos,markers_pos_ecc):
-                        for item in items:
-                            dbidx.write(b(item))
-                    # -- Hash/Ecc encoding of file's content (everything is managed inside stream_compute_ecc_hash)
-                    for ecc_entry in stream_compute_ecc_hash(ecc_manager_variable, hasher, file, max_block_size, header_size, resilience_rates): # then compute the ecc/hash entry for this file's header (each value will be a block, a string of hash+ecc per block of data, because Reed-Solomon is limited to a maximum of 255 bytes, including the original_message+ecc! And in addition we want to use a variable rate for RS that is decreasing along the file)
-                        db.write( b''.join([b(ecc_entry[0]), b(ecc_entry[1])]) ) # note that there's no separator between consecutive blocks, but by calculating the ecc parameters, we will know when decoding the size of each block!
-                        bardisp.update(ecc_entry[2]['message_size'])
-                files_done += 1
-        if bardisp.n > bardisp.total: bardisp.total = bardisp.n # small workaround because n may be higher than total (because of files ending before 'message_size', thus the message is padded and in the end, we have outputted and processed a bit more characters than are really in the files, thus why total can be below n). Doing this allows to keep the trace of the progression bar.
-        bardisp.close()
-        ptee.write("All done! Total number of files processed: %i, skipped: %i" % (files_done, files_skipped))
-        ptee.close()
-        return 0
-
-    # == Error Correction (and checking by hash) mode
-    # For each file, check their headers by block by checking each block against a hash, and if the hash does not match, try to correct with Reed-Solomon and then check the hash again to see if we correctly repaired the block (else the ecc entry might have been corrupted, whether it's the hash or the ecc field, in both cases it's highly unlikely that a wrong repair will match the hash after this wrong repair)
-    elif correct:
-        ptee.write("====================================")
-        ptee.write("Structural adaptive ECC correction, started on %s" % datetime.datetime.now().isoformat())
-        ptee.write("====================================")
-
-        # Prepare the list of files with errors to reduce the scan (only if provided)
-        errors_filelist = []
-        if errors_file:
-            with open(errors_file, 'rb') as efile:
-                for row in csv.DictReader(efile, lineterminator='\n', delimiter='|', quotechar='"', fieldnames=['filepath', 'error']): # need to specify the fieldnames, else the first row in the csv file will be skipped (it will be used as the columns names)
-                    errors_filelist.append(row['filepath'])
-
-        # Read the ecc file
-        dbsize = os.stat(database).st_size # must get db file size before opening it in order not to move the cursor
-        with open(database, 'rb') as db:
-            # Counters
-            files_count = 0
-            files_corrupted = 0
-            files_repaired_partially = 0
-            files_repaired_completely = 0
-            files_skipped = 0
-            bardisp_first_open = True
-
-            # Main loop: process each ecc entry
-            entry = 1 # to start the while loop
-            bardisp = tqdm.tqdm(total=dbsize, file=ptee, leave=True, desc='DBREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
-            while entry:
-
-                # -- Read the next ecc entry (extract the raw string from the ecc file)
-                #if replication_rate == 1:
-                entry_pos = get_next_entry(db, entrymarker)
-                if entry_pos:
-                    if bardisp_first_open: bardisp.n = entry_pos[0]-len(entrymarker) # add the size of the comments in the ecc header
-                    bardisp.update(entry_pos[1]-entry_pos[0]+len(entrymarker)) # update progress bar
-
-                # No entry? Then we finished because this is the end of file (stop condition)
-                if not entry_pos: break
-
-                # -- Extract the fields from the ecc entry
-                entry_p = entry_fields(db, entry_pos, b(field_delim))
-
-                # -- Get file path, check its correctness and correct it by using intra-ecc if necessary
-                relfilepath = entry_p["relfilepath"] # Relative file path, given in the ecc fields
-                relfilepath, fpcorrupted, fpcorrected, fperrmsg = ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, relfilepath, entry_p["relfilepath_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures, max_block_size=max_block_size)
-                # Report errors
-                if fpcorrupted:
-                    if fpcorrected: ptee.write("\n- Fixed error in metadata field at offset %i filepath %s." % (entry_pos[0], filepath))
-                    else: ptee.write("\n- Error in filepath, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], filepath))
-                ptee.write(fperrmsg)
-
-                # Convert to str (so that we can use os.path funcs)
-                relfilepath = relfilepath.decode('latin-1')
-                # Update entry_p
-                entry_p["relfilepath"] = relfilepath
-                # -- End of intra-ecc on filepath
-
-                # -- Get file size, check its correctness and correct it by using intra-ecc if necessary
-                filesize = str(entry_p["filesize"])
-                filesize, fscorrupted, fscorrected, fserrmsg = ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, filesize, entry_p["filesize_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures, max_block_size=max_block_size)
-
-                # Report errors
-                if fscorrupted:
-                    if fscorrected: ptee.write("\n- Fixed error in metadata field at offset %i filesize %s." % (entry_pos[0], filesize))
-                    else: ptee.write("\n- Error in filesize, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], filesize))
-                ptee.write(fserrmsg)
-
-                # Convert filesize intra-field into an int
-                filesize = int(filesize)
-
-                # Update entry_p
-                entry_p["filesize"] = filesize # need to update entry_p because various funcs will directly access filesize this way...
-                # -- End of intra-ecc on filesize
-
-                # Build the absolute file path
-                filepath = os.path.join(rootfolderpath, relfilepath) # Get full absolute filepath from given input folder (because the files may be specified in any folder, in the ecc file the paths are relative, so that the files can be moved around or burnt on optical discs)
-                if errors_filelist and relfilepath not in errors_filelist: continue # if a list of files with errors was supplied (for example by rfigc.py), then we will check only those files and skip the others
-
-                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
-
-                # -- Check filepath
-                # Check that the filepath isn't corrupted (if a silent error erase a character (not only flip a bit), then it will also be detected this way)
-                if relfilepath.find("\x00") >= 0:
-                    ptee.write("Error: ecc entry corrupted on filepath field, please try to manually repair the filepath (filepath: %s - missing/corrupted character at %i)." % (relfilepath, relfilepath.find("\x00")))
-                    files_skipped += 1
-                    continue
-                # Check that file still exists before checking it
-                if not os.path.isfile(filepath):
-                    if not skip_missing: ptee.write("Error: file %s could not be found: either file was moved or the ecc entry was corrupted (filepath is incorrect?)." % relfilepath)
-                    files_skipped += 1
-                    continue
-
-                # -- Checking file size: if the size has changed, the blocks may not match anymore!
-                real_filesize = os.stat(filepath).st_size
-                if filesize != real_filesize:
-                    if ignore_size:
-                        ptee.write("Warning: file %s has a different size: %s (before: %s). Will still try to correct it (but the blocks may not match!)." % (relfilepath, real_filesize, filesize))
-                    else:
-                        ptee.write("Error: file %s has a different size: %s (before: %s). Skipping the file correction because blocks may not match (you can set --ignore_size to still correct even if size is different, maybe just the entry was corrupted)." % (relfilepath, real_filesize, filesize))
-                        files_skipped += 1
-                        continue
-
-                files_count += 1
-                # -- Check blocks and repair if necessary
-                corrupted = False # flag to signal that the file was corrupted and we need to reconstruct it afterwards
-                repaired_partially = False # flag to signal if a file was repaired only partially
-                # Do a first run to check if there's any error. If yes, then we will begin back from the start of the file but this time we will streamline copy the data to an output file.
-                with open(filepath, 'rb') as file:
-                    # For each message block, check the message with hash and repair with ecc if necessary
-                    for i, e in enumerate(stream_entry_assemble(hasher, file, db, entry_p, max_block_size, header_size, resilience_rates)): # Extract and assemble each message block from the original file with its corresponding ecc and hash
-                        # If the message block has a different hash or the message+ecc is corrupted (syndrome is not null), it was corrupted (or the hash is corrupted or one of the characters of the ecc was corrupted, or both). In any case, it's an any clause here (any potential corruption condition triggers the correction).
-                        if hasher.hash(e["message"]) != e["hash"] or (not fast_check and not ecc_manager_variable.check(e["message"], e["ecc"], k=e["ecc_params"]["message_size"])):
-                            corrupted = True
-                            break
-                # -- Reconstruct/Copying the repaired file
-                # If the first run detected a corruption, then we try to repair the file (we create an output file where good blocks will be copied as-is but bad blocks will be repaired, if it's possible)
-                if corrupted:
-                    files_corrupted += 1
-                    repaired_one_block = False # flag to check that we could repair at least one block, else we will delete the output file since we didn't do anything
-                    err_consecutive = True # flag to check if the ecc track is misaligned/misdetected (we only encounter corrupted blocks that we can't fix)
-                    with open(filepath, 'rb') as file:
-                        outfilepath = os.path.join(outputpath, relfilepath) # get the full path to the output file
-                        outfiledir = os.path.dirname(outfilepath)
-                        if not os.path.isdir(outfiledir): os.makedirs(outfiledir) # if the target directory does not exist, create it (and create recursively all parent directories too)
-                        with open(outfilepath, 'wb') as outfile:
-                            # TODO: optimize to copy over what we have already checked, so that we get directly to the first error that triggered the correction
-                            # For each message block, check the message with hash and repair with ecc if necessary
-                            for i, e in enumerate(stream_entry_assemble(hasher, file, db, entry_p, max_block_size, header_size, resilience_rates)): # Extract and assemble each message block from the original file with its corresponding ecc and hash
-                                # If the message block has a different hash, it was corrupted (or the hash is corrupted, or both)
-                                if hasher.hash(e["message"]) == e["hash"] and (fast_check or ecc_manager_variable.check(e["message"], e["ecc"], k=e["ecc_params"]["message_size"])):
-                                    outfile.write(e["message"])
-                                    err_consecutive = False
-                                else:
-                                    # Try to repair the block using ECC
-                                    ptee.write("File %s: corruption in block %i. Trying to fix it." % (relfilepath, i))
-                                    try:
-                                        repaired_block, repaired_ecc = ecc_manager_variable.decode(e["message"], e["ecc"], k=e["ecc_params"]["message_size"], enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
-                                    except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
-                                        repaired_block = None
-                                        repaired_ecc = None
-                                        print("Error: file %s: block %i: %s" % (relfilepath, i, exc))
-                                    # Check if the repair was successful. This is an "all" condition: if all checks fail, then the correction failed. Else, we assume that the checks failed because the ecc entry was partially corrupted (it's highly improbable that any one check success by chance, it's a lot more probable that it's simply that the entry was partially corrupted, eg: the hash was corrupted and thus cannot match anymore).
-                                    hash_ok = False
-                                    ecc_ok = False
-                                    if repaired_block is not None:
-                                        hash_ok = (hasher.hash(repaired_block) == e["hash"])
-                                        ecc_ok = ecc_manager_variable.check(repaired_block, repaired_ecc, k=e["ecc_params"]["message_size"])
-                                    if repaired_block is not None and (hash_ok or ecc_ok): # If the hash now match the repaired message block, we commit the new block
-                                        outfile.write(repaired_block) # save the repaired block
-                                        # Show a precise report about the repair
-                                        if hash_ok and ecc_ok: ptee.write("File %s: block %i repaired!" % (relfilepath, i))
-                                        elif not hash_ok: ptee.write("File %s: block %i probably repaired with matching ecc check but with a hash error (assume the hash was corrupted)." % (relfilepath, i))
-                                        elif not ecc_ok: ptee.write("File %s: block %i probably repaired with matching hash but with ecc check error (assume the ecc was partially corrupted)." % (relfilepath, i))
-                                        # Turn on the repaired flag, to trigger the copying of the file (else it will be removed if all blocks repairs failed in this file)
-                                        repaired_one_block = True
-                                        err_consecutive = False
-                                    else: # Else the hash does not match: the repair failed (either because the ecc is too much tampered, or because the hash is corrupted. Either way, we don't commit).
-                                        outfile.write(e["message"]) # copy the bad block that we can't repair...
-                                        ptee.write("Error: file %s could not repair block %i (both hash and ecc check mismatch). If you know where the errors are, you can set the characters to a null character so that the ecc may correct twice more characters." % (relfilepath, i)) # you need to code yourself to use bit-recover, it's in perl but it should work given the hash computed by this script and the corresponding message block.
-                                        repaired_partially = True
-                                        # Detect if the ecc track is misaligned/misdetected (we encounter only errors that we can't fix)
-                                        if err_consecutive and i >= 10: # threshold is ten consecutive uncorrectable errors
-                                            ptee.write("Failure: Too many consecutive uncorrectable errors for %s. Most likely, the ecc track was misdetected (try to repair the entrymarkers and field delimiters). Skipping this track/file." % relfilepath)
-                                            db.seek(entry_p["ecc_field_pos"][1]) # Optimization: move the reading cursor to the beginning of the next ecc entry, this will save some iterations in get_next_entry()
-                                            break
-                    # Copying the last access time and last modification time from the original file TODO: a more reliable way would be to use the db computed by rfigc.py, because if a software maliciously tampered the data, then the modification date may also have changed (but not if it's a silent error, in that case we're ok).
-                    filestats = os.stat(filepath)
-                    os.utime(outfilepath, (filestats.st_atime, filestats.st_mtime))
-                    # Check that at least one block was repaired, else we couldn't fix anything in the file and thus we should just remove the output file which is an exact copy of the original without any added value
-                    if not repaired_one_block:
-                        os.remove(outfilepath)
-                    # Counters...
-                    elif repaired_partially:
-                        files_repaired_partially += 1
-                    else:
-                        files_repaired_completely += 1
-        # All ecc entries processed for checking and potentally repairing, we're done correcting!
-        bardisp.close() # at the end, the bar may not be 100% because of the headers that are skipped by read_next_entry() and are not accounted in bardisp.
-        ptee.write("All done! Stats:\n- Total files processed: %i\n- Total files corrupted: %i\n- Total files repaired completely: %i\n- Total files repaired partially: %i\n- Total files corrupted but not repaired at all: %i\n- Total files skipped: %i" % (files_count, files_corrupted, files_repaired_completely, files_repaired_partially, files_corrupted - (files_repaired_partially + files_repaired_completely), files_skipped) )
-        ptee.close()
-        if files_corrupted == 0 or files_repaired_completely == files_corrupted:
-            return 0
-        else:
-            return 1
-
-# Calling main function if the script is directly called (not imported as a library in another program)
-if __name__ == "__main__":  # pragma: no cover
-    sys.exit(main())
+#!/usr/bin/env python
+#
+# Structural Adaptive Error Correction Code
+# Copyright (C) 2015-2023 Larroque Stephen
+#
+# Licensed under the MIT License (MIT)
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+#=================================
+#  Structural Adaptive Error Correction Code generator and checker
+#                by Stephen Larroque
+#                       License: MIT
+#              Creation date: 2015-03-10
+#=================================
+#
+# From : http://simple.wikipedia.org/wiki/Reed-Solomon_error_correction
+# The key idea behind a Reed-Solomon code is that the data encoded is first visualized as a polynomial. The code relies on a theorem from algebra that states that any k distinct points uniquely determine a polynomial of degree at most k-1.
+# The sender determines a degree k-1 polynomial, over a finite field, that represents the k data points. The polynomial is then "encoded" by its evaluation at various points, and these values are what is actually sent. During transmission, some of these values may become corrupted. Therefore, more than k points are actually sent. As long as sufficient values are received correctly, the receiver can deduce what the original polynomial was, and decode the original data.
+# In the same sense that one can correct a curve by interpolating past a gap, a Reed-Solomon code can bridge a series of errors in a block of data to recover the coefficients of the polynomial that drew the original curve.
+# This is done automatically using a simple trick: the matrix inversion. For more infos, see this well-explained blog post by Richard Kiss: How to be Minimally Redundant (or "A Splitting Headache") http://blog.richardkiss.com/?p=264
+#
+# codeword_rate: rs = @(n,k) k / n
+# @(n,k) (n - k) / k
+#
+# To get higher resilience against corruption: make the codeword bigger (decrease k and increase n = higher resilience ratio), and use smaller packets (n should be smaller). Source: Kumar, Sanjeev, and Ragini Gupta. "Bit error rate analysis of Reed-Solomon code for efficient communication system." International Journal of Computer Applications 30.12 (2011): 11-15.
+# Note: in the paper, they told that their results indicate to use bigger blocks, but if you look at the Figure 4, this shows the opposite: the best performing parameters is: RS(246, 164) with code rate 0.66667 = resilience ratio 0.25
+# Note2: however, logically, it's more sensible to use bigger blocks to be more resilient. Indeed, since data stream is hashed and ecc'ed per block, this means that if an error burst corrupt a block twice the size of max_block_size (or just the size of max_block_size but exactly overlapping on one block, no shift), then the block will be unrecoverable. In other words, if we have bigger blocks, we raise the required size for an error burst to permanently corrupt our data (thus we diminish the risk). Thus, it would make sense to use error correcting codes with very big blocks. However, this would probably incur a huge performance drop, at least for Reed-Solomon which is basically grounded on matrix operations (thus the complexity would be about O(max_block_size^2), a quadratic complexity).
+#
+#  If 2s + r < 2t (s errors, r erasures) t
+#
+# TODO:
+# - Performance boost in Reed-Solomon libraries. A big work was done, and it's quite fast when using PyPy 2.5.0, but 10x more speedup (to attain 10MB/s encoding rate) would just be perfect! Decoding rate is sufficiently speedy as it is, no need to optimize that part.
+# Use Cauchy Reed-Solomon, which significantly outperforms simple Reed-Solomon?
+# or https://bitbucket.org/kmgreen2/pyeclib with jerasure
+# or http://www.bth.se/fou/cuppsats.nsf/all/bcb2fd16e55a96c2c1257c5e00666323/$file/BTH2013KARLSSON.pdf
+# or https://www.usenix.org/legacy/events/fast09/tech/full_papers/plank/plank_html/
+# - Also backup folders meta-data? (to reconstruct the tree in case a folder is truncated by bit rot)
+#
+
+# Include the lib folder in the python import path (so that packaged modules can be easily called, such as gooey which always call its submodules via gooey parent module)
+import sys, os
+thispathname = os.path.dirname(__file__)
+sys.path.append(os.path.join(thispathname))
+
+# Import necessary libraries
+from lib._compat import _str, _range, _StringIO, b # to support intra-ecc
+from lib.aux_funcs import get_next_entry, is_dir, is_dir_or_file, fullpath, recwalk, sizeof_fmt, path2unix, get_version
+import argparse
+import datetime, time
+import tqdm
+#import itertools
+import math
+#import operator # to get the max out of a dict
+import csv # to process the errors_file from rfigc.py
+import shlex # for string parsing as argv argument to main(), unnecessary otherwise
+from lib.tee import Tee # Redirect print output to the terminal as well as in a log file
+import struct # to support indexes backup file
+from io import BytesIO
+#import pprint # Unnecessary, used only for debugging purposes
+
+# ECC and hashing facade libraries
+from lib.eccman import ECCMan, compute_ecc_params
+from lib.hasher import Hasher
+from reedsolo import ReedSolomonError
+from unireedsolomon.rs import RSCodecError
+
+# Get package version, necessary to input in the ECC the version of pyFileFixity we used, it will be stored in the ecc files,, this can be helpful to disambiguate in the future which version of the software to use for optimal recovery
+__version__ = get_version("__init__.py", thispathname)  # aux function will return root_path as path to lib/ subfolder, so we need to provide our own path here
+
+
+
+#***********************************
+#     AUXILIARY FUNCTIONS
+#***********************************
+
+def feature_scaling(x, xmin, xmax, a=0, b=1):
+    '''Generalized feature scaling (useful for variable error correction rate calculation)'''
+    return a + float(x - xmin) * (b - a) / (xmax - xmin)
+
+#--------------------------------
+
+def entry_fields(file, entry_pos, field_delim="\xFF"):
+    '''From an ecc entry position (a list with starting and ending positions), extract the metadata fields (filename, filesize, ecc for both), and the starting/ending positions of the ecc stream (containing variably encoded blocks of hash and ecc per blocks of the original file's header)'''
+    # Read the the beginning of the ecc entry
+    blocksize = 65535
+    file.seek(entry_pos[0])
+    entry = file.read(blocksize)
+    entry = entry.lstrip(field_delim) # if there was some slight adjustment error (example: the last ecc block of the last file was the field_delim, then we will start with a field_delim, and thus we need to remove the trailing field_delim which is useless and will make the field detection buggy). This is not really a big problem for the previous file's ecc block: the missing ecc characters (which were mistaken for a field_delim), will just be missing (so we will lose a bit of resiliency for the last block of the previous file, but that's not a huge issue, the correction can still rely on the other characters).
+    # TODO: do in a while loop in case the filename is really big (bigger than blocksize) - or in case we add intra-ecc for filename
+
+    # Find metadata fields delimiters positions
+    # TODO: automate this part, just give in argument the number of field_delim to find, and the func will find the x field_delims (the number needs to be specified in argument because the field_delim can maybe be found wrongly inside the ecc stream, which we don't want)
+    first = entry.find(field_delim)
+    second = entry.find(field_delim, first+len(field_delim))
+    third = entry.find(field_delim, second+len(field_delim))
+    fourth = entry.find(field_delim, third+len(field_delim))
+    # Note: we do not try to find all the field delimiters because we optimize here: we just walk the string to find the exact number of field_delim we are looking for, and after we stop, no need to walk through the whole string.
+
+    # Extract the content of the fields
+    # Metadata fields
+    relfilepath = entry[:first]
+    filesize = entry[first+len(field_delim):second]
+    relfilepath_ecc = entry[second+len(field_delim):third]
+    filesize_ecc = entry[third+len(field_delim):fourth]
+    # Ecc stream field (aka ecc blocks)
+    ecc_field_pos = [entry_pos[0]+fourth+len(field_delim), entry_pos[1]] # return the starting and ending position of the rest of the ecc track, which contains blocks of hash/ecc of the original file's content.
+
+    # Place the cursor at the beginning of the ecc_field
+    file.seek(ecc_field_pos[0])
+
+    # Try to convert to an int, an error may happen
+    try:
+        filesize = int(filesize)
+    except Exception as e:
+        print("Exception when trying to detect the filesize in ecc field (it may be corrupted), skipping: ")
+        print(e)
+        #filesize = 0 # avoid setting to 0, we keep as an int so that we can try to fix using intra-ecc
+
+    # entries = [ {"message":, "ecc":, "hash":}, etc.]
+    return {"relfilepath": relfilepath, "relfilepath_ecc": relfilepath_ecc, "filesize": filesize, "filesize_ecc": filesize_ecc, "ecc_field_pos": ecc_field_pos}
+
+def stream_entry_assemble(hasher, file, eccfile, entry_fields, max_block_size, header_size, resilience_rates, constantmode=False):
+    '''From an entry with its parameters (filename, filesize), assemble a list of each block from the original file along with the relative hash and ecc for easy processing later.'''
+    # Cut the header and the ecc entry into blocks, and then assemble them so that we can easily process block by block
+    eccfile.seek(entry_fields["ecc_field_pos"][0])
+    curpos = file.tell()
+    ecc_curpos = eccfile.tell()
+    while (ecc_curpos < entry_fields["ecc_field_pos"][1]): # continue reading the input file until we reach the position of the previously detected ending marker
+        # Compute the current rate, depending on where we are inside the input file (headers? later stage?)
+        if curpos < header_size or constantmode: # header stage: constant rate
+            rate = resilience_rates[0]
+        else: # later stage 2 or 3: progressive rate
+            rate = feature_scaling(curpos, header_size, entry_fields["filesize"], resilience_rates[1], resilience_rates[2]) # find the rate for the current stream of data (interpolate between stage 2 and stage 3 rates depending on the cursor position in the file)
+        # From the rate, compute the ecc parameters
+        ecc_params = compute_ecc_params(max_block_size, rate, hasher)
+        # Extract the message block from input file, given the computed ecc parameters
+        mes = file.read(ecc_params["message_size"])
+        if len(mes) == 0: return # quit if message is empty (reached end-of-file), this is a safeguard if ecc pos ending was miscalculated (we thus only need the starting position to be correct)
+        buf = eccfile.read(ecc_params["hash_size"]+ecc_params["ecc_size"])
+        hash = buf[:ecc_params["hash_size"]]
+        ecc = buf[ecc_params["hash_size"]:]
+
+        yield {"message": mes, "hash": hash, "ecc": ecc, "rate": rate, "ecc_params": ecc_params, "curpos": curpos, "ecc_curpos": ecc_curpos}
+        # Prepare for the next iteration of the loop
+        curpos = file.tell()
+        ecc_curpos = eccfile.tell()
+    # Just a quick safe guard against ecc ending marker misdetection
+    file.seek(0, os.SEEK_END) # alternative way of finding the total size: go to the end of the file
+    size = file.tell()
+    if curpos < size: print("WARNING: end of ecc track reached but not the end of file! Either the ecc ending marker was misdetected, or either the file hash changed! Some blocks maybe may not have been properly checked!")
+
+def stream_compute_ecc_hash(ecc_manager, hasher, file, max_block_size, header_size, resilience_rates):
+    '''Generate a stream of hash/ecc blocks, of variable encoding rate and size, given a file.'''
+    curpos = file.tell() # init the reading cursor at the beginning of the file
+    # Find the total size to know when to stop
+    #size = os.fstat(file.fileno()).st_size # old way of doing it, doesn't work with _StringIO objects
+    file.seek(0, os.SEEK_END) # alternative way of finding the total size: go to the end of the file
+    size = file.tell()
+    file.seek(0, curpos) # place the reading cursor back at the beginning of the file
+    # Main encoding loop
+    while curpos < size: # Continue encoding while we do not reach the end of the file
+        # Calculating the encoding rate
+        if curpos < header_size: # if we are still reading the file's header, we use a constant rate
+            rate = resilience_rates[0]
+        else: # else we use a progressive rate for the rest of the file the we calculate on-the-fly depending on our current reading cursor position in the file
+            rate = feature_scaling(curpos, header_size, size, resilience_rates[1], resilience_rates[2]) # find the rate for the current stream of data (interpolate between stage 2 and stage 3 rates depending on the cursor position in the file)
+
+        # Compute the ecc parameters given the calculated rate
+        ecc_params = compute_ecc_params(max_block_size, rate, hasher)
+        #ecc_manager = ECCMan(max_block_size, ecc_params["message_size"]) # not necessary to create an ecc manager anymore, as it is very costly. Now we can specify a value for k on the fly (tables for all possible values of k are pre-generated in the reed-solomon libraries)
+
+        # Compute the ecc and hash for the current message block
+        mes = file.read(ecc_params["message_size"])
+        hash = hasher.hash(mes)
+        ecc = ecc_manager.encode(mes, k=ecc_params["message_size"])
+        #print("mes %i (%i) - ecc %i (%i) - hash %i (%i)" % (len(mes), message_size, len(ecc), ecc_params["ecc_size"], len(hash), ecc_params["hash_size"])) # DEBUGLINE
+
+        # Return the result
+        yield [b(hash), b(ecc), ecc_params]
+        # Prepare for next iteration
+        curpos = file.tell()
+
+def compute_ecc_hash_from_string(string, ecc_manager, hasher, max_block_size, resilience_rate):
+    '''Generate a concatenated string of ecc stream of hash/ecc blocks, of constant encoding rate, given a string.
+    NOTE: resilience_rate here is constant, you need to supply only one rate, between 0.0 and 1.0. The encoding rate will then be constant, like in header_ecc.py.'''
+    fpfile = BytesIO(b(string))
+    ecc_stream = b''.join( [b(x[1]) for x in stream_compute_ecc_hash(ecc_manager, hasher, fpfile, max_block_size, len(string), [resilience_rate])] ) # "hack" the function by tricking it to always use a constant rate, by setting the header_size=len(relfilepath), and supplying the resilience_rate_intra instead of resilience_rate_s1 (the one for header)
+    return ecc_stream
+
+def ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, field, ecc, entry_pos, enable_erasures=False, erasures_char="\x00", only_erasures=False, max_block_size=65535):
+    """ Correct an intra-field with its corresponding intra-ecc if necessary """
+    # convert strings to _StringIO object so that we can trick our ecc reading functions that normally works only on files
+    fpfile = BytesIO(b(field))
+    fpfile_ecc = BytesIO(b(ecc))
+    fpentry_p = {"ecc_field_pos": [0, len(field)]} # create a fake entry_pos so that the ecc reading function works correctly
+    # Prepare variables
+    field_correct = [] # will store each block of the corrected (or already correct) filepath
+    fcorrupted = False # check if field was corrupted
+    fcorrected = True # check if field was corrected (if it was corrupted)
+    errmsg = ''
+    # Decode each block of the filepath
+    for e in stream_entry_assemble(hasher_intra, fpfile, fpfile_ecc, fpentry_p, max_block_size, len(field), [resilience_rate_intra], constantmode=True):
+        # Check if this block of the filepath is OK, if yes then we just copy it over
+        if ecc_manager_intra.check(e["message"], e["ecc"]):
+            field_correct.append(e["message"])
+        else: # Else this block is corrupted, we will try to fix it using the ecc
+            fcorrupted = True
+            # Repair the message block and the ecc
+            try:
+                repaired_block, repaired_ecc = ecc_manager_intra.decode(e["message"], e["ecc"], enable_erasures=enable_erasures, erasures_char=erasures_char, only_erasures=only_erasures)
+            except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
+                repaired_block = None
+                repaired_ecc = None
+                errmsg += "- Error: unrecoverable corrupted metadata field at offset %i: %s\n" % (entry_pos[0], exc)
+            # Check if the block was successfully repaired: if yes then we copy the repaired block...
+            if repaired_block is not None and ecc_manager_intra.check(repaired_block, repaired_ecc):
+                field_correct.append(repaired_block)
+            else: # ... else it failed, then we copy the original corrupted block and report an error later
+                field_correct.append(e["message"])
+                fcorrected = False
+    # Join all the blocks into one string to build the final filepath
+    field_correct = [b(x) for x in field_correct] # workaround when using --ecc_algo 3 or 4, because we get a list of bytearrays instead of str
+    field = b''.join(field_correct)
+    # Report errors
+    return (field, fcorrupted, fcorrected, errmsg)
+
+
+
+#***********************************
+#        GUI AUX FUNCTIONS
+#***********************************
+
+# Try to import Gooey for GUI display, but manage exception so that we replace the Gooey decorator by a dummy function that will just return the main function as-is, thus keeping the compatibility with command-line usage
+try:  # pragma: no cover
+    import gooey
+except ImportError as exc:
+    # Define a dummy replacement function for Gooey to stay compatible with command-line usage
+    class gooey(object):  # pragma: no cover
+        def Gooey(func):
+            return func
+    # If --gui was specified, then there's a problem
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':  # pragma: no cover
+        print('ERROR: --gui specified but an error happened with lib/gooey, cannot load the GUI (however you can still use this script in commandline). Check that lib/gooey exists and that you have wxpython installed. Here is the error: ')
+        raise(exc)
+
+def conditional_decorator(flag, dec):  # pragma: no cover
+    def decorate(fn):
+        if flag:
+            return dec(fn)
+        else:
+            return fn
+    return decorate
+
+def check_gui_arg():  # pragma: no cover
+    '''Check that the --gui argument was passed, and if true, we remove the --gui option and replace by --gui_launched so that Gooey does not loop infinitely'''
+    if len(sys.argv) > 1 and sys.argv[1] == '--gui':
+        # DEPRECATED since Gooey automatically supply a --ignore-gooey argument when calling back the script for processing
+        #sys.argv[1] = '--gui_launched' # CRITICAL: need to remove/replace the --gui argument, else it will stay in memory and when Gooey will call the script again, it will be stuck in an infinite loop calling back and forth between this script and Gooey. Thus, we need to remove this argument, but we also need to be aware that Gooey was called so that we can call gooey.GooeyParser() instead of argparse.ArgumentParser() (for better fields management like checkboxes for boolean arguments). To solve both issues, we replace the argument --gui by another internal argument --gui_launched.
+        return True
+    else:
+        return False
+
+def AutoGooey(fn):  # pragma: no cover
+    '''Automatically show a Gooey GUI if --gui is passed as the first argument, else it will just run the function as normal'''
+    if check_gui_arg():
+        return gooey.Gooey(fn)
+    else:
+        return fn
+
+
+
+#***********************************
+#                       MAIN
+#***********************************
+
+@AutoGooey
+def main(argv=None, command=None):
+    if argv is None: # if argv is empty, fetch from the commandline
+        argv = sys.argv[1:]
+    elif isinstance(argv, _str): # else if argv is supplied but it's a simple string, we need to parse it to a list of arguments before handing to argparse or any other argument parser
+        argv = shlex.split(argv) # Parse string just like argv using shlex
+
+    #==== COMMANDLINE PARSER ====
+
+    #== Commandline description
+    desc = '''Structural Adaptive Error Correction Code generator and checker
+Description: Given a directory, this application will generate error correcting codes or correct corrupt files, using a structural adaptive approach (the headers will be protected by more ecc bits than subsequent parts, progressively decreasing in the resilience).
+Note: Folders meta-data is NOT accounted, only the files! Use DVDisaster or a similar tool to also cover folders meta-data.
+    '''
+    ep = '''
+Note1: this is a pure-python implementation (except for MD5 hash but a pure-python alternative is provided in lib/md5py.py), thus it may be VERY slow to generate an ecc file. To speed-up things considerably, you can use PyPy v2.5.0 or above, there will be a speed-up of at least 100x from our experiments (you can expect an encoding rate of more than 1MB/s). Feel free to profile using easy_profiler.py and try to optimize the encoding parts of the reed-solomon libraries.
+
+Note2: that Reed-Solomon can correct up to 2*resilience_rate erasures (eg, null bytes, you know where they are) or resilience_rate errors (an error is a corrupted character but you don't know its position) and amount to an additional storage of 2*resilience_rate storage compared to the original total files size.
+'''
+
+    #== Commandline arguments
+    #-- Constructing the parser
+    # Use GooeyParser if we want the GUI because it will provide better widgets
+    if len(argv) > 0 and (argv[0] == '--gui' and not '--ignore-gooey' in argv):  # pragma: no cover
+        # Initialize the Gooey parser
+        main_parser = gooey.GooeyParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define Gooey widget types explicitly (because type auto-detection doesn't work quite well)
+        widget_dir = {"widget": "DirChooser"}
+        widget_filesave = {"widget": "FileSaver"}
+        widget_file = {"widget": "FileChooser"}
+        widget_text = {"widget": "TextField"}
+    else: # Else in command-line usage, use the standard argparse
+        # Delete the special argument to avoid unrecognized argument error in argparse
+        if '--ignore-gooey' in argv: argv.remove('--ignore-gooey') # this argument is automatically fed by Gooey when the user clicks on Start
+        # Initialize the normal argparse parser
+        # Note that prog allows to change the shown calling script, it is necessary to manually set it when it is called as a subcommand (of pff.py). If None, prog will default to sys.argv[0] but with the absolute path removed.
+        main_parser = argparse.ArgumentParser(add_help=True, description=desc, epilog=ep, formatter_class=argparse.RawTextHelpFormatter, prog=command)
+        # Define dummy dict to keep compatibile with command-line usage
+        widget_dir = {}
+        widget_filesave = {}
+        widget_file = {}
+        widget_text = {}
+    # Required arguments
+    main_parser.add_argument('-i', '--input', metavar='/path/to/root/folder', type=is_dir_or_file, nargs=1, required=True,
+                        help='Path to the root folder (or a single file) from where the scanning will occur.', **widget_dir)
+    main_parser.add_argument('-d', '--database', metavar='/some/folder/ecc.txt', type=str, nargs=1, required=True, #type=argparse.FileType('rt')
+                        help='Path to the file containing the ECC informations.', **widget_filesave)
+
+    # Optional general arguments
+    main_parser.add_argument('--ecc_algo', type=int, default=3, required=False,
+                        help='What algorithm use to generate and verify the ECC? Values possible: 1-4. 1 is the formal, fully verified Reed-Solomon in base 3 ; 2 is a faster implementation but still based on the formal base 3 ; 3 is an even faster implementation but based on another library which may not be correct ; 4 is the fastest implementation supporting US FAA ADSB UAT RS FEC standard but is totally incompatible with the other three (a text encoded with any of 1-3 modes will be decodable with any one of them).', **widget_text)
+    main_parser.add_argument('--max_block_size', type=int, default=255, required=False,
+                        help='Reed-Solomon max block size (maximum = 255). It is advised to keep it at the maximum for more resilience (see comments at the top of the script for more info). However, if encoding it too slow, using a smaller value will speed things up greatly, at the expense of more storage space (because hash will relatively take more space - you can use --hash "shortmd5" or --hash "minimd5" to counter balance).', **widget_text)
+    main_parser.add_argument('-s', '--size', type=int, default=1024, required=False,
+                        help='Headers block size to protect with resilience rate stage 1 (eg: 1024 meants that the first 1k of each file will be protected by stage 1).', **widget_text)
+    main_parser.add_argument('-r1', '--resilience_rate_stage1', type=float, default=0.3, required=False,
+                        help='Resilience rate for files headers (eg: 0.3 = 30%% of errors can be recovered but size of codeword will be 60%% of the data block).', **widget_text)
+    main_parser.add_argument('-r2', '--resilience_rate_stage2', type=float, default=0.2, required=False,
+                        help='Resilience rate for stage 2 (after headers, this is the starting rate applied to the rest of the file, which will be gradually lessened towards the end of the file to the stage 3 rate).', **widget_text)
+    main_parser.add_argument('-r3', '--resilience_rate_stage3', type=float, default=0.1, required=False,
+                        help='Resilience rate for stage 3 (rate that will be applied towards the end of the files).', **widget_text)
+    main_parser.add_argument('-ri', '--resilience_rate_intra', type=float, default=0.5, required=False,
+                        help='Resilience rate for intra-ecc (ecc on meta-data, such as filepath, thus this defines the ecc for the critical spots!).', **widget_text)
+    main_parser.add_argument('-l', '--log', metavar='/some/folder/filename.log', type=str, nargs=1, required=False,
+                        help='Path to the log file. (Output will be piped to both the stdout and the log file)', **widget_filesave)
+    main_parser.add_argument('--stats_only', action='store_true', required=False, default=False,
+                        help='Only show the predicted total size of the ECC file given the parameters.')
+    main_parser.add_argument('--hash', metavar='md5;shortmd5;shortsha256...', type=str, required=False,
+                        help='Hash algorithm to use. Choose between: md5, shortmd5, shortsha256, minimd5, minisha256.', **widget_text)
+    main_parser.add_argument('-v', '--verbose', action='store_true', required=False, default=False,
+                        help='Verbose mode (show more output).')
+    main_parser.add_argument('--silent', action='store_true', required=False, default=False,
+                        help='No console output (but if --log specified, the log will still be saved in the specified file).')
+
+    # Correction mode arguments
+    main_parser.add_argument('-c', '--correct', action='store_true', required=False, default=False,
+                        help='Check/Correct the files')
+    main_parser.add_argument('-o', '--output', metavar='/path/to/output/folder', type=is_dir, nargs=1, required=False,
+                        help='Path of the folder where the repaired files will be copied (only repaired corrupted files will be copied there, files that weren\'t corrupted at all won\'t be copied so you have to copy them by yourself afterwards).', **widget_dir)
+    main_parser.add_argument('-e', '--errors_file', metavar='/some/folder/errorsfile.csv', type=str, nargs=1, required=False, #type=argparse.FileType('rt')
+                        help='Path to the error file generated by RFIGC.py. The software will automatically correct those files and only those files.', **widget_file)
+    main_parser.add_argument('--ignore_size', action='store_true', required=False, default=False,
+                        help='On correction, if the file size differs from when the ecc file was generated, ignore and try to correct anyway (this may work with file where data was appended without changing the rest. For compressed formats like zip, this will probably fail).')
+    main_parser.add_argument('--no_fast_check', action='store_true', required=False, default=False,
+                        help='On correction, block corruption is only checked with the hash (the ecc will still be checked after correction, but not before). If no_fast_check is enabled, then ecc will also be checked before. This allows to find blocks corrupted by malicious intent (the block is corrupted but the hash has been corrupted as well to match the corrupted block, because it\'s almost impossible that following a hardware or logical fault, the hash match the corrupted block).')
+    main_parser.add_argument('--skip_missing', action='store_true', required=False, default=False,
+                        help='Skip missing files (no warning).')
+    main_parser.add_argument('--enable_erasures', action='store_true', required=False, default=False,
+                        help='Enable errors-and-erasures correction. Reed-Solomon can correct twice more erasures than errors (eg, if resilience rate is 0.3, then you can correct 30%% errors and 60%% erasures and any combination of errors and erasures between 30%%-60%% corruption). An erasure is a corrupted symbol where we know the position, while errors are not known at all. To find erasures, we will find any symbol that is equal to --erasure_symbol and flag it as an erasure. This is particularly useful if the software you use (eg, a disk scraper) can mark bad sectors with a constant character (eg, null byte). Misdetected erasures will just eat one ecc symbol, and won\'t change the decoded message.')
+    main_parser.add_argument('--only_erasures', action='store_true', required=False, default=False,
+                        help='Enable only erasures correction (no errors). Use this only if you are sure that all corrupted symbols have the same value (eg, if your disk scraper replace bad sectors by null bytes). This will ensure that you can correct up to 2*resilience_rate corrupted symbols.')
+    main_parser.add_argument('--erasure_symbol', type=int, default=0, required=False,
+                        help='Symbol that will be flagged as an erasure. Default: null byte 0. (value must be an integer)', **widget_text)
+
+    # Generate mode arguments
+    main_parser.add_argument('-g', '--generate', action='store_true', required=False, default=False,
+                        help='Generate the ecc file?')
+    main_parser.add_argument('-f', '--force', action='store_true', required=False, default=False,
+                        help='Force overwriting the ecc file even if it already exists (if --generate).')
+    main_parser.add_argument('--skip_size_below', type=int, default=None, required=False,
+                        help='Skip files below the specified size (in bytes).', **widget_text)
+    main_parser.add_argument('--always_include_ext', metavar='txt|jpg|png', type=str, default=None, required=False,
+                        help='Always include files with the specified extensions, useful in combination with --skip_size_below to keep files of certain types even if they are below the size. Format: extensions separated by |.', **widget_text)
+
+    #== Parsing the arguments
+    args = main_parser.parse_args(argv) # Storing all arguments to args
+
+    #-- Set hard-coded variables
+    entrymarker = "\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF" # marker that will signal the beginning of an ecc entry - use an alternating pattern of several characters, this avoids confusion (eg: if you use "AAA" as a pattern, if the ecc block of the previous file ends with "EGA" for example, then the full string for example will be "EGAAAAC:\yourfolder\filea.jpg" and then the entry reader will detect the first "AAA" occurrence as the entry start - this should not make the next entry bug because there is an automatic trim - but the previous ecc block will miss one character that could be used to repair the block because it will be "EG" instead of "EGA"!)
+    field_delim = "\xFA\xFF\xFA\xFF\xFA" # delimiter between fields (filepath, filesize, hash+ecc blocks) inside an ecc entry
+
+    #-- Set variables from arguments
+    inputpath = fullpath(args.input[0]) # path to the files to protect (either a folder or a single file)
+    rootfolderpath = inputpath # path to the root folder (to compute relative paths)
+    database = fullpath(args.database[0])
+    generate = args.generate
+    correct = args.correct
+    force = args.force
+    stats_only = args.stats_only
+    max_block_size = args.max_block_size
+    header_size = args.size
+    resilience_rate_s1 = args.resilience_rate_stage1
+    resilience_rate_s2 = args.resilience_rate_stage2
+    resilience_rate_s3 = args.resilience_rate_stage3
+    resilience_rate_intra = args.resilience_rate_intra
+    enable_erasures = args.enable_erasures
+    only_erasures = args.only_erasures
+    erasure_symbol = args.erasure_symbol
+    ignore_size = args.ignore_size
+    skip_missing = args.skip_missing
+    skip_size_below = args.skip_size_below
+    always_include_ext = args.always_include_ext
+    if always_include_ext: always_include_ext = tuple(['.'+ext for ext in always_include_ext.split('|')]) # prepare a tuple of extensions (prepending with a dot) so that str.endswith() works (it doesn't with a list, only a tuple)
+    hash_algo = args.hash
+    if not hash_algo: hash_algo = "md5"
+    ecc_algo = args.ecc_algo
+    fast_check = not args.no_fast_check
+    verbose = args.verbose
+    silent = args.silent
+
+    if os.path.isfile(inputpath): # if inputpath is a single file (instead of a folder), then define the rootfolderpath as the parent directory (for correct relative path generation, else it will also truncate the filename!)
+        rootfolderpath = os.path.dirname(inputpath)
+
+    if correct:
+        if not args.output:
+            raise NameError('Output path is necessary when in correction mode!')
+        outputpath = fullpath(args.output[0])
+
+    errors_file = None
+    if args.errors_file: errors_file = os.path.basename(fullpath(args.errors_file[0]))
+
+    # -- Checking arguments
+    if not stats_only and not generate and not os.path.isfile(database):
+        raise NameError('Specified database ecc file %s does not exist!' % database)
+    elif generate and os.path.isfile(database) and not force:
+        raise NameError('Specified database ecc file %s already exists! Use --force if you want to overwrite.' % database)
+
+    if resilience_rate_s1 <= 0 or resilience_rate_s2 <= 0 or resilience_rate_s3 <= 0 or resilience_rate_intra <= 0:
+        raise ValueError('Resilience rates cannot be negative nor zero and they must be floating numbers.');
+
+    if max_block_size < 2 or max_block_size > 255:
+        raise ValueError('RS max block size must be between 2 and 255.')
+
+    if header_size < 1:
+        raise ValueError('Header size cannot be negative.')
+
+    if hash_algo not in Hasher.known_algo:
+        raise ValueError("Specified hash algorithm %s is unknown!" % hash_algo)
+
+    # -- Configure the log file if enabled (ptee.write() will write to both stdout/console and to the log file)
+    if args.log:
+        ptee = Tee(args.log[0], 'a', nostdout=silent)
+        #sys.stdout = Tee(args.log[0], 'a')
+        sys.stderr = Tee(args.log[0], 'a', nostdout=silent)
+    else:
+        ptee = Tee(nostdout=silent)
+
+
+    # == PROCESSING BRANCHING == #
+
+    # Precompute some parameters and load up ecc manager objects (big optimization as g_exp and g_log tables calculation is done only once)
+    ptee.write("Initializing the ECC codecs, please wait...")
+    resilience_rates = [resilience_rate_s1, resilience_rate_s2, resilience_rate_s3]
+    hasher = Hasher(hash_algo)
+    hasher_intra = Hasher('none') # for intra_ecc we don't use any hash
+    ecc_params_header = compute_ecc_params(max_block_size, resilience_rate_s1, hasher)
+    ecc_manager_header = ECCMan(max_block_size, ecc_params_header["message_size"], algo=ecc_algo)
+    ecc_manager_variable = ECCMan(max_block_size, 1, algo=ecc_algo)
+    ecc_params_intra = compute_ecc_params(max_block_size, resilience_rate_intra, hasher_intra)
+    ecc_manager_intra = ECCMan(max_block_size, ecc_params_intra["message_size"], algo=ecc_algo)
+    ecc_params_idx = compute_ecc_params(27, 1, hasher_intra)
+    ecc_manager_idx = ECCMan(27, ecc_params_idx["message_size"], algo=ecc_algo)
+    # for stats only
+    ecc_params_variable_average = compute_ecc_params(max_block_size, (resilience_rate_s2 + resilience_rate_s3)/2, hasher) # compute the average variable rate to compute statistics
+    ecc_params_s2 = compute_ecc_params(max_block_size, resilience_rate_s2, hasher)
+    ecc_params_s3 = compute_ecc_params(max_block_size, resilience_rate_s3, hasher)
+
+    # == Precomputation of ecc file size
+    # Precomputing is important so that the user can know what size to expect before starting (and how much time it will take...).
+    filescount = 0
+    sizetotal = 0
+    sizeecc = 0
+    ptee.write("Precomputing list of files and predicted statistics...")
+    for (dirpath, filename) in tqdm.tqdm(recwalk(inputpath), file=ptee):
+        filescount = filescount + 1 # counting the total number of files we will process (so that we can show a progress bar with ETA)
+        # Get full absolute filepath
+        filepath = os.path.join(dirpath, filename)
+        relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
+        # Get the current file's size
+        size = os.stat(filepath).st_size
+        # Check if we must skip this file because size is too small, and then if we still keep it because it's extension is always to be included
+        if skip_size_below and size < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)): continue
+
+        # Compute total size of all files
+        sizetotal = sizetotal + size
+        # Compute predicted size of their headers
+        if size >= header_size: # for big files, we limit the size to the header size
+            filesize_header = header_size
+            filesize_content = size - header_size
+        else: # else for size smaller than the defined header size, it will just be the size of the file
+            filesize_header = size
+            filesize_content = 0
+        # Size of the ecc entry for this file will be: entrymarker-bytes + field_delim-bytes*occurrence + length-filepath-string + length-size-string + length-filepath-ecc + size of the ecc per block for all blocks in file header + size of the hash per block for all blocks in file header.
+        sizeecc += (len(entrymarker) + len(field_delim)*3 + len(relfilepath) + len(str(size)) + int(float(len(relfilepath))*resilience_rate_intra) + (int(math.ceil(float(filesize_header) / ecc_params_header["message_size"])) * (ecc_params_header["ecc_size"]+ecc_params_header["hash_size"])) + (int(math.ceil(float(filesize_content) / ecc_params_variable_average["message_size"])) * (ecc_params_variable_average["ecc_size"]+ecc_params_variable_average["hash_size"])) ) # Compute the total number of bytes we will add with ecc + hash (accounting for the padding of the remaining characters at the end of the sequence in case it doesn't fit with the message_size, by using ceil() )
+    ptee.write("Precomputing done.")
+    if generate: # show statistics only if generating an ecc file
+        # TODO: add the size of the ecc format header? (arguments string + PYHEADERECC identifier)
+        total_pred_percentage = sizeecc * 100 / sizetotal
+        ptee.write("Total ECC size estimation: %s = %g%% of total files size %s." % (sizeof_fmt(sizeecc), total_pred_percentage, sizeof_fmt(sizetotal)))
+        ptee.write("Details per stage:")
+        ptee.write("- Resiliency stage1 of %i%%: For the header (first %i characters) of each file: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate_s1*100, header_size, ecc_params_header["message_size"], ecc_params_header["ecc_size"], int(ecc_params_header["ecc_size"] / 2), ecc_params_header["ecc_size"]))
+        ptee.write("- Resiliency stage2 of %i%%: for the rest of the file, the parameters will start with: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate_s2*100, ecc_params_s2["message_size"], ecc_params_s2["ecc_size"], int(ecc_params_s2["ecc_size"] / 2), ecc_params_s2["ecc_size"]))
+        ptee.write("- Resiliency stage3 of %i%%: progressively towards the end, the parameters will gradually become: each block of %i chars will get an ecc of %i chars (%i errors or %i erasures)." % (resilience_rate_s3*100, ecc_params_s3["message_size"], ecc_params_s3["ecc_size"], int(ecc_params_s3["ecc_size"] / 2), ecc_params_s3["ecc_size"]))
+        if max_block_size > 100: ptee.write("Note: current max_block_size (size of message+ecc blocks) is %i. Consider using a smaller value to greatly speedup the processing (because Reed-Solomon encoding complexity is about O(max_block_size^2)) at the expense of generating a bigger ecc file and less burst error resiliency (because the ecc blocks will be smaller)." % max_block_size)
+
+    if stats_only:
+        ptee.close()
+        return 0
+
+    # == Generation mode
+    # Generate an ecc file, containing ecc entries for every files recursively in the specified root folder.
+    # The file header will be split by blocks depending on max_block_size and resilience_rate, and each of those blocks will be hashed and a Reed-Solomon code will be produced.
+    if generate:
+        ptee.write("====================================")
+        ptee.write("Structural adaptive ECC generation, started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        with open(database, 'wb') as db, open(database+".idx", 'wb') as dbidx:
+            # Write ECC file header identifier (unique string + version)
+            db.write( b("**PYSTRUCTADAPTECCv%s**\n" % (''.join([x * 3 for x in __version__]))) ) # each character in the version will be repeated 3 times, so that in case of tampering, a majority vote can try to disambiguate)
+            # Write the parameters (they are NOT reloaded automatically, you have to specify them at commandline! It's the user role to memorize those parameters (using any means: own brain memory, keep a copy on paper, on email, etc.), so that the parameters are NEVER tampered. The parameters MUST be ultra reliable so that errors in the ECC file can be more efficiently recovered.
+            for i in _range(3): db.write( ("** Parameters: "+" ".join(argv) + "\n").encode() ) # copy them 3 times just to be redundant in case of ecc file corruption
+            db.write( b("** Generated under %s\n" % ecc_manager_variable.description()) )
+            # NOTE: there's NO HEADER for the ecc file! Ecc entries are all independent of each others, you just need to supply the decoding arguments at commandline, and the ecc entries can be decoded. This is done on purpose to be remove the risk of critical spots in ecc file.
+
+            # Compile the list of files to put in the header
+            #filesheader = [':'.join([str(i), str(item[0]), str(item[1])]) for i, item in enumerate(itertools.izip(fileslist, filessizes))]
+            #for i in _range(4): # replicate the headers several times as a safeguard for corruption
+                #db.write("**" + '|'.join(filesheader) + "**\n")
+
+            # Processing ecc on files
+            files_done = 0
+            files_skipped = 0
+            bardisp = tqdm.tqdm(total=sizetotal, file=ptee, leave=True, unit='B', unit_scale=True, mininterval=1)
+            for (dirpath, filename) in recwalk(inputpath):
+                # Get full absolute filepath
+                filepath = os.path.join(dirpath,filename)
+                # Get database relative path (from scanning root folder)
+                relfilepath = path2unix(os.path.relpath(filepath, rootfolderpath)) # File relative path from the root (we truncate the rootfolderpath so that we can easily check the files later even if the absolute path is different)
+                # Get file size
+                filesize = os.stat(filepath).st_size
+                # If skip size is enabled and size is below the skip size, we skip UNLESS the file extension is in the always include list
+                if skip_size_below and filesize < skip_size_below and (not always_include_ext or not relfilepath.lower().endswith(always_include_ext)):
+                    files_skipped += 1
+                    continue
+
+                # Opening the input file's to read its header and compute the ecc/hash blocks
+                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
+                with open(os.path.join(rootfolderpath, filepath), 'rb') as file:
+                    entrymarker_pos = db.tell() # backup the position of the start of this ecc entry
+                    # -- Intra-ecc generation: Compute an ecc for the filepath, to avoid a critical spot here (so that we don't care that the filepath gets corrupted, we have an ecc to fix it!)
+                    relfilepath_ecc = compute_ecc_hash_from_string(relfilepath, ecc_manager_intra, hasher_intra, max_block_size, resilience_rate_intra)
+                    filesize_ecc = compute_ecc_hash_from_string(b(str(filesize)), ecc_manager_intra, hasher_intra, max_block_size, resilience_rate_intra)
+                    db.write( b''.join([b(entrymarker), b(relfilepath), b(field_delim), b(str(filesize)), b(field_delim), b(relfilepath_ecc), b(field_delim), b(filesize_ecc), b(field_delim)]) ) # first save the file's metadata (filename, filesize, ecc for filename, ...), separated with field_delim
+                    # -- External indexes backup: calculate the position of the entrymarker and of each field delimiter, and compute their ecc, and save into the index backup file. This will allow later to retrieve the position of each marker in the ecc file, and repair them if necessary, while just incurring a very cheap storage cost.
+                    # Also, the index backup file is fixed delimited fields sizes, which means that each field has a very specifically delimited size, so that we don't need any marker: we can just compute the total size for each entry, and thus find all entries independently even if one or several are corrupted beyond repair, so that this won't affect other index entries.
+                    markers_pos = [
+                                                    entrymarker_pos,
+                                                    entrymarker_pos+len(entrymarker)+len(relfilepath),
+                                                    entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize)),
+                                                    entrymarker_pos+len(entrymarker)+len(relfilepath)+len(field_delim)+len(str(filesize))+len(field_delim)+len(relfilepath_ecc),
+                                                    db.tell()-len(field_delim)
+                                                    ] # Make the list of all markers positions for this ecc entry. The first and last indexes are the most important (first is the entrymarker, the last is the field_delim just before the ecc track start)
+                    markers_pos = [struct.pack('>Q', x) for x in markers_pos] # Convert to a binary representation in 8 bytes using unsigned long long (up to 16 EB, this should be more than sufficient)
+                    markers_types = [b'1', b'2', b'2', b'2', b'2']
+                    markers_pos_ecc = [ecc_manager_idx.encode(x+y) for x,y in zip(markers_types,markers_pos)] # compute the ecc for each number
+                    # Couple each marker's position with its type and with its ecc, and write them all consecutively into the index backup file
+                    for items in zip(markers_types,markers_pos,markers_pos_ecc):
+                        for item in items:
+                            dbidx.write(b(item))
+                    # -- Hash/Ecc encoding of file's content (everything is managed inside stream_compute_ecc_hash)
+                    for ecc_entry in stream_compute_ecc_hash(ecc_manager_variable, hasher, file, max_block_size, header_size, resilience_rates): # then compute the ecc/hash entry for this file's header (each value will be a block, a string of hash+ecc per block of data, because Reed-Solomon is limited to a maximum of 255 bytes, including the original_message+ecc! And in addition we want to use a variable rate for RS that is decreasing along the file)
+                        db.write( b''.join([b(ecc_entry[0]), b(ecc_entry[1])]) ) # note that there's no separator between consecutive blocks, but by calculating the ecc parameters, we will know when decoding the size of each block!
+                        bardisp.update(ecc_entry[2]['message_size'])
+                files_done += 1
+        if bardisp.n > bardisp.total: bardisp.total = bardisp.n # small workaround because n may be higher than total (because of files ending before 'message_size', thus the message is padded and in the end, we have outputted and processed a bit more characters than are really in the files, thus why total can be below n). Doing this allows to keep the trace of the progression bar.
+        bardisp.close()
+        ptee.write("All done! Total number of files processed: %i, skipped: %i" % (files_done, files_skipped))
+        ptee.close()
+        return 0
+
+    # == Error Correction (and checking by hash) mode
+    # For each file, check their headers by block by checking each block against a hash, and if the hash does not match, try to correct with Reed-Solomon and then check the hash again to see if we correctly repaired the block (else the ecc entry might have been corrupted, whether it's the hash or the ecc field, in both cases it's highly unlikely that a wrong repair will match the hash after this wrong repair)
+    elif correct:
+        ptee.write("====================================")
+        ptee.write("Structural adaptive ECC correction, started on %s" % datetime.datetime.now().isoformat())
+        ptee.write("====================================")
+
+        # Prepare the list of files with errors to reduce the scan (only if provided)
+        errors_filelist = []
+        if errors_file:
+            with open(errors_file, 'rb') as efile:
+                for row in csv.DictReader(efile, lineterminator='\n', delimiter='|', quotechar='"', fieldnames=['filepath', 'error']): # need to specify the fieldnames, else the first row in the csv file will be skipped (it will be used as the columns names)
+                    errors_filelist.append(row['filepath'])
+
+        # Read the ecc file
+        dbsize = os.stat(database).st_size # must get db file size before opening it in order not to move the cursor
+        with open(database, 'rb') as db:
+            # Counters
+            files_count = 0
+            files_corrupted = 0
+            files_repaired_partially = 0
+            files_repaired_completely = 0
+            files_skipped = 0
+            bardisp_first_open = True
+
+            # Main loop: process each ecc entry
+            entry = 1 # to start the while loop
+            bardisp = tqdm.tqdm(total=dbsize, file=ptee, leave=True, desc='DBREAD', unit='B', unit_scale=True) # display progress bar based on reading the database file (since we don't know how many files we will process beforehand nor how many total entries we have)
+            while entry:
+
+                # -- Read the next ecc entry (extract the raw string from the ecc file)
+                #if replication_rate == 1:
+                entry_pos = get_next_entry(db, entrymarker)
+                if entry_pos:
+                    if bardisp_first_open: bardisp.n = entry_pos[0]-len(entrymarker) # add the size of the comments in the ecc header
+                    bardisp.update(entry_pos[1]-entry_pos[0]+len(entrymarker)) # update progress bar
+
+                # No entry? Then we finished because this is the end of file (stop condition)
+                if not entry_pos: break
+
+                # -- Extract the fields from the ecc entry
+                entry_p = entry_fields(db, entry_pos, b(field_delim))
+
+                # -- Get file path, check its correctness and correct it by using intra-ecc if necessary
+                relfilepath = entry_p["relfilepath"] # Relative file path, given in the ecc fields
+                relfilepath, fpcorrupted, fpcorrected, fperrmsg = ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, relfilepath, entry_p["relfilepath_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures, max_block_size=max_block_size)
+                # Report errors
+                if fpcorrupted:
+                    if fpcorrected: ptee.write("\n- Fixed error in metadata field at offset %i filepath %s." % (entry_pos[0], filepath))
+                    else: ptee.write("\n- Error in filepath, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], filepath))
+                ptee.write(fperrmsg)
+
+                # Convert to str (so that we can use os.path funcs)
+                relfilepath = relfilepath.decode('latin-1')
+                # Update entry_p
+                entry_p["relfilepath"] = relfilepath
+                # -- End of intra-ecc on filepath
+
+                # -- Get file size, check its correctness and correct it by using intra-ecc if necessary
+                filesize = str(entry_p["filesize"])
+                filesize, fscorrupted, fscorrected, fserrmsg = ecc_correct_intra_stream(ecc_manager_intra, ecc_params_intra, hasher_intra, resilience_rate_intra, filesize, entry_p["filesize_ecc"], entry_pos, enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures, max_block_size=max_block_size)
+
+                # Report errors
+                if fscorrupted:
+                    if fscorrected: ptee.write("\n- Fixed error in metadata field at offset %i filesize %s." % (entry_pos[0], filesize))
+                    else: ptee.write("\n- Error in filesize, could not correct completely metadata field at offset %i with value: %s. Please fix manually by editing the ecc file or set the corrupted characters to null bytes and --enable_erasures." % (entry_pos[0], filesize))
+                ptee.write(fserrmsg)
+
+                # Convert filesize intra-field into an int
+                filesize = int(filesize)
+
+                # Update entry_p
+                entry_p["filesize"] = filesize # need to update entry_p because various funcs will directly access filesize this way...
+                # -- End of intra-ecc on filesize
+
+                # Build the absolute file path
+                filepath = os.path.join(rootfolderpath, relfilepath) # Get full absolute filepath from given input folder (because the files may be specified in any folder, in the ecc file the paths are relative, so that the files can be moved around or burnt on optical discs)
+                if errors_filelist and relfilepath not in errors_filelist: continue # if a list of files with errors was supplied (for example by rfigc.py), then we will check only those files and skip the others
+
+                if verbose: ptee.write("\n- Processing file %s" % relfilepath)
+
+                # -- Check filepath
+                # Check that the filepath isn't corrupted (if a silent error erase a character (not only flip a bit), then it will also be detected this way)
+                if relfilepath.find("\x00") >= 0:
+                    ptee.write("Error: ecc entry corrupted on filepath field, please try to manually repair the filepath (filepath: %s - missing/corrupted character at %i)." % (relfilepath, relfilepath.find("\x00")))
+                    files_skipped += 1
+                    continue
+                # Check that file still exists before checking it
+                if not os.path.isfile(filepath):
+                    if not skip_missing: ptee.write("Error: file %s could not be found: either file was moved or the ecc entry was corrupted (filepath is incorrect?)." % relfilepath)
+                    files_skipped += 1
+                    continue
+
+                # -- Checking file size: if the size has changed, the blocks may not match anymore!
+                real_filesize = os.stat(filepath).st_size
+                if filesize != real_filesize:
+                    if ignore_size:
+                        ptee.write("Warning: file %s has a different size: %s (before: %s). Will still try to correct it (but the blocks may not match!)." % (relfilepath, real_filesize, filesize))
+                    else:
+                        ptee.write("Error: file %s has a different size: %s (before: %s). Skipping the file correction because blocks may not match (you can set --ignore_size to still correct even if size is different, maybe just the entry was corrupted)." % (relfilepath, real_filesize, filesize))
+                        files_skipped += 1
+                        continue
+
+                files_count += 1
+                # -- Check blocks and repair if necessary
+                corrupted = False # flag to signal that the file was corrupted and we need to reconstruct it afterwards
+                repaired_partially = False # flag to signal if a file was repaired only partially
+                # Do a first run to check if there's any error. If yes, then we will begin back from the start of the file but this time we will streamline copy the data to an output file.
+                with open(filepath, 'rb') as file:
+                    # For each message block, check the message with hash and repair with ecc if necessary
+                    for i, e in enumerate(stream_entry_assemble(hasher, file, db, entry_p, max_block_size, header_size, resilience_rates)): # Extract and assemble each message block from the original file with its corresponding ecc and hash
+                        # If the message block has a different hash or the message+ecc is corrupted (syndrome is not null), it was corrupted (or the hash is corrupted or one of the characters of the ecc was corrupted, or both). In any case, it's an any clause here (any potential corruption condition triggers the correction).
+                        if hasher.hash(e["message"]) != e["hash"] or (not fast_check and not ecc_manager_variable.check(e["message"], e["ecc"], k=e["ecc_params"]["message_size"])):
+                            corrupted = True
+                            break
+                # -- Reconstruct/Copying the repaired file
+                # If the first run detected a corruption, then we try to repair the file (we create an output file where good blocks will be copied as-is but bad blocks will be repaired, if it's possible)
+                if corrupted:
+                    files_corrupted += 1
+                    repaired_one_block = False # flag to check that we could repair at least one block, else we will delete the output file since we didn't do anything
+                    err_consecutive = True # flag to check if the ecc track is misaligned/misdetected (we only encounter corrupted blocks that we can't fix)
+                    with open(filepath, 'rb') as file:
+                        outfilepath = os.path.join(outputpath, relfilepath) # get the full path to the output file
+                        outfiledir = os.path.dirname(outfilepath)
+                        if not os.path.isdir(outfiledir): os.makedirs(outfiledir) # if the target directory does not exist, create it (and create recursively all parent directories too)
+                        with open(outfilepath, 'wb') as outfile:
+                            # TODO: optimize to copy over what we have already checked, so that we get directly to the first error that triggered the correction
+                            # For each message block, check the message with hash and repair with ecc if necessary
+                            for i, e in enumerate(stream_entry_assemble(hasher, file, db, entry_p, max_block_size, header_size, resilience_rates)): # Extract and assemble each message block from the original file with its corresponding ecc and hash
+                                # If the message block has a different hash, it was corrupted (or the hash is corrupted, or both)
+                                if hasher.hash(e["message"]) == e["hash"] and (fast_check or ecc_manager_variable.check(e["message"], e["ecc"], k=e["ecc_params"]["message_size"])):
+                                    outfile.write(e["message"])
+                                    err_consecutive = False
+                                else:
+                                    # Try to repair the block using ECC
+                                    ptee.write("File %s: corruption in block %i. Trying to fix it." % (relfilepath, i))
+                                    try:
+                                        repaired_block, repaired_ecc = ecc_manager_variable.decode(e["message"], e["ecc"], k=e["ecc_params"]["message_size"], enable_erasures=enable_erasures, erasures_char=erasure_symbol, only_erasures=only_erasures)
+                                    except (ReedSolomonError, RSCodecError) as exc: # the reedsolo lib may raise an exception when it can't decode. We ensure that we can still continue to decode the rest of the file, and the other files.
+                                        repaired_block = None
+                                        repaired_ecc = None
+                                        print("Error: file %s: block %i: %s" % (relfilepath, i, exc))
+                                    # Check if the repair was successful. This is an "all" condition: if all checks fail, then the correction failed. Else, we assume that the checks failed because the ecc entry was partially corrupted (it's highly improbable that any one check success by chance, it's a lot more probable that it's simply that the entry was partially corrupted, eg: the hash was corrupted and thus cannot match anymore).
+                                    hash_ok = False
+                                    ecc_ok = False
+                                    if repaired_block is not None:
+                                        hash_ok = (hasher.hash(repaired_block) == e["hash"])
+                                        ecc_ok = ecc_manager_variable.check(repaired_block, repaired_ecc, k=e["ecc_params"]["message_size"])
+                                    if repaired_block is not None and (hash_ok or ecc_ok): # If the hash now match the repaired message block, we commit the new block
+                                        outfile.write(repaired_block) # save the repaired block
+                                        # Show a precise report about the repair
+                                        if hash_ok and ecc_ok: ptee.write("File %s: block %i repaired!" % (relfilepath, i))
+                                        elif not hash_ok: ptee.write("File %s: block %i probably repaired with matching ecc check but with a hash error (assume the hash was corrupted)." % (relfilepath, i))
+                                        elif not ecc_ok: ptee.write("File %s: block %i probably repaired with matching hash but with ecc check error (assume the ecc was partially corrupted)." % (relfilepath, i))
+                                        # Turn on the repaired flag, to trigger the copying of the file (else it will be removed if all blocks repairs failed in this file)
+                                        repaired_one_block = True
+                                        err_consecutive = False
+                                    else: # Else the hash does not match: the repair failed (either because the ecc is too much tampered, or because the hash is corrupted. Either way, we don't commit).
+                                        outfile.write(e["message"]) # copy the bad block that we can't repair...
+                                        ptee.write("Error: file %s could not repair block %i (both hash and ecc check mismatch). If you know where the errors are, you can set the characters to a null character so that the ecc may correct twice more characters." % (relfilepath, i)) # you need to code yourself to use bit-recover, it's in perl but it should work given the hash computed by this script and the corresponding message block.
+                                        repaired_partially = True
+                                        # Detect if the ecc track is misaligned/misdetected (we encounter only errors that we can't fix)
+                                        if err_consecutive and i >= 10: # threshold is ten consecutive uncorrectable errors
+                                            ptee.write("Failure: Too many consecutive uncorrectable errors for %s. Most likely, the ecc track was misdetected (try to repair the entrymarkers and field delimiters). Skipping this track/file." % relfilepath)
+                                            db.seek(entry_p["ecc_field_pos"][1]) # Optimization: move the reading cursor to the beginning of the next ecc entry, this will save some iterations in get_next_entry()
+                                            break
+                    # Copying the last access time and last modification time from the original file TODO: a more reliable way would be to use the db computed by rfigc.py, because if a software maliciously tampered the data, then the modification date may also have changed (but not if it's a silent error, in that case we're ok).
+                    filestats = os.stat(filepath)
+                    os.utime(outfilepath, (filestats.st_atime, filestats.st_mtime))
+                    # Check that at least one block was repaired, else we couldn't fix anything in the file and thus we should just remove the output file which is an exact copy of the original without any added value
+                    if not repaired_one_block:
+                        os.remove(outfilepath)
+                    # Counters...
+                    elif repaired_partially:
+                        files_repaired_partially += 1
+                    else:
+                        files_repaired_completely += 1
+        # All ecc entries processed for checking and potentally repairing, we're done correcting!
+        bardisp.close() # at the end, the bar may not be 100% because of the headers that are skipped by read_next_entry() and are not accounted in bardisp.
+        ptee.write("All done! Stats:\n- Total files processed: %i\n- Total files corrupted: %i\n- Total files repaired completely: %i\n- Total files repaired partially: %i\n- Total files corrupted but not repaired at all: %i\n- Total files skipped: %i" % (files_count, files_corrupted, files_repaired_completely, files_repaired_partially, files_corrupted - (files_repaired_partially + files_repaired_completely), files_skipped) )
+        ptee.close()
+        if files_corrupted == 0 or files_repaired_completely == files_corrupted:
+            return 0
+        else:
+            return 1
+
+# Calling main function if the script is directly called (not imported as a library in another program)
+if __name__ == "__main__":  # pragma: no cover
+    sys.exit(main())
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/aux_tests.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/aux_tests.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,163 +1,163 @@
-""" Auxiliary functions for unit tests """
-
-from __future__ import with_statement
-
-import os
-import shutil
-
-from ..lib._compat import _range, b
-
-def check_eq_files(path1, path2, blocksize=65535, startpos1=0, startpos2=0):
-    """ Return True if both files are identical, False otherwise """
-    flag = True
-    with open(path1, 'rb') as f1, open(path2, 'rb') as f2:
-        buf1 = 1
-        buf2 = 1
-        f1.seek(startpos1)
-        f2.seek(startpos2)
-        while buf1 and buf2:
-            buf1 = f1.read(blocksize)
-            buf2 = f2.read(blocksize)
-            if buf1 != buf2 or (buf1 and not buf2) or (buf2 and not buf1):
-                # Reached end of file or the content is different, then return false
-                flag = False
-                break
-            elif (not buf1 and not buf2):
-                # End of file for both files
-                break
-    return flag
-    #return filecmp.cmp(path1, path2, shallow=False)  # does not work on Travis
-
-def check_eq_dir(path1, path2):
-    """ Return True if both folders have same structure totally identical files, False otherwise """
-    # List files in both directories
-    files1 = []
-    files2 = []
-    for dirpath, dirs, files in os.walk(path1):
-        files1.extend([os.path.relpath(os.path.join(dirpath, file), path1) for file in files])
-    for dirpath, dirs, files in os.walk(path2):
-        files2.extend([os.path.relpath(os.path.join(dirpath, file), path2) for file in files])
-    # Ensure the same order for both lists (filesystem can spit the files in whatever order it wants)
-    files1.sort()
-    files2.sort()
-
-    # Different files in one or both lists: we fail
-    if files1 != files2:
-        return False
-    # Else we need to compare the files contents
-    else:
-        flag = True
-        for i in _range(len(files1)):
-            #print("files: %s %s" % (files1[i], files2[i]))  # debug
-            # If the files contents are different, we fail
-            if not check_eq_files(os.path.join(path1, files1[i]), os.path.join(path2, files2[i])):
-                flag = False
-                break
-        # Else if all files contents were equal and all files are in both lists, success!
-        return flag
-
-def fullpath(relpath):
-    '''Relative path to absolute'''
-    if (type(relpath) is object or hasattr(relpath, 'read')): # relpath is either an object or file-like, try to get its name
-        relpath = relpath.name
-    return os.path.abspath(os.path.expanduser(relpath))
-
-def path_sample_files(type=None, path=None, createdir=False):
-    """ Helper function to return the full path to the test files """
-    subdir = ''
-    if not type:
-        return ''
-    elif type == 'input':
-        subdir = 'files'
-    elif type == 'results':
-        subdir = 'results'
-    elif type == 'output':
-        subdir = 'out'
-
-    dirpath = ''
-    scriptpath = os.path.dirname(os.path.realpath(__file__))
-    if path:
-        dirpath = fullpath(os.path.join(scriptpath, subdir, path))
-    else:
-        dirpath = fullpath(os.path.join(scriptpath, subdir))
-
-    if createdir:
-        create_dir_if_not_exist(dirpath)
-
-    return dirpath
-
-def tamper_file(path, pos=0, replace_str=None):
-    """Tamper a file at the given position and using the given string"""
-    if not replace_str:
-        replace_str = "\x00"
-    try:
-        with open(path, "r+b") as fh:
-            if pos < 0: # if negative, we calculate the position backward from the end of file
-                fsize = os.fstat(fh.fileno()).st_size
-                pos = fsize + pos
-            fh.seek(pos)
-            fh.write(b(replace_str))
-    except IOError:
-        return False
-    finally:
-        try:
-            fh.close()
-        except Exception:
-            pass
-    return True
-
-def find_next_entry(path, marker="\xFF\xFF\xFF\xFF", initpos=0):
-    '''Find the next position of a marker in a file'''
-    blocksize = 65535
-    start = None # start is the relative position of the marker in the current buffer
-    startcursor = None # startcursor is the absolute position of the starting position of the marker in the file
-    buf = 1
-    infile = open(path, 'rb')
-    if initpos > 0: infile.seek(initpos)
-    # Enumerate all markers in a generator
-    while (buf):
-        # Read a long block at once, we will readjust the file cursor after
-        buf = bytearray(infile.read(blocksize))
-        # Find the start marker
-        start = buf.find(marker); # relative position of the starting marker in the currently read string
-        if start >= 0: # assign startcursor only if it's empty (meaning that we did not find the starting entrymarker, else if found we are only looking for 
-            startcursor = infile.tell() - len(buf) + start # absolute position of the starting marker in the file
-            infile.close() # close the file before yielding result, to avoid locking the file
-            yield startcursor
-            infile = open(path, 'rb') # reopen the file just after yield before doing further processing
-            infile.seek(startcursor+len(marker)) # place reading cursor just after the current marker to avoid repeatedly detecting the same marker
-    infile.close() # don't forget to close after the loop!
-
-def create_dir_if_not_exist(path):
-    """Create a directory if it does not already exist, else nothing is done and no error is return"""
-    if not os.path.exists(path):
-        os.makedirs(path)
-
-def remove_if_exist(path):
-    """Delete a file or a directory recursively if it exists, else no exception is raised"""
-    if os.path.exists(path):
-        if os.path.isdir(path):
-            shutil.rmtree(path)
-            return True
-        elif os.path.isfile(path):
-            os.remove(path)
-            return True
-    return False
-
-def get_marker(type=1):
-    """Helper function to store the usual entry and fields markers in ecc files"""
-    if type == 1:
-        return b"\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF"
-    elif type == 2:
-        return b"\xFA\xFF\xFA\xFF\xFA"
-    else:
-        return b''
-
-def dummy_ecc_file_gen(nb_files=1):
-    """ Generate a dummy ecc file, following the specs (of course the ecc tracks are fake!) """
-    # Create header comments
-    fcontent = b'''**SCRIPT_CODE_NAMEv111...000...000**\n** Comment 2\n** Yet another comment\n'''
-    # Create files entries
-    for i in range(1, nb_files+1):
-        fcontent += get_marker(1)+(b"file"+b(str(i))+b".ext")+get_marker(2)+(b"filesize"+b(str(i)))+get_marker(2)+(b"relfilepath"+b(str(i))+b"_ecc")+get_marker(2)+(b"filesize"+b(str(i))+b"_ecc")+get_marker(2)+b"hash-ecc-entry_"*(i*3)
-    return fcontent
+""" Auxiliary functions for unit tests """
+
+from __future__ import with_statement
+
+import os
+import shutil
+
+from ..lib._compat import _range, b
+
+def check_eq_files(path1, path2, blocksize=65535, startpos1=0, startpos2=0):
+    """ Return True if both files are identical, False otherwise """
+    flag = True
+    with open(path1, 'rb') as f1, open(path2, 'rb') as f2:
+        buf1 = 1
+        buf2 = 1
+        f1.seek(startpos1)
+        f2.seek(startpos2)
+        while buf1 and buf2:
+            buf1 = f1.read(blocksize)
+            buf2 = f2.read(blocksize)
+            if buf1 != buf2 or (buf1 and not buf2) or (buf2 and not buf1):
+                # Reached end of file or the content is different, then return false
+                flag = False
+                break
+            elif (not buf1 and not buf2):
+                # End of file for both files
+                break
+    return flag
+    #return filecmp.cmp(path1, path2, shallow=False)  # does not work on Travis
+
+def check_eq_dir(path1, path2):
+    """ Return True if both folders have same structure totally identical files, False otherwise """
+    # List files in both directories
+    files1 = []
+    files2 = []
+    for dirpath, dirs, files in os.walk(path1):
+        files1.extend([os.path.relpath(os.path.join(dirpath, file), path1) for file in files])
+    for dirpath, dirs, files in os.walk(path2):
+        files2.extend([os.path.relpath(os.path.join(dirpath, file), path2) for file in files])
+    # Ensure the same order for both lists (filesystem can spit the files in whatever order it wants)
+    files1.sort()
+    files2.sort()
+
+    # Different files in one or both lists: we fail
+    if files1 != files2:
+        return False
+    # Else we need to compare the files contents
+    else:
+        flag = True
+        for i in _range(len(files1)):
+            #print("files: %s %s" % (files1[i], files2[i]))  # debug
+            # If the files contents are different, we fail
+            if not check_eq_files(os.path.join(path1, files1[i]), os.path.join(path2, files2[i])):
+                flag = False
+                break
+        # Else if all files contents were equal and all files are in both lists, success!
+        return flag
+
+def fullpath(relpath):
+    '''Relative path to absolute'''
+    if (type(relpath) is object or hasattr(relpath, 'read')): # relpath is either an object or file-like, try to get its name
+        relpath = relpath.name
+    return os.path.abspath(os.path.expanduser(relpath))
+
+def path_sample_files(type=None, path=None, createdir=False):
+    """ Helper function to return the full path to the test files """
+    subdir = ''
+    if not type:
+        return ''
+    elif type == 'input':
+        subdir = 'files'
+    elif type == 'results':
+        subdir = 'results'
+    elif type == 'output':
+        subdir = 'out'
+
+    dirpath = ''
+    scriptpath = os.path.dirname(os.path.realpath(__file__))
+    if path:
+        dirpath = fullpath(os.path.join(scriptpath, subdir, path))
+    else:
+        dirpath = fullpath(os.path.join(scriptpath, subdir))
+
+    if createdir:
+        create_dir_if_not_exist(dirpath)
+
+    return dirpath
+
+def tamper_file(path, pos=0, replace_str=None):
+    """Tamper a file at the given position and using the given string"""
+    if not replace_str:
+        replace_str = "\x00"
+    try:
+        with open(path, "r+b") as fh:
+            if pos < 0: # if negative, we calculate the position backward from the end of file
+                fsize = os.fstat(fh.fileno()).st_size
+                pos = fsize + pos
+            fh.seek(pos)
+            fh.write(b(replace_str))
+    except IOError:
+        return False
+    finally:
+        try:
+            fh.close()
+        except Exception:
+            pass
+    return True
+
+def find_next_entry(path, marker="\xFF\xFF\xFF\xFF", initpos=0):
+    '''Find the next position of a marker in a file'''
+    blocksize = 65535
+    start = None # start is the relative position of the marker in the current buffer
+    startcursor = None # startcursor is the absolute position of the starting position of the marker in the file
+    buf = 1
+    infile = open(path, 'rb')
+    if initpos > 0: infile.seek(initpos)
+    # Enumerate all markers in a generator
+    while (buf):
+        # Read a long block at once, we will readjust the file cursor after
+        buf = bytearray(infile.read(blocksize))
+        # Find the start marker
+        start = buf.find(marker); # relative position of the starting marker in the currently read string
+        if start >= 0: # assign startcursor only if it's empty (meaning that we did not find the starting entrymarker, else if found we are only looking for 
+            startcursor = infile.tell() - len(buf) + start # absolute position of the starting marker in the file
+            infile.close() # close the file before yielding result, to avoid locking the file
+            yield startcursor
+            infile = open(path, 'rb') # reopen the file just after yield before doing further processing
+            infile.seek(startcursor+len(marker)) # place reading cursor just after the current marker to avoid repeatedly detecting the same marker
+    infile.close() # don't forget to close after the loop!
+
+def create_dir_if_not_exist(path):
+    """Create a directory if it does not already exist, else nothing is done and no error is return"""
+    if not os.path.exists(path):
+        os.makedirs(path)
+
+def remove_if_exist(path):
+    """Delete a file or a directory recursively if it exists, else no exception is raised"""
+    if os.path.exists(path):
+        if os.path.isdir(path):
+            shutil.rmtree(path)
+            return True
+        elif os.path.isfile(path):
+            os.remove(path)
+            return True
+    return False
+
+def get_marker(type=1):
+    """Helper function to store the usual entry and fields markers in ecc files"""
+    if type == 1:
+        return b"\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF\xFE\xFF"
+    elif type == 2:
+        return b"\xFA\xFF\xFA\xFF\xFA"
+    else:
+        return b''
+
+def dummy_ecc_file_gen(nb_files=1):
+    """ Generate a dummy ecc file, following the specs (of course the ecc tracks are fake!) """
+    # Create header comments
+    fcontent = b'''**SCRIPT_CODE_NAMEv111...000...000**\n** Comment 2\n** Yet another comment\n'''
+    # Create files entries
+    for i in range(1, nb_files+1):
+        fcontent += get_marker(1)+(b"file"+b(str(i))+b".ext")+get_marker(2)+(b"filesize"+b(str(i)))+get_marker(2)+(b"relfilepath"+b(str(i))+b"_ecc")+get_marker(2)+(b"filesize"+b(str(i))+b"_ecc")+get_marker(2)+b"hash-ecc-entry_"*(i*3)
+    return fcontent
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/files/alice.pdf` & `pyFileFixity-3.1.4/pyFileFixity/tests/files/alice.pdf`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/files/sub/Snark.zip` & `pyFileFixity-3.1.4/pyFileFixity/tests/files/sub/Snark.zip`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/files/tux.jpg` & `pyFileFixity-3.1.4/pyFileFixity/tests/files/tux.jpg`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/files/tuxsmall.jpg` & `pyFileFixity-3.1.4/pyFileFixity/tests/files/tuxsmall.jpg`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/resiliency_tester_config_easy.cfg` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/resiliency_tester_config_easy.cfg`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-# IMPORTANT: to be compatible with `python setup.py make alias`, you must make
-# sure that you only put one command per line, and ALWAYS put a line return
-# after an alias and before a command, eg:
-#
-#```
-#all:
-#	test
-#	install
-#test:
-#	nosetest
-#install:
-#	python setup.py install
-#    ```
-#
-# resiliency_tester.py supports a templating system: you can use the following special tags, they will be interpolated at runtime:
-#   - {inputdir}: input directory. Depending on the stage, this is either the untampered files (a copy of the original files), the tampered folder, or even previous repair folders during the repair stage.
-#   - {dbdir}: database directory, where the generated databases will be placed.
-#   - {outputdir}: output directory, where the files generated after executing the current command will be placed in.
-
-before_tamper: # this will be executed before files tampering. Generate your ecc/database files here.
-    python header_ecc.py -i "{inputdir}" -d "{dbdir}/hecc.txt" --size 4096 --ecc_algo 3 -g -f --silent
-    #python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/ecc.txt" -r1 0.3 -r2 0.2 -r3 0.1 -g -f --ecc_algo 3 --silent
-
-tamper: # parameters to tamper the files and even the database files.
-    python filetamper.py -i "{inputdir}" -m "n" -p 0.001 -b "3|6" --header 4096 --silent
-    python filetamper.py -i "{dbdir}" -m "n" -p 0.0001 -b "4|9" --header 4096 --silent
-
-after_tamper: # execute commands after tampering. Can be used to recover
-    python repair_ecc.py -i "{dbdir}/hecc.txt" --index "{dbdir}/hecc.txt.idx" -o "{dbdir}/heccrep.txt" -t 0.4 -f --silent
-    #python repair_ecc.py -i "{dbdir}/ecc.txt" --index "{dbdir}/ecc.txt.idx" -o "{dbdir}/eccrep.txt" -t 0.4 -f --silent
-
-repair:
-    python header_ecc.py -i "{inputdir}" -d "{dbdir}/heccrep.txt" -o "{outputdir}" -c --size 4096 --no_fast_check --ecc_algo 3 --silent
-    #python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/eccrep.txt" -o "{outputdir}" -c -r1 0.3 -r2 0.2 -r3 0.1 -f --ecc_algo 3 --silent
-
-none:
-	# used for unit testing
+# IMPORTANT: to be compatible with `python setup.py make alias`, you must make
+# sure that you only put one command per line, and ALWAYS put a line return
+# after an alias and before a command, eg:
+#
+#```
+#all:
+#	test
+#	install
+#test:
+#	nosetest
+#install:
+#	python setup.py install
+#    ```
+#
+# resiliency_tester.py supports a templating system: you can use the following special tags, they will be interpolated at runtime:
+#   - {inputdir}: input directory. Depending on the stage, this is either the untampered files (a copy of the original files), the tampered folder, or even previous repair folders during the repair stage.
+#   - {dbdir}: database directory, where the generated databases will be placed.
+#   - {outputdir}: output directory, where the files generated after executing the current command will be placed in.
+
+before_tamper: # this will be executed before files tampering. Generate your ecc/database files here.
+    python header_ecc.py -i "{inputdir}" -d "{dbdir}/hecc.txt" --size 4096 --ecc_algo 3 -g -f --silent
+    #python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/ecc.txt" -r1 0.3 -r2 0.2 -r3 0.1 -g -f --ecc_algo 3 --silent
+
+tamper: # parameters to tamper the files and even the database files.
+    python filetamper.py -i "{inputdir}" -m "n" -p 0.001 -b "3|6" --header 4096 --silent
+    python filetamper.py -i "{dbdir}" -m "n" -p 0.0001 -b "4|9" --header 4096 --silent
+
+after_tamper: # execute commands after tampering. Can be used to recover
+    python repair_ecc.py -i "{dbdir}/hecc.txt" --index "{dbdir}/hecc.txt.idx" -o "{dbdir}/heccrep.txt" -t 0.4 -f --silent
+    #python repair_ecc.py -i "{dbdir}/ecc.txt" --index "{dbdir}/ecc.txt.idx" -o "{dbdir}/eccrep.txt" -t 0.4 -f --silent
+
+repair:
+    python header_ecc.py -i "{inputdir}" -d "{dbdir}/heccrep.txt" -o "{outputdir}" -c --size 4096 --no_fast_check --ecc_algo 3 --silent
+    #python structural_adaptive_ecc.py -i "{inputdir}" -d "{dbdir}/eccrep.txt" -o "{outputdir}" -c -r1 0.3 -r2 0.2 -r3 0.1 -f --ecc_algo 3 --silent
+
+none:
+	# used for unit testing
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/resiliency_tester_config_hard.cfg` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/resiliency_tester_config_hard.cfg`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,34 @@
-# IMPORTANT: to be compatible with `python setup.py make alias`, you must make
-# sure that you only put one command per line, and ALWAYS put a line return
-# after an alias and before a command, eg:
-#
-#```
-#all:
-#	test
-#	install
-#test:
-#	nosetest
-#install:
-#	python setup.py install
-#    ```
-#
-# resiliency_tester.py supports a templating system: you can use the following special tags, they will be interpolated at runtime:
-#   - {inputdir}: input directory. Depending on the stage, this is either the untampered files (a copy of the original files), the tampered folder, or even previous repair folders during the repair stage.
-#   - {dbdir}: database directory, where the generated databases will be placed.
-#   - {outputdir}: output directory, where the files generated after executing the current command will be placed in.
-
-before_tamper: # this will be executed before files tampering. Generate your ecc/database files here.
-    #python header_ecc.py -i "{inputdir}" -d "{dbdir}/hecc.txt" --size 4096 --ecc_algo 3 -g -f --silent
-
-tamper: # parameters to tamper the files and even the database files.
-    python filetamper.py -i "{inputdir}" -m "n" -p 0.05 -b "3|6" --silent
-    python filetamper.py -i "{dbdir}" -m "n" -p 0.001 -b "4|9" --silent
-
-after_tamper: # execute commands after tampering. Can be used to recover
-    #python repair_ecc.py -i "{dbdir}/hecc.txt" --index "{dbdir}/hecc.txt.idx" -o "{dbdir}/heccrep.txt" -t 0.4 -f --silent
-
-repair:
-    #python header_ecc.py -i "{inputdir}" -d "{dbdir}/heccrep.txt" -o "{outputdir}" -c --size 4096 --no_fast_check --ecc_algo 3 --silent
-
-none:
-	# used for unit testing
+# IMPORTANT: to be compatible with `python setup.py make alias`, you must make
+# sure that you only put one command per line, and ALWAYS put a line return
+# after an alias and before a command, eg:
+#
+#```
+#all:
+#	test
+#	install
+#test:
+#	nosetest
+#install:
+#	python setup.py install
+#    ```
+#
+# resiliency_tester.py supports a templating system: you can use the following special tags, they will be interpolated at runtime:
+#   - {inputdir}: input directory. Depending on the stage, this is either the untampered files (a copy of the original files), the tampered folder, or even previous repair folders during the repair stage.
+#   - {dbdir}: database directory, where the generated databases will be placed.
+#   - {outputdir}: output directory, where the files generated after executing the current command will be placed in.
+
+before_tamper: # this will be executed before files tampering. Generate your ecc/database files here.
+    #python header_ecc.py -i "{inputdir}" -d "{dbdir}/hecc.txt" --size 4096 --ecc_algo 3 -g -f --silent
+
+tamper: # parameters to tamper the files and even the database files.
+    python filetamper.py -i "{inputdir}" -m "n" -p 0.05 -b "3|6" --silent
+    python filetamper.py -i "{dbdir}" -m "n" -p 0.001 -b "4|9" --silent
+
+after_tamper: # execute commands after tampering. Can be used to recover
+    #python repair_ecc.py -i "{dbdir}/hecc.txt" --index "{dbdir}/hecc.txt.idx" -o "{dbdir}/heccrep.txt" -t 0.4 -f --silent
+
+repair:
+    #python header_ecc.py -i "{inputdir}" -d "{dbdir}/heccrep.txt" -o "{outputdir}" -c --size 4096 --no_fast_check --ecc_algo 3 --silent
+
+none:
+	# used for unit testing
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_algo.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_algo.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_dir.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_dir.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_one_file.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_one_file.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_header_ecc_test_one_file_tamper.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_header_ecc_test_one_file_tamper.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_repair_ecc_check.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_repair_ecc_check.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_repair_ecc_sa_check.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_repair_ecc_sa_check.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_dir.csv` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_dir.csv`

 * *Ordering differences only*

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-path|md5|sha1|last_modification_timestamp|last_modification_date|size|ext
-alice.pdf|298aeefe8c00f2d92d660987bee67260|106e7ad4d3927c5906cd366cc0d5bd887bdc3300|
-testaa.txt|c0d8a5f3a813d488cbfb83f1b147b14b|6ca36c14f68e4eefa47ec23ccc333378b8d0fe73|
-tux.jpg|81e19bbf2efaeb1d6d6473c21c48e4b7|6e38ea91680ef0f960db0fd6a973cf50ef765369|
-tuxsmall.jpg|1c5704dd227e1de7d96b355c6111c764|f8a1f7675ea360bff97d02443c174c102fbcdefa|
-Sub2/testsub2.txt|e80b5017098950fc58aad83c8c14978e|1f8ac10f23c5b5bc1167bda84b833e5c057a77d2|
-sub/Snark.zip|f8435b883eaf03bf84cae75a706a9b8c|e68efd832dd3517d4c80db6a84b98591eeabe864|
-sub/testsub.txt|8ef3d6be5baa449c127aa00083ebbe34|bfb7ef83b23e0791199e4ebe9ae34489a4ef7004|
+path|md5|sha1|last_modification_timestamp|last_modification_date|size|ext
+alice.pdf|298aeefe8c00f2d92d660987bee67260|106e7ad4d3927c5906cd366cc0d5bd887bdc3300|
+testaa.txt|c0d8a5f3a813d488cbfb83f1b147b14b|6ca36c14f68e4eefa47ec23ccc333378b8d0fe73|
+tux.jpg|81e19bbf2efaeb1d6d6473c21c48e4b7|6e38ea91680ef0f960db0fd6a973cf50ef765369|
+tuxsmall.jpg|1c5704dd227e1de7d96b355c6111c764|f8a1f7675ea360bff97d02443c174c102fbcdefa|
+Sub2/testsub2.txt|e80b5017098950fc58aad83c8c14978e|1f8ac10f23c5b5bc1167bda84b833e5c057a77d2|
+sub/Snark.zip|f8435b883eaf03bf84cae75a706a9b8c|e68efd832dd3517d4c80db6a84b98591eeabe864|
+sub/testsub.txt|8ef3d6be5baa449c127aa00083ebbe34|bfb7ef83b23e0791199e4ebe9ae34489a4ef7004|
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_rfigc_test_update_append.csv` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_rfigc_test_update_append.csv`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-path|md5|sha1|last_modification_timestamp|last_modification_date|size|ext
-alice.pdf|298aeefe8c00f2d92d660987bee67260|106e7ad4d3927c5906cd366cc0d5bd887bdc3300|
-testaa.txt|c0d8a5f3a813d488cbfb83f1b147b14b|6ca36c14f68e4eefa47ec23ccc333378b8d0fe73|
-tux.jpg|81e19bbf2efaeb1d6d6473c21c48e4b7|6e38ea91680ef0f960db0fd6a973cf50ef765369|
-tuxsmall.jpg|1c5704dd227e1de7d96b355c6111c764|f8a1f7675ea360bff97d02443c174c102fbcdefa|
-sub/Snark.zip|f8435b883eaf03bf84cae75a706a9b8c|e68efd832dd3517d4c80db6a84b98591eeabe864|
-sub/testsub.txt|8ef3d6be5baa449c127aa00083ebbe34|bfb7ef83b23e0791199e4ebe9ae34489a4ef7004|
-added_file.txt|fad0092ae8c6218c1fb78d281238168d|0a21ef1d2ccc47ffedf45192d0b8c26afd5d552f|
+path|md5|sha1|last_modification_timestamp|last_modification_date|size|ext
+alice.pdf|298aeefe8c00f2d92d660987bee67260|106e7ad4d3927c5906cd366cc0d5bd887bdc3300|
+testaa.txt|c0d8a5f3a813d488cbfb83f1b147b14b|6ca36c14f68e4eefa47ec23ccc333378b8d0fe73|
+tux.jpg|81e19bbf2efaeb1d6d6473c21c48e4b7|6e38ea91680ef0f960db0fd6a973cf50ef765369|
+tuxsmall.jpg|1c5704dd227e1de7d96b355c6111c764|f8a1f7675ea360bff97d02443c174c102fbcdefa|
+sub/Snark.zip|f8435b883eaf03bf84cae75a706a9b8c|e68efd832dd3517d4c80db6a84b98591eeabe864|
+sub/testsub.txt|8ef3d6be5baa449c127aa00083ebbe34|bfb7ef83b23e0791199e4ebe9ae34489a4ef7004|
+added_file.txt|fad0092ae8c6218c1fb78d281238168d|0a21ef1d2ccc47ffedf45192d0b8c26afd5d552f|
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_algo.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_algo.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_dir.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_dir.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file_tamper.db` & `pyFileFixity-3.1.4/pyFileFixity/tests/results/test_structural_adaptive_ecc_test_one_file_tamper.db`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_aux_funcs.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_aux_funcs.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,127 +1,127 @@
-from __future__ import print_function
-
-import unittest
-import sys
-import os
-import shutil
-
-from .aux_tests import get_marker, dummy_ecc_file_gen, path_sample_files, create_dir_if_not_exist
-
-from ..lib import aux_funcs as auxf
-from argparse import ArgumentTypeError
-
-from io import BytesIO
-
-class TestAuxFuncs(unittest.TestCase):
-    def setup_module(self):
-        """ Initialize the tests by emptying the out directory """
-        outfolder = path_sample_files('output')
-        shutil.rmtree(outfolder, ignore_errors=True)
-        create_dir_if_not_exist(outfolder)
-
-    def test_get_next_entry(self):
-        """ aux: test detection of next entry """
-        entries = [
-                b'''file1.ext\xfa\xff\xfa\xff\xfafilesize1\xfa\xff\xfa\xff\xfarelfilepath1_ecc\xfa\xff\xfa\xff\xfafilesize1_ecc\xfa\xff\xfa\xff\xfahash-ecc-entry_hash-ecc-entry_hash-ecc-entry_''',
-                b'''file2.ext\xfa\xff\xfa\xff\xfafilesize2\xfa\xff\xfa\xff\xfarelfilepath2_ecc\xfa\xff\xfa\xff\xfafilesize2_ecc\xfa\xff\xfa\xff\xfahash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_'''
-              ]
-        entries_pos = [
-                        [83, 195],
-                        [205, 362]
-                      ]
-
-        filecontent = dummy_ecc_file_gen(2)
-        fp1 = BytesIO(filecontent)
-        entry = auxf.get_next_entry(fp1, entrymarker=get_marker(1), only_coord=False, blocksize=len(get_marker(1))+1)
-        assert entry == entries[0]
-        entry = auxf.get_next_entry(fp1, entrymarker=get_marker(1), only_coord=False, blocksize=len(get_marker(1))+1)
-        assert entry == entries[1]
-        fp2 = BytesIO(filecontent)
-        entry = auxf.get_next_entry(fp2, entrymarker=get_marker(1), only_coord=True, blocksize=len(get_marker(1))+1)
-        assert entry == entries_pos[0]
-        entry = auxf.get_next_entry(fp2, entrymarker=get_marker(1), only_coord=True, blocksize=len(get_marker(1))+1)
-        assert entry == entries_pos[1]
-
-    def test_sizeof_fmt(self):
-        """ aux: test SI formatting """
-        # Test without SI prefix
-        assert auxf.sizeof_fmt(1023.0, suffix='B', mod=1024.0) == "1023.0B"
-        # Test all possible SI prefixes
-        pows = ['', 'K','M','G','T','P','E','Z', 'Y']
-        for p in range(1, len(pows)):
-            assert auxf.sizeof_fmt(1024.0**p, suffix='B', mod=1024.0) == ("1.0%sB" % pows[p])
-
-    def test_path2unix(self):
-        """ aux: test path2unix """
-        assert auxf.path2unix(r'test\some\folder\file.ext', fromwinpath=True) == r'test/some/folder/file.ext'
-        assert auxf.path2unix(r'test\some\folder\file.ext', nojoin=True, fromwinpath=True) == ['test', 'some', 'folder', 'file.ext']
-        assert auxf.path2unix(r'test/some/folder/file.ext') == r'test/some/folder/file.ext'
-
-    def test_is_file(self):
-        """ aux: test is_file() """
-        indir = path_sample_files('input')
-        infile = path_sample_files('input', 'tux.jpg')
-        assert auxf.is_file(infile)
-        self.assertRaises(ArgumentTypeError, auxf.is_file, indir)
-
-    def test_is_dir(self):
-        """ aux: test is_dir() """
-        indir = path_sample_files('input')
-        infile = path_sample_files('input', 'tux.jpg')
-        assert auxf.is_dir(indir)
-        self.assertRaises(ArgumentTypeError, auxf.is_dir, infile)
-
-    def test_is_dir_or_file(self):
-        """ aux: test is_dir_or_file() """
-        indir = path_sample_files('input')
-        infile = path_sample_files('input', 'tux.jpg')
-        indir_fake = r'path/that/do/not/exist/at/all'
-        infile_fake = path_sample_files('input', 'random_gibberish_file_that_do_not_exists')
-        assert auxf.is_dir_or_file(indir)
-        assert auxf.is_dir_or_file(infile)
-        self.assertRaises(ArgumentTypeError, auxf.is_dir_or_file, indir_fake)
-        self.assertRaises(ArgumentTypeError, auxf.is_dir_or_file, infile_fake)
-
-    def test_recwalk(self):
-        """ aux: test recwalk() """
-        def list_paths_posix(recwalk_result):
-            """ helper function to convert all paths to relative posix like paths (to ease comparison) """
-            return [auxf.path2unix(os.path.join(os.path.relpath(x, pardir),y)) for x,y in recwalk_result]
-        indir = path_sample_files('input')
-        pardir = os.path.dirname(indir)
-        # Compare between sorted and non-sorted path walking (the result should be different! but sorted path should always be the same on all platforms!)
-        res1 = list_paths_posix(auxf.recwalk(indir, sorting=True))
-        res2 = list_paths_posix(auxf.recwalk(indir, sorting=False))
-        # Absolute test: sorted walking should always return the same result on all platforms
-        assert res1 == ['files/alice.pdf', 'files/testaa.txt', 'files/tux.jpg', 'files/tuxsmall.jpg', 'files/Sub2/testsub2.txt', 'files/sub/Snark.zip', 'files/sub/testsub.txt']
-        # Relative test: compare with platform's results
-        if os.name == 'nt':
-            assert res2 != res1
-            assert res2 == ['files/alice.pdf', 'files/testaa.txt', 'files/tux.jpg', 'files/tuxsmall.jpg', 'files/sub/Snark.zip', 'files/sub/testsub.txt', 'files/Sub2/testsub2.txt']
-        elif os.name == 'posix':
-            assert res2 != res1 # BEWARE, do NOT use sets here! On linux, order of generated files can change, although a set is unordered, they will be equal if elements in the sets are the same, contrary to lists, but that's what we are testing here, with ordered walk it should NOT be the same!
-
-    def test_fullpath(self):
-        """ aux: test fullpath() """
-        def relpath(path, pardir):
-            """ helper function to always return a relative posix-like path (ease comparisons) """
-            return auxf.path2unix(os.path.relpath(path, pardir))
-        # Can't really objectively test fullpath() but we can relatively compare the result
-        indir = path_sample_files('input')
-        infile = path_sample_files('input', 'tux.jpg')
-        pardir = os.path.dirname(indir)
-        # Directory test
-        assert relpath(auxf.fullpath(indir), pardir) == 'files'
-        # File test
-        res1 = relpath(auxf.fullpath(infile), pardir)
-        assert res1 == 'files/tux.jpg'
-        # Opened file test
-        with open(infile, 'rb') as fh:
-            res2 = relpath(auxf.fullpath(fh), pardir)
-        assert res1 == res2
-
-    def test_get_version(self):
-        """ aux: test get_version() """
-        thispathname = os.path.dirname(__file__)
-        assert '.' in auxf.get_version('__init__.py', os.path.join(thispathname, '..'))
-        self.assertRaises(RuntimeError, auxf.get_version, 'test_aux_funcs.py', thispathname)
+from __future__ import print_function
+
+import unittest
+import sys
+import os
+import shutil
+
+from .aux_tests import get_marker, dummy_ecc_file_gen, path_sample_files, create_dir_if_not_exist
+
+from ..lib import aux_funcs as auxf
+from argparse import ArgumentTypeError
+
+from io import BytesIO
+
+class TestAuxFuncs(unittest.TestCase):
+    def setup_module(self):
+        """ Initialize the tests by emptying the out directory """
+        outfolder = path_sample_files('output')
+        shutil.rmtree(outfolder, ignore_errors=True)
+        create_dir_if_not_exist(outfolder)
+
+    def test_get_next_entry(self):
+        """ aux: test detection of next entry """
+        entries = [
+                b'''file1.ext\xfa\xff\xfa\xff\xfafilesize1\xfa\xff\xfa\xff\xfarelfilepath1_ecc\xfa\xff\xfa\xff\xfafilesize1_ecc\xfa\xff\xfa\xff\xfahash-ecc-entry_hash-ecc-entry_hash-ecc-entry_''',
+                b'''file2.ext\xfa\xff\xfa\xff\xfafilesize2\xfa\xff\xfa\xff\xfarelfilepath2_ecc\xfa\xff\xfa\xff\xfafilesize2_ecc\xfa\xff\xfa\xff\xfahash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_'''
+              ]
+        entries_pos = [
+                        [83, 195],
+                        [205, 362]
+                      ]
+
+        filecontent = dummy_ecc_file_gen(2)
+        fp1 = BytesIO(filecontent)
+        entry = auxf.get_next_entry(fp1, entrymarker=get_marker(1), only_coord=False, blocksize=len(get_marker(1))+1)
+        assert entry == entries[0]
+        entry = auxf.get_next_entry(fp1, entrymarker=get_marker(1), only_coord=False, blocksize=len(get_marker(1))+1)
+        assert entry == entries[1]
+        fp2 = BytesIO(filecontent)
+        entry = auxf.get_next_entry(fp2, entrymarker=get_marker(1), only_coord=True, blocksize=len(get_marker(1))+1)
+        assert entry == entries_pos[0]
+        entry = auxf.get_next_entry(fp2, entrymarker=get_marker(1), only_coord=True, blocksize=len(get_marker(1))+1)
+        assert entry == entries_pos[1]
+
+    def test_sizeof_fmt(self):
+        """ aux: test SI formatting """
+        # Test without SI prefix
+        assert auxf.sizeof_fmt(1023.0, suffix='B', mod=1024.0) == "1023.0B"
+        # Test all possible SI prefixes
+        pows = ['', 'K','M','G','T','P','E','Z', 'Y']
+        for p in range(1, len(pows)):
+            assert auxf.sizeof_fmt(1024.0**p, suffix='B', mod=1024.0) == ("1.0%sB" % pows[p])
+
+    def test_path2unix(self):
+        """ aux: test path2unix """
+        assert auxf.path2unix(r'test\some\folder\file.ext', fromwinpath=True) == r'test/some/folder/file.ext'
+        assert auxf.path2unix(r'test\some\folder\file.ext', nojoin=True, fromwinpath=True) == ['test', 'some', 'folder', 'file.ext']
+        assert auxf.path2unix(r'test/some/folder/file.ext') == r'test/some/folder/file.ext'
+
+    def test_is_file(self):
+        """ aux: test is_file() """
+        indir = path_sample_files('input')
+        infile = path_sample_files('input', 'tux.jpg')
+        assert auxf.is_file(infile)
+        self.assertRaises(ArgumentTypeError, auxf.is_file, indir)
+
+    def test_is_dir(self):
+        """ aux: test is_dir() """
+        indir = path_sample_files('input')
+        infile = path_sample_files('input', 'tux.jpg')
+        assert auxf.is_dir(indir)
+        self.assertRaises(ArgumentTypeError, auxf.is_dir, infile)
+
+    def test_is_dir_or_file(self):
+        """ aux: test is_dir_or_file() """
+        indir = path_sample_files('input')
+        infile = path_sample_files('input', 'tux.jpg')
+        indir_fake = r'path/that/do/not/exist/at/all'
+        infile_fake = path_sample_files('input', 'random_gibberish_file_that_do_not_exists')
+        assert auxf.is_dir_or_file(indir)
+        assert auxf.is_dir_or_file(infile)
+        self.assertRaises(ArgumentTypeError, auxf.is_dir_or_file, indir_fake)
+        self.assertRaises(ArgumentTypeError, auxf.is_dir_or_file, infile_fake)
+
+    def test_recwalk(self):
+        """ aux: test recwalk() """
+        def list_paths_posix(recwalk_result):
+            """ helper function to convert all paths to relative posix like paths (to ease comparison) """
+            return [auxf.path2unix(os.path.join(os.path.relpath(x, pardir),y)) for x,y in recwalk_result]
+        indir = path_sample_files('input')
+        pardir = os.path.dirname(indir)
+        # Compare between sorted and non-sorted path walking (the result should be different! but sorted path should always be the same on all platforms!)
+        res1 = list_paths_posix(auxf.recwalk(indir, sorting=True))
+        res2 = list_paths_posix(auxf.recwalk(indir, sorting=False))
+        # Absolute test: sorted walking should always return the same result on all platforms
+        assert res1 == ['files/alice.pdf', 'files/testaa.txt', 'files/tux.jpg', 'files/tuxsmall.jpg', 'files/Sub2/testsub2.txt', 'files/sub/Snark.zip', 'files/sub/testsub.txt']
+        # Relative test: compare with platform's results
+        if os.name == 'nt':
+            assert res2 != res1
+            assert res2 == ['files/alice.pdf', 'files/testaa.txt', 'files/tux.jpg', 'files/tuxsmall.jpg', 'files/sub/Snark.zip', 'files/sub/testsub.txt', 'files/Sub2/testsub2.txt']
+        elif os.name == 'posix':
+            assert res2 != res1 # BEWARE, do NOT use sets here! On linux, order of generated files can change, although a set is unordered, they will be equal if elements in the sets are the same, contrary to lists, but that's what we are testing here, with ordered walk it should NOT be the same!
+
+    def test_fullpath(self):
+        """ aux: test fullpath() """
+        def relpath(path, pardir):
+            """ helper function to always return a relative posix-like path (ease comparisons) """
+            return auxf.path2unix(os.path.relpath(path, pardir))
+        # Can't really objectively test fullpath() but we can relatively compare the result
+        indir = path_sample_files('input')
+        infile = path_sample_files('input', 'tux.jpg')
+        pardir = os.path.dirname(indir)
+        # Directory test
+        assert relpath(auxf.fullpath(indir), pardir) == 'files'
+        # File test
+        res1 = relpath(auxf.fullpath(infile), pardir)
+        assert res1 == 'files/tux.jpg'
+        # Opened file test
+        with open(infile, 'rb') as fh:
+            res2 = relpath(auxf.fullpath(fh), pardir)
+        assert res1 == res2
+
+    def test_get_version(self):
+        """ aux: test get_version() """
+        thispathname = os.path.dirname(__file__)
+        assert '.' in auxf.get_version('__init__.py', os.path.join(thispathname, '..'))
+        self.assertRaises(RuntimeError, auxf.get_version, 'test_aux_funcs.py', thispathname)
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_eccman.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_eccman.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,126 +1,126 @@
-from __future__ import print_function
-
-import unittest
-import sys
-import os
-import shutil
-
-from .aux_tests import get_marker, dummy_ecc_file_gen, check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, remove_if_exist
-
-from ..lib.eccman import ECCMan, compute_ecc_params, detect_reedsolomon_parameters
-
-from ..lib._compat import _StringIO, b
-
-class TestECCMan(unittest.TestCase):
-    def setup_module(self):
-        """ Initialize the tests by emptying the out directory """
-        outfolder = path_sample_files('output')
-        shutil.rmtree(outfolder, ignore_errors=True)
-        create_dir_if_not_exist(outfolder)
-
-    def test_eccman_detect_rs_param(self):
-        """ eccman: test reedsolomon param detection """
-        message = b("hello world")
-        mesecc_orig = [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 187, 161, 157, 88, 92, 175, 116, 251, 116]
-        mesecc_orig_tampered = [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 187, 161, 157, 88, 0, 175, 116, 251, 116]
-        n = len(mesecc_orig)
-        k = len(message)
-        params = [n, k, 2, 0x187, 120]
-        res = detect_reedsolomon_parameters(message, mesecc_orig)
-        res2 = detect_reedsolomon_parameters(message, mesecc_orig_tampered)
-        assert ("Hamming distance 0 (0=perfect match):\ngen_nb=%i prim=%i(%s) fcr=%i" % (params[2], params[3], hex(params[3]), params[4])) in res
-        assert ("Hamming distance 1:\ngen_nb=%i prim=%i(%s) fcr=%i" % (params[2], params[3], hex(params[3]), params[4])) in res2
-        res3 = detect_reedsolomon_parameters(message, [-1]*len(mesecc_orig), [3])
-        assert "Parameters could not be automatically detected" in res3
-        self.assertRaises(ValueError, detect_reedsolomon_parameters, [257, 0, 0], [0, 0, 0], c_exp=8)
-        self.assertRaises(ValueError, detect_reedsolomon_parameters, [0, 0, 0], [257, 0, 0], c_exp=8)
-
-    def test_eccman_compute_ecc_params(self):
-        """ eccman: test ecc params computation """
-        class Hasher(object):
-            """ Dummy Hasher """
-            def __len__(self):
-                return 32
-        hasher = Hasher()
-        assert compute_ecc_params(255, 0.5, hasher) == {'ecc_size': 127, 'hash_size': 32, 'message_size': 128}
-        assert compute_ecc_params(255, 0.0, hasher) == {'ecc_size': 0, 'hash_size': 32, 'message_size': 255}
-        assert compute_ecc_params(255, 1.0, hasher) == {'ecc_size': 170, 'hash_size': 32, 'message_size': 85}
-        assert compute_ecc_params(255, 0.3, hasher) == {'ecc_size': 96, 'hash_size': 32, 'message_size': 159}
-        assert compute_ecc_params(255, 0.7, hasher) == {'ecc_size': 149, 'hash_size': 32, 'message_size': 106}
-        assert compute_ecc_params(255, 2.0, hasher) == {'ecc_size': 204, 'hash_size': 32, 'message_size': 51}
-        assert compute_ecc_params(255, 10.0, hasher) == {'ecc_size': 243, 'hash_size': 32, 'message_size': 12}
-        assert compute_ecc_params(140, 10.0, hasher) == {'ecc_size': 133, 'hash_size': 32, 'message_size': 7}
-
-    def test_eccman_codecs(self):
-        """ eccman: test ecc generation and decoding """
-        expected = [
-            [206, 234, 144, 153, 141, 196, 170, 96, 62],
-            [206, 234, 144, 153, 141, 196, 170, 96, 62],
-            [206, 234, 144, 153, 141, 196, 170, 96, 62],
-            [187, 161, 157, 88, 92, 175, 116, 251, 116]
-        ]
-        message = b("hello world")
-        message_eras = b("h\x00ll\x00 world")
-        message_noise = b("h\x00ll\x00 worla")
-        n = 20
-        k = 11
-        for i in range(1,5):
-            eccman = ECCMan(n, k, algo=i)
-            ecc = bytearray(b(eccman.encode(message)))
-            assert list(ecc) == expected[i-1]
-            assert b(eccman.decode(message_eras, ecc)[0]) == message
-            assert b(eccman.decode(message_eras, ecc, enable_erasures=True)[0]) == message
-            assert b(eccman.decode(message_eras, ecc, enable_erasures=True, only_erasures=True)[0]) == message
-            #eccman.decode(message_noise, ecc, enable_erasures=True, only_erasures=True)[0]
-            assert eccman.check(message, ecc)
-            assert not eccman.check(message_eras, ecc)
-            assert "Reed-Solomon with polynomials in Galois field of characteristic" in eccman.description()
-        # Unknown algorithm test
-        self.assertRaises(Exception, ECCMan, n, k, algo=-1)
-        eccman = ECCMan(n, k, algo=1)
-        eccman.algo = -1
-        assert "No description for this ECC algorithm." in eccman.description()
-
-    def test_eccman_pad(self):
-        """ eccman: test ecc padding """
-        message = b("hello world")
-        ecc = b(''.join([chr(x) for x in [206, 234, 144, 153, 141, 196, 170, 96, 62]]))
-        # Oversize parameters compared to the message and ecc
-        n = 22 # should be 20
-        k = 13 # should be 11, but we add +2, which bytes we will pad onto the ecc and the decoding should still work!
-        eccman = ECCMan(n, k, algo=3)
-        # Test left padding (the input message)
-        pmessage = eccman.pad(message)
-        assert pmessage == [b('\x00\x00hello world'), b('\x00\x00')] # format: [padded_message, padonly]
-        assert eccman.check(pmessage[0], ecc)
-        # Test right padding (the ecc block)
-        pecc = eccman.rpad(ecc, 11)
-        assert pecc == [b('\xce\xea\x90\x99\x8d\xc4\xaa`>\x00\x00'), b('\x00\x00')]
-        assert eccman.check(message, pecc[0])
-        # Test decoding with both padding!
-        assert eccman.check(pmessage[0], pecc[0])
-
-    def test_eccman_lpad_decoding(self):
-        """ eccman: test ecc decoding when message needs left padding """
-        message = b("hello world")
-        ecc = b(''.join([chr(x) for x in [206, 234, 144, 153, 141, 196, 170, 96, 62]]))
-        message_eras = b("h\x00ll\x00 world")
-        # Oversize parameters compared to the message and ecc
-        n = 22 # should be 20
-        k = 13 # should be 11, but we add +2, which bytes we will pad onto the ecc and the decoding should still work!
-        eccman = ECCMan(n, k, algo=3)
-        # Test decoding with erasure when the message needs to be padded
-        assert eccman.decode(message_eras, ecc, enable_erasures=True)
-
-    def test_eccman_rpad_decoding(self):
-        """ eccman: test ecc decoding when right padding """
-        message = b("hello world")
-        ecc = b(''.join([chr(x) for x in [206, 234, 144, 153, 141, 196, 170, 96, 62]]))
-        message_eras = b("h\x00ll\x00 world")
-        # Oversize parameters compared to the message and ecc
-        n = 20 # should be 20
-        k = 11 # should be 11, but we add +2, which bytes we will pad onto the ecc and the decoding should still work!
-        eccman = ECCMan(n, k, algo=3)
-        # Test decoding with erasure when the message needs to be padded
-        assert eccman.decode(message_eras, ecc, enable_erasures=True)
-        assert eccman.decode(message_eras, ecc[:-2])
+from __future__ import print_function
+
+import unittest
+import sys
+import os
+import shutil
+
+from .aux_tests import get_marker, dummy_ecc_file_gen, check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, remove_if_exist
+
+from ..lib.eccman import ECCMan, compute_ecc_params, detect_reedsolomon_parameters
+
+from ..lib._compat import _StringIO, b
+
+class TestECCMan(unittest.TestCase):
+    def setup_module(self):
+        """ Initialize the tests by emptying the out directory """
+        outfolder = path_sample_files('output')
+        shutil.rmtree(outfolder, ignore_errors=True)
+        create_dir_if_not_exist(outfolder)
+
+    def test_eccman_detect_rs_param(self):
+        """ eccman: test reedsolomon param detection """
+        message = b("hello world")
+        mesecc_orig = [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 187, 161, 157, 88, 92, 175, 116, 251, 116]
+        mesecc_orig_tampered = [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 187, 161, 157, 88, 0, 175, 116, 251, 116]
+        n = len(mesecc_orig)
+        k = len(message)
+        params = [n, k, 2, 0x187, 120]
+        res = detect_reedsolomon_parameters(message, mesecc_orig)
+        res2 = detect_reedsolomon_parameters(message, mesecc_orig_tampered)
+        assert ("Hamming distance 0 (0=perfect match):\ngen_nb=%i prim=%i(%s) fcr=%i" % (params[2], params[3], hex(params[3]), params[4])) in res
+        assert ("Hamming distance 1:\ngen_nb=%i prim=%i(%s) fcr=%i" % (params[2], params[3], hex(params[3]), params[4])) in res2
+        res3 = detect_reedsolomon_parameters(message, [-1]*len(mesecc_orig), [3])
+        assert "Parameters could not be automatically detected" in res3
+        self.assertRaises(ValueError, detect_reedsolomon_parameters, [257, 0, 0], [0, 0, 0], c_exp=8)
+        self.assertRaises(ValueError, detect_reedsolomon_parameters, [0, 0, 0], [257, 0, 0], c_exp=8)
+
+    def test_eccman_compute_ecc_params(self):
+        """ eccman: test ecc params computation """
+        class Hasher(object):
+            """ Dummy Hasher """
+            def __len__(self):
+                return 32
+        hasher = Hasher()
+        assert compute_ecc_params(255, 0.5, hasher) == {'ecc_size': 127, 'hash_size': 32, 'message_size': 128}
+        assert compute_ecc_params(255, 0.0, hasher) == {'ecc_size': 0, 'hash_size': 32, 'message_size': 255}
+        assert compute_ecc_params(255, 1.0, hasher) == {'ecc_size': 170, 'hash_size': 32, 'message_size': 85}
+        assert compute_ecc_params(255, 0.3, hasher) == {'ecc_size': 96, 'hash_size': 32, 'message_size': 159}
+        assert compute_ecc_params(255, 0.7, hasher) == {'ecc_size': 149, 'hash_size': 32, 'message_size': 106}
+        assert compute_ecc_params(255, 2.0, hasher) == {'ecc_size': 204, 'hash_size': 32, 'message_size': 51}
+        assert compute_ecc_params(255, 10.0, hasher) == {'ecc_size': 243, 'hash_size': 32, 'message_size': 12}
+        assert compute_ecc_params(140, 10.0, hasher) == {'ecc_size': 133, 'hash_size': 32, 'message_size': 7}
+
+    def test_eccman_codecs(self):
+        """ eccman: test ecc generation and decoding """
+        expected = [
+            [206, 234, 144, 153, 141, 196, 170, 96, 62],
+            [206, 234, 144, 153, 141, 196, 170, 96, 62],
+            [206, 234, 144, 153, 141, 196, 170, 96, 62],
+            [187, 161, 157, 88, 92, 175, 116, 251, 116]
+        ]
+        message = b("hello world")
+        message_eras = b("h\x00ll\x00 world")
+        message_noise = b("h\x00ll\x00 worla")
+        n = 20
+        k = 11
+        for i in range(1,5):
+            eccman = ECCMan(n, k, algo=i)
+            ecc = bytearray(b(eccman.encode(message)))
+            assert list(ecc) == expected[i-1]
+            assert b(eccman.decode(message_eras, ecc)[0]) == message
+            assert b(eccman.decode(message_eras, ecc, enable_erasures=True)[0]) == message
+            assert b(eccman.decode(message_eras, ecc, enable_erasures=True, only_erasures=True)[0]) == message
+            #eccman.decode(message_noise, ecc, enable_erasures=True, only_erasures=True)[0]
+            assert eccman.check(message, ecc)
+            assert not eccman.check(message_eras, ecc)
+            assert "Reed-Solomon with polynomials in Galois field of characteristic" in eccman.description()
+        # Unknown algorithm test
+        self.assertRaises(Exception, ECCMan, n, k, algo=-1)
+        eccman = ECCMan(n, k, algo=1)
+        eccman.algo = -1
+        assert "No description for this ECC algorithm." in eccman.description()
+
+    def test_eccman_pad(self):
+        """ eccman: test ecc padding """
+        message = b("hello world")
+        ecc = b(''.join([chr(x) for x in [206, 234, 144, 153, 141, 196, 170, 96, 62]]))
+        # Oversize parameters compared to the message and ecc
+        n = 22 # should be 20
+        k = 13 # should be 11, but we add +2, which bytes we will pad onto the ecc and the decoding should still work!
+        eccman = ECCMan(n, k, algo=3)
+        # Test left padding (the input message)
+        pmessage = eccman.pad(message)
+        assert pmessage == [b('\x00\x00hello world'), b('\x00\x00')] # format: [padded_message, padonly]
+        assert eccman.check(pmessage[0], ecc)
+        # Test right padding (the ecc block)
+        pecc = eccman.rpad(ecc, 11)
+        assert pecc == [b('\xce\xea\x90\x99\x8d\xc4\xaa`>\x00\x00'), b('\x00\x00')]
+        assert eccman.check(message, pecc[0])
+        # Test decoding with both padding!
+        assert eccman.check(pmessage[0], pecc[0])
+
+    def test_eccman_lpad_decoding(self):
+        """ eccman: test ecc decoding when message needs left padding """
+        message = b("hello world")
+        ecc = b(''.join([chr(x) for x in [206, 234, 144, 153, 141, 196, 170, 96, 62]]))
+        message_eras = b("h\x00ll\x00 world")
+        # Oversize parameters compared to the message and ecc
+        n = 22 # should be 20
+        k = 13 # should be 11, but we add +2, which bytes we will pad onto the ecc and the decoding should still work!
+        eccman = ECCMan(n, k, algo=3)
+        # Test decoding with erasure when the message needs to be padded
+        assert eccman.decode(message_eras, ecc, enable_erasures=True)
+
+    def test_eccman_rpad_decoding(self):
+        """ eccman: test ecc decoding when right padding """
+        message = b("hello world")
+        ecc = b(''.join([chr(x) for x in [206, 234, 144, 153, 141, 196, 170, 96, 62]]))
+        message_eras = b("h\x00ll\x00 world")
+        # Oversize parameters compared to the message and ecc
+        n = 20 # should be 20
+        k = 11 # should be 11, but we add +2, which bytes we will pad onto the ecc and the decoding should still work!
+        eccman = ECCMan(n, k, algo=3)
+        # Test decoding with erasure when the message needs to be padded
+        assert eccman.decode(message_eras, ecc, enable_erasures=True)
+        assert eccman.decode(message_eras, ecc[:-2])
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_hasher.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_hasher.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-from __future__ import print_function
-
-import unittest
-import sys
-import os
-import shutil
-
-from .aux_tests import path_sample_files, create_dir_if_not_exist
-
-from ..lib.hasher import Hasher
-
-class TestHasher(unittest.TestCase):
-    def setup_module(self):
-        """ Initialize the tests by emptying the out directory """
-        outfolder = path_sample_files('output')
-        shutil.rmtree(outfolder, ignore_errors=True)
-        create_dir_if_not_exist(outfolder)
-
-    def test_hasher(self):
-        """ hasher: test hashes """
-        instring = "Lorem ipsum and some more stuff\nThe answer to the question of life, universe and everything is... 42."
-        # Put all hashing algo results here (format: "algo_name": [length, result_for_instring])
-        algo_params = {"md5": [32, b'173efbe0280ce506ddbfbfc9aeb44a1a'],
-                       "shortmd5": [8, b'MTczZWZi'],
-                       "shortsha256": [8, b'NjgzMjRk'],
-                       "minimd5":  [4, b'MTcz'],
-                       "minisha256": [4, b'Njgz'],
-                       "none": [0, ''],
-                      }
-        # For each hashing algo, produce a hash and check the length and hash
-        for algo in Hasher.known_algo:
-            h = Hasher(algo)
-            shash = h.hash(instring)
-            #print(algo+": "+shash) # debug
-            assert len(shash) == algo_params[algo][0]
-            assert shash == algo_params[algo][1]
-        # Check that unknown algorithms raise an exception
-        self.assertRaises(NameError, Hasher, "unknown_algo")
-        # Second check of unknown algo raising exception
-        h = Hasher()
-        h.algo = "unknown_algo"
-        self.assertRaises(NameError, h.hash, "abcdef")
+from __future__ import print_function
+
+import unittest
+import sys
+import os
+import shutil
+
+from .aux_tests import path_sample_files, create_dir_if_not_exist
+
+from ..lib.hasher import Hasher
+
+class TestHasher(unittest.TestCase):
+    def setup_module(self):
+        """ Initialize the tests by emptying the out directory """
+        outfolder = path_sample_files('output')
+        shutil.rmtree(outfolder, ignore_errors=True)
+        create_dir_if_not_exist(outfolder)
+
+    def test_hasher(self):
+        """ hasher: test hashes """
+        instring = "Lorem ipsum and some more stuff\nThe answer to the question of life, universe and everything is... 42."
+        # Put all hashing algo results here (format: "algo_name": [length, result_for_instring])
+        algo_params = {"md5": [32, b'173efbe0280ce506ddbfbfc9aeb44a1a'],
+                       "shortmd5": [8, b'MTczZWZi'],
+                       "shortsha256": [8, b'NjgzMjRk'],
+                       "minimd5":  [4, b'MTcz'],
+                       "minisha256": [4, b'Njgz'],
+                       "none": [0, ''],
+                      }
+        # For each hashing algo, produce a hash and check the length and hash
+        for algo in Hasher.known_algo:
+            h = Hasher(algo)
+            shash = h.hash(instring)
+            #print(algo+": "+shash) # debug
+            assert len(shash) == algo_params[algo][0]
+            assert shash == algo_params[algo][1]
+        # Check that unknown algorithms raise an exception
+        self.assertRaises(NameError, Hasher, "unknown_algo")
+        # Second check of unknown algo raising exception
+        h = Hasher()
+        h.algo = "unknown_algo"
+        self.assertRaises(NameError, h.hash, "abcdef")
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_header_ecc.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_header_ecc.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,144 +1,144 @@
-from __future__ import print_function
-
-import sys
-import os
-import itertools
-import hashlib
-
-import shutil
-
-from .. import header_ecc as hecc
-from ..lib.aux_funcs import get_next_entry
-from ..lib.eccman import compute_ecc_params, ECCMan
-from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker, dummy_ecc_file_gen
-
-from ..lib._compat import b
-
-from io import BytesIO
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-
-def test_one_file():
-    """ hecc: test creation and verification of database for one file """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = path_sample_files('output', 'hecc_file.db')
-    fileout = path_sample_files('output', 'tuxsmall.jpg')
-    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
-    fileres = path_sample_files('results', 'test_header_ecc_test_one_file.db')
-    # Generate an ecc file
-    assert hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
-    # Check that generated ecc file is correct
-    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
-    # Check that the ecc file correctly validates the correct files
-    assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
-
-def test_one_file_tamper():
-    """ hecc: test file repair """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = path_sample_files('output', 'hecc_tamper.db')
-    fileout = path_sample_files('output', 'tuxsmall.jpg')
-    fileout2 = path_sample_files('output', 'repaired/tuxsmall.jpg')
-    fileout2_dir = path_sample_files('output', 'repaired')
-    fileres = path_sample_files('results', 'test_header_ecc_test_one_file_tamper.db')
-    create_dir_if_not_exist(fileout2_dir)
-    # Generate an ecc file
-    assert hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
-    # Tamper the file
-    shutil.copyfile(filein, fileout) # Copy it to avoid tampering the original
-    tamper_file(fileout, 4, r'abcde')
-    # Repair the file
-    assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (fileout, filedb, fileout2_dir)) == 0
-    # Check that the file was completely repaired
-    assert check_eq_files(filein, fileout2)
-
-def test_dir():
-    """ hecc: test creation and verification of database for a full directory """
-    filein = path_sample_files('input', )
-    filedb = path_sample_files('output', 'hecc_dir.db')
-    fileout = path_sample_files('output', )
-    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
-    fileres = path_sample_files('results', 'test_header_ecc_test_dir.db')
-    # Generate an ecc file
-    assert hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
-    # Check that generated ecc file is correct
-    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
-    # Check that the ecc file correctly validates the correct files
-    assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
-
-def test_algo():
-    """ hecc: test algorithms equivalence """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = [path_sample_files('output', 'hecc_algo1.db'),
-                path_sample_files('output', 'hecc_algo2.db'),
-                path_sample_files('output', 'hecc_algo3.db'),
-                ]
-    fileres = path_sample_files('results', 'test_header_ecc_test_algo.db')
-    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
-    # For each algorithm
-    for i in range(len(filedb)):
-        # Generate an ecc file
-        assert hecc.main('-i "%s" -d "%s" --ecc_algo=%i -g -f --silent' % (filein, filedb[i], i+1)) == 0
-        # Check file with this ecc algo
-        assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=%i -c --silent' % (filein, filedb[i], fileout_rec, i+1)) == 0
-    for i in range(1, len(filedb)):
-        # Check that generated ecc file is correct
-        startpos1 = next(find_next_entry(filedb[0], get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-        startpos2 = next(find_next_entry(filedb[i], get_marker(type=1)))
-        assert check_eq_files(filedb[0], filedb[i], startpos1=startpos1, startpos2=startpos2)
-    # Check against expected ecc file
-    startpos1 = next(find_next_entry(filedb[0], get_marker(type=1)))
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb[0], fileres, startpos1=startpos1, startpos2=startpos2)
-
-def test_entry_fields():
-    """ hecc: test internal: entry_fields() """
-    ecc = dummy_ecc_file_gen(3)
-    eccf = BytesIO(ecc)
-    ecc_entry = get_next_entry(eccf, get_marker(1), only_coord=False)
-    assert hecc.entry_fields(ecc_entry, field_delim=get_marker(2)) == {'ecc_field': b'hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_', 'filesize_ecc': b'filesize1_ecc', 'relfilepath_ecc': b'relfilepath1_ecc', 'relfilepath': b'file1.ext', 'filesize': b'filesize1'}
-    ecc_entry = get_next_entry(eccf, get_marker(1), only_coord=False)
-    assert hecc.entry_fields(ecc_entry, field_delim=get_marker(2)) == {'ecc_field': b'hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_', 'filesize_ecc': b'filesize2_ecc', 'relfilepath_ecc': b'relfilepath2_ecc', 'relfilepath': b'file2.ext', 'filesize': b'filesize2'}
-
-def test_entry_assemble():
-    """ hecc: test internal: entry_assemble() """
-    class Hasher(object):
-        """ Dummy Hasher """
-        def __len__(self):
-            return 32
-    tempfile = path_sample_files('output', 'hecc_entry_assemble.txt')
-    with open(tempfile, 'wb') as tfile:
-        tfile.write(b("Lorem ipsum\nAnd stuff and stuff and stuff\n"*20))
-    ecc = dummy_ecc_file_gen(3)
-    eccf = BytesIO(ecc)
-    ecc_entry = get_next_entry(eccf, get_marker(1), only_coord=False)
-    entry_fields = hecc.entry_fields(ecc_entry, field_delim=get_marker(2))
-    ecc_params = compute_ecc_params(255, 0.5, Hasher())
-    out = hecc.entry_assemble(entry_fields, ecc_params, 10, tempfile, fileheader=None)
-    assert out == [{'ecc': b'sh-ecc-entry_', 'message': b'Lorem ipsu', 'hash': b'hash-ecc-entry_hash-ecc-entry_ha'}]
-    # TODO: check that several blocks can be assembled, currently we only check one block
-
-def test_compute_ecc_hash():
-    """ hecc: test internal: compute_ecc_hash() """
-    class Hasher(object):
-        """ Dummy Hasher """
-        def hash(self, mes):
-            return "dummyhsh"
-        def __len__(self):
-            return 8
-    n = 20
-    k = 11
-    instring = "hello world!"*20
-    header_size = 1024
-    eccman = ECCMan(n, k, algo=3)
-    out1 = hecc.compute_ecc_hash(eccman, Hasher(), instring[:header_size], 255, 0.5, message_size=None, as_string=False)
-    assert out1 == [[b'dummyhsh', b'\x9b\x18\xeb\xc9z\x01c\xf2\x07'], [b'dummyhsh', b'\xa2Q\xc0Y\xae\xc3b\xd5\x81']]
-    out2 = hecc.compute_ecc_hash(eccman, Hasher(), instring[:header_size], 255, 0.5, message_size=None, as_string=True)
-    assert out2 == [b('dummyhsh\x9b\x18\xeb\xc9z\x01c\xf2\x07'), b('dummyhsh\xa2Q\xc0Y\xae\xc3b\xd5\x81')]
+from __future__ import print_function
+
+import sys
+import os
+import itertools
+import hashlib
+
+import shutil
+
+from .. import header_ecc as hecc
+from ..lib.aux_funcs import get_next_entry
+from ..lib.eccman import compute_ecc_params, ECCMan
+from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker, dummy_ecc_file_gen
+
+from ..lib._compat import b
+
+from io import BytesIO
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+
+def test_one_file():
+    """ hecc: test creation and verification of database for one file """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = path_sample_files('output', 'hecc_file.db')
+    fileout = path_sample_files('output', 'tuxsmall.jpg')
+    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
+    fileres = path_sample_files('results', 'test_header_ecc_test_one_file.db')
+    # Generate an ecc file
+    assert hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
+    # Check that generated ecc file is correct
+    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
+    # Check that the ecc file correctly validates the correct files
+    assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
+
+def test_one_file_tamper():
+    """ hecc: test file repair """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = path_sample_files('output', 'hecc_tamper.db')
+    fileout = path_sample_files('output', 'tuxsmall.jpg')
+    fileout2 = path_sample_files('output', 'repaired/tuxsmall.jpg')
+    fileout2_dir = path_sample_files('output', 'repaired')
+    fileres = path_sample_files('results', 'test_header_ecc_test_one_file_tamper.db')
+    create_dir_if_not_exist(fileout2_dir)
+    # Generate an ecc file
+    assert hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
+    # Tamper the file
+    shutil.copyfile(filein, fileout) # Copy it to avoid tampering the original
+    tamper_file(fileout, 4, r'abcde')
+    # Repair the file
+    assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (fileout, filedb, fileout2_dir)) == 0
+    # Check that the file was completely repaired
+    assert check_eq_files(filein, fileout2)
+
+def test_dir():
+    """ hecc: test creation and verification of database for a full directory """
+    filein = path_sample_files('input', )
+    filedb = path_sample_files('output', 'hecc_dir.db')
+    fileout = path_sample_files('output', )
+    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
+    fileres = path_sample_files('results', 'test_header_ecc_test_dir.db')
+    # Generate an ecc file
+    assert hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
+    # Check that generated ecc file is correct
+    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
+    # Check that the ecc file correctly validates the correct files
+    assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
+
+def test_algo():
+    """ hecc: test algorithms equivalence """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = [path_sample_files('output', 'hecc_algo1.db'),
+                path_sample_files('output', 'hecc_algo2.db'),
+                path_sample_files('output', 'hecc_algo3.db'),
+                ]
+    fileres = path_sample_files('results', 'test_header_ecc_test_algo.db')
+    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
+    # For each algorithm
+    for i in range(len(filedb)):
+        # Generate an ecc file
+        assert hecc.main('-i "%s" -d "%s" --ecc_algo=%i -g -f --silent' % (filein, filedb[i], i+1)) == 0
+        # Check file with this ecc algo
+        assert hecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=%i -c --silent' % (filein, filedb[i], fileout_rec, i+1)) == 0
+    for i in range(1, len(filedb)):
+        # Check that generated ecc file is correct
+        startpos1 = next(find_next_entry(filedb[0], get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+        startpos2 = next(find_next_entry(filedb[i], get_marker(type=1)))
+        assert check_eq_files(filedb[0], filedb[i], startpos1=startpos1, startpos2=startpos2)
+    # Check against expected ecc file
+    startpos1 = next(find_next_entry(filedb[0], get_marker(type=1)))
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb[0], fileres, startpos1=startpos1, startpos2=startpos2)
+
+def test_entry_fields():
+    """ hecc: test internal: entry_fields() """
+    ecc = dummy_ecc_file_gen(3)
+    eccf = BytesIO(ecc)
+    ecc_entry = get_next_entry(eccf, get_marker(1), only_coord=False)
+    assert hecc.entry_fields(ecc_entry, field_delim=get_marker(2)) == {'ecc_field': b'hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_', 'filesize_ecc': b'filesize1_ecc', 'relfilepath_ecc': b'relfilepath1_ecc', 'relfilepath': b'file1.ext', 'filesize': b'filesize1'}
+    ecc_entry = get_next_entry(eccf, get_marker(1), only_coord=False)
+    assert hecc.entry_fields(ecc_entry, field_delim=get_marker(2)) == {'ecc_field': b'hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry_', 'filesize_ecc': b'filesize2_ecc', 'relfilepath_ecc': b'relfilepath2_ecc', 'relfilepath': b'file2.ext', 'filesize': b'filesize2'}
+
+def test_entry_assemble():
+    """ hecc: test internal: entry_assemble() """
+    class Hasher(object):
+        """ Dummy Hasher """
+        def __len__(self):
+            return 32
+    tempfile = path_sample_files('output', 'hecc_entry_assemble.txt')
+    with open(tempfile, 'wb') as tfile:
+        tfile.write(b("Lorem ipsum\nAnd stuff and stuff and stuff\n"*20))
+    ecc = dummy_ecc_file_gen(3)
+    eccf = BytesIO(ecc)
+    ecc_entry = get_next_entry(eccf, get_marker(1), only_coord=False)
+    entry_fields = hecc.entry_fields(ecc_entry, field_delim=get_marker(2))
+    ecc_params = compute_ecc_params(255, 0.5, Hasher())
+    out = hecc.entry_assemble(entry_fields, ecc_params, 10, tempfile, fileheader=None)
+    assert out == [{'ecc': b'sh-ecc-entry_', 'message': b'Lorem ipsu', 'hash': b'hash-ecc-entry_hash-ecc-entry_ha'}]
+    # TODO: check that several blocks can be assembled, currently we only check one block
+
+def test_compute_ecc_hash():
+    """ hecc: test internal: compute_ecc_hash() """
+    class Hasher(object):
+        """ Dummy Hasher """
+        def hash(self, mes):
+            return "dummyhsh"
+        def __len__(self):
+            return 8
+    n = 20
+    k = 11
+    instring = "hello world!"*20
+    header_size = 1024
+    eccman = ECCMan(n, k, algo=3)
+    out1 = hecc.compute_ecc_hash(eccman, Hasher(), instring[:header_size], 255, 0.5, message_size=None, as_string=False)
+    assert out1 == [[b'dummyhsh', b'\x9b\x18\xeb\xc9z\x01c\xf2\x07'], [b'dummyhsh', b'\xa2Q\xc0Y\xae\xc3b\xd5\x81']]
+    out2 = hecc.compute_ecc_hash(eccman, Hasher(), instring[:header_size], 255, 0.5, message_size=None, as_string=True)
+    assert out2 == [b('dummyhsh\x9b\x18\xeb\xc9z\x01c\xf2\x07'), b('dummyhsh\xa2Q\xc0Y\xae\xc3b\xd5\x81')]
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_repair_ecc.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_repair_ecc.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,165 +1,165 @@
-from __future__ import print_function
-
-import sys
-import os
-import itertools
-import hashlib
-
-import shutil
-
-from .. import repair_ecc as recc
-from .. import header_ecc as hecc
-from .. import structural_adaptive_ecc as saecc
-from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker
-
-def get_db():
-    return [path_sample_files('output', 'recc_file.db'), path_sample_files('output', 'recc_file.db_bak')]
-
-def get_db_idx():
-    return [path_sample_files('output', 'recc_file.db.idx'), path_sample_files('output', 'recc_file.db.idx_bak')]
-
-def get_db_sa():
-    return [path_sample_files('output', 'recc_file_sa.db'), path_sample_files('output', 'recc_file_sa.db_bak')]
-
-def get_db_sa_idx():
-    return [path_sample_files('output', 'recc_file_sa.db.idx'), path_sample_files('output', 'recc_file_sa.db.idx_bak')]
-
-def restore_files(type):
-    """ Restore the backup files to clean before the test """
-    if type == 'hecc':
-        filedb, filedb_bak = get_db()
-    elif type == 'hecc_idx':
-        filedb, filedb_bak = get_db_idx()
-    elif type == 'saecc':
-        filedb, filedb_bak = get_db_sa()
-    elif type == 'saecc_idx':
-        filedb, filedb_bak = get_db_sa_idx()
-    os.remove(filedb)
-    shutil.copyfile(filedb_bak, filedb)
-    return 0
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-    # Generate an header_ecc generated ecc file for the repair tests to use
-    filein = path_sample_files('input')
-    filedb, filedb_bak = get_db()
-    filedb_idx, filedb_idx_bak = get_db_idx()
-    hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb))
-    shutil.copyfile(filedb, filedb_bak) # keep a backup, we will reuse it for each test
-    shutil.copyfile(filedb_idx, filedb_idx_bak)
-    # Do the same with structural_adaptive_ecc
-    filedb_sa, filedb_sa_bak = get_db_sa()
-    filedb_sa_idx, filedb_sa_idx_bak = get_db_sa_idx()
-    saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb_sa))
-    shutil.copyfile(filedb_sa, filedb_sa_bak) # keep a backup, we will reuse it for each test
-    shutil.copyfile(filedb_sa_idx, filedb_sa_idx_bak)
-
-def test_check():
-    """ recc: check db and index files are the same as expected """
-    # this also helps to check that restore_files() is working correctly since they are critical for other tests
-    filedb, filedb_bak = get_db()
-    filedb_sa, filedb_sa_bak = get_db_sa()
-    #filedb_idx, filedb_idx_bak = get_db_idx()
-    fileres = path_sample_files('results', 'test_repair_ecc_check.db')
-    fileres_sa = path_sample_files('results', 'test_repair_ecc_sa_check.db')
-    #fileres_idx = path_sample_files('results', 'test_repair_ecc_check.db.idx')
-    # Recopy the original untampered files
-    restore_files('hecc')
-    restore_files('saecc')
-    #restore_files('hecc_idx')
-    # Check that generated files are correct (header_ecc generated)
-    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
-    # assert check_eq_files(filedb_idx, fileres_idx) # cannot check the index file because of the possibly differing comments in the header (this will offset the position of every markers, and thus the index file will be different)
-    # Check that generated files are correct (structural_adaptive_ecc generated)
-    startpos1 = next(find_next_entry(filedb_sa, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    startpos2 = next(find_next_entry(fileres_sa, get_marker(type=1)))
-    assert check_eq_files(filedb_sa, fileres_sa, startpos1=startpos1, startpos2=startpos2)
-
-def test_repair_by_index():
-    """ recc: tamper ecc file and repair by index file """
-    filedb, filedb_bak = get_db()
-    filedb_idx, filedb_idx_bak = get_db_idx()
-    fileout = path_sample_files('output', 'recc_file_repaired_index.db')
-    marker1 = get_marker(type=1)
-    marker2 = get_marker(type=2)
-    restore_files('hecc')
-    restore_files('hecc_idx')
-    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
-    startpos1 = next(find_next_entry(filedb, marker1))
-    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
-    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
-    tamper_file(filedb, startpos1, "a"*len(marker1))
-    tamper_file(filedb, startpos2, "a"*len(marker1))
-    tamper_file(filedb, startpos3, "a"*len(marker2))
-    # Repair ecc file using index file
-    assert recc.main('-i "%s" --index "%s" -o "%s" -t 0.0 -f --silent' % (filedb, filedb_idx, fileout)) == 0
-    assert check_eq_files(filedb_bak, fileout)
-
-def test_repair_by_hamming():
-    """ recc: tamper ecc file and repair by hamming distance """
-    filedb, filedb_bak = get_db()
-    fileout = path_sample_files('output', 'recc_file_repaired.db')
-    marker1 = get_marker(type=1)
-    marker2 = get_marker(type=2)
-    restore_files('hecc')
-    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
-    startpos1 = next(find_next_entry(filedb, marker1))
-    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
-    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
-    tamper_file(filedb, startpos1, "a"*int(len(marker1)*0.3))
-    tamper_file(filedb, startpos2, "a"*int(len(marker1)*0.3))
-    tamper_file(filedb, startpos3, "a"*int(len(marker2)*0.3))
-    # Repair ecc file by hamming similarity
-    assert recc.main('-i "%s" -o "%s" -t 0.3 -f --silent' % (filedb, fileout)) == 0
-    assert check_eq_files(filedb_bak, fileout)
-
-def test_tamper_index():
-    """ recc: tamper index file and see if it can repair itself (hecc) """
-    filedb, filedb_bak = get_db()
-    filedb_idx, filedb_idx_bak = get_db_idx()
-    fileout = path_sample_files('output', 'recc_file_repaired_index.db')
-    marker1 = get_marker(type=1)
-    marker2 = get_marker(type=2)
-    restore_files('hecc')
-    restore_files('hecc_idx')
-    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
-    startpos1 = next(find_next_entry(filedb, marker1))
-    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
-    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
-    tamper_file(filedb, startpos1, "a"*len(marker1))
-    tamper_file(filedb, startpos2, "a"*len(marker1))
-    tamper_file(filedb, startpos3, "a"*len(marker2))
-    # Tamper index file
-    tamper_file(filedb_idx, 0, "abcd")
-    tamper_file(filedb_idx, 9, "abcd")
-    tamper_file(filedb_idx, 27, "abcd")
-    assert recc.main('-i "%s" --index "%s" -o "%s" -t 0.0 -f --silent' % (filedb, filedb_idx, fileout)) == 0
-    assert check_eq_files(filedb_bak, fileout)
-
-def test_tamper_index_saecc():
-    """ recc: tamper index file and see if it can repair itself (saecc) """
-    filedb, filedb_bak = get_db_sa()
-    filedb_idx, filedb_idx_bak = get_db_sa_idx()
-    fileout = path_sample_files('output', 'recc_file_sa_repaired.db')
-    marker1 = get_marker(type=1)
-    marker2 = get_marker(type=2)
-    restore_files('saecc')
-    restore_files('saecc_idx')
-    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
-    startpos1 = next(find_next_entry(filedb, marker1))
-    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
-    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
-    tamper_file(filedb, startpos1, "a"*len(marker1))
-    tamper_file(filedb, startpos2, "a"*len(marker1))
-    tamper_file(filedb, startpos3, "a"*len(marker2))
-    # Tamper index file
-    tamper_file(filedb_idx, 0, "abcd")
-    tamper_file(filedb_idx, 9, "abcd")
-    tamper_file(filedb_idx, 27, "abcd")
-    assert recc.main('-i "%s" --index "%s" -o "%s" -t 0.0 -f --silent' % (filedb, filedb_idx, fileout)) == 0
-    assert check_eq_files(filedb_bak, fileout)
+from __future__ import print_function
+
+import sys
+import os
+import itertools
+import hashlib
+
+import shutil
+
+from .. import repair_ecc as recc
+from .. import header_ecc as hecc
+from .. import structural_adaptive_ecc as saecc
+from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker
+
+def get_db():
+    return [path_sample_files('output', 'recc_file.db'), path_sample_files('output', 'recc_file.db_bak')]
+
+def get_db_idx():
+    return [path_sample_files('output', 'recc_file.db.idx'), path_sample_files('output', 'recc_file.db.idx_bak')]
+
+def get_db_sa():
+    return [path_sample_files('output', 'recc_file_sa.db'), path_sample_files('output', 'recc_file_sa.db_bak')]
+
+def get_db_sa_idx():
+    return [path_sample_files('output', 'recc_file_sa.db.idx'), path_sample_files('output', 'recc_file_sa.db.idx_bak')]
+
+def restore_files(type):
+    """ Restore the backup files to clean before the test """
+    if type == 'hecc':
+        filedb, filedb_bak = get_db()
+    elif type == 'hecc_idx':
+        filedb, filedb_bak = get_db_idx()
+    elif type == 'saecc':
+        filedb, filedb_bak = get_db_sa()
+    elif type == 'saecc_idx':
+        filedb, filedb_bak = get_db_sa_idx()
+    os.remove(filedb)
+    shutil.copyfile(filedb_bak, filedb)
+    return 0
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+    # Generate an header_ecc generated ecc file for the repair tests to use
+    filein = path_sample_files('input')
+    filedb, filedb_bak = get_db()
+    filedb_idx, filedb_idx_bak = get_db_idx()
+    hecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb))
+    shutil.copyfile(filedb, filedb_bak) # keep a backup, we will reuse it for each test
+    shutil.copyfile(filedb_idx, filedb_idx_bak)
+    # Do the same with structural_adaptive_ecc
+    filedb_sa, filedb_sa_bak = get_db_sa()
+    filedb_sa_idx, filedb_sa_idx_bak = get_db_sa_idx()
+    saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb_sa))
+    shutil.copyfile(filedb_sa, filedb_sa_bak) # keep a backup, we will reuse it for each test
+    shutil.copyfile(filedb_sa_idx, filedb_sa_idx_bak)
+
+def test_check():
+    """ recc: check db and index files are the same as expected """
+    # this also helps to check that restore_files() is working correctly since they are critical for other tests
+    filedb, filedb_bak = get_db()
+    filedb_sa, filedb_sa_bak = get_db_sa()
+    #filedb_idx, filedb_idx_bak = get_db_idx()
+    fileres = path_sample_files('results', 'test_repair_ecc_check.db')
+    fileres_sa = path_sample_files('results', 'test_repair_ecc_sa_check.db')
+    #fileres_idx = path_sample_files('results', 'test_repair_ecc_check.db.idx')
+    # Recopy the original untampered files
+    restore_files('hecc')
+    restore_files('saecc')
+    #restore_files('hecc_idx')
+    # Check that generated files are correct (header_ecc generated)
+    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
+    # assert check_eq_files(filedb_idx, fileres_idx) # cannot check the index file because of the possibly differing comments in the header (this will offset the position of every markers, and thus the index file will be different)
+    # Check that generated files are correct (structural_adaptive_ecc generated)
+    startpos1 = next(find_next_entry(filedb_sa, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    startpos2 = next(find_next_entry(fileres_sa, get_marker(type=1)))
+    assert check_eq_files(filedb_sa, fileres_sa, startpos1=startpos1, startpos2=startpos2)
+
+def test_repair_by_index():
+    """ recc: tamper ecc file and repair by index file """
+    filedb, filedb_bak = get_db()
+    filedb_idx, filedb_idx_bak = get_db_idx()
+    fileout = path_sample_files('output', 'recc_file_repaired_index.db')
+    marker1 = get_marker(type=1)
+    marker2 = get_marker(type=2)
+    restore_files('hecc')
+    restore_files('hecc_idx')
+    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
+    startpos1 = next(find_next_entry(filedb, marker1))
+    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
+    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
+    tamper_file(filedb, startpos1, "a"*len(marker1))
+    tamper_file(filedb, startpos2, "a"*len(marker1))
+    tamper_file(filedb, startpos3, "a"*len(marker2))
+    # Repair ecc file using index file
+    assert recc.main('-i "%s" --index "%s" -o "%s" -t 0.0 -f --silent' % (filedb, filedb_idx, fileout)) == 0
+    assert check_eq_files(filedb_bak, fileout)
+
+def test_repair_by_hamming():
+    """ recc: tamper ecc file and repair by hamming distance """
+    filedb, filedb_bak = get_db()
+    fileout = path_sample_files('output', 'recc_file_repaired.db')
+    marker1 = get_marker(type=1)
+    marker2 = get_marker(type=2)
+    restore_files('hecc')
+    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
+    startpos1 = next(find_next_entry(filedb, marker1))
+    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
+    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
+    tamper_file(filedb, startpos1, "a"*int(len(marker1)*0.3))
+    tamper_file(filedb, startpos2, "a"*int(len(marker1)*0.3))
+    tamper_file(filedb, startpos3, "a"*int(len(marker2)*0.3))
+    # Repair ecc file by hamming similarity
+    assert recc.main('-i "%s" -o "%s" -t 0.3 -f --silent' % (filedb, fileout)) == 0
+    assert check_eq_files(filedb_bak, fileout)
+
+def test_tamper_index():
+    """ recc: tamper index file and see if it can repair itself (hecc) """
+    filedb, filedb_bak = get_db()
+    filedb_idx, filedb_idx_bak = get_db_idx()
+    fileout = path_sample_files('output', 'recc_file_repaired_index.db')
+    marker1 = get_marker(type=1)
+    marker2 = get_marker(type=2)
+    restore_files('hecc')
+    restore_files('hecc_idx')
+    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
+    startpos1 = next(find_next_entry(filedb, marker1))
+    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
+    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
+    tamper_file(filedb, startpos1, "a"*len(marker1))
+    tamper_file(filedb, startpos2, "a"*len(marker1))
+    tamper_file(filedb, startpos3, "a"*len(marker2))
+    # Tamper index file
+    tamper_file(filedb_idx, 0, "abcd")
+    tamper_file(filedb_idx, 9, "abcd")
+    tamper_file(filedb_idx, 27, "abcd")
+    assert recc.main('-i "%s" --index "%s" -o "%s" -t 0.0 -f --silent' % (filedb, filedb_idx, fileout)) == 0
+    assert check_eq_files(filedb_bak, fileout)
+
+def test_tamper_index_saecc():
+    """ recc: tamper index file and see if it can repair itself (saecc) """
+    filedb, filedb_bak = get_db_sa()
+    filedb_idx, filedb_idx_bak = get_db_sa_idx()
+    fileout = path_sample_files('output', 'recc_file_sa_repaired.db')
+    marker1 = get_marker(type=1)
+    marker2 = get_marker(type=2)
+    restore_files('saecc')
+    restore_files('saecc_idx')
+    # Completely overwrite a few markers (hence they cannot be recovered by hamming)
+    startpos1 = next(find_next_entry(filedb, marker1))
+    startpos2 = next(find_next_entry(filedb, marker1, startpos1+len(marker1)))
+    startpos3 = next(find_next_entry(filedb, marker2, startpos2+len(marker1)))
+    tamper_file(filedb, startpos1, "a"*len(marker1))
+    tamper_file(filedb, startpos2, "a"*len(marker1))
+    tamper_file(filedb, startpos3, "a"*len(marker2))
+    # Tamper index file
+    tamper_file(filedb_idx, 0, "abcd")
+    tamper_file(filedb_idx, 9, "abcd")
+    tamper_file(filedb_idx, 27, "abcd")
+    assert recc.main('-i "%s" --index "%s" -o "%s" -t 0.0 -f --silent' % (filedb, filedb_idx, fileout)) == 0
+    assert check_eq_files(filedb_bak, fileout)
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_replication_repair.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_replication_repair.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,290 +1,290 @@
-from __future__ import print_function
-
-import sys
-import os
-
-import shutil
-
-from ..lib._compat import _range, b, _StringIO, _ord
-
-from io import BytesIO
-
-from .. import replication_repair as rep
-from .. import rfigc
-from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker
-
-
-def change_letter(s, index, new_char):
-    l = bytearray(s)
-    l[index] = _ord(new_char)
-    return l
-
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-
-def test_relpath_posix():
-    """ repli: test internal: relpath_posix()"""
-    recwalk_result = [r'/test/some/path', r'relative/path/file.ext']
-    pardir = r'/test/some'
-    assert rep.relpath_posix(recwalk_result, pardir, False) == ('/test/some/path', ['path', 'relative', 'path', 'file.ext'])
-    
-    # Can only test the following on a Windows machine, because relpath_posix() uses os.path.relpath to remove the parent directory from the relative path, but Windows path delimiters aren't recognized on a Linux machine, and there's no way to specify the delimiter as an argument...
-    if os.name == 'nt':
-        recwalk_result = [r'C:\test\some\path', r'relative\path\file.ext']
-        pardir = r'C:\test\some'
-        assert rep.relpath_posix(recwalk_result, pardir, True) == ('C:\\test\\some\\path', ['path', 'relative', 'path', 'file.ext'])
-        recwalk_result = [r'/test/some/path', r'relative\path\file.ext']
-        pardir = r'/test/some'
-        assert rep.relpath_posix(recwalk_result, pardir, True) == ('/test/some/path', ['path', 'relative', 'path', 'file.ext'])
-
-def test_sort_dict_of_paths():
-    d = {0: ['testoo.TXT'], 1: ['testoo.TXT'], 2: ['testbb-more.TXT'], 3: ['sub', 'testsub.TXT']}
-    d_sort = rep.sort_dict_of_paths(d)
-    assert d_sort == [(2, ['', 'testbb-more.TXT']),
-                      (0, ['', 'testoo.TXT']),
-                      (1, ['', 'testoo.TXT']),
-                      (3, ['sub', 'testsub.TXT'])]
-
-def test_sort_group():
-    """ repli: test internal: sort_group()"""
-    # Generate an artificial tree, with some relative paths being the same across multiple folders,
-    # and some others in different tree depth (very import to check that it works OK!)
-    curfiles = {
-                0: ['relative', 'path', 'file.ext'],
-                1: ['relative', 'path', 'file.ext'],
-                2: ['relative', 'aaa', 'zzzz.ext'],
-                3: ['zzzz.ext'],
-                4: ['relative', 'zzzz.ext'],
-                5: ['relative', 'path', 'bbbb.ext'],
-               }
-    assert rep.sort_group(curfiles, return_only_first=False) == \
-              [
-               [(3, ['', '', 'zzzz.ext'])],
-               [(4, ['', 'relative', 'zzzz.ext'])],
-               [(2, ['relative', 'aaa', 'zzzz.ext'])],
-               [(5, ['relative', 'path', 'bbbb.ext'])],
-               [(0, ['relative', 'path', 'file.ext']), (1, ['relative', 'path', 'file.ext'])]
-              ]
-    assert rep.sort_group(curfiles, return_only_first=True) == [[(3, ['', '', 'zzzz.ext'])]]
-
-def test_majority_vote_byte_scan():
-    """ repli: test internal: majority_vote_byte_scan()"""
-    
-    def make_filehandles(fileslist):
-        return [BytesIO(f) for f in fileslist]
-    
-    # Necessary variables
-    relfilepath = 'relative/path/file.ext'
-    s = b("Hello world\n"*3)
-    
-    # 1- simple test, only a few characters are tampered/removed towards the end
-    files = []
-    files.append(change_letter(s[:-2], 2, 'X'))
-    files.append(change_letter(s[:-1], 4, 'W'))
-    files.append(s)
-    fileshandles = make_filehandles(files)
-    outfile = BytesIO()
-
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
-    assert errcode == 0
-    assert errmsg == None
-    outfile.seek(0)
-    out = outfile.read()
-    assert out == s
-
-    # 2- Test ambiguity, the first files should take precedence
-    files = []
-    files.append(s)
-    # Set an ambiguity on the 3rd character
-    files.append(change_letter(s, 2, 'X'))
-    files.append(change_letter(s, 2, 'W'))
-
-    # First run: the original string will take precedence since it's first in the list
-    outfile = BytesIO()
-    fileshandles = make_filehandles(files)
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert out == s
-
-    # Second run: put the original string last, now we will have the tampered string taking precedence
-    files.append(files.pop(0))
-    fileshandles = make_filehandles(files)
-    outfile = BytesIO()
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert b('HeXlo world') in out
-
-    # Do the same, the third (tampered) string will take precedence
-    files.append(files.pop(0))
-    fileshandles = make_filehandles(files)
-    outfile = BytesIO()
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert b('HeWlo world') in out
-
-    # Set a default char this time
-    files.append(files.pop(0))
-    fileshandles = make_filehandles(files)
-    outfile = BytesIO()
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=True)
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert b("He\x00lo world") in out
-
-    # Set a custom default char
-    files.append(files.pop(0))
-    fileshandles = make_filehandles(files)
-    outfile = BytesIO()
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null="A")
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert b("HeAlo world") in out
-    
-    # 3- Test ambiguity when some files are already ended, the first still opened file should take precedence
-    files = []
-    files.append(s)
-    # Shorten the second file
-    files.append(s[:-3])
-    # Set an ambiguity on the 2nd character to the end of file
-    files.append(change_letter(s, -2, 'X'))
-
-    # First run: the original string will take precedence since it's first in the list
-    outfile = BytesIO()
-    fileshandles = make_filehandles(files)
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert out == s
-
-    # Move the original string to the last, and see if the tampered string takes precedence (skipping the ended string)
-    files.append(files.pop(0))
-    fileshandles = make_filehandles(files)
-    outfile = BytesIO()
-    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
-    assert errcode == 1
-    outfile.seek(0)
-    out = outfile.read()
-    assert b('Hello worlX') in out
-
-def test_synchronize_files():
-    """ repli: test main() and synchronize_files()"""
-    
-    def quote_paths(inputpaths):
-        return ' '.join(["\"%s\"" % path for path in inputpaths])
-
-    # -- First make replications of test files
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filein2 = path_sample_files('input', 'testaa.txt')
-    filein3 = path_sample_files('input', 'sub/testsub.txt')
-    #filein4 = path_sample_files('input', 'tux.jpg')
-    # Create replications directories
-    inputpaths = [path_sample_files('output', 'repli/dir%i' % (i+1), True) for i in _range(4)]
-    outpath = path_sample_files('output', 'repli/dirout')
-    # Report file
-    report_file = path_sample_files('output', 'repli_report.csv')
-    # Generate the replications
-    fileout = [path_sample_files('output', 'repli/dir%i/tuxsmall.jpg' % (i+1)) for i in _range(4)]
-    fileout2 = [path_sample_files('output', 'repli/dir%i/testaa.txt' % (i+1)) for i in _range(4) if i != 2]
-    path_sample_files('output', 'repli/dir3/sub/', True)
-    fileout3 = path_sample_files('output', 'repli/dir3/sub/testsub.txt')
-    #fileout4 = [path_sample_files('output', 'repli/dir%i/tux.jpg' % (i+1)) for i in _range(4)]
-    for f in fileout:
-        shutil.copy2(filein, f)
-    for f in fileout2:
-        shutil.copy2(filein2, f)
-    shutil.copy2(filein3, fileout3)
-    # Backup of original files (to generate rfigc database)
-    dirorig = path_sample_files('output', 'repli/dirorig', True)
-    shutil.copy2(filein, dirorig)
-    shutil.copy2(filein2, dirorig)
-    shutil.copy2(filein3, path_sample_files('output', 'repli/dirorig/sub', True))
-    # Database file
-    filedb = path_sample_files('output', 'repli_rfigc_db.csv')
-
-    # Tamper the replications
-    # 1st file: tamper all copies but at different positions, to force merge by majority vote
-    for i, f in enumerate(fileout):
-        tamper_file(f, i, "A")
-    # 2nd file: tamper all copies but one, to test rfigc interface
-    for i, f in enumerate(fileout2):
-        if i < 2:
-            tamper_file(f, 0, "A")
-    # 3rd file: no tamper, there's just one file, check that the file is just copied
-    
-    # 1- Majority vote!
-    res1 = b("filepath|dir1|dir2|dir3|dir4|hash-correct|error_code|errors\ntestaa.txt|X|X|-|X|-|OK|-\ntuxsmall.jpg|X|X|X|X|-|OK|-\nsub/testsub.txt|-|-|O|-|-|OK|-\n")
-    errcode = rep.synchronize_files(inputpaths, outpath, report_file=report_file)
-    assert errcode == 0
-    with open(report_file, 'rb') as rfile:
-        rout = rfile.read()
-    assert res1 in rout
-    # Do the same test with the main() func, this should give exactly the same result
-    errcode = rep.main("-i %s -o \"%s\" -r \"%s\" -f --silent" % (quote_paths(inputpaths), outpath, report_file))
-    assert errcode == 0
-    with open(report_file, 'rb') as rfile:
-        rout = rfile.read()
-    assert res1 in rout
-
-    # 2- rfigc database interfacing!
-    res2 = b("filepath|dir1|dir2|dir3|dir4|hash-correct|error_code|errors\ntestaa.txt|X|X|-|O|OK|OK|-\ntuxsmall.jpg|X|X|X|X|OK|OK|-\nsub/testsub.txt|-|-|O|-|OK|OK|-\n")
-    # Generate rfigc database against original files
-    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (dirorig, filedb)) == 0
-    # Replication repair using rfigc database to check results
-    errcode = rep.synchronize_files(inputpaths, outpath, database=filedb, report_file=report_file)
-    assert errcode == 0
-    with open(report_file, 'rb') as rfile:
-        rout = rfile.read()
-    assert res2 in rout
-    # Do the same test with the main() func, this should give exactly the same result
-    errcode = rep.main("-i %s -o \"%s\" -r \"%s\" -d \"%s\" -f --silent" % (quote_paths(inputpaths), outpath, report_file, filedb))
-    assert errcode == 0
-    with open(report_file, 'rb') as rfile:
-        rout = rfile.read()
-    assert res2 in rout
-
-    # 3- Ambiguous voting + bad decoding
-    # Ambiguous tampering
-    # fileout[0] is kept intact, and will take precedence since it's first
-    tamper_file(fileout[1], 10, "B")
-    tamper_file(fileout[2], 10, "C")
-    tamper_file(fileout[3], 10, "D")
-    
-    # Wrong decoding (vote is OK but wrong)
-    tamper_file(fileout2[0], 0, "A")
-    tamper_file(fileout2[1], 0, "A")
-    tamper_file(fileout2[2], 0, "A")
-
-    # Replication repair using rfigc database to check results
-    res3 = b("filepath|dir1|dir2|dir3|dir4|hash-correct|error_code|errors\ntestaa.txt|X|X|-|X|KO|KO| File could not be totally repaired according to rfigc database.\ntuxsmall.jpg|X|X|X|X|OK|KO|Unrecoverable corruptions (because of ambiguity) in file tuxsmall.jpg on characters: ['0xa']. But merged file is correct according to rfigc database.\nsub/testsub.txt|-|-|O|-|OK|OK|-\n")
-    ptee = _StringIO()
-    errcode = rep.synchronize_files(inputpaths, outpath, database=filedb, report_file=report_file, ptee=ptee)
-    assert errcode == 1
-    with open(report_file, 'rb') as rfile:
-        rout = rfile.read()
-    ptee.seek(0)
-    errmsg = ptee.read()
-    assert len(errmsg) > 0
-    assert res3 in rout
-    # Same with main()...
-    errcode = rep.main("-i %s -o \"%s\" -r \"%s\" -d \"%s\" -f --silent" % (quote_paths(inputpaths), outpath, report_file, filedb))
-    assert errcode == 1
-    with open(report_file, 'rb') as rfile:
-        rout = rfile.read()
-    #ptee.seek(0)  # TODO: check if this is not dead code, because I can't see why we are redoing the test after rep.main() which is not provided with ptee anyway so ptee content did not change
-    #errmsg = ptee.read()
-    #assert len(errmsg) > 0
-    assert res3 in rout
-    ptee.close()  # close to avoid tracemalloc complaining
+from __future__ import print_function
+
+import sys
+import os
+
+import shutil
+
+from ..lib._compat import _range, b, _StringIO, _ord
+
+from io import BytesIO
+
+from .. import replication_repair as rep
+from .. import rfigc
+from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker
+
+
+def change_letter(s, index, new_char):
+    l = bytearray(s)
+    l[index] = _ord(new_char)
+    return l
+
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+
+def test_relpath_posix():
+    """ repli: test internal: relpath_posix()"""
+    recwalk_result = [r'/test/some/path', r'relative/path/file.ext']
+    pardir = r'/test/some'
+    assert rep.relpath_posix(recwalk_result, pardir, False) == ('/test/some/path', ['path', 'relative', 'path', 'file.ext'])
+    
+    # Can only test the following on a Windows machine, because relpath_posix() uses os.path.relpath to remove the parent directory from the relative path, but Windows path delimiters aren't recognized on a Linux machine, and there's no way to specify the delimiter as an argument...
+    if os.name == 'nt':
+        recwalk_result = [r'C:\test\some\path', r'relative\path\file.ext']
+        pardir = r'C:\test\some'
+        assert rep.relpath_posix(recwalk_result, pardir, True) == ('C:\\test\\some\\path', ['path', 'relative', 'path', 'file.ext'])
+        recwalk_result = [r'/test/some/path', r'relative\path\file.ext']
+        pardir = r'/test/some'
+        assert rep.relpath_posix(recwalk_result, pardir, True) == ('/test/some/path', ['path', 'relative', 'path', 'file.ext'])
+
+def test_sort_dict_of_paths():
+    d = {0: ['testoo.TXT'], 1: ['testoo.TXT'], 2: ['testbb-more.TXT'], 3: ['sub', 'testsub.TXT']}
+    d_sort = rep.sort_dict_of_paths(d)
+    assert d_sort == [(2, ['', 'testbb-more.TXT']),
+                      (0, ['', 'testoo.TXT']),
+                      (1, ['', 'testoo.TXT']),
+                      (3, ['sub', 'testsub.TXT'])]
+
+def test_sort_group():
+    """ repli: test internal: sort_group()"""
+    # Generate an artificial tree, with some relative paths being the same across multiple folders,
+    # and some others in different tree depth (very import to check that it works OK!)
+    curfiles = {
+                0: ['relative', 'path', 'file.ext'],
+                1: ['relative', 'path', 'file.ext'],
+                2: ['relative', 'aaa', 'zzzz.ext'],
+                3: ['zzzz.ext'],
+                4: ['relative', 'zzzz.ext'],
+                5: ['relative', 'path', 'bbbb.ext'],
+               }
+    assert rep.sort_group(curfiles, return_only_first=False) == \
+              [
+               [(3, ['', '', 'zzzz.ext'])],
+               [(4, ['', 'relative', 'zzzz.ext'])],
+               [(2, ['relative', 'aaa', 'zzzz.ext'])],
+               [(5, ['relative', 'path', 'bbbb.ext'])],
+               [(0, ['relative', 'path', 'file.ext']), (1, ['relative', 'path', 'file.ext'])]
+              ]
+    assert rep.sort_group(curfiles, return_only_first=True) == [[(3, ['', '', 'zzzz.ext'])]]
+
+def test_majority_vote_byte_scan():
+    """ repli: test internal: majority_vote_byte_scan()"""
+    
+    def make_filehandles(fileslist):
+        return [BytesIO(f) for f in fileslist]
+    
+    # Necessary variables
+    relfilepath = 'relative/path/file.ext'
+    s = b("Hello world\n"*3)
+    
+    # 1- simple test, only a few characters are tampered/removed towards the end
+    files = []
+    files.append(change_letter(s[:-2], 2, 'X'))
+    files.append(change_letter(s[:-1], 4, 'W'))
+    files.append(s)
+    fileshandles = make_filehandles(files)
+    outfile = BytesIO()
+
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
+    assert errcode == 0
+    assert errmsg == None
+    outfile.seek(0)
+    out = outfile.read()
+    assert out == s
+
+    # 2- Test ambiguity, the first files should take precedence
+    files = []
+    files.append(s)
+    # Set an ambiguity on the 3rd character
+    files.append(change_letter(s, 2, 'X'))
+    files.append(change_letter(s, 2, 'W'))
+
+    # First run: the original string will take precedence since it's first in the list
+    outfile = BytesIO()
+    fileshandles = make_filehandles(files)
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert out == s
+
+    # Second run: put the original string last, now we will have the tampered string taking precedence
+    files.append(files.pop(0))
+    fileshandles = make_filehandles(files)
+    outfile = BytesIO()
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert b('HeXlo world') in out
+
+    # Do the same, the third (tampered) string will take precedence
+    files.append(files.pop(0))
+    fileshandles = make_filehandles(files)
+    outfile = BytesIO()
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert b('HeWlo world') in out
+
+    # Set a default char this time
+    files.append(files.pop(0))
+    fileshandles = make_filehandles(files)
+    outfile = BytesIO()
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=True)
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert b("He\x00lo world") in out
+
+    # Set a custom default char
+    files.append(files.pop(0))
+    fileshandles = make_filehandles(files)
+    outfile = BytesIO()
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null="A")
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert b("HeAlo world") in out
+    
+    # 3- Test ambiguity when some files are already ended, the first still opened file should take precedence
+    files = []
+    files.append(s)
+    # Shorten the second file
+    files.append(s[:-3])
+    # Set an ambiguity on the 2nd character to the end of file
+    files.append(change_letter(s, -2, 'X'))
+
+    # First run: the original string will take precedence since it's first in the list
+    outfile = BytesIO()
+    fileshandles = make_filehandles(files)
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert out == s
+
+    # Move the original string to the last, and see if the tampered string takes precedence (skipping the ended string)
+    files.append(files.pop(0))
+    fileshandles = make_filehandles(files)
+    outfile = BytesIO()
+    errcode, errmsg = rep.majority_vote_byte_scan(relfilepath, fileshandles, outfile, blocksize=10, default_char_null=False)
+    assert errcode == 1
+    outfile.seek(0)
+    out = outfile.read()
+    assert b('Hello worlX') in out
+
+def test_synchronize_files():
+    """ repli: test main() and synchronize_files()"""
+    
+    def quote_paths(inputpaths):
+        return ' '.join(["\"%s\"" % path for path in inputpaths])
+
+    # -- First make replications of test files
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filein2 = path_sample_files('input', 'testaa.txt')
+    filein3 = path_sample_files('input', 'sub/testsub.txt')
+    #filein4 = path_sample_files('input', 'tux.jpg')
+    # Create replications directories
+    inputpaths = [path_sample_files('output', 'repli/dir%i' % (i+1), True) for i in _range(4)]
+    outpath = path_sample_files('output', 'repli/dirout')
+    # Report file
+    report_file = path_sample_files('output', 'repli_report.csv')
+    # Generate the replications
+    fileout = [path_sample_files('output', 'repli/dir%i/tuxsmall.jpg' % (i+1)) for i in _range(4)]
+    fileout2 = [path_sample_files('output', 'repli/dir%i/testaa.txt' % (i+1)) for i in _range(4) if i != 2]
+    path_sample_files('output', 'repli/dir3/sub/', True)
+    fileout3 = path_sample_files('output', 'repli/dir3/sub/testsub.txt')
+    #fileout4 = [path_sample_files('output', 'repli/dir%i/tux.jpg' % (i+1)) for i in _range(4)]
+    for f in fileout:
+        shutil.copy2(filein, f)
+    for f in fileout2:
+        shutil.copy2(filein2, f)
+    shutil.copy2(filein3, fileout3)
+    # Backup of original files (to generate rfigc database)
+    dirorig = path_sample_files('output', 'repli/dirorig', True)
+    shutil.copy2(filein, dirorig)
+    shutil.copy2(filein2, dirorig)
+    shutil.copy2(filein3, path_sample_files('output', 'repli/dirorig/sub', True))
+    # Database file
+    filedb = path_sample_files('output', 'repli_rfigc_db.csv')
+
+    # Tamper the replications
+    # 1st file: tamper all copies but at different positions, to force merge by majority vote
+    for i, f in enumerate(fileout):
+        tamper_file(f, i, "A")
+    # 2nd file: tamper all copies but one, to test rfigc interface
+    for i, f in enumerate(fileout2):
+        if i < 2:
+            tamper_file(f, 0, "A")
+    # 3rd file: no tamper, there's just one file, check that the file is just copied
+    
+    # 1- Majority vote!
+    res1 = b("filepath|dir1|dir2|dir3|dir4|hash-correct|error_code|errors\ntestaa.txt|X|X|-|X|-|OK|-\ntuxsmall.jpg|X|X|X|X|-|OK|-\nsub/testsub.txt|-|-|O|-|-|OK|-\n")
+    errcode = rep.synchronize_files(inputpaths, outpath, report_file=report_file)
+    assert errcode == 0
+    with open(report_file, 'rb') as rfile:
+        rout = rfile.read()
+    assert res1 in rout
+    # Do the same test with the main() func, this should give exactly the same result
+    errcode = rep.main("-i %s -o \"%s\" -r \"%s\" -f --silent" % (quote_paths(inputpaths), outpath, report_file))
+    assert errcode == 0
+    with open(report_file, 'rb') as rfile:
+        rout = rfile.read()
+    assert res1 in rout
+
+    # 2- rfigc database interfacing!
+    res2 = b("filepath|dir1|dir2|dir3|dir4|hash-correct|error_code|errors\ntestaa.txt|X|X|-|O|OK|OK|-\ntuxsmall.jpg|X|X|X|X|OK|OK|-\nsub/testsub.txt|-|-|O|-|OK|OK|-\n")
+    # Generate rfigc database against original files
+    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (dirorig, filedb)) == 0
+    # Replication repair using rfigc database to check results
+    errcode = rep.synchronize_files(inputpaths, outpath, database=filedb, report_file=report_file)
+    assert errcode == 0
+    with open(report_file, 'rb') as rfile:
+        rout = rfile.read()
+    assert res2 in rout
+    # Do the same test with the main() func, this should give exactly the same result
+    errcode = rep.main("-i %s -o \"%s\" -r \"%s\" -d \"%s\" -f --silent" % (quote_paths(inputpaths), outpath, report_file, filedb))
+    assert errcode == 0
+    with open(report_file, 'rb') as rfile:
+        rout = rfile.read()
+    assert res2 in rout
+
+    # 3- Ambiguous voting + bad decoding
+    # Ambiguous tampering
+    # fileout[0] is kept intact, and will take precedence since it's first
+    tamper_file(fileout[1], 10, "B")
+    tamper_file(fileout[2], 10, "C")
+    tamper_file(fileout[3], 10, "D")
+    
+    # Wrong decoding (vote is OK but wrong)
+    tamper_file(fileout2[0], 0, "A")
+    tamper_file(fileout2[1], 0, "A")
+    tamper_file(fileout2[2], 0, "A")
+
+    # Replication repair using rfigc database to check results
+    res3 = b("filepath|dir1|dir2|dir3|dir4|hash-correct|error_code|errors\ntestaa.txt|X|X|-|X|KO|KO| File could not be totally repaired according to rfigc database.\ntuxsmall.jpg|X|X|X|X|OK|KO|Unrecoverable corruptions (because of ambiguity) in file tuxsmall.jpg on characters: ['0xa']. But merged file is correct according to rfigc database.\nsub/testsub.txt|-|-|O|-|OK|OK|-\n")
+    ptee = _StringIO()
+    errcode = rep.synchronize_files(inputpaths, outpath, database=filedb, report_file=report_file, ptee=ptee)
+    assert errcode == 1
+    with open(report_file, 'rb') as rfile:
+        rout = rfile.read()
+    ptee.seek(0)
+    errmsg = ptee.read()
+    assert len(errmsg) > 0
+    assert res3 in rout
+    # Same with main()...
+    errcode = rep.main("-i %s -o \"%s\" -r \"%s\" -d \"%s\" -f --silent" % (quote_paths(inputpaths), outpath, report_file, filedb))
+    assert errcode == 1
+    with open(report_file, 'rb') as rfile:
+        rout = rfile.read()
+    #ptee.seek(0)  # TODO: check if this is not dead code, because I can't see why we are redoing the test after rep.main() which is not provided with ptee anyway so ptee content did not change
+    #errmsg = ptee.read()
+    #assert len(errmsg) > 0
+    assert res3 in rout
+    ptee.close()  # close to avoid tracemalloc complaining
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_resiliency_tester.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_resiliency_tester.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,197 +1,197 @@
-from __future__ import print_function
-
-import sys
-import os
-
-import shutil
-
-from .. import resiliency_tester as restest
-from .aux_tests import path_sample_files, tamper_file, create_dir_if_not_exist, remove_if_exist
-
-from ..lib._compat import _StringIO
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-
-def test_parse_configfile():
-    """ restest: test internal: parse_configfile() """
-    config = '''
-before_tamper:
-    cmd1 -i "arg1" -o "arg2"
-    cmd2
-
-tamper:
-    cmd3
-    cmd4
-
-after_tamper:
-    cmd5
-    cmd6
-    # a comment
-
-repair:
-    cmd7
-    cmd8
-    '''
-    fconfig = _StringIO(config)
-    parsed = restest.parse_configfile(fconfig)
-    assert parsed == {'tamper': ['cmd3', 'cmd4'], 'after_tamper': ['cmd5', 'cmd6'], 'before_tamper': ['cmd1 -i "arg1" -o "arg2"', 'cmd2'], 'repair': ['cmd7', 'cmd8']}
-
-def test_get_filename_no_ext():
-    """ restest: test internal: get_filename_no_ext() """
-    filepath = '/test/path/to/filename_no_ext.ext'
-    res = restest.get_filename_no_ext(filepath)
-    assert res == 'filename_no_ext'
-
-def test_interpolate_dict():
-    """ restest: test internal: interpolate_dict() """
-    s = 'Some {var1} with {var2} makes for {var3} parties!'
-    d = {'var1': 'wine', 'var2': 'beer', 'var3': 'fun', 'var4': 'Hidden'}
-    res = restest.interpolate_dict(s, interp_args=d)
-    assert res == 'Some wine with beer makes for fun parties!'
-
-def test_get_dbfile():
-    """ restest: test internal: get_dbfile() """
-    res = restest.get_dbfile('databases', 10)
-    assert 'databases' in res
-    assert 'db10' in res
-
-def test_diff_bytes_files():
-    """ restest: test internal: diff_bytes_files() """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    fileout1 = path_sample_files('output', 'bytes_tuxsmall1.jpg')
-    fileout2 = path_sample_files('output', 'bytes_tuxsmall2.jpg')
-    shutil.copy2(filein, fileout1)
-    shutil.copy2(filein, fileout2)
-    res = restest.diff_bytes_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
-    assert res[0] == 0
-    assert res[1] == os.stat(fileout1).st_size
-    tamper_file(fileout2, 0, "X")
-    tamper_file(fileout2, 4, "X")
-    tamper_file(fileout2, 2000, "X")
-    res = restest.diff_bytes_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
-    assert res[0] == 3
-
-def test_diff_count_files():
-    """ restest: test internal: diff_count_files() """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    fileout1 = path_sample_files('output', 'count_tuxsmall1.jpg')
-    fileout2 = path_sample_files('output', 'count_tuxsmall2.jpg')
-    shutil.copy2(filein, fileout1)
-    shutil.copy2(filein, fileout2)
-    res = restest.diff_count_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
-    assert res
-    tamper_file(fileout2, 0, "X")
-    tamper_file(fileout2, 4, "X")
-    tamper_file(fileout2, 2000, "X")
-    res = restest.diff_count_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
-    assert not res
-
-def test_diff_bytes_dir():
-    """ restest: test internal: diff_bytes_dir() """
-    dirin = path_sample_files('input')
-    dirout = path_sample_files('output', 'restest/bytes')
-    fileout = path_sample_files('output', 'restest/bytes/tuxsmall.jpg')
-    fileout2 = path_sample_files('output', 'restest/bytes/testaa.txt')
-    remove_if_exist(dirout)
-    shutil.copytree(dirin, dirout)
-
-    # First compare the two folders that are identical
-    res = restest.diff_bytes_dir(dirin, dirout)
-    assert res[0] == 0
-
-    # Tamper a few bytes of two files
-    tamper_file(fileout, 0, "X")
-    tamper_file(fileout, 4, "X")
-    tamper_file(fileout, 2000, "X")
-    tamper_file(fileout2, 0, "X")
-    res = restest.diff_bytes_dir(dirin, dirout)
-    assert res[0] == 4
-
-    # Now remove a file altogether, its size should be added to the amount of differing bytes
-    filesize = os.stat(fileout).st_size
-    remove_if_exist(fileout)
-    res = restest.diff_bytes_dir(dirin, dirout)
-    assert res[0] == (filesize+1)
-
-def test_diff_count_dir():
-    """ restest: test internal: diff_count_dir() """
-    dirin = path_sample_files('input')
-    dirout = path_sample_files('output', 'restest/count')
-    fileout = path_sample_files('output', 'restest/count/tuxsmall.jpg')
-    fileout2 = path_sample_files('output', 'restest/count/testaa.txt')
-    remove_if_exist(dirout)
-    shutil.copytree(dirin, dirout)
-
-    # First compare the two folders that are identical
-    res = restest.diff_count_dir(dirin, dirout)
-    assert res[0] == 0
-
-    # Tamper a few bytes of two files
-    tamper_file(fileout, 0, "X")
-    tamper_file(fileout, 4, "X")
-    tamper_file(fileout, 2000, "X")
-    tamper_file(fileout2, 0, "X")
-    res = restest.diff_count_dir(dirin, dirout)
-    assert res[0] == 2
-
-    # Now remove a file altogether, its size should be added to the amount of differing bytes
-    filesize = os.stat(fileout).st_size
-    remove_if_exist(fileout)
-    res = restest.diff_count_dir(dirin, dirout)
-    assert res[0] == 2
-
-def test_compute_repair_power():
-    """ restest: test internal: compute_repair_power() """
-    # Note: be careful if you add tests here, the displayed value by print() may be rounded up! Use print(repr(copute_repair_power()))
-    assert restest.compute_repair_power(0.3, 0.5) == 40.0
-    assert restest.compute_repair_power(0.2, 0.8) == 75.0
-    assert restest.compute_repair_power(0.6, 0.3) == -100.0
-    assert restest.compute_repair_power(0.6, 0.0) == 0.6
-
-def test_compute_diff_stats():
-    """ restest: test internal: compute_diff_stats() """
-    dirin = path_sample_files('input')
-    dirout = path_sample_files('output', 'restest/count')
-    fileout = path_sample_files('output', 'restest/count/tuxsmall.jpg')
-    fileout2 = path_sample_files('output', 'restest/count/testaa.txt')
-    remove_if_exist(dirout)
-    shutil.copytree(dirin, dirout)
-
-    # First compare the two folders that are identical
-    res = restest.compute_diff_stats(dirin, dirin, dirout)
-    assert dict(res) == {'diff_bytes': (0, 92955), 'diff_bytes_prev': (0, 92955), 'diff_count': (0, 7), 'diff_count_prev': (0, 7), 'repair_power': 0, 'error': 0.0}
-
-    # Tamper a few bytes of two files
-    tamper_file(fileout, 0, "X")
-    tamper_file(fileout, 4, "X")
-    tamper_file(fileout, 2000, "X")
-    tamper_file(fileout2, 0, "X")
-    res = restest.compute_diff_stats(dirin, dirin, dirout)
-    assert dict(res) == {'diff_bytes': (4, 92955), 'diff_bytes_prev': (4, 92955), 'diff_count': (2, 7), 'diff_count_prev': (2, 7), 'repair_power': 0, 'error': 0.0043031574417729005}
-
-def test_stats_running_average():
-    """ restest: test internal: stats_running_average() """
-    stats1 = {'diff_bytes': (0, 92955), 'diff_bytes_prev': (0, 92955), 'diff_count': (0, 7), 'diff_count_prev': (0, 7), 'repair_power': 0, 'error': 0.0}
-    stats2 = {'diff_bytes': (4, 92955), 'diff_bytes_prev': (4, 92955), 'diff_count': (2, 7), 'diff_count_prev': (2, 7), 'repair_power': 0, 'error': 0.5}
-    assert restest.stats_running_average({"tamper": stats1}, {"tamper": stats2}, 1) == {'tamper': {'diff_count_prev': [1.0, 7.0], 'diff_count': [1.0, 7.0], 'diff_bytes_prev': [2.0, 92955.0], 'error': 0.25, 'repair_power': 0.0, 'diff_bytes': [2.0, 92955.0]}}
-    assert restest.stats_running_average({"tamper": stats1}, {"tamper": stats2}, 3) == {'tamper': {'diff_count_prev': [0.5, 7.0], 'diff_count': [0.5, 7.0], 'diff_bytes_prev': [1.0, 92955.0], 'error': 0.125, 'repair_power': 0.0, 'diff_bytes': [1.0, 92955.0]}}
-
-def test_main():
-    """ restest: test main() """
-    # Change directory so that the config's commands can access pyFileFixity scripts
-    thispathname = os.path.dirname(__file__)
-    sys.path.append(os.path.join(thispathname, '..'))
-    # Setup paths
-    dirin = path_sample_files('input')
-    dirout = path_sample_files('output', 'restest/fulltest')
-    configfile = path_sample_files('results', 'resiliency_tester_config_easy.cfg')
-    configfile_hard = path_sample_files('results', 'resiliency_tester_config_hard.cfg')
-    # Should be no error with the easy scenario (repair should be successful)
-    assert restest.main("-i \"%s\" -o \"%s\" -c \"%s\" -f --silent" % (dirin, dirout, configfile)) == 0
-    # Should be error with the hard scenario
-    assert restest.main("-i \"%s\" -o \"%s\" -c \"%s\" -m 2 -f --silent" % (dirin, dirout, configfile_hard)) == 1
-    # TODO: catch sys.stdout and check for the end stats?
+from __future__ import print_function
+
+import sys
+import os
+
+import shutil
+
+from .. import resiliency_tester as restest
+from .aux_tests import path_sample_files, tamper_file, create_dir_if_not_exist, remove_if_exist
+
+from ..lib._compat import _StringIO
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+
+def test_parse_configfile():
+    """ restest: test internal: parse_configfile() """
+    config = '''
+before_tamper:
+    cmd1 -i "arg1" -o "arg2"
+    cmd2
+
+tamper:
+    cmd3
+    cmd4
+
+after_tamper:
+    cmd5
+    cmd6
+    # a comment
+
+repair:
+    cmd7
+    cmd8
+    '''
+    fconfig = _StringIO(config)
+    parsed = restest.parse_configfile(fconfig)
+    assert parsed == {'tamper': ['cmd3', 'cmd4'], 'after_tamper': ['cmd5', 'cmd6'], 'before_tamper': ['cmd1 -i "arg1" -o "arg2"', 'cmd2'], 'repair': ['cmd7', 'cmd8']}
+
+def test_get_filename_no_ext():
+    """ restest: test internal: get_filename_no_ext() """
+    filepath = '/test/path/to/filename_no_ext.ext'
+    res = restest.get_filename_no_ext(filepath)
+    assert res == 'filename_no_ext'
+
+def test_interpolate_dict():
+    """ restest: test internal: interpolate_dict() """
+    s = 'Some {var1} with {var2} makes for {var3} parties!'
+    d = {'var1': 'wine', 'var2': 'beer', 'var3': 'fun', 'var4': 'Hidden'}
+    res = restest.interpolate_dict(s, interp_args=d)
+    assert res == 'Some wine with beer makes for fun parties!'
+
+def test_get_dbfile():
+    """ restest: test internal: get_dbfile() """
+    res = restest.get_dbfile('databases', 10)
+    assert 'databases' in res
+    assert 'db10' in res
+
+def test_diff_bytes_files():
+    """ restest: test internal: diff_bytes_files() """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    fileout1 = path_sample_files('output', 'bytes_tuxsmall1.jpg')
+    fileout2 = path_sample_files('output', 'bytes_tuxsmall2.jpg')
+    shutil.copy2(filein, fileout1)
+    shutil.copy2(filein, fileout2)
+    res = restest.diff_bytes_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
+    assert res[0] == 0
+    assert res[1] == os.stat(fileout1).st_size
+    tamper_file(fileout2, 0, "X")
+    tamper_file(fileout2, 4, "X")
+    tamper_file(fileout2, 2000, "X")
+    res = restest.diff_bytes_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
+    assert res[0] == 3
+
+def test_diff_count_files():
+    """ restest: test internal: diff_count_files() """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    fileout1 = path_sample_files('output', 'count_tuxsmall1.jpg')
+    fileout2 = path_sample_files('output', 'count_tuxsmall2.jpg')
+    shutil.copy2(filein, fileout1)
+    shutil.copy2(filein, fileout2)
+    res = restest.diff_count_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
+    assert res
+    tamper_file(fileout2, 0, "X")
+    tamper_file(fileout2, 4, "X")
+    tamper_file(fileout2, 2000, "X")
+    res = restest.diff_count_files(fileout1, fileout2, blocksize=1000, startpos1=0, startpos2=0)
+    assert not res
+
+def test_diff_bytes_dir():
+    """ restest: test internal: diff_bytes_dir() """
+    dirin = path_sample_files('input')
+    dirout = path_sample_files('output', 'restest/bytes')
+    fileout = path_sample_files('output', 'restest/bytes/tuxsmall.jpg')
+    fileout2 = path_sample_files('output', 'restest/bytes/testaa.txt')
+    remove_if_exist(dirout)
+    shutil.copytree(dirin, dirout)
+
+    # First compare the two folders that are identical
+    res = restest.diff_bytes_dir(dirin, dirout)
+    assert res[0] == 0
+
+    # Tamper a few bytes of two files
+    tamper_file(fileout, 0, "X")
+    tamper_file(fileout, 4, "X")
+    tamper_file(fileout, 2000, "X")
+    tamper_file(fileout2, 0, "X")
+    res = restest.diff_bytes_dir(dirin, dirout)
+    assert res[0] == 4
+
+    # Now remove a file altogether, its size should be added to the amount of differing bytes
+    filesize = os.stat(fileout).st_size
+    remove_if_exist(fileout)
+    res = restest.diff_bytes_dir(dirin, dirout)
+    assert res[0] == (filesize+1)
+
+def test_diff_count_dir():
+    """ restest: test internal: diff_count_dir() """
+    dirin = path_sample_files('input')
+    dirout = path_sample_files('output', 'restest/count')
+    fileout = path_sample_files('output', 'restest/count/tuxsmall.jpg')
+    fileout2 = path_sample_files('output', 'restest/count/testaa.txt')
+    remove_if_exist(dirout)
+    shutil.copytree(dirin, dirout)
+
+    # First compare the two folders that are identical
+    res = restest.diff_count_dir(dirin, dirout)
+    assert res[0] == 0
+
+    # Tamper a few bytes of two files
+    tamper_file(fileout, 0, "X")
+    tamper_file(fileout, 4, "X")
+    tamper_file(fileout, 2000, "X")
+    tamper_file(fileout2, 0, "X")
+    res = restest.diff_count_dir(dirin, dirout)
+    assert res[0] == 2
+
+    # Now remove a file altogether, its size should be added to the amount of differing bytes
+    filesize = os.stat(fileout).st_size
+    remove_if_exist(fileout)
+    res = restest.diff_count_dir(dirin, dirout)
+    assert res[0] == 2
+
+def test_compute_repair_power():
+    """ restest: test internal: compute_repair_power() """
+    # Note: be careful if you add tests here, the displayed value by print() may be rounded up! Use print(repr(copute_repair_power()))
+    assert restest.compute_repair_power(0.3, 0.5) == 40.0
+    assert restest.compute_repair_power(0.2, 0.8) == 75.0
+    assert restest.compute_repair_power(0.6, 0.3) == -100.0
+    assert restest.compute_repair_power(0.6, 0.0) == 0.6
+
+def test_compute_diff_stats():
+    """ restest: test internal: compute_diff_stats() """
+    dirin = path_sample_files('input')
+    dirout = path_sample_files('output', 'restest/count')
+    fileout = path_sample_files('output', 'restest/count/tuxsmall.jpg')
+    fileout2 = path_sample_files('output', 'restest/count/testaa.txt')
+    remove_if_exist(dirout)
+    shutil.copytree(dirin, dirout)
+
+    # First compare the two folders that are identical
+    res = restest.compute_diff_stats(dirin, dirin, dirout)
+    assert dict(res) == {'diff_bytes': (0, 92955), 'diff_bytes_prev': (0, 92955), 'diff_count': (0, 7), 'diff_count_prev': (0, 7), 'repair_power': 0, 'error': 0.0}
+
+    # Tamper a few bytes of two files
+    tamper_file(fileout, 0, "X")
+    tamper_file(fileout, 4, "X")
+    tamper_file(fileout, 2000, "X")
+    tamper_file(fileout2, 0, "X")
+    res = restest.compute_diff_stats(dirin, dirin, dirout)
+    assert dict(res) == {'diff_bytes': (4, 92955), 'diff_bytes_prev': (4, 92955), 'diff_count': (2, 7), 'diff_count_prev': (2, 7), 'repair_power': 0, 'error': 0.0043031574417729005}
+
+def test_stats_running_average():
+    """ restest: test internal: stats_running_average() """
+    stats1 = {'diff_bytes': (0, 92955), 'diff_bytes_prev': (0, 92955), 'diff_count': (0, 7), 'diff_count_prev': (0, 7), 'repair_power': 0, 'error': 0.0}
+    stats2 = {'diff_bytes': (4, 92955), 'diff_bytes_prev': (4, 92955), 'diff_count': (2, 7), 'diff_count_prev': (2, 7), 'repair_power': 0, 'error': 0.5}
+    assert restest.stats_running_average({"tamper": stats1}, {"tamper": stats2}, 1) == {'tamper': {'diff_count_prev': [1.0, 7.0], 'diff_count': [1.0, 7.0], 'diff_bytes_prev': [2.0, 92955.0], 'error': 0.25, 'repair_power': 0.0, 'diff_bytes': [2.0, 92955.0]}}
+    assert restest.stats_running_average({"tamper": stats1}, {"tamper": stats2}, 3) == {'tamper': {'diff_count_prev': [0.5, 7.0], 'diff_count': [0.5, 7.0], 'diff_bytes_prev': [1.0, 92955.0], 'error': 0.125, 'repair_power': 0.0, 'diff_bytes': [1.0, 92955.0]}}
+
+def test_main():
+    """ restest: test main() """
+    # Change directory so that the config's commands can access pyFileFixity scripts
+    thispathname = os.path.dirname(__file__)
+    sys.path.append(os.path.join(thispathname, '..'))
+    # Setup paths
+    dirin = path_sample_files('input')
+    dirout = path_sample_files('output', 'restest/fulltest')
+    configfile = path_sample_files('results', 'resiliency_tester_config_easy.cfg')
+    configfile_hard = path_sample_files('results', 'resiliency_tester_config_hard.cfg')
+    # Should be no error with the easy scenario (repair should be successful)
+    assert restest.main("-i \"%s\" -o \"%s\" -c \"%s\" -f --silent" % (dirin, dirout, configfile)) == 0
+    # Should be error with the hard scenario
+    assert restest.main("-i \"%s\" -o \"%s\" -c \"%s\" -m 2 -f --silent" % (dirin, dirout, configfile_hard)) == 1
+    # TODO: catch sys.stdout and check for the end stats?
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_rfigc.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_rfigc.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,129 +1,131 @@
-from __future__ import print_function, with_statement
-
-import sys
-import os
-import itertools
-import hashlib
-
-import shutil
-
-from ..lib._compat import b
-
-from .. import rfigc
-from ..lib.aux_funcs import recwalk
-from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, create_dir_if_not_exist
-
-def partial_eq(file, file_partial):
-    """ Do a partial comparison, line by line, we compare only using "line2 in line1", where line2 is from file_partial """
-    flag = True
-    with open(file, 'rb') as outf, open(file_partial, 'rb') as expectedf:
-        out = outf.read().strip(b('\n'))
-        expected = expectedf.read().strip(b('\n')).split(b('\n'))
-        for exp in expected:
-            if not exp in out:
-                flag = False
-                break
-    return flag
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-
-def test_one_file():
-    """ rfigc: test creation and verification of rfigc database for one file """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = path_sample_files('output', 'd_file.csv')
-    fileres = path_sample_files('results', 'test_rfigc_test_one_file.csv')
-    # Generate database file
-    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
-    # Check files are ok
-    assert rfigc.main('-i "%s" -d "%s" --silent' % (filein, filedb)) == 0
-    # Check database file is the same as the pregenerated result
-    with open(filedb, 'rb') as outf, open(fileres, 'rb') as expectedf:
-        # Because of differing timestamps between local and git repo, we must only do a partial comparison (we compare the beginning of the file up to the timestamp)
-        expected = expectedf.read().strip(b("\n"))
-        out = outf.read().strip(b("\n"))
-        assert expected in out
-
-def test_dir():
-    """ rfigc: test creation and verification of database for a full directory """
-    filein = path_sample_files('input', )
-    filedb = path_sample_files('output', 'd_dir.csv')
-    fileres = path_sample_files('results', 'test_rfigc_test_dir.csv')
-    # Generate database file
-    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
-    # Check files are ok
-    assert rfigc.main('-i "%s" -d "%s" --silent' % (filein, filedb)) == 0
-    # Check database file is the same as the pregenerated result
-    # We can't directly compare the two files because of timestamps!
-    # So we manually process the expected results and compare each line to see if it's present in the output
-    assert partial_eq(filedb, fileres)
-    # TODO: add a regular expression to check that all fields are present
-
-def test_error_file():
-    """ rfigc: test tamper file and error file generation """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = path_sample_files('output', 'd.csv')
-    fileout = path_sample_files('output', 'tuxsmall.jpg')
-    fileout2 = path_sample_files('output', 'errors.log')
-    fileres = path_sample_files('results', 'test_rfigc_test_error_file.log')
-    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
-    shutil.copyfile(filein, fileout)
-    tamper_file(fileout, 3)
-    assert rfigc.main('-i "%s" -d "%s" -e "%s" --silent' % (fileout, filedb, fileout2)) == 1
-    check_eq_files(fileout2, fileres)
-
-def test_filescrape():
-    """ rfigc: test --filescraping_recovery """
-    filein_dir = path_sample_files('input', )
-    filedb = path_sample_files('output', 'db_filescrape.csv')
-    fileout_dir = path_sample_files('output', 'filescrape')
-    fileout_dir_rec = path_sample_files('output', 'filescrape_rec')
-    create_dir_if_not_exist(fileout_dir)
-    create_dir_if_not_exist(fileout_dir_rec)
-    # Simulate a filescrape (copy the files but rename them all)
-    i = 0
-    for dirpath, filepath in recwalk(filein_dir):
-        i += 1
-        shutil.copyfile(os.path.join(dirpath, filepath), os.path.join(fileout_dir, "%s.stuff" % i))
-    assert not check_eq_dir(filein_dir, fileout_dir) # check that we correctly filescraped!
-    # Use rfigc to recover from filescrape
-    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein_dir, filedb)) == 0
-    assert rfigc.main('-i "%s" -d "%s" --filescraping_recovery -o "%s" --silent' % (fileout_dir, filedb, fileout_dir_rec)) == 0
-    assert check_eq_dir(filein_dir, fileout_dir_rec) # check that we recovered from filescraping!
-
-def test_update():
-    """ rfigc: test --update """
-    filein = path_sample_files('input', )
-    filedb = path_sample_files('output', 'd_update.csv')
-    fileout_dir = path_sample_files('output', 'update')
-    fileout = path_sample_files('output', 'update/added_file.txt')
-    fileres1 = path_sample_files('results', 'test_rfigc_test_update_append.csv')
-    fileres2 = path_sample_files('results', 'test_rfigc_test_update_remove.csv')
-    # Generate a database from input files
-    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
-    # Create a new file in another folder
-    create_dir_if_not_exist(fileout_dir)
-    with open(fileout, 'wb') as fh:
-        fh.write(b'abcdefABCDEF\n1234598765')
-    # Append file in database
-    assert rfigc.main('-i "%s" -d "%s" --update --append --silent' % (fileout_dir, filedb)) == 0
-    assert partial_eq(filedb, fileres1)
-    # Remove all other files from database
-    assert rfigc.main('-i "%s" -d "%s" --update --remove --silent' % (fileout_dir, filedb)) == 0
-    assert partial_eq(filedb, fileres2)
-
-def test_generate_hashes():
-    """ rfigc: test internal: generate_hashes() """
-    # Test with a file we make on the spot, so this should always be correct!
-    infile0 = path_sample_files('output', 'test_rfigc_generate_hashes.txt')
-    with open(infile0, 'wb') as f0:
-        f0.write(b"Lorem ipsum etc\n"*20)
-    assert rfigc.generate_hashes(infile0) == ('c6e0c87cbb8eeaca8179f22186384e6b', '6f46949be7cda1437bc3fb61fb827a6552beaf8b')
-    # Test with input files, this may change if we change the files
-    infile1 = path_sample_files('input', 'tux.jpg')
-    infile2 = path_sample_files('input', 'alice.pdf')
-    assert rfigc.generate_hashes(infile1) == ('81e19bbf2efaeb1d6d6473c21c48e4b7', '6e38ea91680ef0f960db0fd6a973cf50ef765369')
-    assert rfigc.generate_hashes(infile2) == ('298aeefe8c00f2d92d660987bee67260', '106e7ad4d3927c5906cd366cc0d5bd887bdc3300')
+from __future__ import print_function, with_statement
+
+import sys
+import os
+import itertools
+import hashlib
+
+import shutil
+
+from ..lib._compat import b, _open_csv
+
+from .. import rfigc
+from ..lib.aux_funcs import recwalk
+from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, create_dir_if_not_exist
+
+def partial_eq(file, file_partial):
+    """ Do a partial comparison, line by line, we compare only using "line2 in line1", where line2 is from file_partial """
+    flag = True
+    with _open_csv(file, 'r') as outf, _open_csv(file_partial, 'r') as expectedf:
+        out = outf.read().strip("\r").strip("\n")
+        expected = expectedf.read().split("\n")
+        for exp in expected:
+            if not exp.strip("\n") in out:
+                flag = False
+                break
+    return flag
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+
+def test_one_file():
+    """ rfigc: test creation and verification of rfigc database for one file """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = path_sample_files('output', 'd_file.csv')
+    fileres = path_sample_files('results', 'test_rfigc_test_one_file.csv')
+    # Generate database file
+    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
+    # Check files are ok
+    assert rfigc.main('-i "%s" -d "%s" --silent' % (filein, filedb)) == 0
+    # Check database file is the same as the pregenerated result
+    with _open_csv(filedb, 'r') as outf, _open_csv(fileres, 'r') as expectedf:
+        out = outf.read().strip("\r").strip("\n")
+        # Because of differing timestamps between local and git repo, we must only do a partial comparison (we compare the beginning of the file up to the timestamp)
+        # TODO: to do full comparisons including timestamps, use https://github.com/adamchainz/time-machine or freezegun
+        expected = expectedf.read().split("\n")  # workaround to remove windows carriage return character, it does not always get added but under some strange conditions (in GitHub Actions env, and not all the time, but only on Windows-2019) it can get added by csv writer, ignoring our settings. TODO: remove strip("\r") and try to find a REAL fix.
+        for exp in expected:
+            assert exp.strip("\r").strip("\n") in out
+
+def test_dir():
+    """ rfigc: test creation and verification of database for a full directory """
+    filein = path_sample_files('input', )
+    filedb = path_sample_files('output', 'd_dir.csv')
+    fileres = path_sample_files('results', 'test_rfigc_test_dir.csv')
+    # Generate database file
+    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
+    # Check files are ok
+    assert rfigc.main('-i "%s" -d "%s" --silent' % (filein, filedb)) == 0
+    # Check database file is the same as the pregenerated result
+    # We can't directly compare the two files because of timestamps!
+    # So we manually process the expected results and compare each line to see if it's present in the output
+    assert partial_eq(filedb, fileres)
+    # TODO: add a regular expression to check that all fields are present
+
+def test_error_file():
+    """ rfigc: test tamper file and error file generation """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = path_sample_files('output', 'd.csv')
+    fileout = path_sample_files('output', 'tuxsmall.jpg')
+    fileout2 = path_sample_files('output', 'errors.log')
+    fileres = path_sample_files('results', 'test_rfigc_test_error_file.log')
+    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
+    shutil.copyfile(filein, fileout)
+    tamper_file(fileout, 3)
+    assert rfigc.main('-i "%s" -d "%s" -e "%s" --silent' % (fileout, filedb, fileout2)) == 1
+    check_eq_files(fileout2, fileres)
+
+def test_filescrape():
+    """ rfigc: test --filescraping_recovery """
+    filein_dir = path_sample_files('input', )
+    filedb = path_sample_files('output', 'db_filescrape.csv')
+    fileout_dir = path_sample_files('output', 'filescrape')
+    fileout_dir_rec = path_sample_files('output', 'filescrape_rec')
+    create_dir_if_not_exist(fileout_dir)
+    create_dir_if_not_exist(fileout_dir_rec)
+    # Simulate a filescrape (copy the files but rename them all)
+    i = 0
+    for dirpath, filepath in recwalk(filein_dir):
+        i += 1
+        shutil.copyfile(os.path.join(dirpath, filepath), os.path.join(fileout_dir, "%s.stuff" % i))
+    assert not check_eq_dir(filein_dir, fileout_dir) # check that we correctly filescraped!
+    # Use rfigc to recover from filescrape
+    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein_dir, filedb)) == 0
+    assert rfigc.main('-i "%s" -d "%s" --filescraping_recovery -o "%s" --silent' % (fileout_dir, filedb, fileout_dir_rec)) == 0
+    assert check_eq_dir(filein_dir, fileout_dir_rec) # check that we recovered from filescraping!
+
+def test_update():
+    """ rfigc: test --update """
+    filein = path_sample_files('input', )
+    filedb = path_sample_files('output', 'd_update.csv')
+    fileout_dir = path_sample_files('output', 'update')
+    fileout = path_sample_files('output', 'update/added_file.txt')
+    fileres1 = path_sample_files('results', 'test_rfigc_test_update_append.csv')
+    fileres2 = path_sample_files('results', 'test_rfigc_test_update_remove.csv')
+    # Generate a database from input files
+    assert rfigc.main('-i "%s" -d "%s" -g -f --silent' % (filein, filedb)) == 0
+    # Create a new file in another folder
+    create_dir_if_not_exist(fileout_dir)
+    with open(fileout, 'wb') as fh:
+        fh.write(b'abcdefABCDEF\n1234598765')
+    # Append file in database
+    assert rfigc.main('-i "%s" -d "%s" --update --append --silent' % (fileout_dir, filedb)) == 0
+    assert partial_eq(filedb, fileres1)
+    # Remove all other files from database
+    assert rfigc.main('-i "%s" -d "%s" --update --remove --silent' % (fileout_dir, filedb)) == 0
+    assert partial_eq(filedb, fileres2)
+
+def test_generate_hashes():
+    """ rfigc: test internal: generate_hashes() """
+    # Test with a file we make on the spot, so this should always be correct!
+    infile0 = path_sample_files('output', 'test_rfigc_generate_hashes.txt')
+    with open(infile0, 'wb') as f0:
+        f0.write(b"Lorem ipsum etc\n"*20)
+    assert rfigc.generate_hashes(infile0) == ('c6e0c87cbb8eeaca8179f22186384e6b', '6f46949be7cda1437bc3fb61fb827a6552beaf8b')
+    # Test with input files, this may change if we change the files
+    infile1 = path_sample_files('input', 'tux.jpg')
+    infile2 = path_sample_files('input', 'alice.pdf')
+    assert rfigc.generate_hashes(infile1) == ('81e19bbf2efaeb1d6d6473c21c48e4b7', '6e38ea91680ef0f960db0fd6a973cf50ef765369')
+    assert rfigc.generate_hashes(infile2) == ('298aeefe8c00f2d92d660987bee67260', '106e7ad4d3927c5906cd366cc0d5bd887bdc3300')
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_structural_adaptive_ecc.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_structural_adaptive_ecc.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,145 +1,145 @@
-from __future__ import print_function
-
-import sys
-import os
-import itertools
-import hashlib
-
-import shutil
-
-from .. import structural_adaptive_ecc as saecc
-from ..lib.aux_funcs import get_next_entry
-from ..lib.eccman import ECCMan
-from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker, dummy_ecc_file_gen
-
-from io import BytesIO
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-
-def test_one_file():
-    """ saecc: test creation and verification of database for one file """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = path_sample_files('output', 'saecc_file.db')
-    fileout = path_sample_files('output', 'tuxsmall.jpg')
-    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
-    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_one_file.db')
-    # Generate an ecc file
-    assert saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
-    # Check that generated ecc file is correct
-    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
-    # Check that the ecc file correctly validates the correct files
-    assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
-
-def test_one_file_tamper():
-    """ saecc: test file repair """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = path_sample_files('output', 'saecc_tamper.db')
-    fileout = path_sample_files('output', 'tuxsmall.jpg')
-    fileout2 = path_sample_files('output', 'repaired/tuxsmall.jpg')
-    fileout2_dir = path_sample_files('output', 'repaired')
-    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_one_file_tamper.db')
-    create_dir_if_not_exist(fileout2_dir)
-    # Generate an ecc file
-    assert saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
-    # Tamper the file
-    shutil.copyfile(filein, fileout) # Copy it to avoid tampering the original
-    tamper_file(fileout, 4, r'abcde')
-    tamper_file(fileout, 1100, r'abcde') # tamper outside the range of header
-    tamper_file(fileout, -5, r'abcde') # tamper end of file
-    # Repair the file
-    assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (fileout, filedb, fileout2_dir)) == 0
-    # Check that the file was completely repaired
-    assert check_eq_files(filein, fileout2)
-
-def test_dir():
-    """ saecc: test creation and verification of database for a full directory """
-    filein = path_sample_files('input', )
-    filedb = path_sample_files('output', 'saecc_dir.db')
-    fileout = path_sample_files('output', )
-    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
-    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_dir.db')
-    # Generate an ecc file
-    assert saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
-    # Check that generated ecc file is correct
-    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
-    # Check that the ecc file correctly validates the correct files
-    assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
-
-def test_algo():
-    """ saecc: test algorithms equivalence """
-    filein = path_sample_files('input', 'tuxsmall.jpg')
-    filedb = [path_sample_files('output', 'saecc_algo1.db'),
-                path_sample_files('output', 'saecc_algo2.db'),
-                path_sample_files('output', 'saecc_algo3.db'),
-                ]
-    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_algo.db')
-    fileout_rec = path_sample_files('output', 'rectemp', True)
-    # For each algorithm
-    for i in range(len(filedb)):
-        # Generate an ecc file
-        assert saecc.main('-i "%s" -d "%s" --ecc_algo=%i -g -f --silent' % (filein, filedb[i], i+1)) == 0
-        # Check file with this ecc algo
-        assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=%i -c --silent' % (filein, filedb[i], fileout_rec, i+1)) == 0
-    # Check that all generated ecc are the same, whatever the algo (up to 3)
-    startpos1 = next(find_next_entry(filedb[0], get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
-    for i in range(1, len(filedb)):
-        startpos2 = next(find_next_entry(filedb[i], get_marker(type=1)))
-        assert check_eq_files(filedb[0], filedb[i], startpos1=startpos1, startpos2=startpos2)
-    # Check against expected ecc file
-    startpos1 = next(find_next_entry(filedb[0], get_marker(type=1)))
-    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
-    assert check_eq_files(filedb[0], fileres, startpos1=startpos1, startpos2=startpos2)
-
-def test_entry_fields():
-    """ saecc: test internal: entry_fields() """
-    ecc = dummy_ecc_file_gen(3)
-    eccf = BytesIO(ecc)
-    ecc_entry_pos = get_next_entry(eccf, get_marker(1), only_coord=True)
-    assert saecc.entry_fields(eccf, ecc_entry_pos, field_delim=get_marker(2)) == {'ecc_field_pos': [150, 195], 'filesize_ecc': b'filesize1_ecc', 'relfilepath_ecc': b'relfilepath1_ecc', 'relfilepath': b'file1.ext', 'filesize': b'filesize1'}
-    ecc_entry_pos = get_next_entry(eccf, get_marker(1), only_coord=True)
-    assert saecc.entry_fields(eccf, ecc_entry_pos, field_delim=get_marker(2)) == {'ecc_field_pos': [272, 362], 'filesize_ecc': b'filesize2_ecc', 'relfilepath_ecc': b'relfilepath2_ecc', 'relfilepath': b'file2.ext', 'filesize': b'filesize2'}
-
-def test_stream_entry_assemble():
-    """ saecc: test internal: stream_entry_assemble() """
-    class Hasher(object):
-        """ Dummy Hasher """
-        def __len__(self):
-            return 32
-    tempfile = path_sample_files('output', 'saecc_stream_entry_assemble.txt')
-    with open(tempfile, 'wb') as tfile:
-        tfile.write(b"Lorem ipsum\nAnd stuff and stuff and stuff\n"*20)
-    ecc = dummy_ecc_file_gen(3)
-    eccf = BytesIO(ecc)
-    ecc_entry_pos = get_next_entry(eccf, get_marker(1), only_coord=True)
-    entry_fields = saecc.entry_fields(eccf, ecc_entry_pos, field_delim=get_marker(2))
-    with open(tempfile, 'rb') as tfile:
-        assert list( saecc.stream_entry_assemble(Hasher(), tfile, eccf, entry_fields, 255, 10, [0.7, 0.5, 0.3]) ) == [{'ecc': b'sh-ecc-entry_\xfe\xff\xfe\xff\xfe\xff\xfe\xff\xfe\xfffile2.ext\xfa\xff\xfa\xff\xfafilesize2\xfa\xff\xfa\xff\xfarelfilepath2_ecc\xfa\xff\xfa\xff\xfafilesize2_ecc\xfa\xff\xfa\xff\xfahash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry', 'curpos': 0, 'rate': 0.7, 'ecc_params': {'ecc_size': 149, 'hash_size': 32, 'message_size': 106}, 'ecc_curpos': 150, 'hash': b'hash-ecc-entry_hash-ecc-entry_ha', 'message': b'Lorem ipsum\nAnd stuff and stuff and stuff\nLorem ipsum\nAnd stuff and stuff and stuff\nLorem ipsum\nAnd stuff '}]
-    # TODO: check that several blocks can be assembled, currently we only check one block
-
-def test_stream_compute_ecc_hash():
-    """ saecc: test internal: stream_compute_ecc_hash() and compute_ecc_hash_from_string() """
-    class Hasher(object):
-        """ Dummy Hasher """
-        def hash(self, mes):
-            return "dummyhsh"
-        def __len__(self):
-            return 8
-    n = 20 # aka max_block_size
-    k = 11
-    resilience_rates = [0.7, 0.5, 0.3]
-    instring = b"hello world!"*10
-    tempfile = path_sample_files('output', 'saecc_stream_compute_ecc_hash.txt')
-    with open(tempfile, 'wb') as tfile:
-        tfile.write(instring)
-    eccman = ECCMan(n, k, algo=3)
-    with open(tempfile, 'rb') as tfile:
-        assert list( saecc.stream_compute_ecc_hash(eccman, Hasher(), tfile, n, int(len(instring)/4), resilience_rates) ) == [[b'dummyhsh', bytearray(b'\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'r\x9d\xb7#f\xa3=*\xda\x17WC'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'\xab\x8c\xae\r\xbb\x9b\x93\xbd\xd5\x8f'), {'ecc_size': 10, 'hash_size': 8, 'message_size': 10}], [b'dummyhsh', bytearray(b'\xb8S\x1cz\xb2\xeb\x9fu\x19\x83'), {'ecc_size': 10, 'hash_size': 8, 'message_size': 10}], [b'dummyhsh', bytearray(b'\x07\xc4\xce\xe2\xdf\x0b\t\x17,'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'\xd6(og\xb5}\x06\xe3\xd2'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'v\x9dP\x0c\x01\x03\x83Q!'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'\xc4\x12q\xd9\x0fq\xef\xc2\xba'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'6\xd0\xe8\xe9\xfe(y\x13'), {'ecc_size': 8, 'hash_size': 8, 'message_size': 12}], [b'dummyhsh', bytearray(b'6\xd0\xe8\xe9\xfe(y\x13'), {'ecc_size': 8, 'hash_size': 8, 'message_size': 12}]]
-    assert saecc.compute_ecc_hash_from_string(instring, eccman, Hasher(), n, resilience_rates[0]) == b'\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC'
+from __future__ import print_function
+
+import sys
+import os
+import itertools
+import hashlib
+
+import shutil
+
+from .. import structural_adaptive_ecc as saecc
+from ..lib.aux_funcs import get_next_entry
+from ..lib.eccman import ECCMan
+from .aux_tests import check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, get_marker, dummy_ecc_file_gen
+
+from io import BytesIO
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+
+def test_one_file():
+    """ saecc: test creation and verification of database for one file """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = path_sample_files('output', 'saecc_file.db')
+    fileout = path_sample_files('output', 'tuxsmall.jpg')
+    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
+    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_one_file.db')
+    # Generate an ecc file
+    assert saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
+    # Check that generated ecc file is correct
+    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
+    # Check that the ecc file correctly validates the correct files
+    assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
+
+def test_one_file_tamper():
+    """ saecc: test file repair """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = path_sample_files('output', 'saecc_tamper.db')
+    fileout = path_sample_files('output', 'tuxsmall.jpg')
+    fileout2 = path_sample_files('output', 'repaired/tuxsmall.jpg')
+    fileout2_dir = path_sample_files('output', 'repaired')
+    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_one_file_tamper.db')
+    create_dir_if_not_exist(fileout2_dir)
+    # Generate an ecc file
+    assert saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
+    # Tamper the file
+    shutil.copyfile(filein, fileout) # Copy it to avoid tampering the original
+    tamper_file(fileout, 4, r'abcde')
+    tamper_file(fileout, 1100, r'abcde') # tamper outside the range of header
+    tamper_file(fileout, -5, r'abcde') # tamper end of file
+    # Repair the file
+    assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (fileout, filedb, fileout2_dir)) == 0
+    # Check that the file was completely repaired
+    assert check_eq_files(filein, fileout2)
+
+def test_dir():
+    """ saecc: test creation and verification of database for a full directory """
+    filein = path_sample_files('input', )
+    filedb = path_sample_files('output', 'saecc_dir.db')
+    fileout = path_sample_files('output', )
+    fileout_rec = path_sample_files('output', 'rectemp', True) # temporary folder where repaired files will be placed (we expect none so this should be temporary, empty folder)
+    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_dir.db')
+    # Generate an ecc file
+    assert saecc.main('-i "%s" -d "%s" --ecc_algo=3 -g -f --silent' % (filein, filedb)) == 0
+    # Check that generated ecc file is correct
+    startpos1 = next(find_next_entry(filedb, get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb, fileres, startpos1=startpos1, startpos2=startpos2)
+    # Check that the ecc file correctly validates the correct files
+    assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=3 -c --silent' % (filein, filedb, fileout_rec)) == 0
+
+def test_algo():
+    """ saecc: test algorithms equivalence """
+    filein = path_sample_files('input', 'tuxsmall.jpg')
+    filedb = [path_sample_files('output', 'saecc_algo1.db'),
+                path_sample_files('output', 'saecc_algo2.db'),
+                path_sample_files('output', 'saecc_algo3.db'),
+                ]
+    fileres = path_sample_files('results', 'test_structural_adaptive_ecc_test_algo.db')
+    fileout_rec = path_sample_files('output', 'rectemp', True)
+    # For each algorithm
+    for i in range(len(filedb)):
+        # Generate an ecc file
+        assert saecc.main('-i "%s" -d "%s" --ecc_algo=%i -g -f --silent' % (filein, filedb[i], i+1)) == 0
+        # Check file with this ecc algo
+        assert saecc.main('-i "%s" -d "%s" -o "%s" --ecc_algo=%i -c --silent' % (filein, filedb[i], fileout_rec, i+1)) == 0
+    # Check that all generated ecc are the same, whatever the algo (up to 3)
+    startpos1 = next(find_next_entry(filedb[0], get_marker(type=1))) # need to skip the comments, so we detect where the first entrymarker begins
+    for i in range(1, len(filedb)):
+        startpos2 = next(find_next_entry(filedb[i], get_marker(type=1)))
+        assert check_eq_files(filedb[0], filedb[i], startpos1=startpos1, startpos2=startpos2)
+    # Check against expected ecc file
+    startpos1 = next(find_next_entry(filedb[0], get_marker(type=1)))
+    startpos2 = next(find_next_entry(fileres, get_marker(type=1)))
+    assert check_eq_files(filedb[0], fileres, startpos1=startpos1, startpos2=startpos2)
+
+def test_entry_fields():
+    """ saecc: test internal: entry_fields() """
+    ecc = dummy_ecc_file_gen(3)
+    eccf = BytesIO(ecc)
+    ecc_entry_pos = get_next_entry(eccf, get_marker(1), only_coord=True)
+    assert saecc.entry_fields(eccf, ecc_entry_pos, field_delim=get_marker(2)) == {'ecc_field_pos': [150, 195], 'filesize_ecc': b'filesize1_ecc', 'relfilepath_ecc': b'relfilepath1_ecc', 'relfilepath': b'file1.ext', 'filesize': b'filesize1'}
+    ecc_entry_pos = get_next_entry(eccf, get_marker(1), only_coord=True)
+    assert saecc.entry_fields(eccf, ecc_entry_pos, field_delim=get_marker(2)) == {'ecc_field_pos': [272, 362], 'filesize_ecc': b'filesize2_ecc', 'relfilepath_ecc': b'relfilepath2_ecc', 'relfilepath': b'file2.ext', 'filesize': b'filesize2'}
+
+def test_stream_entry_assemble():
+    """ saecc: test internal: stream_entry_assemble() """
+    class Hasher(object):
+        """ Dummy Hasher """
+        def __len__(self):
+            return 32
+    tempfile = path_sample_files('output', 'saecc_stream_entry_assemble.txt')
+    with open(tempfile, 'wb') as tfile:
+        tfile.write(b"Lorem ipsum\nAnd stuff and stuff and stuff\n"*20)
+    ecc = dummy_ecc_file_gen(3)
+    eccf = BytesIO(ecc)
+    ecc_entry_pos = get_next_entry(eccf, get_marker(1), only_coord=True)
+    entry_fields = saecc.entry_fields(eccf, ecc_entry_pos, field_delim=get_marker(2))
+    with open(tempfile, 'rb') as tfile:
+        assert list( saecc.stream_entry_assemble(Hasher(), tfile, eccf, entry_fields, 255, 10, [0.7, 0.5, 0.3]) ) == [{'ecc': b'sh-ecc-entry_\xfe\xff\xfe\xff\xfe\xff\xfe\xff\xfe\xfffile2.ext\xfa\xff\xfa\xff\xfafilesize2\xfa\xff\xfa\xff\xfarelfilepath2_ecc\xfa\xff\xfa\xff\xfafilesize2_ecc\xfa\xff\xfa\xff\xfahash-ecc-entry_hash-ecc-entry_hash-ecc-entry_hash-ecc-entry', 'curpos': 0, 'rate': 0.7, 'ecc_params': {'ecc_size': 149, 'hash_size': 32, 'message_size': 106}, 'ecc_curpos': 150, 'hash': b'hash-ecc-entry_hash-ecc-entry_ha', 'message': b'Lorem ipsum\nAnd stuff and stuff and stuff\nLorem ipsum\nAnd stuff and stuff and stuff\nLorem ipsum\nAnd stuff '}]
+    # TODO: check that several blocks can be assembled, currently we only check one block
+
+def test_stream_compute_ecc_hash():
+    """ saecc: test internal: stream_compute_ecc_hash() and compute_ecc_hash_from_string() """
+    class Hasher(object):
+        """ Dummy Hasher """
+        def hash(self, mes):
+            return "dummyhsh"
+        def __len__(self):
+            return 8
+    n = 20 # aka max_block_size
+    k = 11
+    resilience_rates = [0.7, 0.5, 0.3]
+    instring = b"hello world!"*10
+    tempfile = path_sample_files('output', 'saecc_stream_compute_ecc_hash.txt')
+    with open(tempfile, 'wb') as tfile:
+        tfile.write(instring)
+    eccman = ECCMan(n, k, algo=3)
+    with open(tempfile, 'rb') as tfile:
+        assert list( saecc.stream_compute_ecc_hash(eccman, Hasher(), tfile, n, int(len(instring)/4), resilience_rates) ) == [[b'dummyhsh', bytearray(b'\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'r\x9d\xb7#f\xa3=*\xda\x17WC'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d'), {'ecc_size': 12, 'hash_size': 8, 'message_size': 8}], [b'dummyhsh', bytearray(b'\xab\x8c\xae\r\xbb\x9b\x93\xbd\xd5\x8f'), {'ecc_size': 10, 'hash_size': 8, 'message_size': 10}], [b'dummyhsh', bytearray(b'\xb8S\x1cz\xb2\xeb\x9fu\x19\x83'), {'ecc_size': 10, 'hash_size': 8, 'message_size': 10}], [b'dummyhsh', bytearray(b'\x07\xc4\xce\xe2\xdf\x0b\t\x17,'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'\xd6(og\xb5}\x06\xe3\xd2'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'v\x9dP\x0c\x01\x03\x83Q!'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'\xc4\x12q\xd9\x0fq\xef\xc2\xba'), {'ecc_size': 9, 'hash_size': 8, 'message_size': 11}], [b'dummyhsh', bytearray(b'6\xd0\xe8\xe9\xfe(y\x13'), {'ecc_size': 8, 'hash_size': 8, 'message_size': 12}], [b'dummyhsh', bytearray(b'6\xd0\xe8\xe9\xfe(y\x13'), {'ecc_size': 8, 'hash_size': 8, 'message_size': 12}]]
+    assert saecc.compute_ecc_hash_from_string(instring, eccman, Hasher(), n, resilience_rates[0]) == b'\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC\x8f=\xae\x11\xe1\xf7F\x94A\xb8\x00\x8d\x97\x13\xe8G*\x15\xb5\xb2hn\xdf\x88r\x9d\xb7#f\xa3=*\xda\x17WC'
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity/tests/test_tee.py` & `pyFileFixity-3.1.4/pyFileFixity/tests/test_tee.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,63 +1,63 @@
-from __future__ import print_function
-
-import sys
-import os
-import shutil
-
-from .aux_tests import get_marker, dummy_ecc_file_gen, check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, remove_if_exist
-
-from ..lib.tee import Tee
-
-from ..lib._compat import _StringIO
-
-def setup_module():
-    """ Initialize the tests by emptying the out directory """
-    outfolder = path_sample_files('output')
-    shutil.rmtree(outfolder, ignore_errors=True)
-    create_dir_if_not_exist(outfolder)
-
-def test_tee_file():
-    """ tee: test tee file output """
-    instring1 = b"First line\nSecond line\n"
-    instring2 = b"Third line\n"
-    filelog = path_sample_files('output', 'tee1.log')
-    remove_if_exist(filelog)
-    # Write first string
-    t = Tee(filelog, 'wb', nostdout=True)
-    t.write(instring1, end='')
-    del t # deleting Tee should close the file
-    with open(filelog, 'rb') as fl:
-        res1 = fl.read()
-    assert res1 == instring1
-    # Write second string while appending
-    t2 = Tee(filelog, 'ab', nostdout=True)
-    t2.write(instring2, end='')
-    del t2 # deleting Tee should close the file
-    with open(filelog, 'rb') as fl:
-        res2 = fl.read()
-    assert res2 == instring1+instring2
-
-def test_tee_stdout():
-    """ tee: test tee stdout """
-    instring1 = "First line\nSecond line\n"
-    instring2 = "Third line\n"
-    filelog = path_sample_files('output', 'tee2.log')
-    remove_if_exist(filelog)
-    # Access stdout and memorize the cursor position just before the test
-    sysout = sys.stdout
-    startpos = sysout.tell()
-    # Write first string
-    t = Tee()
-    t.write(instring1, end='')
-    del t # deleting Tee should close the file
-    # Read stdout and check Tee wrote into stdout
-    sysout.seek(startpos)
-    assert sysout.read() == instring1
-    # Write second string
-    t2 = Tee()
-    t2.write(instring2, end='', flush=False)
-    t2.flush() # try to manually flush by the way
-    del t2 # deleting Tee should close the file
-    # Read stdout and check Tee appended the second string into stdout
-    sysout.seek(startpos)
-    assert sysout.read().startswith(instring1+instring2) # sys.stdout appends a newline return at the second writing, don't know why...
+from __future__ import print_function
+
+import sys
+import os
+import shutil
+
+from .aux_tests import get_marker, dummy_ecc_file_gen, check_eq_files, check_eq_dir, path_sample_files, tamper_file, find_next_entry, create_dir_if_not_exist, remove_if_exist
+
+from ..lib.tee import Tee
+
+from ..lib._compat import _StringIO
+
+def setup_module():
+    """ Initialize the tests by emptying the out directory """
+    outfolder = path_sample_files('output')
+    shutil.rmtree(outfolder, ignore_errors=True)
+    create_dir_if_not_exist(outfolder)
+
+def test_tee_file():
+    """ tee: test tee file output """
+    instring1 = b"First line\nSecond line\n"
+    instring2 = b"Third line\n"
+    filelog = path_sample_files('output', 'tee1.log')
+    remove_if_exist(filelog)
+    # Write first string
+    t = Tee(filelog, 'wb', nostdout=True)
+    t.write(instring1, end='')
+    del t # deleting Tee should close the file
+    with open(filelog, 'rb') as fl:
+        res1 = fl.read()
+    assert res1 == instring1
+    # Write second string while appending
+    t2 = Tee(filelog, 'ab', nostdout=True)
+    t2.write(instring2, end='')
+    del t2 # deleting Tee should close the file
+    with open(filelog, 'rb') as fl:
+        res2 = fl.read()
+    assert res2 == instring1+instring2
+
+def test_tee_stdout():
+    """ tee: test tee stdout """
+    instring1 = "First line\nSecond line\n"
+    instring2 = "Third line\n"
+    filelog = path_sample_files('output', 'tee2.log')
+    remove_if_exist(filelog)
+    # Access stdout and memorize the cursor position just before the test
+    sysout = sys.stdout
+    startpos = sysout.tell()
+    # Write first string
+    t = Tee()
+    t.write(instring1, end='')
+    del t # deleting Tee should close the file
+    # Read stdout and check Tee wrote into stdout
+    sysout.seek(startpos)
+    assert sysout.read() == instring1
+    # Write second string
+    t2 = Tee()
+    t2.write(instring2, end='', flush=False)
+    t2.flush() # try to manually flush by the way
+    del t2 # deleting Tee should close the file
+    # Read stdout and check Tee appended the second string into stdout
+    sysout.seek(startpos)
+    assert sysout.read().startswith(instring1+instring2) # sys.stdout appends a newline return at the second writing, don't know why...
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity.egg-info/PKG-INFO` & `pyFileFixity-3.1.4/README.rst`

 * *Files 23% similar despite different names*

```diff
@@ -1,1122 +1,1272 @@
-Metadata-Version: 2.1
-Name: pyFileFixity
-Version: 3.1.1
-Summary: Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing.
-Author-email: Stephen Karl Larroque <lrq3000@gmail.com>
-Maintainer-email: Stephen Karl Larroque <lrq3000@gmail.com>
-License: MIT License
-Project-URL: Homepage, https://github.com/lrq3000/pyFileFixity
-Project-URL: Documentation, https://github.com/lrq3000/pyFileFixity/blob/master/README.rst
-Project-URL: Source, https://github.com/lrq3000/pyFileFixity
-Project-URL: Tracker, https://github.com/lrq3000/pyFileFixity/issues
-Project-URL: Download, https://github.com/lrq3000/pyFileFixity/releases
-Keywords: file,repair,monitor,change,reed-solomon,error,correction,error correction,parity,parity files,parity bytes,data protection,data recovery,file protection,qr codes,qr code
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Environment :: Console
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: MacOS :: MacOS X
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Programming Language :: Python :: 3.12
-Classifier: Programming Language :: Python :: Implementation :: PyPy
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Classifier: Topic :: System :: Archiving
-Classifier: Topic :: System :: Archiving :: Backup
-Classifier: Topic :: System :: Monitoring
-Classifier: Topic :: System :: Recovery Tools
-Classifier: Topic :: Utilities
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: End Users/Desktop
-Classifier: Intended Audience :: Information Technology
-Classifier: Intended Audience :: System Administrators
-Requires-Python: >=3.7
-Description-Content-Type: text/x-rst
-Provides-Extra: test
-Provides-Extra: testmeta
-License-File: LICENSE
-
-pyFileFixity
-============
-
-|PyPI-Status| |PyPI-Versions| |PyPI-Downloads|
-
-|Build-Status| |Coverage|
-
-pyFileFixity provides a suite of open source, cross-platform, easy
-to use and easy to maintain (readable code) to protect and manage data
-for long term storage/archival, and also test the performance of any data protection algorithm.
-
-The project is done in pure-Python to meet those criteria,
-although cythonized extensions are available for core routines to speed up encoding/decoding,
-but always with a pure python specification available so as to allow long term replication.
-
-Here is an example of what pyFileFixity can do:
-
-|Example|
-
-On the left, this is the original image.
-
-At the center, the same image but
-with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
-which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
-Only a few corrupted bytes are enough to make the image looks like totally
-unrecoverable, and yet we are lucky, because the image could be unreadable at all
-if any of the "magic bytes" were to be corrupted!
-
-At the right, the corrupted image was repaired using `header_ecc.py` of pyFileFixity.
-This repaired only the image header (ie, the first part of the file), so only the first
-3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
-the image looks like it's totally repaired! And the best thing is that it only costed the generation
-of a "ecc repair file", which size is only 3.3KB (17% of the original file)!
-
-This works because most files will store the most important information to read them at
-their beginning, also called "file's header", so repairing this part will almost always ensure
-the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
-you can read it).
-
-Of course, you can also protect the whole file, not only the header, using pyFileFixity's
-`structural_adaptive_ecc.py`. You can also detect any corruption using `rfigc.py`.
-
-------------------------------------------
-
-.. contents:: Table of contents
-   :backlinks: top
-
-Quickstart
-----------
-
-Runs on Python 3 up to Python 3.12-dev. PyPy 3 is also supported.
-
-- To install or update on Python 3:
-
-``pip install --upgrade pyfilefixity``
-
-- For Python 2.7, the latest working version was v3.0.2:
-
-``pip install --upgrade pyfilefixity==3.0.2 reedsolo==1.7.0 unireedsolomon==1.0.5``
-
-- Once installed, the suite of tools can be accessed from a centralized interface script called ``pff`` which provides several subcommands, to list them:
-
-``pff --help``
-
-You should see:
-
-::
-
-    usage: pff [-h]
-               {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
-               ...
-
-    positional arguments:
-      {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
-        hash (rfigc)        Check files integrity fast by hash, size, modification date or by data structure integrity.
-        header (header_ecc, hecc)
-                            Protect/repair files headers with error correction codes
-        whole (structural_adaptive_ecc, saecc, protect, repair)
-                            Protect/repair whole files with error correction codes
-        recover (repair_ecc, recc)
-                            Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery
-                            mode (note: this does NOT recover your files, only the ecc files, which may then be used to
-                            recover your files!)
-        dup (replication_repair)
-                            Repair files from multiple copies of various storage mediums using a majority vote
-        restest (resilience_tester)
-                            Run tests to quantify robustness of a file protection scheme (can be used on any, not just
-                            pyFileFixity)
-        filetamper          Tamper files using various schemes
-        speedtest (ecc_speedtest)
-                            Run error correction encoding and decoding speedtests
-
-    options:
-      -h, --help            show this help message and exit
-
-- Every subcommands provide their own more detailed help instructions, eg for the ``hash`` submodule:
-
-``pff hash --help``
-
-- To generate a monitoring database (to later check very fast which files are corrupted, but cannot repair anything but filesystem metadata):
-
-``pff hash -i "your_folder" -d "dbhash.csv" -g -f -l "log.txt"``
-
-Note: this also works for a single file, just replace "your_folder" by "your_file.ext".
-
-- Later, to check which files were corrupted:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s -e errors.csv``
-
-- To use this monitoring database to recover filesystem metadata such as files names and directory layout by filescraping from files contents:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l "log.txt" -o "output_folder" --filescraping_recovery``
-
-- To protect files headers with a file called ``hecc.txt``:
-
-``pff header -i "your_folder" -d "hecc.txt" -l "log.txt" -g -f --ecc_algo 3``
-
-- To repair files headers and store the repaired files in ``output_folder``:
-
-``pff header -i "your_folder" -d "hecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
-
-- To protect whole files with a file called ``ecc.txt``:
-
-``pff whole -i "your_folder" -d "ecc.txt" -l "log.txt" -g -f -v --ecc_algo 3``
-
-- To repair whole files:
-
-``pff whole -i "your_folder" -d "ecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
-
-Note that ``header`` and ``whole`` can also detect corrupted files and even which blocks inside a file, but they are much slower than ``hash``.
-
-- To try to recover a damaged ecc file ``ecc.txt`` using an index file ``ecc.txt.idx`` (index file is generated automatically with ecc.txt):
-
-``pff recovery -i "ecc.txt" --index "ecc.txt.idx" -o "ecc_repaired.txt" -l "log.txt" -v -f``
-
-- To try to recover a damaged ecc file ``ecc.txt`` without an index file (you can tweak the ``-t`` parameter from 0.0 to 1.0, 1.0 producing many false positives):
-
-``pff recovery -i "ecc.txt" -o "ecc_repaired.txt" -l "log.txt" -v -f -t 0.4``
-
-- To repair your files using multiple duplicated copies that you have stored on different mediums:
-
-``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" --report "rlog.csv" -f -v``
-
-- If you have previously generated a rfigc database, you can use it to enhance the replication repair:
-
-``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" -d "dbhash.csv" --report "rlog.csv" -f -v``
-
-- To run tests on your recovery tools, you can make a Makefile-like configuration file and use the Resiliency Tester submodule:
-
-``pff restest -i "your_folder" -o "test_folder" -c "resiliency_tester_config.txt" -m 3 -l "testlog.txt" -f``
-
-- Internally, ``pff restest`` uses ``pff filetamper`` to tamper files with various schemes, but you can also use ``pff filetamper`` directly.
-
-- To run speedtests of encoding/decoding error correction codes on your machine:
-
-``pff speedtest``
-
-- In case the ``pff`` command does not work, it can be replaced with ``python -m pyFileFixity.pff`` .
-
-The problem of long term storage
---------------------------------
-
-Why are data corrupted with time? One sole reason: entropy.
-Entropy refers to the universal tendency for systems to become
-less ordered over time. Data corruption is exactly that: a disorder
-in bits order. In other words: *the Universe hates your data*.
-
-Long term storage is thus a very difficult topic: it's like fighting with
-death (in this case, the death of data). Indeed, because of entropy,
-data will eventually fade away because of various silent errors such as
-bit rot or cosmic rays. pyFileFixity aims to provide tools to detect any data
-corruption, but also fight data corruption by providing repairing tools.
-
-The only solution is to use a principle of engineering that is long
-known and which makes bridges and planes safe: add some **redundancy**.
-
-There are only 2 ways to add redundancy:
-
--  the simple way is to **duplicate** the object (also called replication),
-   but for data storage, this eats up a lot of storage and is not optimal.
-   However, if storage is cheap, then this is a good solution, as it is
-   much faster than encoding with error correction codes. For replication to work,
-   at least 3 duplicates are necessary at all times, so that if one fails, it must
-   replaced asap. As sailors say: "Either bring 1 compass or 3 compasses, but never
-   two, because then you won't know which one is correct if one fails."
-   Indeed, with 3 duplicates, if you frequently monitor their integrity
-   (eg, with hashes), then if one fails, simply do a majority vote:
-   the bit value given by 2 of the duplicates is probably correct.
--  the second way, the optimal tools ever invented to recover
-   from data corruption, are the **error correction codes** (forward
-   error correction), which are a way to smartly produce redundant codes
-   from your data so that you can later repair your data using these
-   additional pieces of information (ie, an ECC generates n blocks for a
-   file cut in k blocks (with k < n), and then the ecc code can rebuild
-   the whole file with (at least) any k blocks among the total n blocks
-   available). In other words, you can correct up to (n-k) erasures. But
-   error correcting codes can also detect and repair automatically where
-   the errors are (fully automatic data repair for you !), but at the
-   cost that you can then only correct (n-k)/2 errors.
-
-Error correction can seem a bit magical, but for a reasonable intuition,
-it can be seen as a way to average the corruption error rate: on
-average, a bit will still have the same chance to be corrupted, but
-since you have more bits to represent the same data, you lower the
-overall chance to lose this bit.
-
-The problem is that most theoretical and pratical works on error
-correcting codes has been done almost exclusively on channel
-transmission (such as 4G, internet, etc.), but not on data storage,
-which is very different for one reason: whereas in a channel we are in a
-spatial scheme (both the sender and the receiver are different entities
-in space but working at the same timescale), in data storage this is a
-temporal scheme: the sender was you storing the data on your medium at
-time t, and the receiver is again you but now retrieving the data at
-time t+x. Thus, the sender does not exist anymore, thus you cannot ask
-the sender to send again some data if it's too much corrupted: in data
-storage, if a data is corrupted, it's lost for good, whereas in channel theory,
-parts of the data can be submitted again if necessary.
-
-Some attempts were made to translate channel theory and error correcting
-codes theory to data storage, the first being Reed-Solomon which spawned
-the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
-devised for use on optical discs to recover from scratches, which was
-necessary for the technology to be usable for consumers. Since then, new
-less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
-fountain codes such as RaptorQ were invented (or rediscovered), but they
-are still marginally researched for data storage.
-
-This project aims to, first, implement easy tools to evaluate strategies
-(filetamper.py) and file fixity (ie, detect if there are corruptions),
-and then the goal is to provide an open and easy framework to use
-different kinds of error correction codes to protect and repair files.
-
-Also, the ecc file specification is made to be simple and resilient to
-corruption, so that you can process it by your own means if you want to,
-without having to study for hours how the code works (contrary to PAR2
-format).
-
-In practice, both approaches are not exclusive, and the best is to
-combine them: protect the most precious data with error correction codes,
-then duplicate them as well as less sensitive data across multiple storage mediums.
-Hence, this suite of data protection tools, just like any other such suite, is not
-sufficient to guarantee your data is protected, you must have an active data curation
-strategy which includes regularly checking your data and replacing copies that are damaged.
-
-For a primer on storage mediums and data protection strategies, see `this post I wrote <https://web.archive.org/web/20220529125543/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`_.
-
-Why not just use RAID ?
------------------------
-
-RAID is clearly insufficient for long-term data storage, and in fact it
-was primarily meant as a cheap way to get more storage (RAID0) or more
-availability (RAID1) of data, not for archiving data, even on a medium
-timescale:
-
--  RAID 0 is just using multiple disks just like a single one, to extend
-   the available storage. Let's skip this one.
--  RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
-   That's completely useless for long term storage: if either disk
-   fails, or if both disks are partially corrupted, you can't know what
-   are the correct data and which aren't. As an old saying goes: "Never
-   take 2 compasses: either take 3 or 1, because if both compasses show
-   different directions, you will never know which one is correct, nor
-   if both are wrong." That's the principle of Triplication.
--  RAID 5 is based on the triplication idea: you have n disks (but least
-   3), and if one fails you can recover n-1 disks (resilient to only 1
-   disk failure, not more).
--  RAID 6 is an extension of RAID 5 which is closer to error-correction
-   since you can correct n-k disks. However, most (all?) currently
-   commercially available RAID6 devices only implements recovery for at
-   most n-2 (2 disks failures).
--  In any case, RAID cannot detect silent errors automatically, thus you
-   either have to regularly scan, or you risk to lose some of your data
-   permanently, and it's far more common than you can expect (eg, with
-   RAID5, it is enough to have 2 silent errors on two disks on the same
-   bit for the bit to be unrecoverable). That's why a limit of only 1 or
-   2 disks failures is just not enough.
-
-On the opposite, ECC can correct n-k disks (or files). You can configure
-n and k however you want, so that for example you can set k = n/2, which
-means that you can recover all your files from only half of them! (once
-they are encoded with an ecc file of course).
-
-There also are new generation RAID solutions, mainly software based,
-such as SnapRAID or ZFS, which allow you to configure a virtual RAID
-with the value n-k that you want. This is just like an ecc file (but a
-bit less flexible, since it's not a file but a disk mapping, so that you
-can't just copy it around or upload it to a cloud backup hosting). In
-addition to recover (n-k) disks, they can also be configured to recover
-from partial, sectors failures inside the disk and not just the whole
-disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
-and James L. Hafner. "SD codes: erasure codes designed for how storage
-systems really fail." FAST. 2013.).
-
-The other reason RAID is not adapted to long-term storage, is that it
-supposes you store your data on hard-drives exclusively. Hard drives
-aren't a good storage medium for the long term, for two reasons:
-
-| 1- they need a regular plug to keep the internal magnetic disks
-  electrified (else the data will just fade away when there's no
-  residual electricity).
-| 2- the reading instrument is directly included and merged with the
-  data (this is the green electronic board you see from the outside, and
-  the internal head). This is good for quick consumer use (don't need to
-  buy another instrument: the HDD can just be plugged and it works), but
-  it's very bad for long term storage, because the reading instrument is
-  bound to fail, and a lot faster than the data can fade away: this
-  means that even if your magnetic disks inside your HDD still holds
-  your data, if the controller board or the head doesn't work anymore,
-  your data is just lost. And a head (and a controller board) are almost
-  impossible to replace, even by professionals, because the pieces are
-  VERY hard to find (different for each HDD production line) and each
-  HDD has some small physical defects, thus it's impossible to reproduce
-  that too (because the head is so close to the magnetic disk that if
-  you try to do that manually you'll probably fail).
-
-In the end, it's a lot better to just separate the storage medium of
-data, with the reading instrument. The medium I advise is optical disks
-(whether it's BluRay, DVD, CD or whatever), because the reading
-instrument is separate, and the technology (laser reflecting on bumps
-and/or pits) is kind of universal, so that even if the technology is
-lost one day (deprecated by newer technologies, so that you can't find
-the reading instrument anymore because it's not sold anymore), you can
-probably emulate a laser using some software to read your optical disk,
-just like what the CAMiLEON project did to recover data from the
-LaserDiscs of the BBC Domesday Project (see Wikipedia).
-
-Applications included
----------------------
-
-The project currently include the following pure-python applications:
-
--  rfigc.py (subcommand: ``hash``), a hash auditing tool, similar to md5deep/hashdeep, to
-   compute a database of your files along with their metadata, so that
-   later you can check if they were changed/corrupted.
-
--  header\_ecc.py (subcommand: ``header``), an error correction code using Reed-Solomon
-   generator/corrector for files headers. The idea is to supplement
-   other more common redundancy tools such as PAR2 (which is quite
-   reliable), by adding more resiliency only on the critical parts of
-   the files: their headers. Using this script, you can significantly
-   higher the chance of recovering headers, which will allow you to at
-   least open the files.
-
--  structural\_adaptive\_ecc.py (subcommand: ``whole``), a variable error correction rate
-   encoder (kind of a generalization of header\_ecc.py). This script
-   allows to generate an ecc file for the whole content of your files,
-   not just the header part, using a variable resilience rate: the
-   header part will be the most protected, then the rest of each file
-   will be progressively encoded with a smaller and smaller resilience
-   rate. The assumption is that important information is stored first,
-   and then data becomes less and less informative (and thus important,
-   because the end of the file describes less important details). This
-   assumption is very true for all compressed kinds of formats, such as
-   JPG, ZIP, Word, ODT, etc...
-
--  repair\_ecc.py (subcommand: ``recovery``), a script to repair the structure (ie, the entry and
-   fields markers/separators) of an ecc file generated by header\_ecc.py
-   or structural\_adaptive\_ecc.py. The goal is to enhance the
-   resilience of ecc files against corruption by ensuring that their
-   structures can be repaired (up to a certain point which is very high
-   if you use an index backup file, which is a companion file that is
-   generated along an ecc file).
-
--  filetamper.py (subcommand: ``filetamper``) is a quickly made file corrupter, it will erase or
-   change characters in the specified file. This is useful for testing
-   your various protecting strategies and file formats (eg: is PAR2
-   really resilient against corruption? Are zip archives still partially
-   extractable after corruption or are rar archives better? etc.). Do
-   not underestimate the usefulness of this tool, as you should always
-   check the resiliency of your file formats and of your file protection
-   strategies before relying on them.
-
--  replication\_repair.py (subcommand: ``dup``) takes advantage of your multiple copies
-   (replications) of your data over several storage mediums to recover
-   your data in case it gets corrupted. The goal is to take advantage of
-   the storage of your archived files into multiple locations: you will
-   necessarily make replications, so why not use them for repair?
-   Indeed, it's good practice to keep several identical copies of your data
-   on several storage mediums, but in case a corruption happens,
-   usually you will just drop the corrupted copies and keep the intacts ones.
-   However, if all copies are partially corrupted, you're stuck. This script
-   aims to take advantage of these multiple copies to recover your data,
-   without generating a prior ecc file. It works simply by reading through all
-   your different copies of your data, and it casts a majority vote over each
-   byte: the one that is the most often occuring will be kept. In engineering,
-   this is a very common strategy used for very reliable systems such as
-   space rockets, and is called "triple-modular redundancy", because you need
-   at least 3 copies of your data for the majority vote to work (but the more the
-   better).
-
--  resiliency\_tester.py (subcommand: ``restest``) allows you to test the robustness of the
-   corruption correction of the scripts provided here (or any other
-   command-line app). You just have to copy the files you want to test inside a
-   folder, and then the script will copy the files into a test tree, then it
-   will automatically corrupt the files randomly (you can change the parameters
-   like block burst and others), then it will run the file repair command-lines
-   you supply and finally some stats about the repairing power will be
-   generated. This allows you to easily and objectively compare different set
-   of parameters, or even different file repair solutions, on the very data
-   that matters to you, so that you can pick the best option for you.
-
--  ecc\_speedtest.py (subcommand: ``speedtest``) is a simple error correction codes
-   encoder/decoder speedtest. It allows to easily change parameters for the test.
-   This allows to assess how fast your machine can encode/decode with the selected
-   parameters, which can be especially useful to plan ahead for how many files you
-   can reasonably plan to protect with error correction codes (which are time consuming).
-
--  DEPRECATED: easy\_profiler.py is just a quick and simple profiling tool to get
-   you started quickly on what should be optimized to get more speed, if
-   you want to contribute to the project feel free to propose a pull
-   request! (Cython and other optimizations are welcome as long as they
-   are cross-platform and that an alternative pure-python implementation
-   is also available).
-
-Note that all tools are primarily made for command-line usage (type
-script.py --help to get extended info about the accepted arguments), but
-you can also use rfigc.py and header\_ecc.py with a GUI by using the
---gui argument (must be the first and only one argument supplied). The
-GUI is provided as-is and minimal work will be done to maintain it (the
-focus will stay on functionality rather than ergonomy).
-
-IMPORTANT: it is CRITICAL that you use the same parameters for
-correcting mode as when you generated the database/ecc files (this is
-true for all scripts in this bundle). Of course, some options must be
-changed: -g must become -c to correct, and --update is a particular
-case. This works this way on purpose for mainly two reasons: first
-because it is very hard to autodetect the parameters from a database
-file alone and it would produce lots of false positives, and secondly
-(the primary reason) is that storing parameters inside the database file
-is highly unresilient against corruption (if this part of the database
-is tampered, the whole becomes unreadable, while if they are stored
-outside or in your own memory, the database file is always accessible).
-Thus, it is advised to write down the parameters you used to generate
-your database directly on the storage media you will store your database
-file on (eg: if it's an optical disk, write the parameters on the cover
-or directly on the disk using a marker), or better memorize them by
-heart. If you forget them, don't panic, the parameters are always stored
-as comments in the header of the generated ecc files, but you should try
-to store them outside of the ecc files anyway.
-
-For users: what's the advantage of pyFileFixity?
-------------------------------------------------
-
-Pros:
-
--  Open application and open specifications under the MIT license (you
-   can do whatever you want with it and tailor it to your needs if you
-   want to, or add better decoding procedures in the future as science
-   progress so that you can better recover your data from your already
-   generated ecc file).
--  Highly reliable file fixity watcher: rfigc.py will tell you without
-   any ambiguity using several attributes if your files have been
-   corrupted or not, and can even check for images if the header is
-   valid (ie: if the file can still be opened).
--  Readable ecc file format (compared to PAR2 and most other similar
-   specifications).
--  Highly resilient ecc file format against corruption (not only are
-   your data protected by ecc, the ecc file is protected too against
-   critical spots, both because there is no header so that each track is
-   independent and if one track is corrupted beyond repair then other
-   ecc tracks can still be read, and a .idx file will be generated to
-   repair the structure of the ecc file to recover all tracks).
--  Very safe and conservative approach: the recovery process checks that
-   the recovery was successful before committing a repaired block.
--  Partial recovery allowed (even if a file cannot be completely
-   recovered, the parts that can will be repaired and then the rest that
-   can't be repaired will be recopied from the corrupted version).
--  Support directory processing: you can encode an ecc file for a whole
-   directory of files (with any number of sub-directories and depth).
--  No limit on the number of files, and it can recursively protect files
-   in a directory tree.
--  Variable resiliency rate and header-only resilience, ensuring that
-   you can always open your files even if partially corrupted (the
-   structure of your files will be saved, so that you can use other
-   softwares to repair beyond if this set of script is not sufficient to
-   totally repair).
--  Support for erasures (null bytes) and even errors-and-erasures, which
-   literally doubles the repair capabilities. To my knowledge, this is
-   the only freely available parity software that supports erasures.
--  Display the predicted total ecc file size given your parameters,
-   and the total time it will take to encode/decode.
--  Your original files are still accessible as they are, protection files
-   such as ecc files live alongside your original data. Contrary to
-   other data protection schemes such as PAR2 which encode the whole
-   data in par archive files that replace your original files and
-   are not readable without decoding.
--  Opensourced under the very permissive MIT licence, do whatever you
-   want!
-
-Cons:
-
--  Cannot protect meta-data, such as folders paths. The paths are
-   stored, but cannot be recovered (yet? feel free to contribute if you
-   know how). Only files are protected. Thus if your OS or your storage
-   medium crashes and truncate a whole directory tree, the directory
-   tree can't be repaired using the ecc file, and thus you can't access
-   the files neither. However, you can use file scraping to extract the
-   files even if the directory tree is lost, and then use RFIGC.py to
-   reorganize your files correctly. There are alternatives, see the
-   chapters below: you can either package all your files in a single
-   archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
-   DVDisaster as an alternative solution, which is an ecc generator with
-   support for directory trees meta-data (but only on optical disks).
--  Can only repair errors and erasures (characters that are replaced by
-   another character), not deletion nor insertion of characters. However
-   this should not happen with any storage medium (truncation can occur
-   if the file bounds is misdetected, in this case pyFileFixity can
-   partially repair the known parts of the file, but cannot recover the
-   rest past the truncation, except if you used a resiliency rate of at
-   least 0.5, in which case any message block can be recreated with only
-   using the ecc file).
--  Cannot recreate a missing file from other available files (except you
-   have set a resilience\_rate at least 0.5), contrary to Parchives
-   (PAR1/PAR2). Thus, you can only repair a file if you still have it
-   (and its ecc file!) on your filesystem. If it's missing, pyFileFixity
-   cannot do anything (yet, this will be implemented in the future).
-
-Note that the tools were meant for data archival (protect files that you
-won't modify anymore), not for system's files watching nor to protect
-all the files on your computer. To do this, you can use a filesystem
-that directly integrate error correction code capacity, such as ZFS.
-
-Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)
-------------------------------------------------------------------------------
-
-Recursively generate or check the integrity of files by MD5 and SHA1
-hashes, size, modification date or by data structure integrity (only for
-images).
-
-This script is originally meant to be used for data archival, by
-allowing an easy way to check for silent file corruption. Thus, this
-script uses relative paths so that you can easily compute and check the
-same redundant data copied on different mediums (hard drives, optical
-discs, etc.). This script is not meant for system files corruption
-notification, but is more meant to be used from times-to-times to check
-up on your data archives integrity (if you need this kind of application,
-see `avpreserve's fixity <https://github.com/avpreserve/fixity>`_).
-
-Example usage
-~~~~~~~~~~~~~
-
--  To generate the database (only needed once):
-
-``pff hash -i "your_folder" -d "dbhash.csv" -g``
-
--  To check:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s``
-
--  To update your database by appending new files:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -u -a``
-
--  To update your database by appending new files AND removing
-   inexistent files:
-
-``pff hash -i "your_folder" -d "dbhash.csv" -u -a -r``
-
-Note that by default, the script is by default in check mode, to avoid
-wrong manipulations. It will also alert you if you generate over an
-already existing database file.
-
-Arguments
-~~~~~~~~~
-
-::
-
-      -h, --help            show a help message and exit
-      -i /path/to/root/folder, --input /path/to/root/folder
-                            Path to the root folder from where the scanning will occ
-    ur.
-      -d /some/folder/databasefile.csv, --database /some/folder/databasefile.csv
-                            Path to the csv file containing the hash informations.
-      -l /some/folder/filename.log, --log /some/folder/filename.log
-                            Path to the log file. (Output will be piped to both the
-    stdout and the log file)
-      -s, --structure_check
-                            Check images structures for corruption?
-      -e /some/folder/errorsfile.csv, --errors_file /some/folder/errorsfile.csv
-                            Path to the error file, where errors at checking will be
-     stored in CSV for further processing by other softwares (such as file repair so
-    ftwares).
-      -m, --disable_modification_date_checking
-                            Disable modification date checking.
-      --skip_missing        Skip missing files when checking (useful if you split yo
-    ur files into several mediums, for example on optical discs with limited capacit
-    y).
-      -g, --generate        Generate the database? (omit this parameter to check ins
-    tead of generating).
-      -f, --force           Force overwriting the database file even if it already e
-    xists (if --generate).
-      -u, --update          Update database (you must also specify --append or --rem
-    ove).
-      -a, --append          Append new files (if --update).
-      -r, --remove          Remove missing files (if --update).
-      
-      --filescraping_recovery          Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.
-      -o, --output          Path to the output folder where to output the files reorganized after --recover_from_filescraping.
-
-Header Error Correction Code script
------------------------------------
-
-This script was made to be used in combination with other more common
-file redundancy generators (such as PAR2, I advise MultiPar). This is an
-additional layer of protection for your files: by using a higher
-resiliency rate on the headers of your files, you ensure that you will
-be probably able to open them in the future, avoiding the "critical
-spots", also called "fracture-critical" in redundancy engineering (where
-if you modify just one bit, your whole file may become unreadable,
-usually bits residing in the headers - in other words, a single blow
-makes the whole thing collapse, just like non-redundant bridges).
-
-An interesting benefit of this approach is that it has a low storage
-(and computational) overhead that scales linearly to the number of
-files, whatever their size is: for example, if we have a set of 40k
-files for a total size of 60 GB, with a resiliency\_rate of 30% and
-header\_size of 1KB (we limit to the first 1K bytes/characters = our
-file header), then, without counting the hash per block and other
-meta-data, the final ECC file will be about 2 \* resiliency\_rate \*
-number\_of\_files \* header\_size = 24.5 MB. This size can be lower if
-there are many files smaller than 1KB. This is a pretty low storage
-overhead to backup the headers of such a big number of files.
-
-The script is pure-python as are its dependencies: it is thus completely
-cross-platform and open source. The default ecc algo
-(ecc_algo=3 uses `reedsolo <https://github.com/tomerfiliba-org/reedsolomon>`_)
-also provides a speed-optimized C-compiled implementation (``creedsolo``) that will be used
-if available for the user's platform, so pyFileFixity should be fast by default.
-Alternatively, it's possible to use a JIT compiler such as PyPy,
-although this means that ``creedsolo`` will not be useable, so PyPy
-may accelerate other functions but slower ecc encoding/decoding.
-
-Structural Adaptive Error Correction Encoder
---------------------------------------------
-
-This script implements a variable error correction rate encoder: each
-file is ecc encoded using a variable resiliency rate -- using a high
-constant resiliency rate for the header part (resiliency rate stage 1,
-high), then a variable resiliency rate is applied to the rest of the
-file's content, with a higher rate near the beginning of the file
-(resiliency rate stage 2, medium) which progressively decreases until
-the end of file (resiliency rate stage 3, the lowest).
-
-The idea is that the critical parts of files usually are placed at the
-top, and data becomes less and less critical along the file. What is
-meant by critical is both the critical spots (eg: if you tamper only one
-character of a file's header you have good chances of losing your entire
-file, ie, you cannot even open it) and critically encoded information
-(eg: archive formats usually encode compressed symbols as they go along
-the file, which means that the first occurrence is encoded, and then the
-archive simply writes a reference to the symbol. Thus, the first
-occurrence is encoded at the top, and subsequent encoding of this same
-data pattern will just be one symbol, and thus it matters less as long
-as the original symbol is correctly encoded and its information
-preserved, we can always try to restore the reference symbols later).
-Moreover, really redundant data will be placed at the top because they
-can be reused a lot, while data that cannot be too much compressed will
-be placed later, and thus, corruption of this less compressed data is a
-lot less critical because only a few characters will be changed in the
-uncompressed file (since the data is less compressed, a character change
-on the not-so-much compressed data won't have very significant impact on
-the uncompressed data).
-
-This variable error correction rate should allow to protect more the
-critical parts of a file (the header and the beginning of a file, for
-example in compressed file formats such as zip or jpg this is where the
-most importantly strings are encoded) for the same amount of storage as
-a standard constant error correction rate.
-
-Of course, you can set the resiliency rate for each stage to the values
-you want, so that you can even do the opposite: setting a higher
-resiliency rate for stage 3 than stage 2 will produce an ecc that is
-greater towards the end of the contents of your files.
-
-Furthermore, the currently designed format of the ecc file would allow
-two things that are not available in all current file ecc generators
-such as PAR2:
-
-1. it allows to partially repair a file, even if not all
-the blocks can be corrected (in PAR2, a file is repaired only if all
-blocks can be repaired, which is a shame because there are still other
-blocks that could be repaired and thus produce a less corrupted file) ;
-
-2. the ecc file format is quite simple and readable, easy to process by
-any script, which would allow other softwares to also work on it (and it
-was also done in this way to be more resilient against error
-corruptions, so that even if an entry is corrupted, other entries are
-independent and can maybe be used, thus the ecc is very error tolerant.
-This idea was implemented in repair\_ecc.py but it could be extended,
-especially if you know the pattern of the corruption).
-
-The script structural-adaptive-ecc.py implements this idea, which can be
-seen as an extension of header-ecc.py (and in fact the idea was the
-other way around: structural-adaptive-ecc.py was conceived first but was
-too complicated, then header-ecc.py was implemented as a working
-lessened implementation only for headers, and then
-structural-adaptive-ecc.py was finished using header-ecc.py code
-progress). It works, it was a quite well tested for my own needs on
-datasets of hundred of GB, but it's not foolproof so make sure you test
-the script by yourself to see if it's robust enough for your needs (any
-feedback about this would be greatly appreciated!).
-
-ECC Algorithms
---------------
-
-You can specify different ecc algorithms using the ``--ecc_algo`` switch.
-
-For the moment, only Reed-Solomon is implemented, but it's universal
-so you can modify its parameters in lib/eccman.py.
-
-Two Reed-Solomon codecs are available, they are functionally equivalent
-and thoroughly unit tested.
-
--  ``--ecc_algo 1``: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
-   This is the slowest implementation (but also the most easy code to understand).
--  ``--ecc_algo 2``: same as algo 1 but with a faster functions.
--  ``--ecc_algo 3``: use the second codec, which is the fastest.
-   The generated ECC will be compatible with algo 1 and 2.
--  ``--ecc_algo 4``: also use the second, fastest RS codec, but
-   with different parameters (US FAA ADSB UAT RS FEC norm),
-   thus the generated ECC won't be compatible with algo 1 to 3.
-   But do not be scared, the ECC will work just the same.
-
-Note about speed: Also, use a smaller --max\_block\_size to greatly
-speedup the operations! That's the trick used to compute very quickly RS
-ECC on optical discs. You give up a bit of resiliency of course (because
-blocks are smaller, thus you protect a smaller number of characters per
-ECC. In the end, this should not change much about real resiliency, but
-in case you get a big bit error burst on a contiguous block, you may
-lose a whole block at once. That's why using RS255 is better, but it's
-very time consuming. However, the resiliency ratios still hold, so for
-any other case of bit-flipping with average-sized bursts, this should
-not be a problem as long as the size of the bursts is smaller than an
-ecc block.)
-
-In case of a catastrophic event
--------------------------------
-
-TODO: write more here
-
-In case of a catastrophic event of your data due to the failure of your
-storage media (eg: your hard drive crashed), then follow the following
-steps:
-
-1- use dd\_rescue to make a full bit-per-bit verbatim copy of your drive
-before it dies. The nice thing with dd\_rescue is that the copy is
-exact, and also that it can retries or skip in case of bad sectors (it
-won't crash on your suddenly at half the process).
-
-2- Use testdisk to restore partition or to copy files based on partition
-filesystem informations.
-
-3- If you could not recover your files, you can try file scraping using
-`photorec <http://www.cgsecurity.org/wiki/PhotoRec>`_ or
-`plaso  <http://plaso.kiddaland.net/>`_ other similar tools as
-a last resort to extract data based only from files content (no filename,
-often uncorrect filetype, file boundaries may be wrong so some data
-may be cut off, etc.).
-
-4- If you used pyFileFixity before the failure of your storage media,
-you can then use your pre-computed databases to check that files are
-intact (rfigc.py) and if they aren't, you can recover them (using
-header\_ecc.py and structural\_adaptive\_ecc.py). It can also help if
-you recovered your files via data scraping, because your files will be
-totally unorganized, but you can use a previously generated database
-file to recover the full names and directory tree structure using
-rfigc.py --filescraping\_recover.
-
-Also, you can try to fix some of your files using specialized repairing
-tools (but remember that such tool cannot guarantee you the same
-recovering capacity as an error correction code - and in addition, error
-correction code can tell you when it has recovered successfully). For
-example:
-
--  for tar files, you can use `fixtar <https://github.com/BestSolution-at/fixtar>`_.
-   Similar tools (but older): `tarfix <http://www.dmst.aueb.gr/dds/sw/unix/tarfix/>`_
-   and `tar-repair <https://www.datanumen.com/tar-repair/>`_.
--  for RAID mounting and recovery, you can use "Raid faster - recover
-   better" (rfrb) tool by Sabine Seufert and Christian Zoubek:
-   https://github.com/lrq3000/rfrb
--  if your unicode strings were mangled (ie, you see weird symbols),
-   try this script that will automatically demangle them:
-   https://github.com/LuminosoInsight/python-ftfy
--  to repair tabular (2D) data such as .csv, try
-   `Carpenter <https://pypi.python.org/pypi/Carpenter/>`_.
--  tool to identify corrupted files in ddrescue images: 
-   `ddrescue-ffile <https://github.com/Salamek/ddrescue-ffile>`_
-
-Protecting directory tree meta-data
------------------------------------
-
-One main current limitation of pyFileFixity is that it cannot protect
-the directory tree meta-data. This means that in the worst case, if a
-silent error happens on the inode pointing to the root directory that
-you protected with an ecc, the whole directory will vanish, and all the
-files inside too. In less worst cases, sub-directories can vanish, but
-it's still pretty bad, and since the ecc file doesn't store any
-information about inodes, you can't recover the full path.
-
-The inability to store these meta-data is because of two choices in the
-design: 1- portability: we want the ecc file to work even if we move the
-root directory to another place or another storage medium (and of
-course, the inode would change), 2- cross-platform compatibility:
-there's no way to get and store directory meta-data for all platforms,
-but of course we could implement specific instructions for each main
-platform, so this point is not really a problem.
-
-To workaround this issue (directory meta-data are critical spots), other
-softwares use a one-time storage medium (ie, writing your data along
-with generating and writing the ecc). This way, they can access at
-the bit level the inode info, and they are guaranted that the inodes
-won't ever change. This is the approach taken by DVDisaster: by using
-optical mediums, it can compute inodes that will be permanent, and thus
-also encode that info in the ecc file. Another approach is to create a
-virtual filesystem specifically to store just your files, so that you
-manage the inode yourself, and you can then copy the whole filesystem
-around (which is really just a file, just like a zip file - which can
-also be considered as a mini virtual file system in fact) like
-`rsbep <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`_.
-
-Here the portability principle of pyFileFixity prevents this approach.
-But you can mimic this workaround on your hard drive for pyFileFixity to
-work: you just need to package all your files into one file. This way,
-you sort of create a virtual file system: inside the archive, files and
-directories have meta-data just like in a filesystem, but from the
-outside it's just one file, composed of bytes that we can just encode to
-generate an ecc file - in other words, we removed the inodes portability
-problem, since this meta-data is stored relatively inside the archive,
-the archive manage it, and we can just encode this info like any other
-stream of data! The usual way to make an archive from several files is
-to use TAR, but this will generate a solid archive which will prevent
-partial recovery. An alternative is to use DAR, which is a non-solid
-archive version of TAR, with lots of other features too. If you also
-want to compress, you can just use ZIP (with DEFLATE algorithm) your
-files (this also generates a non-solid archive). You can then use
-pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
-will then protect both your files just like before and the directories
-meta-data too now.
-
-Tools like pyFileFixity (or which can be used as complements)
--------------------------------------------------------------
-
-Here are some tools with a similar philosophy to pyFileFixity, which you
-can use if they better fit your needs, either as a replacement of
-pyFileFixity or as a complement (pyFileFixity can always be used to
-generate an ecc file):
-
--  `DAR (Disk ARchive) <http://dar.linux.free.fr/>`__: similar to tar
-   but non-solid thus allows for partial recovery and per-file access,
-   plus it saves the directory tree meta-data -- see catalog isolation
-   -- plus it can handle error correction natively using PAR2 and
-   encryption. Also supports incremental backup, thus it's a very nice
-   versatile tool. Crossplatform and opensource.
--  `DVDisaster <http://dvdisaster.net/>`__: error correction at the bit
-   level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
-   it also protects directory tree meta-data and is resilient to
-   corruption (v2 still has some critical spots but v3 won't have any).
--  rsbep tool that is part of dvbackup package in Debian: allows to
-   generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
-   for your backups, if you're on unix or using cygwin.
--  `rsbep modification by Thanassis
-   Tsiodras <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`__:
-   enhanced rsbep to avoid critical spots and faster speed. Also
-   includes a "freeze" script to encode your files into a virtual
-   filesystem (using Python/FUSE) so that even meta-data such as
-   directory tree are fully protected by the ecc. Great script, but not
-   maintained, it needs some intensive testing by someone knowledgeable
-   to guarantee this script is reliable enough for production.
--  Parchive (PAR1, PAR2, MultiPar): well known error correction file
-   generator. The big advantage of Parchives is that an ecc block
-   depends on multiple files: this allows to completely reconstruct a
-   missing file from scratch using files that are still available. Works
-   good for most people, but most available Parchive generators are not
-   satisfiable for me because 1- they do not allow to generate an ecc
-   for a directory tree recursively (except MultiPar, and even if it is
-   allowed in the PAR2 specs), 2- they can be very slow to generate
-   (even with multiprocessor extensions, because the galois field is
-   over 2^16 instead of 2^8, which is very costly), 3- the spec is not
-   very resilient to errors and tampering over the ecc file, as it
-   assumes the ecc file won't be corrupted (I also tested, it's still a
-   bit resilient, but it could be a lot more with some tweaking of the
-   spec), 4- it doesn't allow for partial recovery (recovering blocks
-   that we can and pass the others that are unrecoverable): with PAR2, a
-   file can be restored fully or it cannot be at all.
--  Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
-   create non-solid archives which are readable by most computers
-   (ubiquitous algorithm). Non-solid archive means that a zip file can
-   still unzip correct files even if it is corrupted, because files are
-   encoded in blocks, and thus even if some blocks are corrupted, the
-   decoding can happen. A `fast implementation with enhanced compression
-   is available in pure Go <https://github.com/klauspost/compress>`__
-   (good for long storage).
--  TestDisk: for file scraping, when nothing else worked.
--  dd\_rescue: for disk scraping (allows to forcefully read a whole disk
-   at the bit level and copy everything it can, passing bad sector with
-   options to retry them later on after a first full pass over the
-   correct sectors).
--  ZFS: a file system which includes ecc correction directly. The whole
-   filesystem, including directory tree meta-data, are protected. If you
-   want ecc protection on your computer for all your files, this is the
-   way to go.
--  Encryption: technically, you can encrypt your files without losing
-   too much redundancy, as long as you use an encryption scheme that is
-   block-based such as DES: if one block gets corrupted, it won't be
-   decryptable, but the rest of the files' encrypted blocks should be
-   decryptable without any problem. So encrypting with such algorithms
-   leads to similar files as non-solid archives such as deflate zip. Of
-   course, for very long term storage, it's better to avoid encryption
-   and compression (because you raise the information contained in a
-   single block of data, thus if you lose one block, you lose more
-   data), but if it's really necessary to you, you can still maintain
-   high chances of recovering your files by using block-based
-   encryption/compression (note: block-based encryption can
-   be seen as the equivalent of non-solid archives for compression,
-   because the data is compressed/encrypted in independent blocks,
-   thus allowing partial uncompression/decryption).
--  `SnapRAID <http://snapraid.sourceforge.net/>`__
--  `par2ools <https://github.com/jmoiron/par2ools>`__: a set of
-   additional tools to manage par2 archives
--  `Checkm <https://pypi.python.org/pypi/Checkm/0.4>`__: a tool similar
-   to rfigc.py
--  `BagIt <https://en.wikipedia.org/wiki/BagIt>`__ with two python
-   implementations `here <https://pypi.python.org/pypi/pybagit/>`__ and
-   `here <https://pypi.python.org/pypi/bagit/>`__: this is a file
-   packaging format for sharing and storing archives for long term
-   preservation, it just formalizes a few common procedures and meta
-   data that are usually added to files for long term archival (such as
-   MD5 digest).
--  `RSArmor <https://github.com/jap/rsarm>`__ a tool based on
-   Reed-Solomon to encode binary data files into hexadecimal, so that
-   you can print the characters on paper. May be interesting for small
-   datasets (below 100 MB).
--  `Ent <https://github.com/lsauer/entropy>`__ a tool to analyze the
-   entropy of your files. Can be very interesting to optimize the error
-   correction algorithm, or your compression tools.
--  `HashFS <https://pypi.python.org/pypi/hashfs/>`_ is a non-redundant,
-   duplication free filesystem, in Python. **Data deduplication** is very
-   important for large scale long term storage: since you want your data
-   to be redundant, this means you will use an additional storage space
-   for your redundant copies that will be proportional to your original data.
-   Having duplicated data will consume more storage and more processing
-   time, for no benefit. That's why it's a good idea to deduplicate your data
-   prior to create redundant copies: this will be faster and save you money.
-   Deduplication can either be done manually (by using duplicates removers)
-   or systematically and automatically using specific filesystems such as
-   zfs (with deduplication enabled) or hashfs.
--  Paper as a storage medium: paper is not a great storage medium,
-   because it has low storage density (ie, you can only store at most 
-   about 100 KB) and it can also degrade just like other storage mediums,
-   but you cannot check that automatically since it's not digital. However,
-   if you are interested, here are a few softwares that do that:
-   `Paper key <http://en.wikipedia.org/wiki/Paper_key>`_,
-   `Paperbak <http://www.ollydbg.de/Paperbak/index.html>`_,
-   `Optar <http://ronja.twibright.com/optar/>`_,
-   `dpaper <https://github.com/penma/dpaper>`_,
-   `QR Backup <http://blog.liw.fi/posts/qr-backup/>`_,
-   `QR Backup (another) <http://blog.shuningbian.net/2009/10/qrbackup.php>`_,
-   `QR Backup (again another) <http://git.pictorii.com/index.php?p=qrbackup.git&a=summary>`_,
-   `QR Backup (again) <http://hansmi.ch/software/qrbackup>`_,
-   `and finally a related paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&rep=rep1&type=pdf>`_.
--  AVPreserve tools, most notably `fixity <https://github.com/avpreserve/fixity>`_ 
-   to monitor for file changes (similarly to rfigc, but actively as a daemon)
-   and `interstitial <https://github.com/avpreserve/interstitial>`_ to detect
-   interstitial errors in audio digitization workflows (great to ensure you
-   correctly digitized a whole audio file into WAV without any error).
-
-FAQ
----
-
--  Can I compress my data files and my ecc file?
-
-As a rule of thumb, you should ALWAYS keep your ecc file in clear
-text, so under no compression nor encryption. This is because in case
-the ecc file gets corrupted, if compressed/encrypted, the
-decompression/decrypting of the corrupted parts may completely flaw
-the whole structure of the ecc file.
-
-Your data files, that you want to protect, *should* remain in clear
-text, but you may choose to compress them if it drastically reduces
-the size of your files, and if you raise the resilience rate of your
-ecc file (so compression may be a good option if you have an
-opportunity to trade the file size reduction for more ecc file
-resilience). Also, make sure to choose a non-solid compression
-algorithm like DEFLATE (zip) so that you can still decode correct
-parts even if some are corrupted (else with a solid archive, if one
-byte is corrupted, the whole archive may become unreadable).
-
-However, in the case that you compress your files, you should generate
-the ecc file only *after* compression, so that the ecc file applies to
-the compressed archive instead of the uncompressed files, else you
-risk being unable to correct your files because the uncompression of
-corrupted parts may output gibberish, and length extended corrupted
-parts (and if the size is different, Reed-Solomon will just freak
-out).
-
--  Can I encrypt my data files and my ecc file ?
-
-NEVER encrypt your ecc file, this is totally useless and
-counterproductive.
-
-You can encrypt your data files, but choose a non-solid algorithm
-(like AES if I'm not mistaken) so that corrupted parts do not prevent
-the decoding of subsequent correct parts. Of course, you're lowering a
-bit your chances of recovering your data files by encrypting them (the
-best chance to keep data for the long term is to keep them in clear
-text), but if it's really necessary, using a non-solid encrypting
-scheme is a good compromise.
-
-You can generate an ecc file on your encrypted data files, thus
-*after* encryption, and keep the ecc file in clear text (never encrypt
-nor compress it). This is not a security risk at all since the ecc
-file does not give any information on the content inside your
-encrypted files, but rather just redundant info to correct corrupted
-bytes (however if you generate the ecc file on the data files before
-encryption, then it's clearly a security risk, and someone could
-recover your data without your permission).
-
-- What medium should I use to store my data?
-
-The details are long and a bit complicated (I may write a complete article
-about it in the future), but the tl;dr answer is that you should use *optical disks*,
-because it decouples the storage medium and the reading hardware
-(eg, at the opposite we have hard drives, which contains both the reading
-hardware and the storage medium, so if one fails, you lose both)
-and because it's most likely future-proof (you only need a laser, which
-is universal, the laser's parameters can always be tweaked).
-
-From scientific studies, it seems that, at the time of writing this (2015),
-BluRay HTL disks are the most resilient against environmental degradation.
-To raise the duration, you can also put optical disks in completely opaque boxes
-(to avoid light degradation) and in addition you can put any storage medium
-(not only optical disks, but also hard drives and anything really) in
-*completely* air-tight and water-tight bags or box and put in a fridge or a freezer.
-This is a law of nature: lower the temperature, lower will be the entropy, in other
-words lower will be the degradation over time. It works the same with digital data.
-
-- What file formats are the most recoverable?
-
-It's difficult to advise a specific format. What we can do is advise the characteristics
-of a good file format:
-
-  * future-proof (should be readable in the future).
-  * non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn't cause a problem to the decoding of other blocks).
-  * open source implementation available.
-  * minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).
-  * No magic bytes or header importance (ie, corrupting the header won't prevent opening the file).
-
-There are a few studies about the most resilient file formats, such as:
-
-  * `"Just one bit in a million: On the effects of data corruption in files" by Volker Heydegger <http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1>`_.
-  * `"Analysing the impact of file formats on data integrity" by Volker Heydegger <http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf>`_.
-  * `"A guide to formats", by The UK national archives <http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf>`_ (you want to look at the Recoverability entry in each table).
-
-- What is Reed-Solomon?
-
-If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon
-
-Also, you may want to read the following resources:
-
-  * "`Reed-Solomon codes for coders <https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders>`_", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.
-  * "Algebraic codes for data transmission", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books <https://books.google.fr/books?id=eQs2i-R9-oYC&lpg=PR11&ots=atCPQJm3OJ&dq=%22Algebraic%20codes%20for%20data%20transmission%22%2C%20Blahut%2C%20Richard%20E.%2C%202003%2C%20Cambridge%20university%20press.&lr&hl=fr&pg=PA193#v=onepage&q=%22Algebraic%20codes%20for%20data%20transmission%22,%20Blahut,%20Richard%20E.,%202003,%20Cambridge%20university%20press.&f=false>`_.
-
-
-.. |Example| image:: https://raw.githubusercontent.com/lrq3000/pyFileFixity/master/tux-example.jpg
-   :scale: 60 %
-   :alt: Image corruption and repair example
-.. |PyPI-Status| image:: https://img.shields.io/pypi/v/pyfilefixity.svg
-   :target: https://pypi.org/project/pyfilefixity
-.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/pyfilefixity.svg?logo=python&logoColor=white
-   :target: https://pypi.org/project/pyfilefixity
-.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/pyfilefixity.svg?label=pypi%20downloads&logo=python&logoColor=white
-   :target: https://pypi.org/project/pyfilefixity
-.. |Build-Status| image:: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml/badge.svg?event=push
-   :target: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml
-.. |Coverage| image:: https://codecov.io/github/lrq3000/pyFileFixity/coverage.svg?branch=master
-   :target: https://codecov.io/github/lrq3000/pyFileFixity?branch=master
+pyFileFixity
+============
+
+|PyPI-Status| |PyPI-Versions| |PyPI-Downloads|
+
+|Build-Status| |Coverage|
+
+pyFileFixity provides a suite of open source, cross-platform, easy
+to use and easy to maintain (readable code) to protect and manage data
+for long term storage/archival, and also test the performance of any data protection algorithm.
+
+The project is done in pure-Python to meet those criteria,
+although cythonized extensions are available for core routines to speed up encoding/decoding,
+but always with a pure python specification available so as to allow long term replication.
+
+Here is an example of what pyFileFixity can do:
+
+|Example|
+
+On the left, this is the original image.
+
+At the center, the same image but
+with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
+which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
+Only a few corrupted bytes are enough to make the image looks like totally
+unrecoverable, and yet we are lucky, because the image could be unreadable at all
+if any of the "magic bytes" were to be corrupted!
+
+At the right, the corrupted image was repaired using ``pff header`` command of pyFileFixity.
+This repaired only the image header (ie, the first part of the file), so only the first
+3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
+the image looks indistinguishable from the untampered original! And the best thing is that
+it only costed the generation of a "ecc repair file" for the header, which size is only a
+constant 3.3KB per file, regardless of the protected file's size!
+
+This works because most files will store the most important information to read them at
+their beginning, also called "file's header", so repairing this part will almost always ensure
+the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
+you can read it). This works especially well for images, compressed files, formatted documents such as
+DOCX and ODT, etc.
+
+Of course, you can also protect the whole file, not only the header, using pyFileFixity's
+``pff whole`` command. You can also detect any corruption using ``pff hash``.
+
+------------------------------------------
+
+.. contents:: Table of contents
+   :backlinks: top
+
+Quickstart
+----------
+
+Runs on Python 3 up to Python 3.12-dev. PyPy 3 is also supported.
+
+- To install or update on Python 3:
+
+``pip install --upgrade pyfilefixity``
+
+- For Python 2.7, the latest working version was v3.0.2:
+
+``pip install --upgrade pyfilefixity==3.0.2 reedsolo==1.7.0 unireedsolomon==1.0.5``
+
+- Once installed, the suite of tools can be accessed from a centralized interface script called ``pff`` which provides several subcommands, to list them:
+
+``pff --help``
+
+You should see:
+
+::
+
+    usage: pff [-h]
+               {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
+               ...
+
+    positional arguments:
+      {hash,rfigc,header,header_ecc,hecc,whole,structural_adaptive_ecc,saecc,protect,repair,recover,repair_ecc,recc,dup,replication_repair,restest,resilience_tester,filetamper,speedtest,ecc_speedtest}
+        hash (rfigc)        Check files integrity fast by hash, size, modification date or by data structure integrity.
+        header (header_ecc, hecc)
+                            Protect/repair files headers with error correction codes
+        whole (structural_adaptive_ecc, saecc, protect, repair)
+                            Protect/repair whole files with error correction codes
+        recover (repair_ecc, recc)
+                            Utility to try to recover damaged ecc files using a failsafe mechanism, a sort of recovery
+                            mode (note: this does NOT recover your files, only the ecc files, which may then be used to
+                            recover your files!)
+        dup (replication_repair)
+                            Repair files from multiple copies of various storage mediums using a majority vote
+        restest (resilience_tester)
+                            Run tests to quantify robustness of a file protection scheme (can be used on any, not just
+                            pyFileFixity)
+        filetamper          Tamper files using various schemes
+        speedtest (ecc_speedtest)
+                            Run error correction encoding and decoding speedtests
+
+    options:
+      -h, --help            show this help message and exit
+
+- Every subcommands provide their own more detailed help instructions, eg for the ``hash`` submodule:
+
+``pff hash --help``
+
+- To generate a monitoring database (to later check very fast which files are corrupted, but cannot repair anything but filesystem metadata):
+
+``pff hash -i "your_folder" -d "dbhash.csv" -g -f -l "log.txt"``
+
+Note: this also works for a single file, just replace "your_folder" by "your_file.ext".
+
+- To update this monitoring database (check for new files, but does not remove files that do not exist anymore - replace ``--append`` with ``--remove`` for the latter):
+
+``pff hash -i "your_folder -d "dbhash.csv" --update --append``
+
+- Later, to check which files were corrupted:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s -e errors.csv``
+
+- To use this monitoring database to recover filesystem metadata such as files names and directory layout by filescraping from files contents:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l "log.txt" -o "output_folder" --filescraping_recovery``
+
+- To protect files headers with a file called ``hecc.txt``:
+
+``pff header -i "your_folder" -d "hecc.txt" -l "log.txt" -g -f --ecc_algo 3``
+
+- To repair files headers and store the repaired files in ``output_folder``:
+
+``pff header -i "your_folder" -d "hecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
+
+- To protect whole files with a file called ``ecc.txt``:
+
+``pff whole -i "your_folder" -d "ecc.txt" -l "log.txt" -g -f -v --ecc_algo 3``
+
+- To repair whole files:
+
+``pff whole -i "your_folder" -d "ecc.txt" -o "output_folder" -l "log.txt" -c -v --ecc_algo 3``
+
+Note that ``header`` and ``whole`` can also detect corrupted files and even which blocks inside a file, but they are much slower than ``hash``.
+
+- To try to recover a damaged ecc file ``ecc.txt`` using an index file ``ecc.txt.idx`` (index file is generated automatically with ecc.txt):
+
+``pff recovery -i "ecc.txt" --index "ecc.txt.idx" -o "ecc_repaired.txt" -l "log.txt" -v -f``
+
+- To try to recover a damaged ecc file ``ecc.txt`` without an index file (you can tweak the ``-t`` parameter from 0.0 to 1.0, 1.0 producing many false positives):
+
+``pff recovery -i "ecc.txt" -o "ecc_repaired.txt" -l "log.txt" -v -f -t 0.4``
+
+- To repair your files using multiple duplicated copies that you have stored on different mediums:
+
+``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" --report "rlog.csv" -f -v``
+
+- If you have previously generated a rfigc database, you can use it to enhance the replication repair:
+
+``pff dup -i "path/to/dir1" "path/to/dir2" "path/to/dir3" -o "path/to/output" -d "dbhash.csv" --report "rlog.csv" -f -v``
+
+- To run tests on your recovery tools, you can make a Makefile-like configuration file and use the Resiliency Tester submodule:
+
+``pff restest -i "your_folder" -o "test_folder" -c "resiliency_tester_config.txt" -m 3 -l "testlog.txt" -f``
+
+- Internally, ``pff restest`` uses ``pff filetamper`` to tamper files with various schemes, but you can also use ``pff filetamper`` directly.
+
+- To run speedtests of encoding/decoding error correction codes on your machine:
+
+``pff speedtest``
+
+- In case the ``pff`` command does not work, it can be replaced with ``python -m pyFileFixity.pff`` .
+
+The problem of long term storage
+--------------------------------
+
+Why are data corrupted with time? One sole reason: entropy.
+Entropy refers to the universal tendency for systems to become
+less ordered over time. Data corruption is exactly that: a disorder
+in bits order. In other words: *the Universe hates your data*.
+
+Long term storage is thus a very difficult topic: it's like fighting with
+death (in this case, the death of data). Indeed, because of entropy,
+data will eventually fade away because of various silent errors such as
+bit rot or cosmic rays. pyFileFixity aims to provide tools to detect any data
+corruption, but also fight data corruption by providing repairing tools.
+
+The only solution is to use a principle of engineering that is long
+known and which makes bridges and planes safe: add some **redundancy**.
+
+There are only 2 ways to add redundancy:
+
+-  the simple way is to **duplicate** the object (also called replication),
+   but for data storage, this eats up a lot of storage and is not optimal.
+   However, if storage is cheap, then this is a good solution, as it is
+   much faster than encoding with error correction codes. For replication to work,
+   at least 3 duplicates are necessary at all times, so that if one fails, it must
+   replaced asap. As sailors say: "Either bring 1 compass or 3 compasses, but never
+   two, because then you won't know which one is correct if one fails."
+   Indeed, with 3 duplicates, if you frequently monitor their integrity
+   (eg, with hashes), then if one fails, simply do a majority vote:
+   the bit value given by 2 of the duplicates is probably correct.
+-  the second way, the optimal tools ever invented to recover
+   from data corruption, are the **error correction codes** (forward
+   error correction), which are a way to smartly produce redundant codes
+   from your data so that you can later repair your data using these
+   additional pieces of information (ie, an ECC generates n blocks for a
+   file cut in k blocks (with k < n), and then the ecc code can rebuild
+   the whole file with (at least) any k blocks among the total n blocks
+   available). In other words, you can correct up to (n-k) erasures. But
+   error correcting codes can also detect and repair automatically where
+   the errors are (fully automatic data repair for you !), but at the
+   cost that you can then only correct (n-k)/2 errors.
+
+Error correction can seem a bit magical, but for a reasonable intuition,
+it can be seen as a way to average the corruption error rate: on
+average, a bit will still have the same chance to be corrupted, but
+since you have more bits to represent the same data, you lower the
+overall chance to lose this bit.
+
+The problem is that most theoretical and pratical works on error
+correcting codes has been done almost exclusively on channel
+transmission (such as 4G, internet, etc.), but not on data storage,
+which is very different for one reason: whereas in a channel we are in a
+spatial scheme (both the sender and the receiver are different entities
+in space but working at the same timescale), in data storage this is a
+temporal scheme: the sender was you storing the data on your medium at
+time t, and the receiver is again you but now retrieving the data at
+time t+x. Thus, the sender does not exist anymore, thus you cannot ask
+the sender to send again some data if it's too much corrupted: in data
+storage, if a data is corrupted, it's lost for good, whereas in channel theory,
+parts of the data can be submitted again if necessary.
+
+Some attempts were made to translate channel theory and error correcting
+codes theory to data storage, the first being Reed-Solomon which spawned
+the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
+devised for use on optical discs to recover from scratches, which was
+necessary for the technology to be usable for consumers. Since then, new
+less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
+fountain codes such as RaptorQ were invented (or rediscovered), but they
+are still marginally researched for data storage.
+
+This project aims to, first, implement easy tools to evaluate strategies
+(filetamper.py) and file fixity (ie, detect if there are corruptions),
+and then the goal is to provide an open and easy framework to use
+different kinds of error correction codes to protect and repair files.
+
+Also, the ecc file specification is made to be simple and resilient to
+corruption, so that you can process it by your own means if you want to,
+without having to study for hours how the code works (contrary to PAR2
+format).
+
+In practice, both approaches are not exclusive, and the best is to
+combine them: protect the most precious data with error correction codes,
+then duplicate them as well as less sensitive data across multiple storage mediums.
+Hence, this suite of data protection tools, just like any other such suite, is not
+sufficient to guarantee your data is protected, you must have an active (but infrequent and hence not time consuming)
+data curation strategy that includes regularly checking your data and replacing copies that are damaged every few years.
+
+For a primer on storage mediums and data protection strategies, see `this post I wrote <https://web.archive.org/web/20220529125543/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`_.
+
+Why not just use RAID ?
+-----------------------
+
+RAID is clearly insufficient for long-term data storage, and in fact it
+was primarily meant as a cheap way to get more storage (RAID0) or more
+availability (RAID1) of data, not for archiving data, even on a medium
+timescale:
+
+-  RAID 0 is just using multiple disks just like a single one, to extend
+   the available storage. Let's skip this one.
+-  RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
+   That's completely useless for long term storage: if either disk
+   fails, or if both disks are partially corrupted, you can't know what
+   are the correct data and which aren't. As an old saying goes: "Never
+   take 2 compasses: either take 3 or 1, because if both compasses show
+   different directions, you will never know which one is correct, nor
+   if both are wrong." That's the principle of Triplication.
+-  RAID 5 is based on the triplication idea: you have n disks (but least
+   3), and if one fails you can recover n-1 disks (resilient to only 1
+   disk failure, not more).
+-  RAID 6 is an extension of RAID 5 which is closer to error-correction
+   since you can correct n-k disks. However, most (all?) currently
+   commercially available RAID6 devices only implements recovery for at
+   most n-2 (2 disks failures).
+-  In any case, RAID cannot detect silent errors automatically, thus you
+   either have to regularly scan, or you risk to lose some of your data
+   permanently, and it's far more common than you can expect (eg, with
+   RAID5, it is enough to have 2 silent errors on two disks on the same
+   bit for the bit to be unrecoverable). That's why a limit of only 1 or
+   2 disks failures is just not enough.
+-  Finally, it's worth noting that `hard drives do implement ECC codes <https://superuser.com/a/1554342/157556>`__
+   to be resilient against bad sectors (otherwise we would lose data
+   all the time!), but they only have limited corrective capacity,
+   mainly because the ECC code is short and not configurable.
+
+On the opposite, ECC can correct n-k disks (or files). You can configure
+n and k however you want, so that for example you can set k = n/2, which
+means that you can recover all your files from only half of them! (once
+they are encoded with an ecc file of course).
+
+There also are new generation RAID solutions, mainly software based,
+such as SnapRAID or ZFS, which allow you to configure a virtual RAID
+with the value n-k that you want. This is just like an ecc file (but a
+bit less flexible, since it's not a file but a disk mapping, so that you
+can't just copy it around or upload it to a cloud backup hosting). In
+addition to recover (n-k) disks, they can also be configured to recover
+from partial, sectors failures inside the disk and not just the whole
+disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
+and James L. Hafner. "SD codes: erasure codes designed for how storage
+systems really fail." FAST. 2013.).
+
+The other reason RAID is not adapted to long-term storage, is that it
+supposes you store your data on hard-drives exclusively. Hard drives
+aren't a good storage medium for the long term, for two reasons:
+
+| 1- they need a regular plug to keep the internal magnetic disks
+  electrified (else the data will just fade away when there's no
+  residual electricity).
+| 2- the reading instrument is directly included and merged with the
+  data (this is the green electronic board you see from the outside, and
+  the internal head). This is good for quick consumer use (don't need to
+  buy another instrument: the HDD can just be plugged and it works), but
+  it's very bad for long term storage, because the reading instrument is
+  bound to fail, and a lot faster than the data can fade away: this
+  means that even if your magnetic disks inside your HDD still holds
+  your data, if the controller board or the head doesn't work anymore,
+  your data is just lost. And a head (and a controller board) are almost
+  impossible to replace, even by professionals, because the pieces are
+  VERY hard to find (different for each HDD production line) and each
+  HDD has some small physical defects, thus it's impossible to reproduce
+  that too (because the head is so close to the magnetic disk that if
+  you try to do that manually you'll probably fail).
+
+In the end, it's a lot better to just separate the storage medium of
+data, with the reading instrument.
+
+We will talk later about what storage mediums can be used instead.
+
+Applications included
+---------------------
+
+The pyFileFixity suite currently include the following pure-python applications:
+
+-  rfigc.py (subcommand: ``hash``), a hash auditing tool, similar to md5deep/hashdeep, to
+   compute a database of your files along with their metadata, so that
+   later you can check if they were changed/corrupted.
+
+-  header\_ecc.py (subcommand: ``header``), an error correction code using Reed-Solomon
+   generator/corrector for files headers. The idea is to supplement
+   other more common redundancy tools such as PAR2 (which is quite
+   reliable), by adding more resiliency only on the critical parts of
+   the files: their headers. Using this script, you can significantly
+   higher the chance of recovering headers, which will allow you to at
+   least open the files.
+
+-  structural\_adaptive\_ecc.py (subcommand: ``whole``), a variable error correction rate
+   encoder (kind of a generalization of header\_ecc.py). This script
+   allows to generate an ecc file for the whole content of your files,
+   not just the header part, using a variable resilience rate: the
+   header part will be the most protected, then the rest of each file
+   will be progressively encoded with a smaller and smaller resilience
+   rate. The assumption is that important information is stored first,
+   and then data becomes less and less informative (and thus important,
+   because the end of the file describes less important details). This
+   assumption is very true for all compressed kinds of formats, such as
+   JPG, ZIP, Word, ODT, etc...
+
+-  repair\_ecc.py (subcommand: ``recovery``), a script to repair the structure (ie, the entry and
+   fields markers/separators) of an ecc file generated by header\_ecc.py
+   or structural\_adaptive\_ecc.py. The goal is to enhance the
+   resilience of ecc files against corruption by ensuring that their
+   structures can be repaired (up to a certain point which is very high
+   if you use an index backup file, which is a companion file that is
+   generated along an ecc file).
+
+-  filetamper.py (subcommand: ``filetamper``) is a quickly made file corrupter, it will erase or
+   change characters in the specified file. This is useful for testing
+   your various protecting strategies and file formats (eg: is PAR2
+   really resilient against corruption? Are zip archives still partially
+   extractable after corruption or are rar archives better? etc.). Do
+   not underestimate the usefulness of this tool, as you should always
+   check the resiliency of your file formats and of your file protection
+   strategies before relying on them.
+
+-  replication\_repair.py (subcommand: ``dup``) takes advantage of your multiple copies
+   (replications) of your data over several storage mediums to recover
+   your data in case it gets corrupted. The goal is to take advantage of
+   the storage of your archived files into multiple locations: you will
+   necessarily make replications, so why not use them for repair?
+   Indeed, it's good practice to keep several identical copies of your data
+   on several storage mediums, but in case a corruption happens,
+   usually you will just drop the corrupted copies and keep the intacts ones.
+   However, if all copies are partially corrupted, you're stuck. This script
+   aims to take advantage of these multiple copies to recover your data,
+   without generating a prior ecc file. It works simply by reading through all
+   your different copies of your data, and it casts a majority vote over each
+   byte: the one that is the most often occuring will be kept. In engineering,
+   this is a very common strategy used for very reliable systems such as
+   space rockets, and is called "triple-modular redundancy", because you need
+   at least 3 copies of your data for the majority vote to work (but the more the
+   better).
+
+-  resiliency\_tester.py (subcommand: ``restest``) allows you to test the robustness of the
+   corruption correction of the scripts provided here (or any other
+   command-line app). You just have to copy the files you want to test inside a
+   folder, and then the script will copy the files into a test tree, then it
+   will automatically corrupt the files randomly (you can change the parameters
+   like block burst and others), then it will run the file repair command-lines
+   you supply and finally some stats about the repairing power will be
+   generated. This allows you to easily and objectively compare different set
+   of parameters, or even different file repair solutions, on the very data
+   that matters to you, so that you can pick the best option for you.
+
+-  ecc\_speedtest.py (subcommand: ``speedtest``) is a simple error correction codes
+   encoder/decoder speedtest. It allows to easily change parameters for the test.
+   This allows to assess how fast your machine can encode/decode with the selected
+   parameters, which can be especially useful to plan ahead for how many files you
+   can reasonably plan to protect with error correction codes (which are time consuming).
+
+-  DEPRECATED: easy\_profiler.py is just a quick and simple profiling tool to get
+   you started quickly on what should be optimized to get more speed, if
+   you want to contribute to the project feel free to propose a pull
+   request! (Cython and other optimizations are welcome as long as they
+   are cross-platform and that an alternative pure-python implementation
+   is also available).
+
+Note that all tools are primarily made for command-line usage (type
+pff <subcommand> --help to get extended info about the accepted arguments)
+
+IMPORTANT: it is CRITICAL that you use the same parameters for
+correcting mode as when you generated the database/ecc files (this is
+true for all scripts in this bundle). Of course, some options must be
+changed: -g must become -c to correct, and --update is a particular
+case. This works this way on purpose for mainly two reasons: first
+because it is very hard to autodetect the parameters from a database
+file alone and it would produce lots of false positives, and secondly
+(the primary reason) is that storing parameters inside the database file
+is highly unresilient against corruption (if this part of the database
+is tampered, the whole becomes unreadable, while if they are stored
+outside or in your own memory, the database file is always accessible).
+Thus, it is advised to write down the parameters you used to generate
+your database directly on the storage media you will store your database
+file on (eg: if it's an optical disk, write the parameters on the cover
+or directly on the disk using a marker), or better memorize them by
+heart. If you forget them, don't panic, the parameters are always stored
+as comments in the header of the generated ecc files, but you should try
+to store them outside of the ecc files anyway.
+
+For users: what are the advantages of pyFileFixity?
+---------------------------------------------------
+
+Pros:
+
+-  Open application and open specifications under the MIT license (you
+   can do whatever you want with it and tailor it to your needs if you
+   want to, or add better decoding procedures in the future as science
+   progress so that you can better recover your data from your already
+   generated ecc file).
+-  Highly reliable file fixity watcher: rfigc.py will tell you without
+   any ambiguity using several attributes if your files have been
+   corrupted or not, and can even check for images if the header is
+   valid (ie: if the file can still be opened).
+-  Readable ecc file format (compared to PAR2 and most other similar
+   specifications).
+-  Highly resilient ecc file format against corruption (not only are
+   your data protected by ecc, the ecc file is protected too against
+   critical spots, both because there is no header so that each track is
+   independent and if one track is corrupted beyond repair then other
+   ecc tracks can still be read, and a .idx file will be generated to
+   repair the structure of the ecc file to recover all tracks).
+-  Very safe and conservative approach: the recovery process checks that
+   the recovery was successful before committing a repaired block.
+-  Partial recovery allowed (even if a file cannot be completely
+   recovered, the parts that can will be repaired and then the rest that
+   can't be repaired will be recopied from the corrupted version).
+-  Support directory processing: you can encode an ecc file for a whole
+   directory of files (with any number of sub-directories and depth).
+-  No limit on the number of files, and it can recursively protect files
+   in a directory tree.
+-  Variable resiliency rate and header-only resilience, ensuring that
+   you can always open your files even if partially corrupted (the
+   structure of your files will be saved, so that you can use other
+   softwares to repair beyond if this set of script is not sufficient to
+   totally repair).
+-  Support for erasures (null bytes) and even errors-and-erasures, which
+   literally doubles the repair capabilities. To my knowledge, this is
+   the only freely available parity software that supports erasures.
+-  Display the predicted total ecc file size given your parameters,
+   and the total time it will take to encode/decode.
+-  Your original files are still accessible as they are, protection files
+   such as ecc files live alongside your original data. Contrary to
+   other data protection schemes such as PAR2 which encode the whole
+   data in par archive files that replace your original files and
+   are not readable without decoding.
+-  Opensourced under the very permissive MIT licence, do whatever you
+   want!
+
+Cons:
+
+-  Cannot protect meta-data, such as folders paths. The paths are
+   stored, but cannot be recovered (yet? feel free to contribute if you
+   know how). Only files are protected. Thus if your OS or your storage
+   medium crashes and truncate a whole directory tree, the directory
+   tree can't be repaired using the ecc file, and thus you can't access
+   the files neither. However, you can use file scraping to extract the
+   files even if the directory tree is lost, and then use RFIGC.py to
+   reorganize your files correctly. There are alternatives, see the
+   chapters below: you can either package all your files in a single
+   archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
+   DVDisaster as an alternative solution, which is an ecc generator with
+   support for directory trees meta-data (but only on optical disks).
+-  Can only repair errors and erasures (characters that are replaced by
+   another character), not deletion nor insertion of characters. However
+   this should not happen with any storage medium (truncation can occur
+   if the file bounds is misdetected, in this case pyFileFixity can
+   partially repair the known parts of the file, but cannot recover the
+   rest past the truncation, except if you used a resiliency rate of at
+   least 0.5, in which case any message block can be recreated with only
+   using the ecc file).
+-  Cannot recreate a missing file from other available files (except you
+   have set a resilience\_rate at least 0.5), contrary to Parchives
+   (PAR1/PAR2). Thus, you can only repair a file if you still have it
+   (and its ecc file!) on your filesystem. If it's missing, pyFileFixity
+   cannot do anything (yet, this will be implemented in the future).
+
+Note that the tools were meant for data archival (protect files that you
+won't modify anymore), not for system's files watching nor to protect
+all the files on your computer. To do this, you can use a filesystem
+that directly integrate error correction code capacity, such as ZFS.
+
+Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)
+------------------------------------------------------------------------------
+
+Recursively generate or check the integrity of files by MD5 and SHA1
+hashes, size, modification date or by data structure integrity (only for
+images).
+
+This script is originally meant to be used for data archival, by
+allowing an easy way to check for silent file corruption. Thus, this
+script uses relative paths so that you can easily compute and check the
+same redundant data copied on different mediums (hard drives, optical
+discs, etc.). This script is not meant for system files corruption
+notification, but is more meant to be used from times-to-times to check
+up on your data archives integrity (if you need this kind of application,
+see `avpreserve's fixity <https://github.com/avpreserve/fixity>`_).
+
+Example usage
+~~~~~~~~~~~~~
+
+-  To generate the database (only needed once):
+
+``pff hash -i "your_folder" -d "dbhash.csv" -g``
+
+-  To check:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -l log.txt -s``
+
+-  To update your database by appending new files:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -u -a``
+
+-  To update your database by appending new files AND removing
+   inexistent files:
+
+``pff hash -i "your_folder" -d "dbhash.csv" -u -a -r``
+
+Note that by default, the script is by default in check mode, to avoid
+wrong manipulations. It will also alert you if you generate over an
+already existing database file.
+
+Arguments
+~~~~~~~~~
+
+::
+
+      -h, --help            show a help message and exit
+      -i /path/to/root/folder, --input /path/to/root/folder
+                            Path to the root folder from where the scanning will occ
+    ur.
+      -d /some/folder/databasefile.csv, --database /some/folder/databasefile.csv
+                            Path to the csv file containing the hash informations.
+      -l /some/folder/filename.log, --log /some/folder/filename.log
+                            Path to the log file. (Output will be piped to both the
+    stdout and the log file)
+      -s, --structure_check
+                            Check images structures for corruption?
+      -e /some/folder/errorsfile.csv, --errors_file /some/folder/errorsfile.csv
+                            Path to the error file, where errors at checking will be
+     stored in CSV for further processing by other softwares (such as file repair so
+    ftwares).
+      -m, --disable_modification_date_checking
+                            Disable modification date checking.
+      --skip_missing        Skip missing files when checking (useful if you split yo
+    ur files into several mediums, for example on optical discs with limited capacit
+    y).
+      -g, --generate        Generate the database? (omit this parameter to check ins
+    tead of generating).
+      -f, --force           Force overwriting the database file even if it already e
+    xists (if --generate).
+      -u, --update          Update database (you must also specify --append or --rem
+    ove).
+      -a, --append          Append new files (if --update).
+      -r, --remove          Remove missing files (if --update).
+      
+      --filescraping_recovery          Given a folder of unorganized files, compare to the database and restore the filename and directory structure into the output folder.
+      -o, --output          Path to the output folder where to output the files reorganized after --recover_from_filescraping.
+
+Header Error Correction Code script
+-----------------------------------
+
+This script was made to be used in combination with other more common
+file redundancy generators (such as PAR2, I advise MultiPar). This is an
+additional layer of protection for your files: by using a higher
+resiliency rate on the headers of your files, you ensure that you will
+be probably able to open them in the future, avoiding the "critical
+spots", also called "fracture-critical" in redundancy engineering (where
+if you modify just one bit, your whole file may become unreadable,
+usually bits residing in the headers - in other words, a single blow
+makes the whole thing collapse, just like non-redundant bridges).
+
+An interesting benefit of this approach is that it has a low storage
+(and computational) overhead that scales linearly to the number of
+files, whatever their size is: for example, if we have a set of 40k
+files for a total size of 60 GB, with a resiliency\_rate of 30% and
+header\_size of 1KB (we limit to the first 1K bytes/characters = our
+file header), then, without counting the hash per block and other
+meta-data, the final ECC file will be about 2 \* resiliency\_rate \*
+number\_of\_files \* header\_size = 24.5 MB. This size can be lower if
+there are many files smaller than 1KB. This is a pretty low storage
+overhead to backup the headers of such a big number of files.
+
+The script is pure-python as are its dependencies: it is thus completely
+cross-platform and open source. The default ecc algo
+(ecc_algo=3 uses `reedsolo <https://github.com/tomerfiliba-org/reedsolomon>`_)
+also provides a speed-optimized C-compiled implementation (``creedsolo``) that will be used
+if available for the user's platform, so pyFileFixity should be fast by default.
+Alternatively, it's possible to use a JIT compiler such as PyPy,
+although this means that ``creedsolo`` will not be useable, so PyPy
+may accelerate other functions but slower ecc encoding/decoding.
+
+Structural Adaptive Error Correction Encoder
+--------------------------------------------
+
+This script implements a variable error correction rate encoder: each
+file is ecc encoded using a variable resiliency rate -- using a high
+constant resiliency rate for the header part (resiliency rate stage 1,
+high), then a variable resiliency rate is applied to the rest of the
+file's content, with a higher rate near the beginning of the file
+(resiliency rate stage 2, medium) which progressively decreases until
+the end of file (resiliency rate stage 3, the lowest).
+
+The idea is that the critical parts of files usually are placed at the
+top, and data becomes less and less critical along the file. What is
+meant by critical is both the critical spots (eg: if you tamper only one
+character of a file's header you have good chances of losing your entire
+file, ie, you cannot even open it) and critically encoded information
+(eg: archive formats usually encode compressed symbols as they go along
+the file, which means that the first occurrence is encoded, and then the
+archive simply writes a reference to the symbol. Thus, the first
+occurrence is encoded at the top, and subsequent encoding of this same
+data pattern will just be one symbol, and thus it matters less as long
+as the original symbol is correctly encoded and its information
+preserved, we can always try to restore the reference symbols later).
+Moreover, really redundant data will be placed at the top because they
+can be reused a lot, while data that cannot be too much compressed will
+be placed later, and thus, corruption of this less compressed data is a
+lot less critical because only a few characters will be changed in the
+uncompressed file (since the data is less compressed, a character change
+on the not-so-much compressed data won't have very significant impact on
+the uncompressed data).
+
+This variable error correction rate should allow to protect more the
+critical parts of a file (the header and the beginning of a file, for
+example in compressed file formats such as zip or jpg this is where the
+most importantly strings are encoded) for the same amount of storage as
+a standard constant error correction rate.
+
+Of course, you can set the resiliency rate for each stage to the values
+you want, so that you can even do the opposite: setting a higher
+resiliency rate for stage 3 than stage 2 will produce an ecc that is
+greater towards the end of the contents of your files.
+
+Furthermore, the currently designed format of the ecc file would allow
+two things that are not available in all current file ecc generators
+such as PAR2:
+
+1. it allows to partially repair a file, even if not all
+the blocks can be corrected (in PAR2, a file is repaired only if all
+blocks can be repaired, which is a shame because there are still other
+blocks that could be repaired and thus produce a less corrupted file) ;
+
+2. the ecc file format is quite simple and readable, easy to process by
+any script, which would allow other softwares to also work on it (and it
+was also done in this way to be more resilient against error
+corruptions, so that even if an entry is corrupted, other entries are
+independent and can maybe be used, thus the ecc is very error tolerant.
+This idea was implemented in repair\_ecc.py but it could be extended,
+especially if you know the pattern of the corruption).
+
+The script structural-adaptive-ecc.py implements this idea, which can be
+seen as an extension of header-ecc.py (and in fact the idea was the
+other way around: structural-adaptive-ecc.py was conceived first but was
+too complicated, then header-ecc.py was implemented as a working
+lessened implementation only for headers, and then
+structural-adaptive-ecc.py was finished using header-ecc.py code
+progress). It works, it was a quite well tested for my own needs on
+datasets of hundred of GB, but it's not foolproof so make sure you test
+the script by yourself to see if it's robust enough for your needs (any
+feedback about this would be greatly appreciated!).
+
+ECC Algorithms
+--------------
+
+You can specify different ecc algorithms using the ``--ecc_algo`` switch.
+
+For the moment, only Reed-Solomon is implemented, but it's universal
+so you can modify its parameters in lib/eccman.py.
+
+Two Reed-Solomon codecs are available, they are functionally equivalent
+and thoroughly unit tested.
+
+-  ``--ecc_algo 1``: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
+   This is the slowest implementation (but also the most easy code to understand).
+-  ``--ecc_algo 2``: same as algo 1 but with a faster functions.
+-  ``--ecc_algo 3``: use the second codec, which is the fastest.
+   The generated ECC will be compatible with algo 1 and 2.
+-  ``--ecc_algo 4``: also use the second, fastest RS codec, but
+   with different parameters (US FAA ADSB UAT RS FEC norm),
+   thus the generated ECC won't be compatible with algo 1 to 3.
+   But do not be scared, the ECC will work just the same.
+
+Note about speed: Also, use a smaller --max\_block\_size to greatly
+speedup the operations! That's the trick used to compute very quickly RS
+ECC on optical discs. You give up a bit of resiliency of course (because
+blocks are smaller, thus you protect a smaller number of characters per
+ECC. In the end, this should not change much about real resiliency, but
+in case you get a big bit error burst on a contiguous block, you may
+lose a whole block at once. That's why using RS255 is better, but it's
+very time consuming. However, the resiliency ratios still hold, so for
+any other case of bit-flipping with average-sized bursts, this should
+not be a problem as long as the size of the bursts is smaller than an
+ecc block.)
+
+In case of a catastrophic event
+-------------------------------
+
+TODO: write more here
+
+In case of a catastrophic event of your data due to the failure of your
+storage media (eg: your hard drive crashed), then follow the following
+steps:
+
+1- use dd\_rescue to make a full bit-per-bit verbatim copy of your drive
+before it dies. The nice thing with dd\_rescue is that the copy is
+exact, and also that it can retries or skip in case of bad sectors (it
+won't crash on your suddenly at half the process).
+
+2- Use testdisk to restore partition or to copy files based on partition
+filesystem informations.
+
+3- If you could not recover your files, you can try file scraping using
+`photorec <http://www.cgsecurity.org/wiki/PhotoRec>`_ or
+`plaso  <http://plaso.kiddaland.net/>`_ other similar tools as
+a last resort to extract data based only from files content (no filename,
+often uncorrect filetype, file boundaries may be wrong so some data
+may be cut off, etc.).
+
+4- If you used pyFileFixity before the failure of your storage media,
+you can then use your pre-computed databases to check that files are
+intact (rfigc.py) and if they aren't, you can recover them (using
+header\_ecc.py and structural\_adaptive\_ecc.py). It can also help if
+you recovered your files via data scraping, because your files will be
+totally unorganized, but you can use a previously generated database
+file to recover the full names and directory tree structure using
+rfigc.py --filescraping\_recover.
+
+Also, you can try to fix some of your files using specialized repairing
+tools (but remember that such tool cannot guarantee you the same
+recovering capacity as an error correction code - and in addition, error
+correction code can tell you when it has recovered successfully). For
+example:
+
+-  for tar files, you can use `fixtar <https://github.com/BestSolution-at/fixtar>`_.
+   Similar tools (but older): `tarfix <http://www.dmst.aueb.gr/dds/sw/unix/tarfix/>`_
+   and `tar-repair <https://www.datanumen.com/tar-repair/>`_.
+-  for RAID mounting and recovery, you can use "Raid faster - recover
+   better" (rfrb) tool by Sabine Seufert and Christian Zoubek:
+   https://github.com/lrq3000/rfrb
+-  if your unicode strings were mangled (ie, you see weird symbols),
+   try this script that will automatically demangle them:
+   https://github.com/LuminosoInsight/python-ftfy
+-  to repair tabular (2D) data such as .csv, try
+   `Carpenter <https://pypi.python.org/pypi/Carpenter/>`_.
+-  tool to identify corrupted files in ddrescue images: 
+   `ddrescue-ffile <https://github.com/Salamek/ddrescue-ffile>`_
+
+Protecting directory tree meta-data
+-----------------------------------
+
+One main current limitation of pyFileFixity is that it cannot protect
+the directory tree meta-data. This means that in the worst case, if a
+silent error happens on the inode pointing to the root directory that
+you protected with an ecc, the whole directory will vanish, and all the
+files inside too. In less worst cases, sub-directories can vanish, but
+it's still pretty bad, and since the ecc file doesn't store any
+information about inodes, you can't recover the full path.
+
+The inability to store these meta-data is because of two choices in the
+design:
+
+1.  portability: we want the ecc file to work even if we move the
+    root directory to another place or another storage medium (and of
+    course, the inode would change),
+
+2.  cross-platform compatibility: there's no way to get and store
+    directory meta-data for all platforms, but of course we could implement specific instructions for each main
+    platform, so this point is not really a problem.
+
+To workaround this issue (directory meta-data are critical spots), other
+softwares use a one-time storage medium (ie, writing your data along
+with generating and writing the ecc). This way, they can access at
+the bit level the inode info, and they are guaranted that the inodes
+won't ever change. This is the approach taken by DVDisaster: by using
+optical mediums, it can compute inodes that will be permanent, and thus
+also encode that info in the ecc file. Another approach is to create a
+virtual filesystem specifically to store just your files, so that you
+manage the inode yourself, and you can then copy the whole filesystem
+around (which is really just a file, just like a zip file - which can
+also be considered as a mini virtual file system in fact) like
+`rsbep <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`_.
+
+Here the portability principle of pyFileFixity prevents this approach.
+But you can mimic this workaround on your hard drive for pyFileFixity to
+work: you just need to package all your files into one file. This way,
+you sort of create a virtual file system: inside the archive, files and
+directories have meta-data just like in a filesystem, but from the
+outside it's just one file, composed of bytes that we can just encode to
+generate an ecc file - in other words, we removed the inodes portability
+problem, since this meta-data is stored relatively inside the archive,
+the archive manage it, and we can just encode this info like any other
+stream of data! The usual way to make an archive from several files is
+to use TAR, but this will generate a solid archive which will prevent
+partial recovery. An alternative is to use DAR, which is a non-solid
+archive version of TAR, with lots of other features too. If you also
+want to compress, you can just use ZIP (with DEFLATE algorithm) your
+files (this also generates a non-solid archive). You can then use
+pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
+will then protect both your files just like before and the directories
+meta-data too now.
+
+Which storage medium to use
+---------------------------
+Since hard drives have a relatively short timespan (5-10 years, often less)
+and require regular plugging to an electrical outlet to keep the magnetic
+plates from decaying, other solutions are more advisable.
+
+The medium I used to advise was optical disks (whether it's BluRay, DVD - not CDs!),
+because the reading instrument is distinct from the storage medium, and
+the technology (laser reflecting on bumps and/or pits) is kind of universal,
+so that even if the technology is lost one day (deprecated by newer technologies,
+so that you can't find the reading instrument anymore because it's not sold anymore),
+you can probably emulate a laser using some software to read your optical disk,
+just like what the CAMiLEON project did to recover data from the
+LaserDiscs of the BBC Domesday Project (see Wikipedia). BluRays have an estimated
+lifespan of 20-50 years depending on if they are "gold archival grade", whereas
+DVD should live up from 10-30 years. CDs are only required to live a minimum of 1 year
+up to 10 years max, hence are not fit for archival. Archival optimized optical discs
+such as M-Discs boast about being able to live up to 100 years, but there is no
+independent scientific backing of these claims currently. For more details, you can read
+a longer explanation I wrote with references on
+`StackOverflow <https://web.archive.org/web/20230424112000/https://superuser.com/questions/374609/what-medium-should-be-used-for-long-term-high-volume-data-storage-archival/873260>`__.
+
+However, limitations of optical discs include their limited storage space, low
+transfer speed, and limited rewriteability.
+
+A more convenient solution is to use magnetic tape, especially with an open standard
+such as `Linear Tape Open (LTO) <https://en.wikipedia.org/wiki/Linear_Tape-Open>`__,
+which ensures interoperability between manufacturers
+and hence also reduces cost because of competition. LTO works as a two components
+system: the tape drive, and the cartridges (with the magnetic bands). There
+are lots of versions of LTO, each generation improving on the previous one.
+LTO cartridges have a shorter lifespan than optical discs, being 15-30 years on average,
+but they are much more convenient to use:
+
+-  they provide extremely big storage space (one cartridge being several TB as of LTO-4,
+   and the storage capacity approximately doubles every few years with every new version!),
+-  are fast to write (about 5h to write the full cartridge, speed increases with new versions
+   so the total time to fill a cartridge stays about the same),
+-  the storage medium (cartridges) is also distinct from the reading/writing instrument (LTO tape drive), 
+-  are easily rewriteable, although it is necessary to reformat to free up space, but the idea is
+   that "full mirror backups" can be made regularly by overwriting an old tape.
+-  being an open standard, drives to read older versions 25 years old (LTO-1 is from 2000)
+   are still available.
+-  15-30 years of lifespan is still great for archival! But requires active curation (ie, checking
+   cartridges every 5 years and making a full new copy on a new cartridge each decade should be largely sufficient).
+-  Cartridges are cheap: LTO7 cartridges allowing storage of up to 15 TB cost only 60 bucks brand new, often
+   much less in refurbished (already used, but can be overwritten and reused). This is MUCH less expensive
+   than hard drives.
+-  Fit for cold storage: unlike hard drives (using magnetic platters) and like optical discs,
+   the cartridges do not need to be plugged to an electrical outlet regularly, the magnetic band does not
+   decay without electrical current, so the cartridges can be cold stored in air-tight, temperature-proofed
+   and humidity-proof containers, which can be stored off-site (fire-proof data recovery plan).
+-  Recovery of failed LTO cartridges is
+   `inexpensive and readily available <https://www.quora.com/I-have-an-old-LTO-tape-Can-I-recover-its-data-and-save-it-into-a-hard-drive>`__,
+   whereas recovering the magnetic signal from failed hard drives costs
+   `thousands of euros/dollars <https://www.quora.com/Is-there-any-way-of-recovering-data-from-dead-hard-disk>`__.
+   LTO tapes are also fully compatible with DAR archives, improving chances of recovery with error correction codes
+   and non-solid archives that can be partially recovered.
+
+Sounds perfect, right? Well, nothing is, LTO also has several disadvantages:
+
+-  Initial cost of starting is very expensive: a brand new LTO drive of latest generations
+   cost several thousand euros/dollars. Second-hand or refurbished drives of older generations
+   are much less expensive, but they are difficult to setup, as it is unlikely you will find them
+   in an all-in-one package, you will have to get the tape drive separately from the computer system
+   to plug it to (more on that in the next section).
+-  Limited retrocompatibility: the LTO standard specifies that each generation of drives
+   only need to support the current gen and one past gen. However, this is counterbalanced by the fact that
+   the LTO standard is open, so anybody can make LTO drives, including in the future, and it is possible someday
+   a manufacturer will make a LTO drive that supports multiple past generations (just like there are old tapes
+   digitizers that can be connected in USB, for archival purposes). Until then, in practice,
+   it means that ideally when upgrading your LTO system, you need to upgrade by one generation at a time,
+   or if you get a drive of 2+ later gens, you need to keep or buy a drive of the older gen you had to
+   read your tapes to then transfer to the latest gen you have. As of 2023, there are still LTO1 tape drives
+   available for cheap in second-hand, a technology that was published in 2000 and already deprecated
+   in 2001 by LTO2, so this shows that LTO tape drives of older generations should still be plentily available.
+-  LTO is a sequential technology: it is very fast to write and read sequentially, but if you want to
+   download a specific file, the tape has to be fully read up to where the file is stored, contrary to
+   hard drives with random access that can access in linear or sublinear time.
+-  (Old fixed issue) Before LTO-5, which introduced the LTFS standardized filesystem that allows mounting on
+   any operating file system such as Windows, Linux and MacOS, the various LTO drives
+   manufacturers used their own closed-source filesystems that were often incompatible with each others.
+   Hence, make sure to get an LTO-5 drive or above to ensure future access to your long term archives.
+
+Given all the above characteristics, LTO>=5 appears to be the best practical solution
+for long term archival, if coupled with an active (but infrequent) curation process.
+
+There is however one exception: if you need to cold store the medium in a non temperate
+environment (outside of 10-40°C), then using optical discs may be more resilient,
+although LTO cartridges should also be able to sustain a wider range of temperature
+but you need to wait while they "warm up" in the environment where the reader is
+before reading, so that the magnetic elements have time to stabilize at normal temperature.
+
+How to get a LTO tape drive and system running
+----------------------------------------------
+
+To get started with LTO tape drives and which one to choose and how to make your own
+rig, `Matthew Millman made an excellent tutorial <https://www.mattmillman.com/attaching-lto-tape-drives-via-usb-or-thunderbolt/>`__
+on which we build upon below, so you should read this tutorial and then read the instructions below.
+
+The process is as follows: first find a second-hand/refurbished LTO drive with the highest revision you can for your budget,
+then find a server of a similar generation, or make an eGPU + SAS card of the highest speed the tape drive can support.
+Generally, you can aim for a LTO drive 3-4 generations older than the latest one (eg, if current is LTO9, you can expect
+cheap - 150-300 dollars per drive) for a LTO5 or LTO6). Aim only for LTO5+, because only LTFS did not exist before LTO5,
+but keep in mind some LTO5 drives need a firmware update to support LTFS, whereas all LTO6 drives support out of the box.
+
+Once you find a second-hand LTO drive, consult its user manual beforehand to see
+what SAS or fibre cable (FC) you need (if SAS, any version should work, even greater versions, but older
+versions will just limit the read/write speed performance). For example, here is the manual for the
+`HP LTO6 drive <https://docs.oracle.com/cd/E38452_01/en/LTO6_Vol1_E1_D7/LTO6_Vol1_E1_D7.pdf>`__.
+All LTO drives are compatible with all computers provided you have the adequate connectivity (a SAS or FC adapter).
+
+Once you have a LTO drive, then you can look for a computer to plug your LTO to. Essentially, you just need a computer that supports SAS. If not, then at least a free PCIe or mini-PCIe slot to be able to connect a SAS adapter.
+
+The general outline is that you just need to have a computer with a PCIe slot, and get a SAS or FC adapter (depending
+on whether your LTO drive is SAS or FC) so that you can plug your LTO drive. There is
+currently no SAS to USB adapter, and only one manufacturer makes LTO drives with USB ports but
+they are super expensive, so just stick with internal SAS or FC drives (usually you want SAS,
+FC are better for long range connections, whereas SAS is compatible with SATA and SCSI drives,
+so you can also plug all your other hard drives plus the LTO tape drive on the same SAS adapter with this protocol).
+
+In practice, there are 2 different available cost-effective approaches:
+
+-  If you have an external tape drive, then the best is to get a (second-hand) eGPU casing, and a PCIe SAS adapter, that you will plug in the eGPU casing instead of a GPU card. The eGPU casing should support Thunderbolt so this is how you will connect to the SAS and hence to your tape drive: you connect your laptop to the eGPU casing, and the eGPU casing to the external tape drive via the SAS adapter in the eGPU casing. This usually costs about 150-200 euros/dollars as of 2023.
+
+  *  An alternative is to buy a low footprint PCIe dock such as `EXP GDC <https://wiki.geekworm.com/GDC>`__ produces, which essentially replaces the eGPU casing. The disadvantage is that your PCIe SAS adapter will be exposed, but this can be more cost effective (especially in second hand, you can get them at 20-40 euros/dollars instead of 120-150 euros/dollars brand new). But remember you also need to buy a power supply unit!
+
+-  If you got an internal tape drive, which are usually cheaper than external ones, then the approach is different: instead of configuring a sort of SAS-to-Thunderbolt bridge, here you get a standalone computer with either a motherboard that natively supports SAS (which is usually the case of computers meant to be servers), or at least a motherboard with a PCIe slot to buy separately a PCIe SAS adapter, and you plug your internal drive inside. So you will not be able to connect your laptop directly to the tape drive, you will have to pilot the server (which is just a standard desktop computer). Given these requirements, you can either make such a server yourself, but then keep in mind you have to build the whole computer, with a motherboard, a power supply, RAM, CPU, network, etc. Or, the easiest and usually cheapest route, is to just buy an old server with SAS hard drives second-hand (and every other components already in it), of a similar or later generation than your tape drive. Indeed, if the server has SAS hard drives, then it means you can connect your SAS tape drive too, no need for an adapter! Usually you can get them for cheap, for example if you get a 3-4 previous gen tape drive (eg, LTO-6 when current is LTO-9), then you can easily get a server computer of a similar generation for 100-250 euros/dollars, and everything is ready for you. Just make sure not to get a rack/blade computer, get one in tower form, easier to manipulate. Search on second hand websites: "server sas", then check that the SAS speed is on par with what your tape drive can accept, but if lower or higher, no biggie, it will just be slower, but it should work nevertheless. May also have to buy the right connectors but not an issue, just check the manual of your tape drive. Note: avoid HP Enterprise (HPE) servers, as there is a suspicion of programmed obsolescence in the `Smart Array's Smart Storage Battery <https://www.youtube.com/watch?v=6jxdGXA0RYk>`__.
+
+The consumables, the tapes, can also be easily found second-hand and usually are very cheap, eg, LTO6 tapes are sold at 10-20 euros/dollars one, for a storage space of 3TB to 6.25TB per tape.
+
+With both approaches, expect at the cheapest a total cost of about 500 euros/dollars for the tape drive and attachment system (eGPU casing or dedicated server) as of 2023, which is very good and amortizable very fast with just a few tapes, even compared to the cheapest hard drives!
+
+A modern data curation strategy for individuals
+-----------------------------------------------
+
+Here is an example curation strategy, which is accessible to individuals and not just
+big data centers:
+
+-  Get a LTO>=5 drive. Essentially, the idea with LTO is that you can just dump a copy
+   of your whole hard drives, since the cartridges are big and inexpensive. And you can
+   regularly reformat and overwrite the previous copy with a newer one. Store some LTO cartridges
+   out of side to be robust against fires.
+-  If you want additional protection, especially by adding error-correction codes,
+   DAR can be used to compress the data with PAR2 and is
+   `compatible <https://superuser.com/questions/963246/how-to-read-an-dar-archive-via-lto-6-tape>`__
+   with LTO. Alternatively, pyFileFixity can also be used to generate ECC codes, that can
+   either be stored on the same cartridge alongside the files or on a separate cartridge depending
+   on your threat model.
+-  Two kinds of archival plans are possible:
+
+  1.  either only use LTO cartridges, then try to use cartridges of different brands
+      (to avoid them failing at the same time - cartridges produced by the same industrial
+      line will tend to include the same defects and similar lifespan)
+      and store your data on at least 3 different copies/cartridges, per the redundancy principle
+      (ie, "either bring one compass or three, but never two, because you will never know which one is correct").
+
+  2.  either use LTO cartridges as ONE archival medium, and use other kinds of storage
+      for the additional 2 copies you need: one can be an external hard drive, and the last one
+      a cloud backup solution such as SpiderOak. The advantage of this solution is that
+      it is more convenient: use your external hard drive to frequently backup,
+      then also use your cloud backup to auto backup your most critical data online (off-site),
+      and finally from time to time update your last copy on a LTO cartridge by mirroring your
+      external hard drive.
+
+-  Curation strategy is then the same for all plans:
+
+  1.  Every 5 years, the "small checkup": check your 3 copies, either by scanning sectors or by your own
+      precomputed hashes (pyFileFixity's ``hash`` command).
+
+  2.  If there is an error, assume the whole medium is dead and needs to be replaced
+      and your data needs to be recovered: first using your error correction codes if you have,
+      and then using pyFileFixity ``dup`` command to use a majority vote to reconstruct one valid copy out of the 3 copies.
+
+  3.  Every 10 years, the "big checkup": even if the mediums did not fail, replace them by newer ones: mirror the old hard drive to
+      a new one, the old LTO cartridge to a new one (it can be on a newer LTO version, so that you keep pace with the technology), etc.
+
+With the above strategy, you should be able to preserve your data for as long as you can actively curate it. In case you want
+more robustness against accidents or the risk that 2 copies get corrupted under 5 years, then you can make more copies, preferably
+as LTO cartridges, but it can be other hard drives.
+
+For more information on how to cold store LTO drives, read pp32-33 "Caring for Cartridges" instruction of this
+`user manual <https://docs.oracle.com/cd/E38452_01/en/LTO6_Vol1_E1_D7/LTO6_Vol1_E1_D7.pdf>`__. For HP LTO6 drives,
+Matthew Millman made an open-source commandline tool to do advanced LTO manipulations on Windows:
+`ltfscmd <https://github.com/inaxeon/ltfscmd>`__.
+
+In case you cannot afford a LTO drive, you can replace these by external hard drives, as they are less expensive to start with,
+but then your curation strategy should be done more frequently (ie, every 2-3 years a small checkup, and every 5 years, a big checkup).
+
+Tools like pyFileFixity (or which can be used as complements)
+-------------------------------------------------------------
+
+Here are some tools with a similar philosophy to pyFileFixity, which you
+can use if they better fit your needs, either as a replacement of
+pyFileFixity or as a complement (pyFileFixity can always be used to
+generate an ecc file):
+
+-  `DAR (Disk ARchive) <http://dar.linux.free.fr/>`__: similar to tar
+   but non-solid thus allows for partial recovery and per-file access,
+   plus it saves the directory tree meta-data -- see catalog isolation
+   -- plus it can handle error correction natively using PAR2 and
+   encryption. Also supports incremental backup, thus it's a very nice
+   versatile tool. Crossplatform and opensource. Compatible with
+   `Linear Tape Open (LTO) <https://en.wikipedia.org/wiki/Linear_Tape-Open>`__
+   magnetic bands storage (see instructions
+   `here <https://superuser.com/questions/963246/how-to-read-an-dar-archive-via-lto-6-tape>`__)
+-  `DVDisaster <http://dvdisaster.net/>`__: error correction at the bit
+   level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
+   it also protects directory tree meta-data and is resilient to
+   corruption (v2 still has some critical spots but v3 won't have any).
+-  rsbep tool that is part of dvbackup package in Debian: allows to
+   generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
+   for your backups, if you're on unix or using cygwin.
+-  `rsbep modification by Thanassis
+   Tsiodras <http://users.softlab.ntua.gr/~ttsiod/rsbep.html>`__:
+   enhanced rsbep to avoid critical spots and faster speed. Also
+   includes a "freeze" script to encode your files into a virtual
+   filesystem (using Python/FUSE) so that even meta-data such as
+   directory tree are fully protected by the ecc. Great script, but not
+   maintained, it needs some intensive testing by someone knowledgeable
+   to guarantee this script is reliable enough for production.
+-  Parchive (PAR1, PAR2, MultiPar): well known error correction file
+   generator. The big advantage of Parchives is that an ecc block
+   depends on multiple files: this allows to completely reconstruct a
+   missing file from scratch using files that are still available. Works
+   good for most people, but most available Parchive generators are not
+   satisfiable for me because 1- they do not allow to generate an ecc
+   for a directory tree recursively (except MultiPar, and even if it is
+   allowed in the PAR2 specs), 2- they can be very slow to generate
+   (even with multiprocessor extensions, because the galois field is
+   over 2^16 instead of 2^8, which is very costly), 3- the spec is not
+   very resilient to errors and tampering over the ecc file, as it
+   assumes the ecc file won't be corrupted (I also tested, it's still a
+   bit resilient, but it could be a lot more with some tweaking of the
+   spec), 4- it doesn't allow for partial recovery (recovering blocks
+   that we can and pass the others that are unrecoverable): with PAR2, a
+   file can be restored fully or it cannot be at all.
+-  Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
+   create non-solid archives which are readable by most computers
+   (ubiquitous algorithm). Non-solid archive means that a zip file can
+   still unzip correct files even if it is corrupted, because files are
+   encoded in blocks, and thus even if some blocks are corrupted, the
+   decoding can happen. A `fast implementation with enhanced compression
+   is available in pure Go <https://github.com/klauspost/compress>`__
+   (good for long storage).
+-  TestDisk: for file scraping, when nothing else worked.
+-  dd\_rescue: for disk scraping (allows to forcefully read a whole disk
+   at the bit level and copy everything it can, passing bad sector with
+   options to retry them later on after a first full pass over the
+   correct sectors).
+-  ZFS: a file system which includes ecc correction directly. The whole
+   filesystem, including directory tree meta-data, are protected. If you
+   want ecc protection on your computer for all your files, this is the
+   way to go.
+-  Encryption: technically, you can encrypt your files without losing
+   too much redundancy, as long as you use an encryption scheme that is
+   block-based such as DES: if one block gets corrupted, it won't be
+   decryptable, but the rest of the files' encrypted blocks should be
+   decryptable without any problem. So encrypting with such algorithms
+   leads to similar files as non-solid archives such as deflate zip. Of
+   course, for very long term storage, it's better to avoid encryption
+   and compression (because you raise the information contained in a
+   single block of data, thus if you lose one block, you lose more
+   data), but if it's really necessary to you, you can still maintain
+   high chances of recovering your files by using block-based
+   encryption/compression (note: block-based encryption can
+   be seen as the equivalent of non-solid archives for compression,
+   because the data is compressed/encrypted in independent blocks,
+   thus allowing partial uncompression/decryption).
+-  `SnapRAID <http://snapraid.sourceforge.net/>`__
+-  `par2ools <https://github.com/jmoiron/par2ools>`__: a set of
+   additional tools to manage par2 archives
+-  `Checkm <https://pypi.python.org/pypi/Checkm/0.4>`__: a tool similar
+   to rfigc.py
+-  `BagIt <https://en.wikipedia.org/wiki/BagIt>`__ with two python
+   implementations `here <https://pypi.python.org/pypi/pybagit/>`__ and
+   `here <https://pypi.python.org/pypi/bagit/>`__: this is a file
+   packaging format for sharing and storing archives for long term
+   preservation, it just formalizes a few common procedures and meta
+   data that are usually added to files for long term archival (such as
+   MD5 digest).
+-  `RSArmor <https://github.com/jap/rsarm>`__ a tool based on
+   Reed-Solomon to encode binary data files into hexadecimal, so that
+   you can print the characters on paper. May be interesting for small
+   datasets (below 100 MB).
+-  `Ent <https://github.com/lsauer/entropy>`__ a tool to analyze the
+   entropy of your files. Can be very interesting to optimize the error
+   correction algorithm, or your compression tools.
+-  `HashFS <https://pypi.python.org/pypi/hashfs/>`_ is a non-redundant,
+   duplication free filesystem, in Python. **Data deduplication** is very
+   important for large scale long term storage: since you want your data
+   to be redundant, this means you will use an additional storage space
+   for your redundant copies that will be proportional to your original data.
+   Having duplicated data will consume more storage and more processing
+   time, for no benefit. That's why it's a good idea to deduplicate your data
+   prior to create redundant copies: this will be faster and save you money.
+   Deduplication can either be done manually (by using duplicates removers)
+   or systematically and automatically using specific filesystems such as
+   zfs (with deduplication enabled) or hashfs.
+-  Paper as a storage medium: paper is not a great storage medium,
+   because it has low storage density (ie, you can only store at most 
+   about 100 KB) and it can also degrade just like other storage mediums,
+   but you cannot check that automatically since it's not digital. However,
+   if you are interested, here are a few softwares that do that:
+   `Paper key <http://en.wikipedia.org/wiki/Paper_key>`_,
+   `Paperbak <http://www.ollydbg.de/Paperbak/index.html>`_,
+   `Optar <http://ronja.twibright.com/optar/>`_,
+   `dpaper <https://github.com/penma/dpaper>`_,
+   `QR Backup <http://blog.liw.fi/posts/qr-backup/>`_,
+   `QR Backup (another) <http://blog.shuningbian.net/2009/10/qrbackup.php>`_,
+   `QR Backup (again another) <http://git.pictorii.com/index.php?p=qrbackup.git&a=summary>`_,
+   `QR Backup (again) <http://hansmi.ch/software/qrbackup>`_,
+   `and finally a related paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&rep=rep1&type=pdf>`_.
+-  AVPreserve tools, most notably `fixity <https://github.com/avpreserve/fixity>`_ 
+   to monitor for file changes (similarly to rfigc, but actively as a daemon)
+   and `interstitial <https://github.com/avpreserve/interstitial>`_ to detect
+   interstitial errors in audio digitization workflows (great to ensure you
+   correctly digitized a whole audio file into WAV without any error).
+
+FAQ
+---
+
+-  Can I compress my data files and my ecc file?
+
+As a rule of thumb, you should ALWAYS keep your ecc file in clear
+text, so under no compression nor encryption. This is because in case
+the ecc file gets corrupted, if compressed/encrypted, the
+decompression/decrypting of the corrupted parts may completely flaw
+the whole structure of the ecc file.
+
+Your data files, that you want to protect, *should* remain in clear
+text, but you may choose to compress them if it drastically reduces
+the size of your files, and if you raise the resilience rate of your
+ecc file (so compression may be a good option if you have an
+opportunity to trade the file size reduction for more ecc file
+resilience). Also, make sure to choose a non-solid compression
+algorithm like DEFLATE (zip) so that you can still decode correct
+parts even if some are corrupted (else with a solid archive, if one
+byte is corrupted, the whole archive may become unreadable).
+
+However, in the case that you compress your files, you should generate
+the ecc file only *after* compression, so that the ecc file applies to
+the compressed archive instead of the uncompressed files, else you
+risk being unable to correct your files because the uncompression of
+corrupted parts may output gibberish, and length extended corrupted
+parts (and if the size is different, Reed-Solomon will just freak
+out).
+
+-  Can I encrypt my data files and my ecc file ?
+
+NEVER encrypt your ecc file, this is totally useless and
+counterproductive.
+
+You can encrypt your data files, but choose a non-solid algorithm
+(like AES if I'm not mistaken) so that corrupted parts do not prevent
+the decoding of subsequent correct parts. Of course, you're lowering a
+bit your chances of recovering your data files by encrypting them (the
+best chance to keep data for the long term is to keep them in clear
+text), but if it's really necessary, using a non-solid encrypting
+scheme is a good compromise.
+
+You can generate an ecc file on your encrypted data files, thus
+*after* encryption, and keep the ecc file in clear text (never encrypt
+nor compress it). This is not a security risk at all since the ecc
+file does not give any information on the content inside your
+encrypted files, but rather just redundant info to correct corrupted
+bytes (however if you generate the ecc file on the data files before
+encryption, then it's clearly a security risk, and someone could
+recover your data without your permission).
+
+- What medium should I use to store my data?
+
+The details are long and a bit complicated (I may write a complete article
+about it in the future), but the tl;dr answer is that you should use *optical disks*,
+because it decouples the storage medium and the reading hardware
+(eg, at the opposite we have hard drives, which contains both the reading
+hardware and the storage medium, so if one fails, you lose both)
+and because it's most likely future-proof (you only need a laser, which
+is universal, the laser's parameters can always be tweaked).
+
+From scientific studies, it seems that, at the time of writing this (2015),
+BluRay HTL disks are the most resilient against environmental degradation.
+To raise the duration, you can also put optical disks in completely opaque boxes
+(to avoid light degradation) and in addition you can put any storage medium
+(not only optical disks, but also hard drives and anything really) in
+*completely* air-tight and water-tight bags or box and put in a fridge or a freezer.
+This is a law of nature: lower the temperature, lower will be the entropy, in other
+words lower will be the degradation over time. It works the same with digital data.
+
+- What file formats are the most recoverable?
+
+It's difficult to advise a specific format. What we can do is advise the characteristics
+of a good file format:
+
+  * future-proof (should be readable in the future).
+  * non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn't cause a problem to the decoding of other blocks).
+  * open source implementation available.
+  * minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).
+  * No magic bytes or header importance (ie, corrupting the header won't prevent opening the file).
+
+There are a few studies about the most resilient file formats, such as:
+
+  * `"Just one bit in a million: On the effects of data corruption in files" by Volker Heydegger <http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1>`_.
+  * `"Analysing the impact of file formats on data integrity" by Volker Heydegger <http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf>`_.
+  * `"A guide to formats", by The UK national archives <http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf>`_ (you want to look at the Recoverability entry in each table).
+
+- What is Reed-Solomon?
+
+If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon
+
+Also, you may want to read the following resources:
+
+  * "`Reed-Solomon codes for coders <https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders>`_", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.
+  * "Algebraic codes for data transmission", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books <https://books.google.fr/books?id=eQs2i-R9-oYC&lpg=PR11&ots=atCPQJm3OJ&dq=%22Algebraic%20codes%20for%20data%20transmission%22%2C%20Blahut%2C%20Richard%20E.%2C%202003%2C%20Cambridge%20university%20press.&lr&hl=fr&pg=PA193#v=onepage&q=%22Algebraic%20codes%20for%20data%20transmission%22,%20Blahut,%20Richard%20E.,%202003,%20Cambridge%20university%20press.&f=false>`_.
+
+
+.. |Example| image:: https://raw.githubusercontent.com/lrq3000/pyFileFixity/master/tux-example.jpg
+   :scale: 60 %
+   :alt: Image corruption and repair example
+.. |PyPI-Status| image:: https://img.shields.io/pypi/v/pyfilefixity.svg
+   :target: https://pypi.org/project/pyfilefixity
+.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/pyfilefixity.svg?logo=python&logoColor=white
+   :target: https://pypi.org/project/pyfilefixity
+.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/pyfilefixity.svg?label=pypi%20downloads&logo=python&logoColor=white
+   :target: https://pypi.org/project/pyfilefixity
+.. |Build-Status| image:: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml/badge.svg?event=push
+   :target: https://github.com/lrq3000/pyFileFixity/actions/workflows/ci-build.yml
+.. |Coverage| image:: https://codecov.io/github/lrq3000/pyFileFixity/coverage.svg?branch=master
+   :target: https://codecov.io/github/lrq3000/pyFileFixity?branch=master
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pyFileFixity-3.1.1/pyFileFixity.egg-info/SOURCES.txt` & `pyFileFixity-3.1.4/pyFileFixity.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `pyFileFixity-3.1.1/pyproject.toml` & `pyFileFixity-3.1.4/pyproject.toml`

 * *Files 16% similar despite different names*

```diff
@@ -1,249 +1,249 @@
-# SPDX-License-Identifier: MIT
-# Inspired by: https://hynek.me/articles/python-recursive-optional-dependencies/
-# We here use a single-module distribution style https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#single-module-distribution
-
-[build-system]
-# never uppercap requirements unless we have evidence it won't work https://iscinumpy.dev/post/bound-version-constraints/ 
-# cython cannot be placed in optional-dependencies, Cython won't be able to do its magic to make it importable in setup.py
-# setuptools>=61 is necessary to support dynamic version in pyproject.toml: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/ -- but only setuptools up to 44.1.1 is available on Py2, so we define setuptools>44 as the highest version for Py2, and this works because on Py2, pyproject.toml is only used for build-system (because this table only exists in pyproject.toml, see PEP 518), and then pip/setuptools switch to setup.cfg on Py2 for the rest of the build process. For more infos on dependency specification, see: https://peps.python.org/pep-0508/
-requires = ["setuptools>=44;python_version<'3'", "setuptools>=61;python_version>='3'"]
-build-backend = "setuptools.build_meta"
-
-[project]  # beware if using setuptools: setup.py still gets executed, and even if pyproject.toml fields take precedence, if there is any code error in setup.py, building will fail!
-name = "pyFileFixity"
-dynamic = ["version"]  # see PEP 440 https://peps.python.org/pep-0440/#pre-releases and https://packaging.python.org/en/latest/guides/single-sourcing-package-version/
-description = "Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing."
-authors = [
-    {name = "Stephen Karl Larroque", email = "lrq3000@gmail.com"},
-    ]
-maintainers = [
-    {name = "Stephen Karl Larroque", email = "lrq3000@gmail.com"},
-    ]
-requires-python = ">=3.7"
-license = {text = "MIT License"} # { file = "LICENSE" }
-keywords = ["file", "repair", "monitor", "change", "reed-solomon", "error", "correction", "error correction", "parity", "parity files", "parity bytes", "data protection", "data recovery", "file protection", "qr codes", "qr code"]
-classifiers = [
-    'Development Status :: 5 - Production/Stable',
-    'License :: OSI Approved :: MIT License',
-    'Environment :: Console',
-    'Operating System :: Microsoft :: Windows',
-    'Operating System :: MacOS :: MacOS X',
-    'Operating System :: POSIX :: Linux',
-    'Programming Language :: Python',
-    'Programming Language :: Python :: 3',
-    'Programming Language :: Python :: 3.7',
-    'Programming Language :: Python :: 3.8',
-    'Programming Language :: Python :: 3.9',
-    'Programming Language :: Python :: 3.10',
-    'Programming Language :: Python :: 3.11',
-    'Programming Language :: Python :: 3.12',
-    'Programming Language :: Python :: Implementation :: PyPy',
-    'Topic :: Software Development :: Libraries',
-    'Topic :: Software Development :: Libraries :: Python Modules',
-    'Topic :: System :: Archiving',
-    'Topic :: System :: Archiving :: Backup',
-    'Topic :: System :: Monitoring',
-    'Topic :: System :: Recovery Tools',
-    'Topic :: Utilities',
-    'Intended Audience :: Developers',
-    'Intended Audience :: End Users/Desktop',
-    'Intended Audience :: Information Technology',
-    'Intended Audience :: System Administrators',
-]
-dependencies = [
-    #"typing-extensions; python_version<'3.8'",
-    #"importlib_metadata;python_version<'3.8'",
-    "pathlib2",
-    "argparse",
-    "sortedcontainers",
-    "tqdm",
-    "distance",
-    "reedsolo>=2.0.0b1",  # for Py3 (Py2 will use setup.cfg)
-    "unireedsolomon",  # for Py3 (Py2 will use setup.cfg)
-]
-
-[tool.setuptools.dynamic]
-version = {attr = "pyFileFixity.__version__"}  # see: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/
-
-[project.urls]
-Homepage = "https://github.com/lrq3000/pyFileFixity"
-Documentation = "https://github.com/lrq3000/pyFileFixity/blob/master/README.rst"
-"Source" = "https://github.com/lrq3000/pyFileFixity"
-Tracker = "https://github.com/lrq3000/pyFileFixity/issues"
-Download = "https://github.com/lrq3000/pyFileFixity/releases"
-#Changelog = "https://url/changelog"
-
-[project.optional-dependencies]
-# tests_require was deprecated in setup.py by setuptools, because anyway downstream the user wants to test in their own environment, not an isolated env, so the only practical replacement is to have test requirements defined as an extras/optional-dependency [test] so that they can be installed in the user's environment if they want to: https://discuss.python.org/t/providing-a-way-to-specify-how-to-run-tests-and-docs/15016
-test = [  # minimum dependencies to run tests
-    "pytest",
-    "pytest-cov",
-    #"coveralls",
-    "py-make",  # necessary to run the config files in tests/results/*.cfg
-]
-testmeta = [  # dependencies to test meta-data. Note that some of these dependencies make cibuildwheel choke on cryptography
-    "build",
-    "twine",
-    "validate-pyproject",
-    "rstcheck",
-]
-
-[project.readme]
-# Do NOT use .. code:: python for interactive code session, otherwise syntax error due to prompt handle >>>, see https://svn.python.org/projects/external/Sphinx-1.2/doc/markup/code.rst and https://thomas-cokelaer.info/tutorials/sphinx/rest_syntax.html#python-docstrings
-# also use rstcheck
-file = "README.rst"
-content-type = "text/x-rst"
-
-[project.scripts]
-pff = "pyFileFixity.pff:main"
-
-#[tool.setuptools]
-#package-dir = {"" = "src"}
-
-[tool.setuptools.packages.find]
-# IMPORTANT: systematically delete `src/<project.name>.egg-info` folder before rebuilding, otherwise the list of included files will not get updated (it's in `SOURCES.txt` file in this folder)
-where = [""]
-include = ["pyFileFixity"]
-#exclude = ["test*"]
-#namespaces = true
-
-[tool.setuptools.package-data]
-# Check the <mypkg>.egg-info/SOURCES.txt file generated after a `build` or `pip install` to check if the following files are correctly included in the sdist.
-# Check also the list of files included by default: https://packaging.python.org/en/latest/guides/using-manifest-in/
-"*" = [
-    "*.rst",
-    "LICENSE*",
-    "README*",
-    "*.pyx",  # include the cython implementation sourcecode in the wheel, so that the user can cythonize later, this is necessary, it's not included by default even with extensions in setup.py
-    "*.c",  # include the cythonized intermediary .c file for source distributions such as Gentoo, so that they do not need to install Cython v3 - no need since we use src-layout, it's automatically included
-    #"tests",  # to help linux distros package builders, they may want to run the unit test to check their package is working OK - no need, tests folder is packaged by default in sdist nowadays
-    #"*tests/files/*",  # this does NOT work, more than one level of folder is not supported this way, need to put in a separate line to define a package path, eg, "mypkg.tests" = ["files/*"].
-    #"*lib/*",
-]
-"pyFileFixity" = [
-    "ecc_specification.txt",
-    "resiliency_tester_config.txt",
-]
-#"pyFileFixity.tests" = [  # does NOt work, need MANIFEST.in, see: https://github.com/pypa/setuptools/issues/3341
-#    "files/*",  # attach necessary files to run tests
-#    "results/*",  # attach necessary py-make config and resulting database files to run and compare tests results
-#]
-
-#[tool.setuptools.exclude-package-data]
-#"*" = [
-#    "docs/_build",
-#    "tests/__pycache__",
-#    "tests/.mypy_cache",
-#]
-
-[tool.pytest.ini_options]
-addopts = [
-    "--import-mode=importlib",
-    "-ra",
-    "--strict-markers",
-]
-xfail_strict = true
-testpaths = "pyFileFixity/tests"  # default path to look for tests if nothing is specified in commandline
-filterwarnings = [
-    "once::Warning",
-]
-required_plugins = "pytest-cov"
-
-[tool.coverage.run]
-branch = true
-relative_files = true
-include = [
-    "pyFileFixity/lib/aux_funcs.py",
-    "pyFileFixity/lib/eccman.py",
-    "pyFileFixity/lib/hasher.py",
-    "pyFileFixity/lib/tee.py",
-    "pyFileFixity/_infos.py",
-    "pyFileFixity/header_ecc.py",
-    "pyFileFixity/repair_ecc.py",
-    "pyFileFixity/replication_repair.py",
-    "pyFileFixity/resiliency_tester.py",
-    "pyFileFixity/rfigc.py",
-    "pyFileFixity/structural_adaptive_ecc.py",
-    ]
-exclude = [
-    "pyFileFixity/tests/*",
-    "pyFileFixity/__init__.py",
-    "pyFileFixity/easy_profiler.py",
-    "pyFileFixity/ecc_speedtest.py",
-    "pyFileFixity/filetamper.py",
-    "pycleaner.py",
-    "setup.py",
-]
-
-[tool.coverage.paths]
-source = ["pyFileFixity"]
-
-[tool.coverage.report]  # Beware: you need to delete .coveragerc if you have one, otherwise .coveragerc will take precedence!
-show_missing = true
-include = [
-    "*.py",
-]
-omit = [
-    "*/python?.?/*",
-    "*/site-packages/nose/*",
-    "*/opt/python/pypy*",
-    "*/tests/*",
-]
-exclude_lines = [
-    # a more strict default pragma
-    "\\# pragma: no cover\\b",
-
-    # allow defensive code
-    "^\\s*raise AssertionError\\b",
-    "^\\s*raise NotImplementedError\\b",
-    "^\\s*return NotImplemented\\b",
-    "^\\s*raise$",
-
-    # typing-related code
-    "^if (False|TYPE_CHECKING):",
-    ": \\.\\.\\.(\\s*#.*)?$",
-    "^ +\\.\\.\\.$",
-    "-> ['\"]?NoReturn['\"]?:",
-]
-
-[tool.cibuildwheel]
-#build = "*"
-#skip = ""
-#test-skip = ""
-
-archs = ["auto"]
-#build-frontend = "pip"
-#dependency-versions = "pinned"
-#environment = {""}
-#environment-pass = []
-build-verbosity = "1"
-
-#before-all = ""
-#before-build = ""
-#repair-wheel-command = ""
-
-test-command = "pytest {package}/tests"
-#before-test = ""
-#test-requires = []
-test-extras = ["test"]  # reuse the [test] extras optional dependencies to install test dependencies, instead of redefining in test-requires
-
-#[tool.cibuildwheel.config-settings]
-#--build-option = "--cythonize"
-
-# NOTE: you have to use single-quoted strings in TOML for regular expressions.
-# It's the equivalent of r-strings in Python.  Multiline strings are treated as
-# verbose regular expressions by Black.  Use [ ] to denote a significant space
-# character.
-
-# While black is awesome, it does not support cython, so we avoid for the moment
-# to keep a similar formatting between the pure python and the cython implementations.
-# see: https://github.com/psf/black/issues/359
-
-#[tool.black]
-#line-length = 88
-#target-version = ['py39', 'py310', 'py311']
-#include = '\.pyi?$'
-# We use preview style for formatting Black itself. If you
-# want stable formatting across releases, you should keep
-# this off.
-#preview = false
+# SPDX-License-Identifier: MIT
+# Inspired by: https://hynek.me/articles/python-recursive-optional-dependencies/
+# We here use a flat-layout distribution style https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
+
+[build-system]
+# never uppercap requirements unless we have evidence it won't work https://iscinumpy.dev/post/bound-version-constraints/ 
+# cython cannot be placed in optional-dependencies, Cython won't be able to do its magic to make it importable in setup.py
+# setuptools>=61 is necessary to support dynamic version in pyproject.toml: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/ -- but only setuptools up to 44.1.1 is available on Py2, so we define setuptools>44 as the highest version for Py2, and this works because on Py2, pyproject.toml is only used for build-system (because this table only exists in pyproject.toml, see PEP 518), and then pip/setuptools switch to setup.cfg on Py2 for the rest of the build process. For more infos on dependency specification, see: https://peps.python.org/pep-0508/
+requires = ["setuptools>=44;python_version<'3'", "setuptools>=61;python_version>='3'"]
+build-backend = "setuptools.build_meta"
+
+[project]  # beware if using setuptools: setup.py still gets executed, and even if pyproject.toml fields take precedence, if there is any code error in setup.py, building will fail!
+name = "pyFileFixity"
+dynamic = ["version"]  # see PEP 440 https://peps.python.org/pep-0440/#pre-releases and https://packaging.python.org/en/latest/guides/single-sourcing-package-version/
+description = "Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing."
+authors = [
+    {name = "Stephen Karl Larroque", email = "lrq3000@gmail.com"},
+    ]
+maintainers = [
+    {name = "Stephen Karl Larroque", email = "lrq3000@gmail.com"},
+    ]
+requires-python = ">=3.7"
+license = {text = "MIT License"} # { file = "LICENSE" }
+keywords = ["file", "repair", "monitor", "change", "reed-solomon", "error", "correction", "error correction", "parity", "parity files", "parity bytes", "data protection", "data recovery", "file protection", "qr codes", "qr code"]
+classifiers = [
+    'Development Status :: 5 - Production/Stable',
+    'License :: OSI Approved :: MIT License',
+    'Environment :: Console',
+    'Operating System :: Microsoft :: Windows',
+    'Operating System :: MacOS :: MacOS X',
+    'Operating System :: POSIX :: Linux',
+    'Programming Language :: Python',
+    'Programming Language :: Python :: 3',
+    'Programming Language :: Python :: 3.7',
+    'Programming Language :: Python :: 3.8',
+    'Programming Language :: Python :: 3.9',
+    'Programming Language :: Python :: 3.10',
+    'Programming Language :: Python :: 3.11',
+    'Programming Language :: Python :: 3.12',
+    'Programming Language :: Python :: Implementation :: PyPy',
+    'Topic :: Software Development :: Libraries',
+    'Topic :: Software Development :: Libraries :: Python Modules',
+    'Topic :: System :: Archiving',
+    'Topic :: System :: Archiving :: Backup',
+    'Topic :: System :: Monitoring',
+    'Topic :: System :: Recovery Tools',
+    'Topic :: Utilities',
+    'Intended Audience :: Developers',
+    'Intended Audience :: End Users/Desktop',
+    'Intended Audience :: Information Technology',
+    'Intended Audience :: System Administrators',
+]
+dependencies = [
+    #"typing-extensions; python_version<'3.8'",
+    #"importlib_metadata;python_version<'3.8'",
+    "pathlib2",
+    "argparse",
+    "sortedcontainers",
+    "tqdm",
+    "distance",
+    "reedsolo>=2.0.0b1",  # for Py3 (Py2 will use setup.cfg)
+    "unireedsolomon",  # for Py3 (Py2 will use setup.cfg)
+]
+
+[tool.setuptools.dynamic]
+version = {attr = "pyFileFixity.__version__"}  # see: https://packaging.python.org/en/latest/guides/single-sourcing-package-version/
+
+[project.urls]
+Homepage = "https://github.com/lrq3000/pyFileFixity"
+Documentation = "https://github.com/lrq3000/pyFileFixity/blob/master/README.rst"
+"Source" = "https://github.com/lrq3000/pyFileFixity"
+Tracker = "https://github.com/lrq3000/pyFileFixity/issues"
+Download = "https://github.com/lrq3000/pyFileFixity/releases"
+#Changelog = "https://url/changelog"
+
+[project.optional-dependencies]
+# tests_require was deprecated in setup.py by setuptools, because anyway downstream the user wants to test in their own environment, not an isolated env, so the only practical replacement is to have test requirements defined as an extras/optional-dependency [test] so that they can be installed in the user's environment if they want to: https://discuss.python.org/t/providing-a-way-to-specify-how-to-run-tests-and-docs/15016
+test = [  # minimum dependencies to run tests
+    "pytest",
+    "pytest-cov",
+    #"coveralls",
+    "py-make",  # necessary to run the config files in tests/results/*.cfg
+]
+testmeta = [  # dependencies to test meta-data. Note that some of these dependencies make cibuildwheel choke on cryptography
+    "build",
+    "twine",
+    "validate-pyproject",
+    "rstcheck",
+]
+
+[project.readme]
+# Do NOT use .. code:: python for interactive code session, otherwise syntax error due to prompt handle >>>, see https://svn.python.org/projects/external/Sphinx-1.2/doc/markup/code.rst and https://thomas-cokelaer.info/tutorials/sphinx/rest_syntax.html#python-docstrings
+# also use rstcheck
+file = "README.rst"
+content-type = "text/x-rst"
+
+[project.scripts]
+pff = "pyFileFixity.pff:main"
+
+#[tool.setuptools]
+#package-dir = {"" = "src"}
+
+[tool.setuptools.packages.find]
+# IMPORTANT: systematically delete `src/<project.name>.egg-info` folder before rebuilding, otherwise the list of included files will not get updated (it's in `SOURCES.txt` file in this folder)
+where = [""]
+include = ["pyFileFixity*"]
+#exclude = ["pyFileFixity.lib.profilers"]
+#namespaces = true
+
+[tool.setuptools.package-data]
+# Check the <mypkg>.egg-info/SOURCES.txt file generated after a `build` or `pip install` to check if the following files are correctly included in the sdist.
+# Check also the list of files included by default: https://packaging.python.org/en/latest/guides/using-manifest-in/
+"*" = [
+    "*.rst",
+    "LICENSE*",
+    "README*",
+    "*.pyx",  # include the cython implementation sourcecode in the wheel, so that the user can cythonize later, this is necessary, it's not included by default even with extensions in setup.py
+    "*.c",  # include the cythonized intermediary .c file for source distributions such as Gentoo, so that they do not need to install Cython v3 - no need since we use src-layout, it's automatically included
+    #"tests",  # to help linux distros package builders, they may want to run the unit test to check their package is working OK - no need, tests folder is packaged by default in sdist nowadays
+    #"*tests/files/*",  # this does NOT work, more than one level of folder is not supported this way, need to put in a separate line to define a package path, eg, "mypkg.tests" = ["files/*"].
+    #"*lib/*",
+]
+"pyFileFixity" = [
+    "ecc_specification.txt",
+    "resiliency_tester_config.txt",
+]
+#"pyFileFixity.tests" = [  # does NOt work, need MANIFEST.in, see: https://github.com/pypa/setuptools/issues/3341
+#    "files/*",  # attach necessary files to run tests
+#    "results/*",  # attach necessary py-make config and resulting database files to run and compare tests results
+#]
+
+#[tool.setuptools.exclude-package-data]
+#"*" = [
+#    "docs/_build",
+#    "tests/__pycache__",
+#    "tests/.mypy_cache",
+#]
+
+[tool.pytest.ini_options]
+addopts = [
+    "--import-mode=importlib",
+    "-ra",
+    "--strict-markers",
+]
+xfail_strict = true
+testpaths = "pyFileFixity/tests"  # default path to look for tests if nothing is specified in commandline
+filterwarnings = [
+    "once::Warning",
+]
+required_plugins = "pytest-cov"
+
+[tool.coverage.run]
+branch = true
+relative_files = true
+include = [
+    "pyFileFixity/lib/aux_funcs.py",
+    "pyFileFixity/lib/eccman.py",
+    "pyFileFixity/lib/hasher.py",
+    "pyFileFixity/lib/tee.py",
+    "pyFileFixity/_infos.py",
+    "pyFileFixity/header_ecc.py",
+    "pyFileFixity/repair_ecc.py",
+    "pyFileFixity/replication_repair.py",
+    "pyFileFixity/resiliency_tester.py",
+    "pyFileFixity/rfigc.py",
+    "pyFileFixity/structural_adaptive_ecc.py",
+    ]
+exclude = [
+    "pyFileFixity/tests/*",
+    "pyFileFixity/__init__.py",
+    "pyFileFixity/easy_profiler.py",
+    "pyFileFixity/ecc_speedtest.py",
+    "pyFileFixity/filetamper.py",
+    "pycleaner.py",
+    "setup.py",
+]
+
+[tool.coverage.paths]
+source = ["pyFileFixity"]
+
+[tool.coverage.report]  # Beware: you need to delete .coveragerc if you have one, otherwise .coveragerc will take precedence!
+show_missing = true
+include = [
+    "*.py",
+]
+omit = [
+    "*/python?.?/*",
+    "*/site-packages/nose/*",
+    "*/opt/python/pypy*",
+    "*/tests/*",
+]
+exclude_lines = [
+    # a more strict default pragma
+    "\\# pragma: no cover\\b",
+
+    # allow defensive code
+    "^\\s*raise AssertionError\\b",
+    "^\\s*raise NotImplementedError\\b",
+    "^\\s*return NotImplemented\\b",
+    "^\\s*raise$",
+
+    # typing-related code
+    "^if (False|TYPE_CHECKING):",
+    ": \\.\\.\\.(\\s*#.*)?$",
+    "^ +\\.\\.\\.$",
+    "-> ['\"]?NoReturn['\"]?:",
+]
+
+[tool.cibuildwheel]
+#build = "*"
+#skip = ""
+#test-skip = ""
+
+archs = ["auto"]
+#build-frontend = "pip"
+#dependency-versions = "pinned"
+#environment = {""}
+#environment-pass = []
+build-verbosity = "1"
+
+#before-all = ""
+#before-build = ""
+#repair-wheel-command = ""
+
+test-command = "pytest {package}/tests"
+#before-test = ""
+#test-requires = []
+test-extras = ["test"]  # reuse the [test] extras optional dependencies to install test dependencies, instead of redefining in test-requires
+
+#[tool.cibuildwheel.config-settings]
+#--build-option = "--cythonize"
+
+# NOTE: you have to use single-quoted strings in TOML for regular expressions.
+# It's the equivalent of r-strings in Python.  Multiline strings are treated as
+# verbose regular expressions by Black.  Use [ ] to denote a significant space
+# character.
+
+# While black is awesome, it does not support cython, so we avoid for the moment
+# to keep a similar formatting between the pure python and the cython implementations.
+# see: https://github.com/psf/black/issues/359
+
+#[tool.black]
+#line-length = 88
+#target-version = ['py39', 'py310', 'py311']
+#include = '\.pyi?$'
+# We use preview style for formatting Black itself. If you
+# want stable formatting across releases, you should keep
+# this off.
+#preview = false
```

### Comparing `pyFileFixity-3.1.1/tox.ini` & `pyFileFixity-3.1.4/tox.ini`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-# Isolated package builds test from a temporary directory via tox
-# Thanks to Paul Ganssle for the minimal example, see: https://blog.ganssle.io/articles/2019/08/test-as-installed.html and https://github.com/pganssle/tox-examples/blob/master/changedir/tox.ini
-# Use `tox -e py`
-#
-# Tox (http://tox.testrun.org/) is a tool for running tests
-# in multiple virtualenvs. This configuration file will run the
-# test suite on all supported python versions. To use it, "pip install tox"
-
-[tox]
-minversion=3.13.0
-isolated_build=True
-envlist = py27, py32, py34, pypy, pypy3, setup.py
-
-[testenv]
-description = Run the tests under {basepython}
-deps = pytest
-changedir = {envtmpdir}  # use a temporary directory to ensure we test the built package, not the repository version: https://blog.ganssle.io/articles/2019/08/test-as-installed.html and https://github.com/pganssle/tox-examples/blob/master/changedir/tox.ini
-commands = python -m pytest {posargs} {toxinidir}
-
-[testenvpy2]
-deps =
-    #jpeg pillow # to support rfigc.py --structure_check
-    nose
-    nose-timer
-    coverage<4
-    coveralls
-commands =
-    nosetests pyFileFixity/tests/ --with-coverage --cover-package=pyFileFixity -d -v --with-timer
-    coveralls
-
-[testenv:pypy2]
-#basepython=C:\Program Files (x86)\pypy-4.0.0-win32\pypy.exe
-# No coverage for PyPy, too slow...
-deps =
-    #pypy-tk # necessary for pypy to install pillow
-    #jpeg pillow # to support rfigc.py --structure_check
-    nose
-    nose-timer
-commands =
-    pypy --version
-    nosetests pyFileFixity/tests/ -d -v --with-timer
-
-[testenv:pypy3]
-# No coverage for PyPy, too slow...
-deps =
-    nose
-    nose-timer
-commands =
-    pypy --version
-    nosetests pyFileFixity/tests/ -d -v --with-timer
+# Isolated package builds test from a temporary directory via tox
+# Thanks to Paul Ganssle for the minimal example, see: https://blog.ganssle.io/articles/2019/08/test-as-installed.html and https://github.com/pganssle/tox-examples/blob/master/changedir/tox.ini
+# Use `tox -e py`
+#
+# Tox (http://tox.testrun.org/) is a tool for running tests
+# in multiple virtualenvs. This configuration file will run the
+# test suite on all supported python versions. To use it, "pip install tox"
+
+[tox]
+minversion=3.13.0
+isolated_build=True
+envlist = py27, py32, py34, pypy, pypy3, setup.py
+
+[testenv]
+description = Run the tests under {basepython}
+deps = pytest
+changedir = {envtmpdir}  # use a temporary directory to ensure we test the built package, not the repository version: https://blog.ganssle.io/articles/2019/08/test-as-installed.html and https://github.com/pganssle/tox-examples/blob/master/changedir/tox.ini
+commands = python -m pytest {posargs} {toxinidir}
+
+[testenvpy2]
+deps =
+    #jpeg pillow # to support rfigc.py --structure_check
+    nose
+    nose-timer
+    coverage<4
+    coveralls
+commands =
+    nosetests pyFileFixity/tests/ --with-coverage --cover-package=pyFileFixity -d -v --with-timer
+    coveralls
+
+[testenv:pypy2]
+#basepython=C:\Program Files (x86)\pypy-4.0.0-win32\pypy.exe
+# No coverage for PyPy, too slow...
+deps =
+    #pypy-tk # necessary for pypy to install pillow
+    #jpeg pillow # to support rfigc.py --structure_check
+    nose
+    nose-timer
+commands =
+    pypy --version
+    nosetests pyFileFixity/tests/ -d -v --with-timer
+
+[testenv:pypy3]
+# No coverage for PyPy, too slow...
+deps =
+    nose
+    nose-timer
+commands =
+    pypy --version
+    nosetests pyFileFixity/tests/ -d -v --with-timer
```

